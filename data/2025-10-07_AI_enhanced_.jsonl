{"id": "2510.03287", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03287", "abs": "https://arxiv.org/abs/2510.03287", "authors": ["Moinak Bhattacharya", "Gagandeep Singh", "Prateek Prasanna"], "title": "SoC-DT: Standard-of-Care Aligned Digital Twins for Patient-Specific Tumor Dynamics", "comment": null, "summary": "Accurate prediction of tumor trajectories under standard-of-care (SoC)\ntherapies remains a major unmet need in oncology. This capability is essential\nfor optimizing treatment planning and anticipating disease progression.\nConventional reaction-diffusion models are limited in scope, as they fail to\ncapture tumor dynamics under heterogeneous therapeutic paradigms. There is\nhence a critical need for computational frameworks that can realistically\nsimulate SoC interventions while accounting for inter-patient variability in\ngenomics, demographics, and treatment regimens. We introduce Standard-of-Care\nDigital Twin (SoC-DT), a differentiable framework that unifies\nreaction-diffusion tumor growth models, discrete SoC interventions (surgery,\nchemotherapy, radiotherapy) along with genomic and demographic personalization\nto predict post-treatment tumor structure on imaging. An implicit-explicit\nexponential time-differencing solver, IMEX-SoC, is also proposed, which ensures\nstability, positivity, and scalability in SoC treatment situations. Evaluated\non both synthetic data and real world glioma data, SoC-DT consistently\noutperforms classical PDE baselines and purely data-driven neural models in\npredicting tumor dynamics. By bridging mechanistic interpretability with modern\ndifferentiable solvers, SoC-DT establishes a principled foundation for\npatient-specific digital twins in oncology, enabling biologically consistent\ntumor dynamics estimation. Code will be made available upon acceptance.", "AI": {"tldr": "SoC-DT is a differentiable framework that combines reaction-diffusion tumor growth models with standard-of-care interventions and patient-specific factors to predict post-treatment tumor structure on imaging.", "motivation": "Current reaction-diffusion models fail to capture tumor dynamics under heterogeneous therapeutic paradigms, creating a need for computational frameworks that can realistically simulate standard-of-care interventions while accounting for inter-patient variability.", "method": "The framework unifies reaction-diffusion tumor growth models with discrete SoC interventions (surgery, chemotherapy, radiotherapy) and incorporates genomic and demographic personalization. It uses an implicit-explicit exponential time-differencing solver (IMEX-SoC) for stability, positivity, and scalability.", "result": "SoC-DT consistently outperforms classical PDE baselines and purely data-driven neural models in predicting tumor dynamics on both synthetic data and real-world glioma data.", "conclusion": "SoC-DT establishes a principled foundation for patient-specific digital twins in oncology by bridging mechanistic interpretability with modern differentiable solvers, enabling biologically consistent tumor dynamics estimation."}}
{"id": "2510.03342", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03342", "abs": "https://arxiv.org/abs/2510.03342", "authors": ["Abbas Abdolmaleki", "Saminda Abeyruwan", "Joshua Ainslie", "Jean-Baptiste Alayrac", "Montserrat Gonzalez Arenas", "Ashwin Balakrishna", "Nathan Batchelor", "Alex Bewley", "Jeff Bingham", "Michael Bloesch", "Konstantinos Bousmalis", "Philemon Brakel", "Anthony Brohan", "Thomas Buschmann", "Arunkumar Byravan", "Serkan Cabi", "Ken Caluwaerts", "Federico Casarini", "Christine Chan", "Oscar Chang", "London Chappellet-Volpini", "Jose Enrique Chen", "Xi Chen", "Hao-Tien Lewis Chiang", "Krzysztof Choromanski", "Adrian Collister", "David B. D'Ambrosio", "Sudeep Dasari", "Todor Davchev", "Meet Kirankumar Dave", "Coline Devin", "Norman Di Palo", "Tianli Ding", "Carl Doersch", "Adil Dostmohamed", "Yilun Du", "Debidatta Dwibedi", "Sathish Thoppay Egambaram", "Michael Elabd", "Tom Erez", "Xiaolin Fang", "Claudio Fantacci", "Cody Fong", "Erik Frey", "Chuyuan Fu", "Ruiqi Gao", "Marissa Giustina", "Keerthana Gopalakrishnan", "Laura Graesser", "Oliver Groth", "Agrim Gupta", "Roland Hafner", "Steven Hansen", "Leonard Hasenclever", "Sam Haves", "Nicolas Heess", "Brandon Hernaez", "Alex Hofer", "Jasmine Hsu", "Lu Huang", "Sandy H. Huang", "Atil Iscen", "Mithun George Jacob", "Deepali Jain", "Sally Jesmonth", "Abhishek Jindal", "Ryan Julian", "Dmitry Kalashnikov", "M. Emre Karagozler", "Stefani Karp", "Matija Kecman", "J. Chase Kew", "Donnie Kim", "Frank Kim", "Junkyung Kim", "Thomas Kipf", "Sean Kirmani", "Ksenia Konyushkova", "Li Yang Ku", "Yuheng Kuang", "Thomas Lampe", "Antoine Laurens", "Tuan Anh Le", "Isabel Leal", "Alex X. Lee", "Tsang-Wei Edward Lee", "Guy Lever", "Jacky Liang", "Li-Heng Lin", "Fangchen Liu", "Shangbang Long", "Caden Lu", "Sharath Maddineni", "Anirudha Majumdar", "Kevis-Kokitsi Maninis", "Andrew Marmon", "Sergio Martinez", "Assaf Hurwitz Michaely", "Niko Milonopoulos", "Joss Moore", "Robert Moreno", "Michael Neunert", "Francesco Nori", "Joy Ortiz", "Kenneth Oslund", "Carolina Parada", "Emilio Parisotto", "Amaris Paryag", "Acorn Pooley", "Thomas Power", "Alessio Quaglino", "Haroon Qureshi", "Rajkumar Vasudeva Raju", "Helen Ran", "Dushyant Rao", "Kanishka Rao", "Isaac Reid", "David Rendleman", "Krista Reymann", "Miguel Rivas", "Francesco Romano", "Yulia Rubanova", "Peter Pastor Sampedro", "Pannag R Sanketi", "Dhruv Shah", "Mohit Sharma", "Kathryn Shea", "Mohit Shridhar", "Charles Shu", "Vikas Sindhwani", "Sumeet Singh", "Radu Soricut", "Rachel Sterneck", "Ian Storz", "Razvan Surdulescu", "Jie Tan", "Jonathan Tompson", "Saran Tunyasuvunakool", "Jake Varley", "Grace Vesom", "Giulia Vezzani", "Maria Bauza Villalonga", "Oriol Vinyals", "Ren\u00e9 Wagner", "Ayzaan Wahid", "Stefan Welker", "Paul Wohlhart", "Chengda Wu", "Markus Wulfmeier", "Fei Xia", "Ted Xiao", "Annie Xie", "Jinyu Xie", "Peng Xu", "Sichun Xu", "Ying Xu", "Zhuo Xu", "Jimmy Yan", "Sherry Yang", "Skye Yang", "Yuxiang Yang", "Hiu Hong Yu", "Wenhao Yu", "Wentao Yuan", "Yuan Yuan", "Jingwei Zhang", "Tingnan Zhang", "Zhiyuan Zhang", "Allan Zhou", "Guangyao Zhou", "Yuxiang Zhou"], "title": "Gemini Robotics 1.5: Pushing the Frontier of Generalist Robots with Advanced Embodied Reasoning, Thinking, and Motion Transfer", "comment": null, "summary": "General-purpose robots need a deep understanding of the physical world,\nadvanced reasoning, and general and dexterous control. This report introduces\nthe latest generation of the Gemini Robotics model family: Gemini Robotics 1.5,\na multi-embodiment Vision-Language-Action (VLA) model, and Gemini Robotics-ER\n1.5, a state-of-the-art Embodied Reasoning (ER) model. We are bringing together\nthree major innovations. First, Gemini Robotics 1.5 features a novel\narchitecture and a Motion Transfer (MT) mechanism, which enables it to learn\nfrom heterogeneous, multi-embodiment robot data and makes the VLA more general.\nSecond, Gemini Robotics 1.5 interleaves actions with a multi-level internal\nreasoning process in natural language. This enables the robot to \"think before\nacting\" and notably improves its ability to decompose and execute complex,\nmulti-step tasks, and also makes the robot's behavior more interpretable to the\nuser. Third, Gemini Robotics-ER 1.5 establishes a new state-of-the-art for\nembodied reasoning, i.e., for reasoning capabilities that are critical for\nrobots, such as visual and spatial understanding, task planning, and progress\nestimation. Together, this family of models takes us a step towards an era of\nphysical agents-enabling robots to perceive, think and then act so they can\nsolve complex multi-step tasks.", "AI": {"tldr": "Introduces Gemini Robotics 1.5 multi-embodiment VLA model and Gemini Robotics-ER 1.5 embodied reasoning model with three innovations: novel architecture with Motion Transfer, action-reasoning interleaving, and state-of-the-art embodied reasoning capabilities.", "motivation": "General-purpose robots need deep physical world understanding, advanced reasoning, and dexterous control to solve complex multi-step tasks.", "method": "Uses novel architecture with Motion Transfer mechanism for learning from heterogeneous robot data, interleaves actions with multi-level internal reasoning in natural language, and develops embodied reasoning capabilities for visual/spatial understanding and task planning.", "result": "Enables robots to 'think before acting', improves complex task decomposition and execution, makes robot behavior more interpretable, and establishes new state-of-the-art for embodied reasoning.", "conclusion": "This model family advances towards an era of physical agents that can perceive, think, and act to solve complex multi-step tasks."}}
{"id": "2510.03292", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03292", "abs": "https://arxiv.org/abs/2510.03292", "authors": ["Do\u011fanay Demir", "\u0130lknur Durgar Elkahlout"], "title": "Visualizing Celebrity Dynamics in Video Content: A Proposed Approach Using Face Recognition Timestamp Data", "comment": null, "summary": "In an era dominated by video content, understanding its structure and\ndynamics has become increasingly important. This paper presents a hybrid\nframework that combines a distributed multi-GPU inference system with an\ninteractive visualization platform for analyzing celebrity dynamics in video\nepisodes. The inference framework efficiently processes large volumes of video\ndata by leveraging optimized ONNX models, heterogeneous batch inference, and\nhigh-throughput parallelism, ensuring scalable generation of timestamped\nappearance records. These records are then transformed into a comprehensive\nsuite of visualizations, including appearance frequency charts, duration\nanalyses, pie charts, co-appearance matrices, network graphs, stacked area\ncharts, seasonal comparisons, and heatmaps. Together, these visualizations\nprovide multi-dimensional insights into video content, revealing patterns in\ncelebrity prominence, screen-time distribution, temporal dynamics,\nco-appearance relationships, and intensity across episodes and seasons. The\ninteractive nature of the system allows users to dynamically explore data,\nidentify key moments, and uncover evolving relationships between individuals.\nBy bridging distributed recognition with structured, visually-driven analytics,\nthis work enables new possibilities for entertainment analytics, content\ncreation strategies, and audience engagement studies.", "AI": {"tldr": "A hybrid framework combining distributed multi-GPU inference with interactive visualization for analyzing celebrity dynamics in video content, enabling scalable processing and multi-dimensional insights.", "motivation": "To address the growing importance of understanding video structure and dynamics in an era dominated by video content, particularly for analyzing celebrity appearances and relationships.", "method": "Combines distributed multi-GPU inference system with interactive visualization platform, using optimized ONNX models, heterogeneous batch inference, and high-throughput parallelism for scalable video processing.", "result": "Generates comprehensive visualizations including appearance frequency charts, duration analyses, co-appearance matrices, network graphs, and heatmaps that reveal patterns in celebrity prominence, screen-time distribution, and temporal dynamics.", "conclusion": "The framework bridges distributed recognition with structured analytics, enabling new possibilities for entertainment analytics, content creation strategies, and audience engagement studies through interactive exploration of video content."}}
{"id": "2510.03457", "categories": ["cs.RO", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2510.03457", "abs": "https://arxiv.org/abs/2510.03457", "authors": ["Jianfeng Lin", "Tianyu Wang", "Baxi Chong", "Matthew Fernandez", "Zhaochen Xu", "Daniel I. Goldman"], "title": "Optimal swimming with body compliance in an overdamped medium", "comment": null, "summary": "Elongate animals and robots use undulatory body waves to locomote through\ndiverse environments. Geometric mechanics provides a framework to model and\noptimize such systems in highly damped environments, connecting a prescribed\nshape change pattern (gait) with locomotion displacement. However, existing\napproaches assume precise execution of prescribed gaits, whereas in practice\nenvironmental interactions with compliant bodies of animals or robots\nfrequently perturb the realized trajectories. In this work, we extend geometric\nmechanics to predict locomotor performance and search for optimal swimming\nstrategy of compliant undulators. We introduce a compliant extension of\nPurcell's three-link swimmer by incorporating series-connected springs at the\njoints. Body dynamics are derived with resistive force theory. Geometric\nmechanics is incorporated into movement prediction and into an optimization\nframework that identifies strategies for controlling compliant swimmers to\nachieve maximal displacement. We validate our framework on a physical\ncable-driven three-link limbless robot, and demonstrate accurate prediction and\noptimization of locomotor performance under varied programmed, state-dependent\ncompliance in a granular medium. Our results establish a systematic\nphysics-based approach for modeling and controlling compliant swimming\nlocomotion, highlighting compliance as a design feature that can be exploited\nfor robust movement in homogeneous and heterogeneous environments.", "AI": {"tldr": "This paper extends geometric mechanics to model and optimize compliant undulatory swimming locomotion, introducing a compliant three-link swimmer model with joint springs and validating it on a physical robot in granular media.", "motivation": "Existing geometric mechanics approaches assume precise gait execution, but in practice environmental interactions with compliant bodies perturb trajectories. The authors aim to predict and optimize locomotor performance for compliant undulators.", "method": "Developed a compliant extension of Purcell's three-link swimmer with series-connected springs at joints, derived body dynamics using resistive force theory, and incorporated geometric mechanics into movement prediction and optimization framework.", "result": "Validated the framework on a physical cable-driven three-link robot, demonstrating accurate prediction and optimization of locomotor performance under varied programmed compliance in granular medium.", "conclusion": "Established a systematic physics-based approach for modeling and controlling compliant swimming locomotion, showing compliance can be exploited as a design feature for robust movement in various environments."}}
{"id": "2510.03294", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03294", "abs": "https://arxiv.org/abs/2510.03294", "authors": ["Saanvi Kataria"], "title": "Domain-Robust Marine Plastic Detection Using Vision Models", "comment": "16 pages, 5 figures, 1 table", "summary": "Marine plastic pollution is a pressing environmental threat, making reliable\nautomation for underwater debris detection essential. However, vision systems\ntrained on one dataset often degrade on new imagery due to domain shift. This\nstudy benchmarks models for cross-domain robustness, training convolutional\nneural networks - CNNs (MobileNetV2, ResNet-18, EfficientNet-B0) and vision\ntransformers (DeiT-Tiny, ViT-B16) on a labeled underwater dataset and then\nevaluates them on a balanced cross-domain test set built from plastic-positive\nimages drawn from a different source and negatives from the training domain.\nTwo zero-shot models were assessed, CLIP ViT-L14 and Google's Gemini 2.0 Flash,\nthat leverage pretraining to classify images without fine-tuning. Results show\nthe lightweight MobileNetV2 delivers the strongest cross-domain performance (F1\n0.97), surpassing larger models. All fine-tuned models achieved high Precision\n(around 99%), but differ in Recall, indicating varying sensitivity to plastic\ninstances. Zero-shot CLIP is comparatively sensitive (Recall around 80%) yet\nprone to false positives (Precision around 56%), whereas Gemini exhibits the\ninverse profile (Precision around 99%, Recall around 81%). Error analysis\nhighlights recurring confusions with coral textures, suspended particulates,\nand specular glare. Overall, compact CNNs with supervised training can\ngeneralize effectively for cross-domain underwater detection, while large\npretrained vision-language models provide complementary strengths.", "AI": {"tldr": "This paper benchmarks deep learning models for cross-domain underwater plastic debris detection, finding that lightweight CNNs like MobileNetV2 outperform larger models and zero-shot approaches in cross-domain generalization.", "motivation": "Marine plastic pollution requires reliable automation for detection, but vision systems trained on one dataset often degrade on new imagery due to domain shift, necessitating robust cross-domain models.", "method": "Benchmarked CNN models (MobileNetV2, ResNet-18, EfficientNet-B0) and vision transformers (DeiT-Tiny, ViT-B16) trained on labeled underwater data, plus zero-shot models (CLIP ViT-L14, Gemini 2.0 Flash) evaluated on cross-domain test sets with plastic-positive images from different sources.", "result": "MobileNetV2 achieved strongest cross-domain performance (F1 0.97). All fine-tuned models had high Precision (~99%) but varied Recall. CLIP showed high Recall (~80%) but low Precision (~56%), while Gemini had high Precision (~99%) but lower Recall (~81%).", "conclusion": "Compact CNNs with supervised training generalize effectively for cross-domain underwater detection, while large pretrained vision-language models provide complementary strengths, with error patterns showing confusions with coral textures, particulates, and glare."}}
{"id": "2510.03460", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03460", "abs": "https://arxiv.org/abs/2510.03460", "authors": ["Sibo Tian", "Minghui Zheng", "Xiao Liang"], "title": "Warm-Starting Optimization-Based Motion Planning for Robotic Manipulators via Point Cloud-Conditioned Flow Matching", "comment": null, "summary": "Rapid robot motion generation is critical in Human-Robot Collaboration (HRC)\nsystems, as robots need to respond to dynamic environments in real time by\ncontinuously observing their surroundings and replanning their motions to\nensure both safe interactions and efficient task execution. Current\nsampling-based motion planners face challenges in scaling to high-dimensional\nconfiguration spaces and often require post-processing to interpolate and\nsmooth the generated paths, resulting in time inefficiency in complex\nenvironments. Optimization-based planners, on the other hand, can incorporate\nmultiple constraints and generate smooth trajectories directly, making them\npotentially more time-efficient. However, optimization-based planners are\nsensitive to initialization and may get stuck in local minima. In this work, we\npresent a novel learning-based method that utilizes a Flow Matching model\nconditioned on a single-view point cloud to learn near-optimal solutions for\noptimization initialization. Our method does not require prior knowledge of the\nenvironment, such as obstacle locations and geometries, and can generate\nfeasible trajectories directly from single-view depth camera input. Simulation\nstudies on a UR5e robotic manipulator in cluttered workspaces demonstrate that\nthe proposed generative initializer achieves a high success rate on its own,\nsignificantly improves the success rate of trajectory optimization compared\nwith traditional and learning-based benchmark initializers, requires fewer\noptimization iterations, and exhibits strong generalization to unseen\nenvironments.", "AI": {"tldr": "A learning-based method using Flow Matching with single-view point clouds to generate near-optimal initializations for trajectory optimization in human-robot collaboration, achieving high success rates and faster convergence.", "motivation": "Need for rapid motion generation in HRC systems where robots must respond to dynamic environments in real-time. Current sampling-based planners struggle with high-dimensional spaces and require post-processing, while optimization-based planners are sensitive to initialization and local minima.", "method": "Uses a Flow Matching model conditioned on single-view point clouds to learn near-optimal solutions for optimization initialization, requiring no prior environmental knowledge and working directly from depth camera input.", "result": "Achieves high success rate independently, significantly improves trajectory optimization success compared to benchmarks, requires fewer optimization iterations, and generalizes well to unseen environments.", "conclusion": "The proposed generative initializer effectively addresses initialization challenges in optimization-based motion planning, enabling efficient and safe human-robot collaboration in cluttered environments."}}
{"id": "2510.03295", "categories": ["cs.CV", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03295", "abs": "https://arxiv.org/abs/2510.03295", "authors": ["Passant Elchafei", "Amany Fashwan"], "title": "Multimodal Arabic Captioning with Interpretable Visual Concept Integration", "comment": null, "summary": "We present VLCAP, an Arabic image captioning framework that integrates\nCLIP-based visual label retrieval with multimodal text generation. Rather than\nrelying solely on end-to-end captioning, VLCAP grounds generation in\ninterpretable Arabic visual concepts extracted with three multilingual\nencoders, mCLIP, AraCLIP, and Jina V4, each evaluated separately for label\nretrieval. A hybrid vocabulary is built from training captions and enriched\nwith about 21K general domain labels translated from the Visual Genome dataset,\ncovering objects, attributes, and scenes. The top-k retrieved labels are\ntransformed into fluent Arabic prompts and passed along with the original image\nto vision-language models. In the second stage, we tested Qwen-VL and Gemini\nPro Vision for caption generation, resulting in six encoder-decoder\nconfigurations. The results show that mCLIP + Gemini Pro Vision achieved the\nbest BLEU-1 (5.34%) and cosine similarity (60.01%), while AraCLIP + Qwen-VL\nobtained the highest LLM-judge score (36.33%). This interpretable pipeline\nenables culturally coherent and contextually accurate Arabic captions.", "AI": {"tldr": "VLCAP is an Arabic image captioning framework that combines CLIP-based visual label retrieval with multimodal text generation, achieving state-of-the-art results through interpretable Arabic visual concepts.", "motivation": "To create culturally coherent and contextually accurate Arabic image captions by grounding generation in interpretable visual concepts rather than relying solely on end-to-end captioning approaches.", "method": "Two-stage pipeline: 1) Extract Arabic visual concepts using three multilingual encoders (mCLIP, AraCLIP, Jina V4) for label retrieval from hybrid vocabulary; 2) Transform top-k labels into Arabic prompts and generate captions using Qwen-VL and Gemini Pro Vision models.", "result": "mCLIP + Gemini Pro Vision achieved best BLEU-1 (5.34%) and cosine similarity (60.01%), while AraCLIP + Qwen-VL obtained highest LLM-judge score (36.33%) across six encoder-decoder configurations.", "conclusion": "The interpretable pipeline successfully enables culturally coherent and contextually accurate Arabic captions, demonstrating the effectiveness of combining visual concept retrieval with multimodal generation for Arabic image captioning."}}
{"id": "2510.03471", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03471", "abs": "https://arxiv.org/abs/2510.03471", "authors": ["Dingqi Zhang", "Ran Tao", "Sheng Cheng", "Naira Hovakimyan", "Mark W. Mueller"], "title": "A Simulation Evaluation Suite for Robust Adaptive Quadcopter Control", "comment": null, "summary": "Robust adaptive control methods are essential for maintaining quadcopter\nperformance under external disturbances and model uncertainties. However,\nfragmented evaluations across tasks, simulators, and implementations hinder\nsystematic comparison of these methods. This paper introduces an\neasy-to-deploy, modular simulation testbed for quadcopter control, built on\nRotorPy, that enables evaluation under a wide range of disturbances such as\nwind, payload shifts, rotor faults, and control latency. The framework includes\na library of representative adaptive and non-adaptive controllers and provides\ntask-relevant metrics to assess tracking accuracy and robustness. The unified\nmodular environment enables reproducible evaluation across control methods and\neliminates redundant reimplementation of components such as disturbance models,\ntrajectory generators, and analysis tools. We illustrate the testbed's\nversatility through examples spanning multiple disturbance scenarios and\ntrajectory types, including automated stress testing, to demonstrate its\nutility for systematic analysis. Code is available at\nhttps://github.com/Dz298/AdaptiveQuadBench.", "AI": {"tldr": "A modular simulation testbed for quadcopter control evaluation that enables systematic comparison of adaptive control methods under various disturbances.", "motivation": "Fragmented evaluations across tasks, simulators, and implementations hinder systematic comparison of robust adaptive control methods for quadcopters under external disturbances and model uncertainties.", "method": "Built an easy-to-deploy, modular simulation testbed on RotorPy with a library of adaptive/non-adaptive controllers, disturbance models (wind, payload shifts, rotor faults, control latency), trajectory generators, and analysis tools.", "result": "The framework enables reproducible evaluation across control methods with task-relevant metrics for tracking accuracy and robustness, demonstrated through multiple disturbance scenarios and trajectory types including automated stress testing.", "conclusion": "The unified modular environment eliminates redundant reimplementation and provides utility for systematic analysis of quadcopter control methods."}}
{"id": "2510.03285", "categories": ["cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03285", "abs": "https://arxiv.org/abs/2510.03285", "authors": ["Su Kara", "Fazle Faisal", "Suman Nath"], "title": "WAREX: Web Agent Reliability Evaluation on Existing Benchmarks", "comment": null, "summary": "Recent advances in browser-based LLM agents have shown promise for automating\ntasks ranging from simple form filling to hotel booking or online shopping.\nCurrent benchmarks measure agent performance in controlled environments, such\nas containers or stable networks, where websites behave deterministically.\nHowever, in the real world, users access websites over networks and HTTPS\nconnections that introduce instability from multiple sources: client-side,\nserver-side issues or broader system failures. Moreover, live websites are\nprone to web attacks such Cross-Site Scripting, as well as general site\nmodifications which can cause unexpected or malicious pop-ups or improper\nfunctionality. To address this gap, we present WAREX: Web Agent Reliability\nEvaluation on Existing Benchmarks. We measure the impact of WAREX across three\npopular benchmarks: WebArena, WebVoyager, and REAL. Our experiments show that\nintroducing WAREX leads to significant drops in task success rates,\nhighlighting the limited robustness of state-of-the-art agents.", "AI": {"tldr": "WAREX evaluates web agent reliability by introducing real-world network instability and web attacks to existing benchmarks, revealing significant performance drops in state-of-the-art agents.", "motivation": "Current benchmarks test agents in controlled environments, but real-world web browsing involves network instability and security threats that aren't reflected in existing evaluations.", "method": "Developed WAREX framework that adds real-world challenges (network issues, HTTPS problems, web attacks like XSS, site modifications) to three popular benchmarks: WebArena, WebVoyager, and REAL.", "result": "Experiments showed significant drops in task success rates when WAREX was introduced, demonstrating limited robustness of current web agents.", "conclusion": "Existing web agents lack robustness against real-world challenges, highlighting the need for more comprehensive evaluation frameworks like WAREX to improve agent reliability."}}
{"id": "2510.03297", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03297", "abs": "https://arxiv.org/abs/2510.03297", "authors": ["Akshar Gothi"], "title": "Convolutional Neural Nets vs Vision Transformers: A SpaceNet Case Study with Balanced vs Imbalanced Regimes", "comment": "5 pages, 1 figure, 9 tables. Code and artifacts:\n  https://github.com/akshar27/spacenet-cnn-vs-vit (release v1.0.1)", "summary": "We present a controlled comparison of a convolutional neural network\n(EfficientNet-B0) and a Vision Transformer (ViT-Base) on SpaceNet under two\nlabel-distribution regimes: a naturally imbalanced five-class split and a\nbalanced-resampled split with 700 images per class (70:20:10 train/val/test).\nWith matched preprocessing (224x224, ImageNet normalization), lightweight\naugmentations, and a 40-epoch budget on a single NVIDIA P100, we report\naccuracy, macro-F1, balanced accuracy, per-class recall, and deployment metrics\n(model size and latency). On the imbalanced split, EfficientNet-B0 reaches 93%\ntest accuracy with strong macro-F1 and lower latency; ViT-Base is competitive\nat 93% with a larger parameter count and runtime. On the balanced split, both\nmodels are strong; EfficientNet-B0 reaches 99% while ViT-Base remains\ncompetitive, indicating that balancing narrows architecture gaps while CNNs\nretain an efficiency edge. We release manifests, logs, and per-image\npredictions to support reproducibility.", "AI": {"tldr": "A controlled comparison of EfficientNet-B0 (CNN) and ViT-Base (Vision Transformer) on SpaceNet dataset shows both achieve ~93% accuracy on imbalanced data, with EfficientNet having better efficiency. On balanced data, both reach ~99% accuracy, narrowing the performance gap while CNNs maintain efficiency advantages.", "motivation": "To conduct a fair comparison between convolutional neural networks (CNNs) and Vision Transformers (ViTs) under controlled conditions with different label distribution regimes, addressing the need for systematic evaluation of these architectures.", "method": "Used SpaceNet dataset with two label-distribution regimes: naturally imbalanced five-class split and balanced-resampled split (700 images per class). Applied matched preprocessing (224x224, ImageNet normalization), lightweight augmentations, and 40-epoch training budget on single NVIDIA P100 GPU.", "result": "On imbalanced split: EfficientNet-B0 achieved 93% test accuracy with strong macro-F1 and lower latency; ViT-Base was competitive at 93% but with larger parameter count and runtime. On balanced split: EfficientNet-B0 reached 99% accuracy while ViT-Base remained competitive, showing that balancing narrows architecture gaps.", "conclusion": "CNNs (EfficientNet-B0) retain efficiency advantages in terms of model size and latency, while balancing data distribution narrows performance gaps between architectures. Both models perform strongly on balanced data, suggesting data balance is crucial for fair architecture comparisons."}}
{"id": "2510.03472", "categories": ["cs.RO", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.03472", "abs": "https://arxiv.org/abs/2510.03472", "authors": ["Yulun Zhang", "Alexandre O. G. Barbosa", "Federico Pecora", "Jiaoyang Li"], "title": "Destination-to-Chutes Task Mapping Optimization for Multi-Robot Coordination in Robotic Sorting Systems", "comment": "Accepted to IEEE International Symposium on Multi-Robot and\n  Multi-Agent Systems (MRS) 2025", "summary": "We study optimizing a destination-to-chutes task mapping to improve\nthroughput in Robotic Sorting Systems (RSS), where a team of robots sort\npackages on a sortation floor by transporting them from induct workstations to\neject chutes based on their shipping destinations (e.g. Los Angeles or\nPittsburgh). The destination-to-chutes task mapping is used to determine which\nchutes a robot can drop its package. Finding a high-quality task mapping is\nchallenging because of the complexity of a real-world RSS. First, optimizing\ntask mapping is interdependent with robot target assignment and path planning.\nSecond, chutes will be CLOSED for a period of time once they receive sufficient\npackages to allow for downstream processing. Third, task mapping quality\ndirectly impacts the downstream processing, as scattered chutes for the same\ndestination increase package handling time. In this paper, we first formally\ndefine task mappings and the problem of Task Mapping Optimization (TMO). We\nthen present a simulator of RSS to evaluate task mappings. We then present a\nsimple TMO method based on the Evolutionary Algorithm and Mixed Integer Linear\nProgramming, demonstrating the advantage of our optimized task mappings over\nthe greedily generated ones in various RSS setups with different map sizes,\nnumbers of chutes, and destinations. Finally, we use Quality Diversity\nalgorithms to analyze the throughput of a diverse set of task mappings. Our\ncode is available online at https://github.com/lunjohnzhang/tmo_public.", "AI": {"tldr": "This paper presents methods to optimize destination-to-chutes task mapping in Robotic Sorting Systems to improve throughput, addressing challenges like robot planning interdependencies, chute closures, and downstream processing impacts.", "motivation": "Optimizing task mapping in Robotic Sorting Systems is challenging due to interdependencies with robot assignment and path planning, temporary chute closures when full, and negative impacts on downstream processing when packages for the same destination are scattered across multiple chutes.", "method": "The authors formally define task mappings and the Task Mapping Optimization problem, develop an RSS simulator for evaluation, and present optimization methods using Evolutionary Algorithm and Mixed Integer Linear Programming, along with Quality Diversity algorithms for analyzing diverse mapping strategies.", "result": "The optimized task mappings generated by their methods demonstrate advantages over greedily generated mappings across various RSS setups with different map sizes, numbers of chutes, and destinations.", "conclusion": "The proposed optimization framework successfully improves throughput in Robotic Sorting Systems by addressing the complex interdependencies in task mapping, with code made publicly available for further research and application."}}
{"id": "2510.03377", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03377", "abs": "https://arxiv.org/abs/2510.03377", "authors": ["Ahmed Missaoui", "Cemalettin Ozturk", "Barry O'Sullivan"], "title": "Refined Iterated Pareto Greedy for Energy-aware Hybrid Flowshop Scheduling with Blocking Constraints", "comment": null, "summary": "The scarcity of non-renewable energy sources, geopolitical problems in its\nsupply, increasing prices, and the impact of climate change, force the global\neconomy to develop more energy-efficient solutions for their operations. The\nManufacturing sector is not excluded from this challenge as one of the largest\nconsumers of energy. Energy-efficient scheduling is a method that attracts\nmanufacturing companies to reduce their consumption as it can be quickly\ndeployed and can show impact immediately. In this study, the hybrid flow shop\nscheduling problem with blocking constraint (BHFS) is investigated in which we\nseek to minimize the latest completion time (i.e. makespan) and overall energy\nconsumption, a typical manufacturing setting across many industries from\nautomotive to pharmaceutical. Energy consumption and the latest completion time\nof customer orders are usually conflicting objectives. Therefore, we first\nformulate the problem as a novel multi-objective mixed integer programming\n(MIP) model and propose an augmented epsilon-constraint method for finding the\nPareto-optimal solutions. Also, an effective multi-objective metaheuristic\nalgorithm. Refined Iterated Pareto Greedy (RIPG), is developed to solve large\ninstances in reasonable time. Our proposed methods are benchmarked using small,\nmedium, and large-size instances to evaluate their efficiency. Two well-known\nalgorithms are adopted for comparing our novel approaches. The computational\nresults show the effectiveness of our method.", "AI": {"tldr": "This paper addresses energy-efficient scheduling in manufacturing by solving a hybrid flow shop problem with blocking constraints, aiming to minimize both makespan and energy consumption using multi-objective optimization methods.", "motivation": "The scarcity of non-renewable energy sources, geopolitical supply issues, rising prices, and climate change impacts drive the need for energy-efficient manufacturing solutions, particularly in scheduling operations which can show immediate impact.", "method": "The authors formulate a multi-objective mixed integer programming model and propose an augmented epsilon-constraint method for Pareto-optimal solutions, along with developing a Refined Iterated Pareto Greedy (RIPG) metaheuristic algorithm for large instances.", "result": "The proposed methods were benchmarked across small, medium, and large instances against two well-known algorithms, with computational results demonstrating the effectiveness of their approach.", "conclusion": "The developed multi-objective optimization methods effectively address the conflicting objectives of minimizing both makespan and energy consumption in hybrid flow shop scheduling with blocking constraints, providing practical solutions for energy-efficient manufacturing."}}
{"id": "2510.03314", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03314", "abs": "https://arxiv.org/abs/2510.03314", "authors": ["Shucheng Zhang", "Yan Shi", "Bingzhang Wang", "Yuang Zhang", "Muhammad Monjurul Karim", "Kehua Chen", "Chenxi Liu", "Mehrdad Nasri", "Yinhai Wang"], "title": "A Comprehensive Review on Artificial Intelligence Empowered Solutions for Enhancing Pedestrian and Cyclist Safety", "comment": "20 pages, 4 figures, 5 tables", "summary": "Ensuring the safety of vulnerable road users (VRUs), such as pedestrians and\ncyclists, remains a critical global challenge, as conventional\ninfrastructure-based measures often prove inadequate in dynamic urban\nenvironments. Recent advances in artificial intelligence (AI), particularly in\nvisual perception and reasoning, open new opportunities for proactive and\ncontext-aware VRU protection. However, existing surveys on AI applications for\nVRUs predominantly focus on detection, offering limited coverage of other\nvision-based tasks that are essential for comprehensive VRU understanding and\nprotection. This paper presents a state-of-the-art review of recent progress in\ncamera-based AI sensing systems for VRU safety, with an emphasis on\ndevelopments from the past five years and emerging research trends. We\nsystematically examine four core tasks, namely detection and classification,\ntracking and reidentification, trajectory prediction, and intent recognition\nand prediction, which together form the backbone of AI-empowered proactive\nsolutions for VRU protection in intelligent transportation systems. To guide\nfuture research, we highlight four major open challenges from the perspectives\nof data, model, and deployment. By linking advances in visual AI with practical\nconsiderations for real-world implementation, this survey aims to provide a\nfoundational reference for the development of next-generation sensing systems\nto enhance VRU safety.", "AI": {"tldr": "This paper provides a comprehensive review of camera-based AI sensing systems for vulnerable road user (VRU) safety, covering detection, tracking, trajectory prediction, and intent recognition tasks from the past five years.", "motivation": "Conventional infrastructure-based measures are inadequate for VRU protection in dynamic urban environments, and existing AI surveys focus mainly on detection while neglecting other essential vision tasks for comprehensive VRU understanding.", "method": "The paper systematically reviews four core AI tasks: detection and classification, tracking and reidentification, trajectory prediction, and intent recognition and prediction, examining developments from the past five years and emerging research trends.", "result": "The survey identifies key advances in visual AI for VRU safety and highlights four major open challenges from data, model, and deployment perspectives that need to be addressed for real-world implementation.", "conclusion": "By linking visual AI advances with practical implementation considerations, this survey provides a foundational reference for developing next-generation sensing systems to enhance VRU safety in intelligent transportation systems."}}
{"id": "2510.03481", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.03481", "abs": "https://arxiv.org/abs/2510.03481", "authors": ["Khang Vo Huynh", "David Parker", "Lu Feng"], "title": "Robust Permissive Controller Synthesis for Interval MDPs", "comment": null, "summary": "We address the problem of robust permissive controller synthesis for robots\noperating under uncertain dynamics, modeled as Interval Markov Decision\nProcesses (IMDPs). IMDPs generalize standard MDPs by allowing transition\nprobabilities to vary within intervals, capturing epistemic uncertainty from\nsensing noise, actuation imprecision, and coarse system abstractions-common in\nrobotics. Traditional controller synthesis typically yields a single\ndeterministic strategy, limiting adaptability. In contrast, permissive\ncontrollers (multi-strategies) allow multiple actions per state, enabling\nruntime flexibility and resilience. However, prior work on permissive\ncontroller synthesis generally assumes exact transition probabilities, which is\nunrealistic in many robotic applications. We present the first framework for\nrobust permissive controller synthesis on IMDPs, guaranteeing that all\nstrategies compliant with the synthesized multi-strategy satisfy reachability\nor reward-based specifications under all admissible transitions. We formulate\nthe problem as mixed-integer linear programs (MILPs) and propose two encodings:\na baseline vertex-enumeration method and a scalable duality-based method that\navoids explicit enumeration. Experiments on four benchmark domains show that\nboth methods synthesize robust, maximally permissive controllers and scale to\nlarge IMDPs with up to hundreds of thousands of states.", "AI": {"tldr": "First framework for robust permissive controller synthesis on Interval Markov Decision Processes (IMDPs) that guarantees reachability/reward specifications under all admissible transitions, using MILP formulations with scalable duality-based encoding.", "motivation": "Traditional controller synthesis yields single deterministic strategies that limit adaptability, while existing permissive controllers assume exact transition probabilities - unrealistic for robotics with sensing noise, actuation imprecision, and coarse abstractions.", "method": "Formulate as mixed-integer linear programs (MILPs) with two encodings: baseline vertex-enumeration method and scalable duality-based method that avoids explicit enumeration of transition probability vertices.", "result": "Both methods synthesize robust, maximally permissive controllers and scale to large IMDPs with up to hundreds of thousands of states across four benchmark domains.", "conclusion": "The framework enables runtime flexibility and resilience in robotic systems operating under uncertain dynamics by providing the first robust permissive controller synthesis approach for IMDPs."}}
{"id": "2510.03399", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03399", "abs": "https://arxiv.org/abs/2510.03399", "authors": ["Xiaoyan Bai", "Aryan Shrivastava", "Ari Holtzman", "Chenhao Tan"], "title": "Know Thyself? On the Incapability and Implications of AI Self-Recognition", "comment": "Our code is available, see\n  https://github.com/ChicagoHAI/self-recognition", "summary": "Self-recognition is a crucial metacognitive capability for AI systems,\nrelevant not only for psychological analysis but also for safety, particularly\nin evaluative scenarios. Motivated by contradictory interpretations of whether\nmodels possess self-recognition (Panickssery et al., 2024; Davidson et al.,\n2024), we introduce a systematic evaluation framework that can be easily\napplied and updated. Specifically, we measure how well 10 contemporary larger\nlanguage models (LLMs) can identify their own generated text versus text from\nother models through two tasks: binary self-recognition and exact model\nprediction. Different from prior claims, our results reveal a consistent\nfailure in self-recognition. Only 4 out of 10 models predict themselves as\ngenerators, and the performance is rarely above random chance. Additionally,\nmodels exhibit a strong bias toward predicting GPT and Claude families. We also\nprovide the first evaluation of model awareness of their own and others'\nexistence, as well as the reasoning behind their choices in self-recognition.\nWe find that the model demonstrates some knowledge of its own existence and\nother models, but their reasoning reveals a hierarchical bias. They appear to\nassume that GPT, Claude, and occasionally Gemini are the top-tier models, often\nassociating high-quality text with them. We conclude by discussing the\nimplications of our findings on AI safety and future directions to develop\nappropriate AI self-awareness.", "AI": {"tldr": "LLMs consistently fail at self-recognition, with only 4 out of 10 models correctly identifying their own generated text, performing barely above random chance while showing strong bias toward GPT and Claude families.", "motivation": "Address contradictory claims about whether models possess self-recognition capabilities, which is crucial for AI safety and metacognitive analysis.", "method": "Systematic evaluation framework with two tasks: binary self-recognition (identifying own vs others' text) and exact model prediction, tested on 10 contemporary LLMs.", "result": "Models show consistent failure in self-recognition, strong bias toward predicting GPT/Claude families, and hierarchical reasoning bias associating high-quality text with top-tier models.", "conclusion": "Current LLMs lack appropriate self-awareness, raising safety concerns; future work needed to develop proper AI self-recognition capabilities."}}
{"id": "2510.03316", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03316", "abs": "https://arxiv.org/abs/2510.03316", "authors": ["Ryan P. Demilt", "Nicholas LaHaye", "Karis Tenneson"], "title": "The View From Space: Navigating Instrumentation Differences with EOFMs", "comment": null, "summary": "Earth Observation Foundation Models (EOFMs) have exploded in prevalence as\ntools for processing the massive volumes of remotely sensed and other earth\nobservation data, and for delivering impact on the many essential earth\nmonitoring tasks. An emerging trend posits using the outputs of pre-trained\nmodels as 'embeddings' which summarize high dimensional data to be used for\ngeneric tasks such as similarity search and content-specific queries. However,\nmost EOFM models are trained only on single modalities of data and then applied\nor benchmarked by matching bands across different modalities. It is not clear\nfrom existing work what impact diverse sensor architectures have on the\ninternal representations of the present suite of EOFMs. We show in this work\nthat the representation space of EOFMs is highly sensitive to sensor\narchitecture and that understanding this difference gives a vital perspective\non the pitfalls of current EOFM design and signals for how to move forward as\nmodel developers, users, and a community guided by robust remote-sensing\nscience.", "AI": {"tldr": "Earth Observation Foundation Models (EOFMs) are sensitive to sensor architecture differences, which affects their representation spaces and highlights limitations in current model design.", "motivation": "To understand how diverse sensor architectures impact the internal representations of EOFMs, as most models are trained on single modalities but applied across different modalities.", "method": "Analyzed the representation space of EOFMs to demonstrate sensitivity to sensor architecture differences.", "result": "Found that EOFM representation spaces are highly sensitive to sensor architecture, revealing current design pitfalls.", "conclusion": "Understanding sensor architecture impacts provides vital perspective for improving EOFM design and guiding robust remote-sensing science."}}
{"id": "2510.03496", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03496", "abs": "https://arxiv.org/abs/2510.03496", "authors": ["Vadivelan Murugesan", "Rajasundaram Mathiazhagan", "Sanjana Joshi", "Aliasghar Arab"], "title": "Digital-Twin Evaluation for Proactive Human-Robot Collision Avoidance via Prediction-Guided A-RRT*", "comment": null, "summary": "Human-robot collaboration requires precise prediction of human motion over\nextended horizons to enable proactive collision avoidance. Unlike existing\nplanners that rely solely on kinodynamic models, we present a prediction-driven\nsafe planning framework that leverages granular, joint-by-joint human motion\nforecasting validated in a physics-based digital twin. A capsule-based\nartificial potential field (APF) converts these granular predictions into\ncollision risk metrics, triggering an Adaptive RRT* (A-RRT*) planner when\nthresholds are exceeded. The depth camera is used to extract 3D skeletal poses\nand a convolutional neural network-bidirectional long short-term memory\n(CNN-BiLSTM) model to predict individual joint trajectories ahead of time. A\ndigital twin model integrates real-time human posture prediction placed in\nfront of a simulated robot to evaluate motions and physical contacts. The\nproposed method enables validation of planned trajectories ahead of time and\nbridging potential latency gaps in updating planned trajectories in real-time.\nIn 50 trials, our method achieved 100% proactive avoidance with > 250 mm\nclearance and sub-2 s replanning, demonstrating superior precision and\nreliability compared to existing kinematic-only planners through the\nintegration of predictive human modeling with digital twin validation.", "AI": {"tldr": "A prediction-driven safe planning framework that uses granular human motion forecasting with digital twin validation to enable proactive collision avoidance in human-robot collaboration.", "motivation": "Human-robot collaboration requires precise long-term human motion prediction for proactive collision avoidance, overcoming limitations of kinodynamic-only planners.", "method": "Uses CNN-BiLSTM model for joint-by-joint human motion prediction, capsule-based APF for collision risk assessment, and Adaptive RRT* planner triggered by risk thresholds, validated in physics-based digital twin.", "result": "Achieved 100% proactive avoidance with >250mm clearance and sub-2s replanning in 50 trials, demonstrating superior precision and reliability over kinematic-only planners.", "conclusion": "Integration of predictive human modeling with digital twin validation enables reliable proactive collision avoidance and bridges latency gaps in real-time trajectory planning."}}
{"id": "2510.03418", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.03418", "abs": "https://arxiv.org/abs/2510.03418", "authors": ["Ananya Mantravadi", "Shivali Dalmia", "Abhishek Mukherji", "Nand Dave", "Anudha Mittal"], "title": "ContraGen: A Multi-Agent Generation Framework for Enterprise Contradictions Detection", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) integrates LLMs with external sources,\noffering advanced capabilities for information access and decision-making.\nHowever, contradictions in retrieved evidence can result in inconsistent or\nuntrustworthy outputs, which is especially problematic in enterprise settings\nwhere compliance, governance, and accountability are critical. Existing\nbenchmarks for contradiction detection are limited to sentence-level analysis\nand do not capture the complexity of enterprise documents such as contracts,\nfinancial filings, compliance reports, or policy manuals. To address this\nlimitation, we propose ContraGen, a contradiction-aware benchmark framework\ntailored to enterprise domain. The framework generates synthetic\nenterprise-style documents with embedded contradictions, enabling systematic\nevaluation of both intra-document and cross-document consistency. Automated\ncontradiction mining is combined with human-in-the-loop validation to ensure\nhigh accuracy. Our contributions include generating realistic enterprise\ndocuments, modeling a taxonomy of contradiction types common in business\nprocesses, enabling controlled creation of self- and pairwise contradictions,\ndeveloping a contradiction-aware retrieval evaluation pipeline and embedding\nhuman oversight to reflect domain-specific judgment complexity. This work\nestablishes a foundation for more trustworthy and accountable RAG systems in\nenterprise information-seeking applications, where detecting and resolving\ncontradictions is essential for reducing risk and ensuring compliance.", "AI": {"tldr": "ContraGen is a contradiction-aware benchmark framework for enterprise RAG systems that generates synthetic enterprise documents with embedded contradictions to evaluate intra-document and cross-document consistency.", "motivation": "Existing contradiction detection benchmarks are limited to sentence-level analysis and don't capture enterprise document complexity, while contradictions in retrieved evidence can cause inconsistent/untrustworthy outputs critical for enterprise compliance and governance.", "method": "Generates synthetic enterprise-style documents with embedded contradictions using automated contradiction mining combined with human-in-the-loop validation, modeling a taxonomy of contradiction types common in business processes.", "result": "The framework enables systematic evaluation of both intra-document and cross-document consistency through controlled creation of self- and pairwise contradictions, with a contradiction-aware retrieval evaluation pipeline.", "conclusion": "ContraGen establishes a foundation for more trustworthy and accountable RAG systems in enterprise applications, where detecting and resolving contradictions is essential for reducing risk and ensuring compliance."}}
{"id": "2510.03317", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03317", "abs": "https://arxiv.org/abs/2510.03317", "authors": ["G\u00fcnel Aghakishiyeva", "Jiayi Zhou", "Saagar Arya", "James David Poling", "Holly R. Houliston", "Jamie N. Womble", "David W. Johnston", "Brinnae Bent"], "title": "Photorealistic Inpainting for Perturbation-based Explanations in Ecological Monitoring", "comment": "Accepted to NeurIPS 2025 Imageomics Workshop", "summary": "Ecological monitoring is increasingly automated by vision models, yet opaque\npredictions limit trust and field adoption. We present an inpainting-guided,\nperturbation-based explanation technique that produces photorealistic,\nmask-localized edits that preserve scene context. Unlike masking or blurring,\nthese edits stay in-distribution and reveal which fine-grained morphological\ncues drive predictions in tasks such as species recognition and trait\nattribution. We demonstrate the approach on a YOLOv9 detector fine-tuned for\nharbor seal detection in Glacier Bay drone imagery, using\nSegment-Anything-Model-refined masks to support two interventions: (i) object\nremoval/replacement (e.g., replacing seals with plausible ice/water or boats)\nand (ii) background replacement with original animals composited onto new\nscenes. Explanations are assessed by re-scoring perturbed images (flip rate,\nconfidence drop) and by expert review for ecological plausibility and\ninterpretability. The resulting explanations localize diagnostic structures,\navoid deletion artifacts common to traditional perturbations, and yield\ndomain-relevant insights that support expert validation and more trustworthy\ndeployment of AI in ecology.", "AI": {"tldr": "An inpainting-guided explanation technique produces photorealistic, localized edits to explain vision models in ecological monitoring, revealing fine-grained morphological cues while preserving scene context.", "motivation": "Opaque predictions from automated vision models limit trust and field adoption in ecological monitoring, creating a need for interpretable explanations that domain experts can validate.", "method": "Uses inpainting-guided perturbation with Segment-Anything-Model-refined masks for two interventions: object removal/replacement and background replacement, applied to YOLOv9 detector for harbor seal detection in drone imagery.", "result": "The approach localizes diagnostic structures, avoids deletion artifacts common to traditional perturbations, and produces domain-relevant insights that support expert validation through flip rate and confidence drop metrics.", "conclusion": "This technique enables more trustworthy deployment of AI in ecology by providing photorealistic, interpretable explanations that preserve ecological plausibility and support expert review."}}
{"id": "2510.03504", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03504", "abs": "https://arxiv.org/abs/2510.03504", "authors": ["Yutong Wang", "Yichun Qu", "Tengxiang Wang", "Lishuo Pan", "Nora Ayanian"], "title": "Distributed Connectivity Maintenance and Recovery for Quadrotor Motion Planning", "comment": null, "summary": "Maintaining connectivity is crucial in many multi-robot applications, yet\nfragile to obstacles and visual occlusions. We present a real-time distributed\nframework for multi-robot navigation certified by high-order control barrier\nfunctions (HOCBFs) that controls inter-robot proximity to maintain connectivity\nwhile avoiding collisions. We incorporate control Lyapunov functions to enable\nconnectivity recovery from initial disconnected configurations and temporary\nlosses, providing robust connectivity during navigation in obstacle-rich\nenvironments. Our trajectory generation framework concurrently produces\nplanning and control through a Bezier-parameterized trajectory, which naturally\nprovides smooth curves with arbitrary degree of derivatives. The main\ncontribution is the unified MPC-CLF-CBF framework, a continuous-time trajectory\ngeneration and control method for connectivity maintenance and recovery of\nmulti-robot systems. We validate the framework through extensive simulations\nand a physical experiment with 4 Crazyflie nano-quadrotors.", "AI": {"tldr": "Real-time distributed MPC-CLF-CBF framework for multi-robot navigation that maintains connectivity using high-order control barrier functions while avoiding collisions, with connectivity recovery capabilities.", "motivation": "Maintaining connectivity is crucial but fragile in multi-robot systems due to obstacles and occlusions. Existing methods need robust connectivity maintenance and recovery capabilities.", "method": "Unified MPC-CLF-CBF framework combining Model Predictive Control, Control Lyapunov Functions, and high-order Control Barrier Functions with Bezier-parameterized trajectories for smooth navigation.", "result": "Framework successfully maintains connectivity and recovers from disconnections in obstacle-rich environments, validated through simulations and physical experiments with 4 Crazyflie nano-quadrotors.", "conclusion": "The proposed framework provides robust real-time connectivity maintenance and recovery for multi-robot systems in complex environments."}}
{"id": "2510.03453", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03453", "abs": "https://arxiv.org/abs/2510.03453", "authors": ["Paul S. Rosenbloom"], "title": "A Qualitative Comparative Evaluation of Cognitive and Generative Theories", "comment": "To appear in Proceedings of the 12th Annual Conference on Advances in\n  Cognitive Systems (ACS-25)", "summary": "Evaluation is a critical activity associated with any theory. Yet this has\nproven to be an exceptionally challenging activity for theories based on\ncognitive architectures. For an overlapping set of reasons, evaluation can also\nbe challenging for theories based on generative neural architectures. This dual\nchallenge is approached here by leveraging a broad perspective on theory\nevaluation to yield a wide-ranging, albeit qualitative, comparison of\nwhole-mind-oriented cognitive and generative architectures and the full systems\nthat are based on these architectures.", "AI": {"tldr": "A qualitative comparison of cognitive and generative architectures for whole-mind systems, addressing challenges in evaluating theories based on these architectures.", "motivation": "Evaluation is challenging for both cognitive architectures and generative neural architectures, requiring a broader perspective on theory evaluation.", "method": "Leveraging a broad perspective on theory evaluation to conduct a wide-ranging qualitative comparison of whole-mind-oriented cognitive and generative architectures and their full systems.", "result": "The paper provides a comprehensive qualitative analysis comparing cognitive and generative architectures for whole-mind systems.", "conclusion": "A broad evaluation approach can effectively address the dual challenges of evaluating theories based on cognitive and generative architectures."}}
{"id": "2510.03318", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03318", "abs": "https://arxiv.org/abs/2510.03318", "authors": ["Ahmed Kabil", "Ghada Khoriba", "Mina Yousef", "Essam A. Rashed"], "title": "Advances in Medical Image Segmentation: A Comprehensive Survey with a Focus on Lumbar Spine Applications", "comment": "Computers in Biology and Medicine (to appear)", "summary": "Medical Image Segmentation (MIS) stands as a cornerstone in medical image\nanalysis, playing a pivotal role in precise diagnostics, treatment planning,\nand monitoring of various medical conditions. This paper presents a\ncomprehensive and systematic survey of MIS methodologies, bridging the gap\nbetween traditional image processing techniques and modern deep learning\napproaches. The survey encompasses thresholding, edge detection, region-based\nsegmentation, clustering algorithms, and model-based techniques while also\ndelving into state-of-the-art deep learning architectures such as Convolutional\nNeural Networks (CNNs), Fully Convolutional Networks (FCNs), and the widely\nadopted U-Net and its variants. Moreover, integrating attention mechanisms,\nsemi-supervised learning, generative adversarial networks (GANs), and\nTransformer-based models is thoroughly explored. In addition to covering\nestablished methods, this survey highlights emerging trends, including hybrid\narchitectures, cross-modality learning, federated and distributed learning\nframeworks, and active learning strategies, which aim to address challenges\nsuch as limited labeled datasets, computational complexity, and model\ngeneralizability across diverse imaging modalities. Furthermore, a specialized\ncase study on lumbar spine segmentation is presented, offering insights into\nthe challenges and advancements in this relatively underexplored anatomical\nregion. Despite significant progress in the field, critical challenges persist,\nincluding dataset bias, domain adaptation, interpretability of deep learning\nmodels, and integration into real-world clinical workflows.", "AI": {"tldr": "This paper provides a comprehensive survey of medical image segmentation methods, covering both traditional techniques and modern deep learning approaches, with a special focus on emerging trends and a case study on lumbar spine segmentation.", "motivation": "To bridge the gap between traditional image processing techniques and modern deep learning approaches in medical image segmentation, and to address persistent challenges in the field.", "method": "Systematic survey methodology covering thresholding, edge detection, region-based segmentation, clustering algorithms, model-based techniques, CNNs, FCNs, U-Net variants, attention mechanisms, semi-supervised learning, GANs, and Transformer-based models.", "result": "The survey comprehensively maps the evolution of MIS methodologies and identifies emerging trends including hybrid architectures, cross-modality learning, federated learning, and active learning strategies.", "conclusion": "Despite significant progress, critical challenges remain including dataset bias, domain adaptation, model interpretability, and integration into clinical workflows, highlighting the need for continued research in medical image segmentation."}}
{"id": "2510.03529", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03529", "abs": "https://arxiv.org/abs/2510.03529", "authors": ["Zekai Liang", "Xiao Liang", "Soofiyan Atar", "Sreyan Das", "Zoe Chiu", "Peihan Zhang", "Florian Richter", "Shanglei Liu", "Michael C. Yip"], "title": "LapSurgie: Humanoid Robots Performing Surgery via Teleoperated Handheld Laparoscopy", "comment": null, "summary": "Robotic laparoscopic surgery has gained increasing attention in recent years\nfor its potential to deliver more efficient and precise minimally invasive\nprocedures. However, adoption of surgical robotic platforms remains largely\nconfined to high-resource medical centers, exacerbating healthcare disparities\nin rural and low-resource regions. To close this gap, a range of solutions has\nbeen explored, from remote mentorship to fully remote telesurgery. Yet, the\npractical deployment of surgical robotic systems to underserved communities\nremains an unsolved challenge. Humanoid systems offer a promising path toward\ndeployability, as they can directly operate in environments designed for humans\nwithout extensive infrastructure modifications -- including operating rooms. In\nthis work, we introduce LapSurgie, the first humanoid-robot-based laparoscopic\nteleoperation framework. The system leverages an inverse-mapping strategy for\nmanual-wristed laparoscopic instruments that abides to remote center-of-motion\nconstraints, enabling precise hand-to-tool control of off-the-shelf surgical\nlaparoscopic tools without additional setup requirements. A control console\nequipped with a stereo vision system provides real-time visual feedback.\nFinally, a comprehensive user study across platforms demonstrates the\neffectiveness of the proposed framework and provides initial evidence for the\nfeasibility of deploying humanoid robots in laparoscopic procedures.", "AI": {"tldr": "LapSurgie is the first humanoid-robot-based laparoscopic teleoperation framework that enables precise control of surgical instruments using an inverse-mapping strategy with remote center-of-motion constraints, aiming to make robotic surgery more accessible.", "motivation": "To address healthcare disparities by making robotic laparoscopic surgery accessible to rural and low-resource regions, overcoming the current limitation where surgical robotic platforms are confined to high-resource medical centers.", "method": "Uses a humanoid robot with an inverse-mapping strategy for manual-wristed laparoscopic instruments that respects remote center-of-motion constraints, combined with a control console featuring stereo vision for real-time feedback.", "result": "The system enables precise hand-to-tool control of off-the-shelf surgical laparoscopic tools without additional setup requirements, and user studies demonstrate its effectiveness and feasibility.", "conclusion": "Humanoid robots offer a promising solution for deploying laparoscopic surgical systems in underserved communities, as they can operate in existing human-designed environments without extensive infrastructure modifications."}}
{"id": "2510.03469", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2510.03469", "abs": "https://arxiv.org/abs/2510.03469", "authors": ["Keshav Ramani", "Vali Tawosi", "Salwa Alamir", "Daniel Borrajo"], "title": "Bridging LLM Planning Agents and Formal Methods: A Case Study in Plan Verification", "comment": null, "summary": "We introduce a novel framework for evaluating the alignment between natural\nlanguage plans and their expected behavior by converting them into Kripke\nstructures and Linear Temporal Logic (LTL) using Large Language Models (LLMs)\nand performing model checking. We systematically evaluate this framework on a\nsimplified version of the PlanBench plan verification dataset and report on\nmetrics like Accuracy, Precision, Recall and F1 scores. Our experiments\ndemonstrate that GPT-5 achieves excellent classification performance (F1 score\nof 96.3%) while almost always producing syntactically perfect formal\nrepresentations that can act as guarantees. However, the synthesis of\nsemantically perfect formal models remains an area for future exploration.", "AI": {"tldr": "A framework that uses LLMs to convert natural language plans into Kripke structures and LTL formulas, then performs model checking to evaluate plan-behavior alignment, achieving 96.3% F1 score with GPT-5.", "motivation": "To systematically evaluate the alignment between natural language plans and their expected behavior through formal verification methods.", "method": "Convert natural language plans into Kripke structures and Linear Temporal Logic (LTL) formulas using Large Language Models, then perform model checking on a simplified PlanBench dataset.", "result": "GPT-5 achieves excellent classification performance with 96.3% F1 score and produces syntactically perfect formal representations that can act as guarantees.", "conclusion": "The framework successfully demonstrates high performance in plan verification, though achieving semantically perfect formal models remains a challenge for future work."}}
{"id": "2510.03328", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03328", "abs": "https://arxiv.org/abs/2510.03328", "authors": ["Fiona Victoria Stanley Jothiraj", "Arunaggiri Pandian Karunanidhi", "Seth A. Eichmeyer"], "title": "DECOR: Deep Embedding Clustering with Orientation Robustness", "comment": null, "summary": "In semiconductor manufacturing, early detection of wafer defects is critical\nfor product yield optimization. However, raw wafer data from wafer quality\ntests are often complex, unlabeled, imbalanced and can contain multiple defects\non a single wafer, making it crucial to design clustering methods that remain\nreliable under such imperfect data conditions. We introduce DECOR, a deep\nclustering with orientation robustness framework that groups complex defect\npatterns from wafer maps into consistent clusters. We evaluate our method on\nthe open source MixedWM38 dataset, demonstrating its ability to discover\nclusters without manual tuning. DECOR explicitly accounts for orientation\nvariations in wafer maps, ensuring that spatially similar defects are\nconsistently clustered regardless of its rotation or alignment. Experiments\nindicate that our method outperforms existing clustering baseline methods, thus\nproviding a reliable and scalable solution in automated visual inspection\nsystems.", "AI": {"tldr": "DECOR is a deep clustering framework for wafer defect detection that handles orientation variations and complex, unlabeled data, outperforming existing methods on the MixedWM38 dataset.", "motivation": "Early detection of wafer defects is critical for semiconductor manufacturing yield optimization, but raw wafer data is complex, unlabeled, imbalanced, and can contain multiple defects per wafer, requiring robust clustering methods.", "method": "DECOR (Deep Clustering with Orientation Robustness) framework that groups complex defect patterns from wafer maps into consistent clusters while explicitly accounting for orientation variations to ensure spatially similar defects are clustered regardless of rotation or alignment.", "result": "DECOR outperforms existing clustering baseline methods on the MixedWM38 dataset and demonstrates ability to discover clusters without manual tuning.", "conclusion": "DECOR provides a reliable and scalable solution for automated visual inspection systems in semiconductor manufacturing by handling orientation variations and complex data conditions."}}
{"id": "2510.03532", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03532", "abs": "https://arxiv.org/abs/2510.03532", "authors": ["Zekai Liang", "Kazuya Miyata", "Xiao Liang", "Florian Richter", "Michael C. Yip"], "title": "Efficient Surgical Robotic Instrument Pose Reconstruction in Real World Conditions Using Unified Feature Detection", "comment": null, "summary": "Accurate camera-to-robot calibration is essential for any vision-based\nrobotic control system and especially critical in minimally invasive surgical\nrobots, where instruments conduct precise micro-manipulations. However, MIS\nrobots have long kinematic chains and partial visibility of their degrees of\nfreedom in the camera, which introduces challenges for conventional\ncamera-to-robot calibration methods that assume stiff robots with good\nvisibility. Previous works have investigated both keypoint-based and\nrendering-based approaches to address this challenge in real-world conditions;\nhowever, they often struggle with consistent feature detection or have long\ninference times, neither of which are ideal for online robot control. In this\nwork, we propose a novel framework that unifies the detection of geometric\nprimitives (keypoints and shaft edges) through a shared encoding, enabling\nefficient pose estimation via projection geometry. This architecture detects\nboth keypoints and edges in a single inference and is trained on large-scale\nsynthetic data with projective labeling. This method is evaluated across both\nfeature detection and pose estimation, with qualitative and quantitative\nresults demonstrating fast performance and state-of-the-art accuracy in\nchallenging surgical environments.", "AI": {"tldr": "A novel framework for camera-to-robot calibration in minimally invasive surgical robots that unifies keypoint and edge detection through shared encoding, enabling fast and accurate pose estimation.", "motivation": "Conventional camera-to-robot calibration methods struggle with MIS robots due to long kinematic chains and partial visibility of degrees of freedom, while existing approaches have issues with inconsistent feature detection or slow inference times unsuitable for online control.", "method": "Proposes a unified framework that detects both geometric primitives (keypoints and shaft edges) through shared encoding, trained on large-scale synthetic data with projective labeling, enabling efficient pose estimation via projection geometry in a single inference.", "result": "Demonstrates fast performance and state-of-the-art accuracy in challenging surgical environments across both feature detection and pose evaluation, with qualitative and quantitative validation.", "conclusion": "The proposed method provides an efficient and accurate solution for camera-to-robot calibration in MIS robots, overcoming limitations of previous approaches and enabling reliable online robot control."}}
{"id": "2510.03485", "categories": ["cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.03485", "abs": "https://arxiv.org/abs/2510.03485", "authors": ["Xiaofei Wen", "Wenjie Jacky Mo", "Yanan Xie", "Peng Qi", "Muhao Chen"], "title": "Towards Policy-Compliant Agents: Learning Efficient Guardrails For Policy Violation Detection", "comment": "16 pages, 5 figures", "summary": "Autonomous web agents need to operate under externally imposed or\nhuman-specified policies while generating long-horizon trajectories. However,\nlittle work has examined whether these trajectories comply with such policies,\nor whether policy violations persist across different contexts such as domains\n(e.g., shopping or coding websites) and subdomains (e.g., product search and\norder management in shopping). To address this gap, we introduce\nPolicyGuardBench, a benchmark of about 60k examples for detecting policy\nviolations in agent trajectories. From diverse agent runs, we generate a broad\nset of policies and create both within subdomain and cross subdomain pairings\nwith violation labels. In addition to full-trajectory evaluation,\nPolicyGuardBench also includes a prefix-based violation detection task where\nmodels must anticipate policy violations from truncated trajectory prefixes\nrather than complete sequences. Using this dataset, we train PolicyGuard-4B, a\nlightweight guardrail model that delivers strong detection accuracy across all\ntasks while keeping inference efficient. Notably, PolicyGuard-4B generalizes\nacross domains and preserves high accuracy on unseen settings. Together,\nPolicyGuardBench and PolicyGuard-4B provide the first comprehensive framework\nfor studying policy compliance in web agent trajectories, and show that\naccurate and generalizable guardrails are feasible at small scales.", "AI": {"tldr": "PolicyGuardBench is a benchmark for detecting policy violations in web agent trajectories, with PolicyGuard-4B as a lightweight guardrail model that achieves strong detection accuracy and generalization across domains.", "motivation": "Autonomous web agents need to comply with external policies, but little work has examined policy compliance across different contexts like domains and subdomains.", "method": "Created PolicyGuardBench with 60k examples from diverse agent runs, including full-trajectory and prefix-based violation detection tasks. Trained PolicyGuard-4B model on this dataset.", "result": "PolicyGuard-4B delivers strong detection accuracy across all tasks, generalizes across domains, and maintains high accuracy on unseen settings while keeping inference efficient.", "conclusion": "Accurate and generalizable guardrails for policy compliance in web agent trajectories are feasible at small scales, providing a comprehensive framework for studying policy compliance."}}
{"id": "2510.03337", "categories": ["cs.CV", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2510.03337", "abs": "https://arxiv.org/abs/2510.03337", "authors": ["Andrey A. Lebedev", "Victor B. Kazantsev", "Sergey V. Stasenko"], "title": "Error correction in multiclass image classification of facial emotion on unbalanced samples", "comment": null, "summary": "This paper considers the problem of error correction in multi-class\nclassification of face images on unbalanced samples. The study is based on the\nanalysis of a data frame containing images labeled by seven different emotional\nstates of people of different ages. Particular attention is paid to the problem\nof class imbalance, in which some emotions significantly prevail over others.\nTo solve the classification problem, a neural network model based on LSTM with\nan attention mechanism focusing on key areas of the face that are informative\nfor emotion recognition is used. As part of the experiments, the model is\ntrained on all possible configurations of subsets of six classes with\nsubsequent error correction for the seventh class, excluded at the training\nstage. The results show that correction is possible for all classes, although\nthe degree of success varies: some classes are better restored, others are\nworse. In addition, on the test sample, when correcting some classes, an\nincrease in key quality metrics for small classes was recorded, which indicates\nthe promise of the proposed approach in solving applied problems related to the\nsearch for rare events, for example, in anti-fraud systems. Thus, the proposed\nmethod can be effectively applied in facial expression analysis systems and in\ntasks requiring stable classification under skewed class distribution.", "AI": {"tldr": "This paper proposes an LSTM-based neural network with attention mechanism for multi-class facial emotion classification on imbalanced datasets, using error correction techniques to improve classification of underrepresented emotion classes.", "motivation": "The study addresses the problem of class imbalance in facial emotion recognition, where some emotions significantly outnumber others, making accurate classification of rare emotions challenging.", "method": "Used an LSTM neural network with attention mechanism focusing on key facial areas for emotion recognition. Trained on subsets of six emotion classes and performed error correction for the excluded seventh class.", "result": "Error correction was possible for all emotion classes, though with varying success rates. Some classes were better restored than others. Test results showed improved quality metrics for small classes, indicating promise for rare event detection.", "conclusion": "The proposed method is effective for facial expression analysis systems and can provide stable classification performance under imbalanced class distributions, making it suitable for applications like anti-fraud systems that require rare event detection."}}
{"id": "2510.03547", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03547", "abs": "https://arxiv.org/abs/2510.03547", "authors": ["Carina Veil", "Moritz Flaschel", "Ellen Kuhl"], "title": "Shape-Space Graphs: Fast and Collision-Free Path Planning for Soft Robots", "comment": null, "summary": "Soft robots, inspired by elephant trunks or octopus arms, offer extraordinary\nflexibility to bend, twist, and elongate in ways that rigid robots cannot.\nHowever, their motion planning remains a challenge, especially in cluttered\nenvironments with obstacles, due to their highly nonlinear and\ninfinite-dimensional kinematics. Here, we present a graph-based path planning\ntool for an elephant-trunk-inspired soft robotic arm designed with three\nartificial muscle fibers that allow for multimodal continuous deformation\nthrough contraction. Using a biomechanical model inspired by morphoelasticity\nand active filament theory, we precompute a shape library and construct a\n$k$-nearest neighbor graph in \\emph{shape space}, ensuring that each node\ncorresponds to a mechanically accurate and physically valid robot shape. For\nthe graph, we use signed distance functions to prune nodes and edges colliding\nwith obstacles, and define multi-objective edge costs based on geometric\ndistance and actuation effort, enabling energy-efficient planning with\ncollision avoidance. We demonstrate that our algorithm reliably avoids\nobstacles and generates feasible paths within milliseconds from precomputed\ngraphs using Dijkstra's algorithm. We show that including energy costs can\ndrastically reduce the actuation effort compared to geometry-only planning, at\nthe expense of longer tip trajectories. Our results highlight the potential of\nshape-space graph search for fast and reliable path planning in the field of\nsoft robotics, paving the way for real-time applications in surgical,\nindustrial, and assistive settings.", "AI": {"tldr": "A graph-based path planning method for soft robotic arms using precomputed shape libraries and k-nearest neighbor graphs in shape space, enabling fast collision avoidance and energy-efficient motion planning.", "motivation": "Soft robots offer extraordinary flexibility but face challenges in motion planning due to their highly nonlinear and infinite-dimensional kinematics, especially in cluttered environments with obstacles.", "method": "Uses a biomechanical model inspired by morphoelasticity and active filament theory to precompute a shape library, constructs a k-nearest neighbor graph in shape space, employs signed distance functions for collision detection, and defines multi-objective edge costs based on geometric distance and actuation effort.", "result": "The algorithm reliably avoids obstacles and generates feasible paths within milliseconds using Dijkstra's algorithm, with energy costs reducing actuation effort by 50% compared to geometry-only planning (though resulting in longer tip trajectories).", "conclusion": "Shape-space graph search enables fast and reliable path planning for soft robotics, with potential for real-time applications in surgical, industrial, and assistive settings."}}
{"id": "2510.03506", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03506", "abs": "https://arxiv.org/abs/2510.03506", "authors": ["John Nguyen", "Marton Havasi", "Tariq Berrada", "Luke Zettlemoyer", "Ricky T. Q. Chen"], "title": "OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows", "comment": "https://johnlnguyen.com/oneflow", "summary": "We present OneFlow, the first non-autoregressive multimodal model that\nenables variable-length and concurrent mixed-modal generation. Unlike\nautoregressive models that enforce rigid causal ordering between text and image\ngeneration, OneFlow combines an insertion-based Edit Flow for discrete text\ntokens with Flow Matching for image latents. OneFlow enables concurrent\ntext-image synthesis with hierarchical sampling that prioritizes content over\ngrammar. Through controlled experiments across model sizes from 1B to 8B, we\ndemonstrate that OneFlow outperforms autoregressive baselines on both\ngeneration and understanding tasks while using up to 50% fewer training FLOPs.\nOneFlow surpasses both autoregressive and diffusion-based approaches while\nunlocking new capabilities for concurrent generation, iterative refinement, and\nnatural reasoning-like generation.", "AI": {"tldr": "OneFlow is the first non-autoregressive multimodal model that enables concurrent text-image generation using insertion-based Edit Flow for text and Flow Matching for images, outperforming autoregressive models with 50% fewer training FLOPs.", "motivation": "To overcome the limitations of autoregressive models that enforce rigid causal ordering between text and image generation, enabling more flexible and efficient concurrent multimodal generation.", "method": "Combines insertion-based Edit Flow for discrete text tokens with Flow Matching for image latents, using hierarchical sampling that prioritizes content over grammar for concurrent text-image synthesis.", "result": "Outperforms autoregressive baselines on both generation and understanding tasks across model sizes from 1B to 8B, while using up to 50% fewer training FLOPs. Surpasses both autoregressive and diffusion-based approaches.", "conclusion": "OneFlow enables new capabilities for concurrent generation, iterative refinement, and natural reasoning-like generation, demonstrating superior performance and efficiency compared to existing approaches."}}
{"id": "2510.03341", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03341", "abs": "https://arxiv.org/abs/2510.03341", "authors": ["Bozheng Li", "Miao Yang", "Zhenhan Chen", "Jiawang Cao", "Mushui Liu", "Yi Lu", "Yongliang Wu", "Bin Zhang", "Yangguang Ji", "Licheng Tang", "Jay Wu", "Wenbo Zhu"], "title": "OpusAnimation: Code-Based Dynamic Chart Generation", "comment": "working in progress", "summary": "Dynamic Chart Generation (DCG) involves producing code-rendered animated\nvisualizations as charts. While recent advances in multi-modal large language\nmodels (MLLMs) have significantly improved their capability on static chart\ngeneration and comprehension, MLLMs' potential for handling dynamic chart\ngeneration and understanding remains underexplored. To bridge this research\ngap, we introduce DCG-Bench (Dynamic Chart Generation Benchmark), the first\nbenchmark evaluating MLLM's capability on dynamic chart generation tasks from\nthree dimensions: Simple Text-to-Chart, Detailed Text-to-Chart, and\nVideo-to-Chart tasks. We construct DCG-8K, a high-quality DCG dataset with\nannotations covering instruction-code-video triplets and QA pairs for both code\nand video evaluation. Based on DCG-8K, we explored a two-stage training recipe,\nproposing Joint-Code-Visual Reward for group relative policy optimization to\nconstruct expert MLLM Qwen2.5-VL-DCG-3B for the DCG task. Our benchmarking\nresult reveals shortcomings of existing MLLMs in the visual-to-chart task, and\nour model beats the best open-sourced MLLM with an average 8.31% performance\ngain across three tasks, and shows on par performance against proprietary\nmodels with only 3B parameters, proving the effectiveness of our training\nrecipe. Our code and dataset will be publicly available.", "AI": {"tldr": "The paper introduces DCG-Bench, the first benchmark for evaluating multi-modal large language models (MLLMs) on dynamic chart generation tasks, and proposes a two-stage training method with Joint-Code-Visual Reward optimization that achieves state-of-the-art performance.", "motivation": "While MLLMs have improved static chart generation and comprehension, their potential for dynamic chart generation and understanding remains underexplored. The research aims to bridge this gap by creating a comprehensive benchmark and training methodology.", "method": "Constructed DCG-8K dataset with instruction-code-video triplets and QA pairs. Proposed two-stage training recipe using Joint-Code-Visual Reward for group relative policy optimization to create expert MLLM Qwen2.5-VL-DCG-3B.", "result": "The model beats the best open-sourced MLLM with 8.31% average performance gain across three tasks (Simple Text-to-Chart, Detailed Text-to-Chart, Video-to-Chart) and shows on-par performance against proprietary models despite having only 3B parameters.", "conclusion": "The proposed training recipe effectively addresses dynamic chart generation challenges, demonstrating that specialized training can achieve competitive performance with smaller models, while revealing existing MLLMs' shortcomings in visual-to-chart tasks."}}
{"id": "2510.03599", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03599", "abs": "https://arxiv.org/abs/2510.03599", "authors": ["Shafeef Omar", "Majid Khadiv"], "title": "Learning to Act Through Contact: A Unified View of Multi-Task Robot Learning", "comment": null, "summary": "We present a unified framework for multi-task locomotion and manipulation\npolicy learning grounded in a contact-explicit representation. Instead of\ndesigning different policies for different tasks, our approach unifies the\ndefinition of a task through a sequence of contact goals-desired contact\npositions, timings, and active end-effectors. This enables leveraging the\nshared structure across diverse contact-rich tasks, leading to a single policy\nthat can perform a wide range of tasks. In particular, we train a\ngoal-conditioned reinforcement learning (RL) policy to realise given contact\nplans. We validate our framework on multiple robotic embodiments and tasks: a\nquadruped performing multiple gaits, a humanoid performing multiple biped and\nquadrupedal gaits, and a humanoid executing different bimanual object\nmanipulation tasks. Each of these scenarios is controlled by a single policy\ntrained to execute different tasks grounded in contacts, demonstrating\nversatile and robust behaviours across morphologically distinct systems. Our\nresults show that explicit contact reasoning significantly improves\ngeneralisation to unseen scenarios, positioning contact-explicit policy\nlearning as a promising foundation for scalable loco-manipulation.", "AI": {"tldr": "A unified framework for multi-task locomotion and manipulation using contact-explicit representations, enabling a single policy to handle diverse tasks across different robot embodiments.", "motivation": "To overcome the limitation of designing separate policies for different tasks by leveraging shared structure across contact-rich tasks through a unified contact-based representation.", "method": "Goal-conditioned reinforcement learning policy trained to realize contact plans defined by sequences of contact goals (positions, timings, active end-effectors).", "result": "Single policies successfully controlled quadruped gaits, humanoid biped/quadrupedal gaits, and bimanual object manipulation across different robotic embodiments with robust performance.", "conclusion": "Explicit contact reasoning significantly improves generalization to unseen scenarios, making contact-explicit policy learning a promising foundation for scalable loco-manipulation."}}
{"id": "2510.03605", "categories": ["cs.AI", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03605", "abs": "https://arxiv.org/abs/2510.03605", "authors": ["Adel Javanmard", "Baharan Mirzasoleiman", "Vahab Mirrokni"], "title": "Understanding the Role of Training Data in Test-Time Scaling", "comment": "24 pages, 4 figures", "summary": "Test-time scaling improves the reasoning capabilities of large language\nmodels (LLMs) by allocating extra compute to generate longer Chains-of-Thoughts\n(CoTs). This enables models to tackle more complex problem by breaking them\ndown into additional steps, backtracking, and correcting mistakes. Despite its\nstrong performance--demonstrated by OpenAI's o1 and DeepSeek R1, the conditions\nin the training data under which long CoTs emerge, and when such long CoTs\nimprove the performance, remain unclear. In this paper, we study the\nperformance of test-time scaling for transformers trained on an in-context\nweight prediction task for linear regression. Our analysis provides a\ntheoretical explanation for several intriguing observations: First, at any\nfixed test error, increasing test-time compute allows us to reduce the number\nof in-context examples (context length) in training prompts. Second, if the\nskills required to solve a downstream task are not sufficiently present in the\ntraining data, increasing test-time compute can harm performance. Finally, we\ncharacterize task hardness via the smallest eigenvalue of its feature\ncovariance matrix and show that training on a diverse, relevant, and hard set\nof tasks results in best performance for test-time scaling. We confirm our\nfindings with experiments on large, nonlinear transformer architectures.", "AI": {"tldr": "Test-time scaling improves LLM reasoning by generating longer Chains-of-Thought, but its effectiveness depends on training data conditions. The paper analyzes when long CoTs help or harm performance using linear regression tasks.", "motivation": "To understand when and why test-time scaling (longer Chains-of-Thought) improves reasoning performance, as current understanding is unclear despite strong empirical results from models like OpenAI's o1 and DeepSeek R1.", "method": "Theoretical analysis using transformers trained on in-context weight prediction for linear regression, characterizing task hardness via feature covariance eigenvalues, and experimental validation on nonlinear transformers.", "result": "Found that: 1) More test-time compute reduces needed training context length for same error; 2) Insufficient training data skills can make test-time scaling harmful; 3) Diverse, relevant, hard tasks yield best test-time scaling performance.", "conclusion": "Test-time scaling effectiveness depends critically on training data quality - it requires sufficient relevant skills in training data and performs best on diverse, challenging tasks."}}
{"id": "2510.03348", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03348", "abs": "https://arxiv.org/abs/2510.03348", "authors": ["Vlardimir Yugay", "Duy-Kien Nguyen", "Theo Gevers", "Cees G. M. Snoek", "Martin R. Oswald"], "title": "Visual Odometry with Transformers", "comment": null, "summary": "Modern monocular visual odometry methods typically combine pre-trained deep\nlearning components with optimization modules, resulting in complex pipelines\nthat rely heavily on camera calibration and hyperparameter tuning, and often\nstruggle in unseen real-world scenarios. Recent large-scale 3D models trained\non massive amounts of multi-modal data have partially alleviated these\nchallenges, providing generalizable dense reconstruction and camera pose\nestimation. Still, they remain limited in handling long videos and providing\naccurate per-frame estimates, which are required for visual odometry. In this\nwork, we demonstrate that monocular visual odometry can be addressed\neffectively in an end-to-end manner, thereby eliminating the need for\nhandcrafted components such as bundle adjustment, feature matching, camera\ncalibration, or dense 3D reconstruction. We introduce VoT, short for Visual\nodometry Transformer, which processes sequences of monocular frames by\nextracting features and modeling global relationships through temporal and\nspatial attention. Unlike prior methods, VoT directly predicts camera motion\nwithout estimating dense geometry and relies solely on camera poses for\nsupervision. The framework is modular and flexible, allowing seamless\nintegration of various pre-trained encoders as feature extractors. Experimental\nresults demonstrate that VoT scales effectively with larger datasets, benefits\nsubstantially from stronger pre-trained backbones, generalizes across diverse\ncamera motions and calibration settings, and outperforms traditional methods\nwhile running more than 3 times faster. The code will be released.", "AI": {"tldr": "VoT (Visual odometry Transformer) is an end-to-end monocular visual odometry method that uses temporal and spatial attention to directly predict camera motion without traditional components like bundle adjustment or dense 3D reconstruction.", "motivation": "Traditional monocular visual odometry methods rely on complex pipelines with pre-trained components and optimization modules, requiring camera calibration and hyperparameter tuning, and struggle in unseen scenarios. Existing large-scale 3D models have limitations in handling long videos and providing accurate per-frame estimates.", "method": "VoT processes sequences of monocular frames by extracting features and modeling global relationships through temporal and spatial attention. It directly predicts camera motion without estimating dense geometry, uses only camera poses for supervision, and allows integration of various pre-trained encoders as feature extractors.", "result": "VoT scales effectively with larger datasets, benefits from stronger pre-trained backbones, generalizes across diverse camera motions and calibration settings, outperforms traditional methods, and runs more than 3 times faster.", "conclusion": "Monocular visual odometry can be effectively addressed in an end-to-end manner, eliminating the need for handcrafted components like bundle adjustment, feature matching, camera calibration, or dense 3D reconstruction."}}
{"id": "2510.03640", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.03640", "abs": "https://arxiv.org/abs/2510.03640", "authors": ["Mostafa Emam", "Matthias Gerdts"], "title": "Safety-Oriented Dynamic Path Planning for Automated Vehicles", "comment": "Published in 2025 IEEE 101st Vehicular Technology Conference\n  (VTC2025-Spring), Oslo, Norway, June 17-20, 2025. Received Best Conference\n  Paper Award", "summary": "Ensuring safety in autonomous vehicles necessitates advanced path planning\nand obstacle avoidance capabilities, particularly in dynamic environments. This\npaper introduces a bi-level control framework that efficiently augments road\nboundaries by incorporating time-dependent grid projections of obstacle\nmovements, thus enabling precise and adaptive path planning. The main control\nloop utilizes Nonlinear Model Predictive Control (NMPC) for real-time path\noptimization, wherein homotopy-based constraint relaxation is employed to\nimprove the solvability of the optimal control problem (OCP). Furthermore, an\nindependent backup loop runs concurrently to provide safe fallback trajectories\nwhen an optimal trajectory cannot be computed by the main loop within a\ncritical time frame, thus enhancing safety and real-time performance. Our\nevaluation showcases the benefits of the proposed methods in various driving\nscenarios, highlighting the real-time applicability and robustness of our\napproach. Overall, the framework represents a significant step towards safer\nand more reliable autonomous driving in complex and dynamic environments.", "AI": {"tldr": "A bi-level control framework for autonomous vehicles that combines NMPC-based path planning with homotopy constraint relaxation and a backup safety loop for real-time obstacle avoidance in dynamic environments.", "motivation": "To ensure safety in autonomous vehicles through advanced path planning and obstacle avoidance capabilities, particularly in dynamic environments where real-time performance is critical.", "method": "Uses a bi-level control framework with: 1) Main loop with Nonlinear Model Predictive Control (NMPC) for real-time path optimization using homotopy-based constraint relaxation, and 2) Independent backup loop for safe fallback trajectories when main loop fails within critical time.", "result": "Evaluation shows benefits in various driving scenarios, demonstrating real-time applicability and robustness of the approach.", "conclusion": "The framework represents a significant step towards safer and more reliable autonomous driving in complex and dynamic environments."}}
{"id": "2510.03612", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.03612", "abs": "https://arxiv.org/abs/2510.03612", "authors": ["Tanqiu Jiang", "Min Bai", "Nikolaos Pappas", "Yanjun Qi", "Sandesh Swamy"], "title": "Cross-Modal Content Optimization for Steering Web Agent Preferences", "comment": null, "summary": "Vision-language model (VLM)-based web agents increasingly power high-stakes\nselection tasks like content recommendation or product ranking by combining\nmultimodal perception with preference reasoning. Recent studies reveal that\nthese agents are vulnerable against attackers who can bias selection outcomes\nthrough preference manipulations using adversarial pop-ups, image\nperturbations, or content tweaks. Existing work, however, either assumes strong\nwhite-box access, with limited single-modal perturbations, or uses impractical\nsettings. In this paper, we demonstrate, for the first time, that joint\nexploitation of visual and textual channels yields significantly more powerful\npreference manipulations under realistic attacker capabilities. We introduce\nCross-Modal Preference Steering (CPS) that jointly optimizes imperceptible\nmodifications to an item's visual and natural language descriptions, exploiting\nCLIP-transferable image perturbations and RLHF-induced linguistic biases to\nsteer agent decisions. In contrast to prior studies that assume gradient\naccess, or control over webpages, or agent memory, we adopt a realistic\nblack-box threat setup: a non-privileged adversary can edit only their own\nlisting's images and textual metadata, with no insight into the agent's model\ninternals. We evaluate CPS on agents powered by state-of-the-art proprietary\nand open source VLMs including GPT-4.1, Qwen-2.5VL and Pixtral-Large on both\nmovie selection and e-commerce tasks. Our results show that CPS is\nsignificantly more effective than leading baseline methods. For instance, our\nresults show that CPS consistently outperforms baselines across all models\nwhile maintaining 70% lower detection rates, demonstrating both effectiveness\nand stealth. These findings highlight an urgent need for robust defenses as\nagentic systems play an increasingly consequential role in society.", "AI": {"tldr": "Cross-Modal Preference Steering (CPS) is a black-box attack method that jointly optimizes visual and textual perturbations to manipulate VLM-based web agents' selection decisions, achieving high effectiveness while maintaining stealth.", "motivation": "VLM-based web agents are vulnerable to preference manipulations, but existing attacks assume unrealistic settings like white-box access or single-modal perturbations. This paper aims to demonstrate more powerful attacks under realistic black-box conditions.", "method": "CPS jointly optimizes imperceptible modifications to both visual and textual content, exploiting CLIP-transferable image perturbations and RLHF-induced linguistic biases to steer agent decisions without requiring gradient access or agent internals.", "result": "CPS significantly outperforms baseline methods across state-of-the-art VLMs (GPT-4.1, Qwen-2.5VL, Pixtral-Large) on movie selection and e-commerce tasks, while maintaining 70% lower detection rates.", "conclusion": "The findings reveal critical vulnerabilities in VLM-based web agents and highlight the urgent need for robust defenses as these systems play increasingly important societal roles."}}
{"id": "2510.03352", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03352", "abs": "https://arxiv.org/abs/2510.03352", "authors": ["Mahdi Farahbakhsh", "Vishnu Teja Kunde", "Dileep Kalathil", "Krishna Narayanan", "Jean-Francois Chamberland"], "title": "Inference-Time Search using Side Information for Diffusion-based Image Reconstruction", "comment": null, "summary": "Diffusion models have emerged as powerful priors for solving inverse\nproblems. However, existing approaches typically overlook side information that\ncould significantly improve reconstruction quality, especially in severely\nill-posed settings. In this work, we propose a novel inference-time search\nalgorithm that guides the sampling process using the side information in a\nmanner that balances exploration and exploitation. This enables more accurate\nand reliable reconstructions, providing an alternative to the gradient-based\nguidance that is prone to reward-hacking artifacts. Our approach can be\nseamlessly integrated into a wide range of existing diffusion-based image\nreconstruction pipelines. Through extensive experiments on a number of inverse\nproblems, such as box inpainting, super-resolution, and various deblurring\ntasks including motion, Gaussian, nonlinear, and blind deblurring, we show that\nour approach consistently improves the qualitative and quantitative performance\nof diffusion-based image reconstruction algorithms. We also show the superior\nperformance of our approach with respect to other baselines, including reward\ngradient-based guidance algorithms. The code is available at\n\\href{https://github.com/mhdfb/sideinfo-search-reconstruction}{this\nrepository}.", "AI": {"tldr": "A novel inference-time search algorithm that uses side information to guide diffusion models for solving inverse problems, improving reconstruction quality without gradient-based guidance artifacts.", "motivation": "Existing diffusion-based approaches for inverse problems overlook valuable side information that could significantly enhance reconstruction quality, especially in severely ill-posed settings.", "method": "Proposed an inference-time search algorithm that guides the sampling process using side information, balancing exploration and exploitation to avoid reward-hacking artifacts common in gradient-based guidance methods.", "result": "The approach consistently improves qualitative and quantitative performance across various inverse problems including box inpainting, super-resolution, and multiple deblurring tasks (motion, Gaussian, nonlinear, blind), outperforming reward gradient-based guidance baselines.", "conclusion": "The method provides an effective alternative to gradient-based guidance that can be seamlessly integrated into existing diffusion-based image reconstruction pipelines, enabling more accurate and reliable reconstructions."}}
{"id": "2510.03644", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03644", "abs": "https://arxiv.org/abs/2510.03644", "authors": ["Mohammadjavad Javadi", "Robin Chhabra"], "title": "Geometrically Exact Hard Magneto-Elastic Cosserat Shells: Static Formulation for Shape Morphing", "comment": null, "summary": "Cosserat rod theory is the popular approach to modeling ferromagnetic soft\nrobots as 1-Dimensional (1D) slender structures in most applications, such as\nbiomedical. However, recent soft robots designed for locomotion and\nmanipulation often exhibit a large width-to-length ratio that categorizes them\nas 2D shells. For analysis and shape-morphing control purposes, we develop an\nefficient coordinate-free static model of hard-magnetic shells found in soft\nmagnetic grippers and walking soft robots. The approach is based on a novel\nformulation of Cosserat shell theory on the Special Euclidean group\n($\\mathbf{SE}(3)$). The shell is assumed to be a 2D manifold of material points\nwith six degrees of freedom (position & rotation) suitable for capturing the\nbehavior of a uniformly distributed array of spheroidal hard magnetic particles\nembedded in the rheological elastomer. The shell's configuration manifold is\nthe space of all smooth embeddings $\\mathbb{R}^2\\rightarrow\\mathbf{SE}(3)$.\nAccording to a novel definition of local deformation gradient based on the Lie\ngroup structure of $\\mathbf{SE}(3)$, we derive the strong and weak forms of\nequilibrium equations, following the principle of virtual work. We extract the\nlinearized version of the weak form for numerical implementations. The\nresulting finite element approach can avoid well-known challenges such as\nsingularity and locking phenomenon in modeling shell structures. The proposed\nmodel is analytically and experimentally validated through a series of test\ncases that demonstrate its superior efficacy, particularly when the shell\nundergoes severe rotations and displacements.", "AI": {"tldr": "The paper develops a coordinate-free static model for hard-magnetic shells using Cosserat shell theory on the Special Euclidean group (SE(3)), enabling efficient analysis of 2D soft magnetic robots with large width-to-length ratios.", "motivation": "Traditional Cosserat rod theory is insufficient for modeling modern soft robots with large width-to-length ratios that behave as 2D shells, particularly in applications like magnetic grippers and walking robots.", "method": "A novel formulation of Cosserat shell theory on SE(3) group, treating the shell as a 2D manifold with six degrees of freedom (position and rotation), using Lie group structure for deformation gradient, and deriving equilibrium equations via virtual work principle with finite element implementation.", "result": "The model avoids common numerical challenges like singularity and locking phenomena, and is validated through analytical and experimental test cases showing superior efficacy for shells undergoing severe rotations and displacements.", "conclusion": "The proposed coordinate-free Cosserat shell model provides an efficient and robust framework for analyzing and controlling shape-morphing in hard-magnetic shell structures used in advanced soft robotics applications."}}
{"id": "2510.03632", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03632", "abs": "https://arxiv.org/abs/2510.03632", "authors": ["Jiaxi Li", "Yucheng Shi", "Jin Lu", "Ninghao Liu"], "title": "MITS: Enhanced Tree Search Reasoning for LLMs via Pointwise Mutual Information", "comment": "18 pages", "summary": "Tree search has become as a representative framework for test-time reasoning\nwith large language models (LLMs), exemplified by methods such as\nTree-of-Thought and Monte Carlo Tree Search that explore multiple reasoning\npaths. However, it remains difficult to provide instant and reliable\nquantitative assessments of intermediate reasoning step quality, and extensive\npath exploration is computationally costly. To address this, we propose Mutual\nInformation Tree Search (MITS), a novel framework that guides reasoning with\ninformation-theoretic principles. MITS introduces an effective scoring function\nbased on pointwise mutual information (PMI), which enables step-wise evaluation\nof reasoning paths and search tree expansion via beam search without expensive\nlook-ahead simulations, achieving superior reasoning performances while\nmaintaining computational efficiency. The framework is complemented by an\nentropy-based dynamic sampling strategy that adaptively allocates computational\nresources to uncertain reasoning steps where exploration is most beneficial.\nFor final prediction, MITS employs a weighted voting scheme that combines PMI\nscores with prediction consensus. Through comprehensive experiments on diverse\nreasoning benchmarks, MITS consistently surpasses baseline methods,\nestablishing a principled and efficient framework for LLM reasoning.", "AI": {"tldr": "MITS is a novel tree search framework for LLM reasoning that uses pointwise mutual information (PMI) for step-wise path evaluation and beam search, achieving superior performance with computational efficiency through entropy-based dynamic sampling and weighted voting.", "motivation": "Existing tree search methods for LLM reasoning face challenges in providing instant quantitative assessments of intermediate reasoning steps and suffer from high computational costs due to extensive path exploration.", "method": "Proposes Mutual Information Tree Search (MITS) with PMI-based scoring function for step-wise evaluation, beam search for tree expansion without look-ahead simulations, entropy-based dynamic sampling for adaptive resource allocation, and weighted voting for final predictions.", "result": "MITS consistently surpasses baseline methods across diverse reasoning benchmarks while maintaining computational efficiency.", "conclusion": "MITS establishes a principled and efficient framework for LLM reasoning that combines information-theoretic principles with practical computational considerations."}}
{"id": "2510.03353", "categories": ["cs.CV", "I.4.9; I.5.0; H.3.1; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.03353", "abs": "https://arxiv.org/abs/2510.03353", "authors": ["Larissa S. Gomes", "Gustavo P. Almeida", "Bryan U. Moreira", "Marco Quiroz", "Breno Xavier", "Lucas Soares", "Stephanie L. Bri\u00e3o", "Felipe G. Oliveira", "Paulo L. J. Drews-Jr"], "title": "Sonar Image Datasets: A Comprehensive Survey of Resources, Challenges, and Applications", "comment": "Published in the Conference on Graphics, Patterns and Images\n  (SIBGRAPI). This 4-page paper presents a timeline of publicly available\n  datasets up to the year 2025", "summary": "Sonar images are relevant for advancing underwater exploration, autonomous\nnavigation, and ecosystem monitoring. However, the progress depends on data\navailability. The scarcity of publicly available, well-annotated sonar image\ndatasets creates a significant bottleneck for the development of robust machine\nlearning models. This paper presents a comprehensive and concise review of the\ncurrent landscape of sonar image datasets, seeking not only to catalog existing\nresources but also to contextualize them, identify gaps, and provide a clear\nroadmap, serving as a base guide for researchers of any kind who wish to start\nor advance in the field of underwater acoustic data analysis. We mapped\npublicly accessible datasets across various sonar modalities, including Side\nScan Sonar (SSS), Forward-Looking Sonar (FLS), Synthetic Aperture Sonar (SAS),\nMultibeam Echo Sounder (MBES), and Dual-Frequency Identification Sonar\n(DIDSON). An analysis was conducted on applications such as classification,\ndetection, segmentation, and 3D reconstruction. This work focuses on\nstate-of-the-art advancements, incorporating newly released datasets. The\nfindings are synthesized into a master table and a chronological timeline,\noffering a clear and accessible comparison of characteristics, sizes, and\nannotation details datasets.", "AI": {"tldr": "This paper provides a comprehensive review of publicly available sonar image datasets across various modalities (SSS, FLS, SAS, MBES, DIDSON) and applications (classification, detection, segmentation, 3D reconstruction), identifying gaps and offering a roadmap for researchers in underwater acoustic data analysis.", "motivation": "The scarcity of publicly available, well-annotated sonar image datasets creates a significant bottleneck for developing robust machine learning models in underwater exploration, autonomous navigation, and ecosystem monitoring.", "method": "Conducted a comprehensive review by mapping publicly accessible datasets across various sonar modalities, analyzing applications, and synthesizing findings into a master table and chronological timeline for clear comparison of dataset characteristics, sizes, and annotation details.", "result": "Created a systematic catalog of existing sonar image datasets with contextual analysis, identified gaps in available resources, and developed accessible comparison tools (master table and timeline) for researchers.", "conclusion": "This review serves as a foundational guide for researchers entering or advancing in underwater acoustic data analysis by providing a clear roadmap of available datasets and highlighting areas needing further development."}}
{"id": "2510.03660", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03660", "abs": "https://arxiv.org/abs/2510.03660", "authors": ["Mohammadjavad Javadi", "Charlie Wadds", "Robin Chhabra"], "title": "An Amphibious Untethered Inchworm Soft Robot for Fast Crawling Locomotion", "comment": null, "summary": "Untethered soft robots are essential for advancing the real-world deployment\nof soft robotic systems in diverse and multitasking environments. Inspired by\nsoft-bodied inchworm, we present a fully untethered soft robot with a curved,\nflexible structure actuated by magnetic forces. The robot has a total mass of\n102.63 g and demonstrates multimodal locomotion, achieving a maximum walking\nspeed of 3.74 cm/s and a swimming speed of 0.82 cm/s. A compact and lightweight\nonboard control circuit enables wireless command transmission, while an\nintegrated camera provides environmental perception. Through structural\noptimization and system-level integration, the robot successfully performs\nwalking, steering, swimming, and payload transport without reliance on external\ninfrastructure. The robot's dynamic performance and locomotion capabilities are\nsystematically validated through experimental characterization.", "AI": {"tldr": "A fully untethered soft robot inspired by inchworms, using magnetic actuation to achieve multimodal locomotion including walking, swimming, and payload transport with onboard wireless control and camera.", "motivation": "To advance real-world deployment of soft robots in diverse environments by creating untethered systems that don't rely on external infrastructure.", "method": "Developed a curved, flexible soft robot actuated by magnetic forces, featuring structural optimization, onboard wireless control circuit, and integrated camera for environmental perception.", "result": "Robot achieved maximum walking speed of 3.74 cm/s and swimming speed of 0.82 cm/s, successfully performing walking, steering, swimming, and payload transport with total mass of 102.63 g.", "conclusion": "The study demonstrates successful development of a fully untethered soft robot with multimodal locomotion capabilities, validated through systematic experimental characterization."}}
{"id": "2510.03680", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03680", "abs": "https://arxiv.org/abs/2510.03680", "authors": ["Bumjun Kim", "Dongjae Jeon", "Dueun Kim", "Wonje Jeung", "Albert No"], "title": "Rainbow Padding: Mitigating Early Termination in Instruction-Tuned Diffusion LLMs", "comment": "25 pages. Project page available\n  at~\\url{https://ai-isl.github.io/rainbow-padding}", "summary": "Diffusion large language models (dLLMs) have emerged as a promising\nalternative to autoregressive models, offering flexible generation orders and\nstrong performance on complex reasoning tasks. However, instruction-tuned dLLMs\nexhibit a critical vulnerability we term \\texttt{<eos>} overflow: as allocated\nsequence length increases, responses paradoxically become shorter, collapsing\ninto early termination or degenerating into streams of \\texttt{<eos>} tokens.\nAlthough noticed in practice, this issue has not been systematically analyzed.\nWe trace its root cause to the dual role of \\texttt{<eos>} as both termination\nand padding, which concentrates probability mass on \\texttt{<eos>} at later\npositions and propagates backward to trigger early termination. To address\nthis, we introduce Rainbow Padding, a simple remedy that replaces repeated\n\\texttt{<eos>} placeholders with a repeating cycle of distinct padding tokens,\ndistributing probability mass and breaking \\texttt{<eos>} dominance.\nExperiments show that Rainbow Padding substantially improves length robustness\nand output quality, with as few as seven padding tokens sufficient to prevent\nearly termination. Moreover, the method integrates efficiently into existing\ninstruction-tuned models: LoRA fine-tuning for a single epoch on minimal data\nyields significant improvements, making this solution highly practical. The\ncode is publicly available at https://github.com/quasar529/rainbow-padding.", "AI": {"tldr": "Diffusion LLMs suffer from 'eos overflow' where longer allocated sequences cause shorter responses due to eos token dominance. Rainbow Padding fixes this by using multiple distinct padding tokens instead of repeated eos tokens.", "motivation": "Instruction-tuned diffusion LLMs exhibit a critical vulnerability where increasing sequence length paradoxically shortens responses due to eos token dominance, which hasn't been systematically analyzed.", "method": "Rainbow Padding replaces repeated eos placeholders with a repeating cycle of distinct padding tokens to distribute probability mass and break eos dominance.", "result": "Rainbow Padding substantially improves length robustness and output quality, with just seven padding tokens sufficient to prevent early termination. It integrates efficiently via single-epoch LoRA fine-tuning on minimal data.", "conclusion": "Rainbow Padding provides a simple, practical solution to eos overflow in diffusion LLMs, enabling robust generation across varying sequence lengths with minimal training overhead."}}
{"id": "2510.03356", "categories": ["cs.CV", "cs.ET"], "pdf": "https://arxiv.org/pdf/2510.03356", "abs": "https://arxiv.org/abs/2510.03356", "authors": ["Ziyang Chen", "Yuta Itoh", "Kaan Ak\u015fit"], "title": "Learned Display Radiance Fields with Lensless Cameras", "comment": null, "summary": "Calibrating displays is a basic and regular task that content creators must\nperform to maintain optimal visual experience, yet it remains a troublesome\nissue. Measuring display characteristics from different viewpoints often\nrequires specialized equipment and a dark room, making it inaccessible to most\nusers. To avoid specialized hardware requirements in display calibrations, our\nwork co-designs a lensless camera and an Implicit Neural Representation based\nalgorithm for capturing display characteristics from various viewpoints. More\nspecifically, our pipeline enables efficient reconstruction of light fields\nemitted from a display from a viewing cone of 46.6{\\deg} X 37.6{\\deg}. Our\nemerging pipeline paves the initial steps towards effortless display\ncalibration and characterization.", "AI": {"tldr": "The paper presents a lensless camera and neural algorithm system for display calibration without specialized hardware, enabling light field reconstruction from multiple viewpoints.", "motivation": "Traditional display calibration requires specialized equipment and dark rooms, making it inaccessible to most users. The authors aim to eliminate hardware requirements for display characterization.", "method": "Co-design of a lensless camera and Implicit Neural Representation algorithm to capture display characteristics from various viewpoints, enabling light field reconstruction from a 46.6\u00b0 \u00d7 37.6\u00b0 viewing cone.", "result": "The pipeline successfully reconstructs light fields emitted from displays across a wide viewing angle, providing an efficient method for display characterization.", "conclusion": "This emerging pipeline represents initial progress toward effortless display calibration and characterization, potentially making the process more accessible without requiring specialized hardware."}}
{"id": "2510.03677", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03677", "abs": "https://arxiv.org/abs/2510.03677", "authors": ["Salim Rezvani", "Ammar Jaleel Mahmood", "Robin Chhabra"], "title": "Robust Visual Embodiment: How Robots Discover Their Bodies in Real Environments", "comment": null, "summary": "Robots with internal visual self-models promise unprecedented adaptability,\nyet existing autonomous modeling pipelines remain fragile under realistic\nsensing conditions such as noisy imagery and cluttered backgrounds. This paper\npresents the first systematic study quantifying how visual\ndegradations--including blur, salt-and-pepper noise, and Gaussian noise--affect\nrobotic self-modeling. Through both simulation and physical experiments, we\ndemonstrate their impact on morphology prediction, trajectory planning, and\ndamage recovery in state-of-the-art pipelines. To overcome these challenges, we\nintroduce a task-aware denoising framework that couples classical restoration\nwith morphology-preserving constraints, ensuring retention of structural cues\ncritical for self-modeling. In addition, we integrate semantic segmentation to\nrobustly isolate robots from cluttered and colorful scenes. Extensive\nexperiments show that our approach restores near-baseline performance across\nsimulated and physical platforms, while existing pipelines degrade\nsignificantly. These contributions advance the robustness of visual\nself-modeling and establish practical foundations for deploying self-aware\nrobots in unpredictable real-world environments.", "AI": {"tldr": "This paper studies how visual degradations (blur, noise) affect robotic self-modeling and introduces a task-aware denoising framework with morphology-preserving constraints and semantic segmentation to maintain performance.", "motivation": "Existing autonomous visual self-modeling pipelines are fragile under realistic sensing conditions like noisy imagery and cluttered backgrounds, limiting robot adaptability in real-world environments.", "method": "Introduces a task-aware denoising framework that combines classical restoration with morphology-preserving constraints, plus semantic segmentation to isolate robots from cluttered scenes.", "result": "The approach restores near-baseline performance across simulated and physical platforms, while existing pipelines degrade significantly under visual degradations.", "conclusion": "These contributions advance the robustness of visual self-modeling and establish practical foundations for deploying self-aware robots in unpredictable real-world environments."}}
{"id": "2510.03696", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03696", "abs": "https://arxiv.org/abs/2510.03696", "authors": ["Deepak Babu Piskala", "Sharlene Chen", "Udita Patel", "Parul Kalra", "Rafael Castrillo"], "title": "Mind the Goal: Data-Efficient Goal-Oriented Evaluation of Conversational Agents and Chatbots using Teacher Models", "comment": null, "summary": "Evaluating the quality of multi-turn chatbot interactions remains\nchallenging, as most existing methods assess interactions at the turn level\nwithout addressing whether a user's overarching goal was fulfilled. A ``goal''\nhere refers to an information need or task, such as asking for policy\ninformation or applying for leave. We propose a comprehensive framework for\ngoal-oriented evaluation of multi-agent systems (MAS), introducing the\n\\textbf{Goal Success Rate (GSR)} to measure the percentage of fulfilled goals,\nand a \\textbf{Root Cause of Failure (RCOF)} taxonomy to identify reasons for\nfailure in multi-agent chatbots. Our method segments conversations by user\ngoals and evaluates success using all relevant turns. We present a model-based\nevaluation system combining teacher LLMs, where domain experts define goals,\nset quality standards serving as a guidance for the LLMs. The LLMs use\n``thinking tokens'' to produce interpretable rationales, enabling\n\\textit{explainable}, \\textit{data-efficient} evaluations. In an enterprise\nsetting, we apply our framework to evaluate AIDA, a zero-to-one employee\nconversational agent system built as a ground-up multi-agent conversational\nagent, and observe GSR improvement from 63\\% to 79\\% over six months since its\ninception. Our framework is generic and offers actionable insights through a\ndetailed defect taxonomy based on analysis of failure points in multi-agent\nchatbots, diagnosing overall success, identifying key failure modes, and\ninforming system improvements.", "AI": {"tldr": "Proposes a goal-oriented evaluation framework for multi-agent chatbots using Goal Success Rate (GSR) and Root Cause of Failure (RCOF) taxonomy to measure whether user goals are fulfilled and diagnose failures.", "motivation": "Existing chatbot evaluation methods assess interactions at turn level but don't address whether users' overarching goals (information needs or tasks) are actually fulfilled.", "method": "Segments conversations by user goals, uses teacher LLMs with domain expert-defined goals and quality standards, employs \"thinking tokens\" for interpretable rationales, and applies GSR and RCOF taxonomy.", "result": "Applied to AIDA enterprise chatbot system, achieved GSR improvement from 63% to 79% over six months, providing actionable insights through detailed failure analysis.", "conclusion": "The framework offers explainable, data-efficient evaluation that diagnoses overall success, identifies key failure modes, and informs system improvements for multi-agent chatbots."}}
{"id": "2510.03361", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03361", "abs": "https://arxiv.org/abs/2510.03361", "authors": ["Ali Kayyam", "Anusha Madan Gopal", "M. Anthony Lewis"], "title": "Provenance Networks: End-to-End Exemplar-Based Explainability", "comment": null, "summary": "We introduce provenance networks, a novel class of neural models designed to\nprovide end-to-end, training-data-driven explainability. Unlike conventional\npost-hoc methods, provenance networks learn to link each prediction directly to\nits supporting training examples as part of the model's normal operation,\nembedding interpretability into the architecture itself. Conceptually, the\nmodel operates similarly to a learned KNN, where each output is justified by\nconcrete exemplars weighted by relevance in the feature space. This approach\nfacilitates systematic investigations of the trade-off between memorization and\ngeneralization, enables verification of whether a given input was included in\nthe training set, aids in the detection of mislabeled or anomalous data points,\nenhances resilience to input perturbations, and supports the identification of\nsimilar inputs contributing to the generation of a new data point. By jointly\noptimizing the primary task and the explainability objective, provenance\nnetworks offer insights into model behavior that traditional deep networks\ncannot provide. While the model introduces additional computational cost and\ncurrently scales to moderately sized datasets, it provides a complementary\napproach to existing explainability techniques. In particular, it addresses\ncritical challenges in modern deep learning, including model opaqueness,\nhallucination, and the assignment of credit to data contributors, thereby\nimproving transparency, robustness, and trustworthiness in neural models.", "AI": {"tldr": "Provenance networks are neural models that provide end-to-end explainability by linking predictions directly to supporting training examples, embedding interpretability into the architecture itself.", "motivation": "To address model opaqueness, hallucination, and credit assignment issues in deep learning by providing training-data-driven explainability that traditional deep networks cannot offer.", "method": "The model operates like a learned KNN, jointly optimizing the primary task and explainability objective. It links each prediction to relevant training examples weighted by feature space relevance, embedding interpretability directly into the architecture.", "result": "Enables systematic investigation of memorization vs. generalization trade-offs, verification of training set inclusion, detection of mislabeled/anomalous data, enhanced resilience to input perturbations, and identification of similar inputs contributing to new data generation.", "conclusion": "While introducing additional computational cost and currently scaling to moderately sized datasets, provenance networks provide a complementary approach that improves transparency, robustness, and trustworthiness in neural models by addressing critical explainability challenges."}}
{"id": "2510.03706", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03706", "abs": "https://arxiv.org/abs/2510.03706", "authors": ["Eadom Dessalene", "Pavan Mantripragada", "Michael Maynord", "Yiannis Aloimonos"], "title": "EmbodiSwap for Zero-Shot Robot Imitation Learning", "comment": "Video link:\n  https://drive.google.com/file/d/1UccngwgPqUwPMhBja7JrXfZoTquCx_Qe/view?usp=sharing", "summary": "We introduce EmbodiSwap - a method for producing photorealistic synthetic\nrobot overlays over human video. We employ EmbodiSwap for zero-shot imitation\nlearning, bridging the embodiment gap between in-the-wild ego-centric human\nvideo and a target robot embodiment. We train a closed-loop robot manipulation\npolicy over the data produced by EmbodiSwap. We make novel use of V-JEPA as a\nvisual backbone, repurposing V-JEPA from the domain of video understanding to\nimitation learning over synthetic robot videos. Adoption of V-JEPA outperforms\nalternative vision backbones more conventionally used within robotics. In\nreal-world tests, our zero-shot trained V-JEPA model achieves an $82\\%$ success\nrate, outperforming a few-shot trained $\\pi_0$ network as well as $\\pi_0$\ntrained over data produced by EmbodiSwap. We release (i) code for generating\nthe synthetic robot overlays which takes as input human videos and an arbitrary\nrobot URDF and generates a robot dataset, (ii) the robot dataset we synthesize\nover EPIC-Kitchens, HOI4D and Ego4D, and (iii) model checkpoints and inference\ncode, to facilitate reproducible research and broader adoption.", "AI": {"tldr": "EmbodiSwap is a method for creating photorealistic synthetic robot overlays on human videos to enable zero-shot imitation learning, bridging the embodiment gap between human videos and target robots.", "motivation": "To address the embodiment gap between in-the-wild ego-centric human video data and target robot embodiments, enabling zero-shot imitation learning without requiring real robot demonstration data.", "method": "Uses EmbodiSwap to generate synthetic robot overlays on human videos, trains closed-loop robot manipulation policies on this synthetic data, and employs V-JEPA as a visual backbone repurposed from video understanding to imitation learning.", "result": "The zero-shot trained V-JEPA model achieves 82% success rate in real-world tests, outperforming few-shot trained alternatives and other methods using EmbodiSwap data.", "conclusion": "EmbodiSwap with V-JEPA backbone enables effective zero-shot imitation learning, outperforming conventional approaches, and the authors release code, datasets, and models to facilitate broader adoption."}}
{"id": "2510.03700", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03700", "abs": "https://arxiv.org/abs/2510.03700", "authors": ["Seungseop Lim", "Gibaeg Kim", "Hyunkyung Lee", "Wooseok Han", "Jean Seo", "Jaehyo Yoo", "Eunho Yang"], "title": "H-DDx: A Hierarchical Evaluation Framework for Differential Diagnosis", "comment": "GenAI4Health @NeurIPS 2025", "summary": "An accurate differential diagnosis (DDx) is essential for patient care,\nshaping therapeutic decisions and influencing outcomes. Recently, Large\nLanguage Models (LLMs) have emerged as promising tools to support this process\nby generating a DDx list from patient narratives. However, existing evaluations\nof LLMs in this domain primarily rely on flat metrics, such as Top-k accuracy,\nwhich fail to distinguish between clinically relevant near-misses and\ndiagnostically distant errors. To mitigate this limitation, we introduce H-DDx,\na hierarchical evaluation framework that better reflects clinical relevance.\nH-DDx leverages a retrieval and reranking pipeline to map free-text diagnoses\nto ICD-10 codes and applies a hierarchical metric that credits predictions\nclosely related to the ground-truth diagnosis. In benchmarking 22 leading\nmodels, we show that conventional flat metrics underestimate performance by\noverlooking clinically meaningful outputs, with our results highlighting the\nstrengths of domain-specialized open-source models. Furthermore, our framework\nenhances interpretability by revealing hierarchical error patterns,\ndemonstrating that LLMs often correctly identify the broader clinical context\neven when the precise diagnosis is missed.", "AI": {"tldr": "H-DDx is a hierarchical evaluation framework for LLMs in differential diagnosis that better reflects clinical relevance by crediting predictions closely related to ground-truth diagnoses, showing conventional flat metrics underestimate performance.", "motivation": "Existing evaluations of LLMs for differential diagnosis rely on flat metrics that fail to distinguish between clinically relevant near-misses and diagnostically distant errors, limiting their clinical utility.", "method": "H-DDx uses a retrieval and reranking pipeline to map free-text diagnoses to ICD-10 codes and applies hierarchical metrics that credit predictions closely related to ground-truth diagnoses.", "result": "Benchmarking 22 leading models showed conventional flat metrics underestimate performance by overlooking clinically meaningful outputs, with domain-specialized open-source models performing well. The framework revealed LLMs often correctly identify broader clinical context even when missing precise diagnoses.", "conclusion": "H-DDx provides a more clinically relevant evaluation framework for LLMs in differential diagnosis, enhancing interpretability and revealing hierarchical error patterns that conventional metrics miss."}}
{"id": "2510.03363", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.03363", "abs": "https://arxiv.org/abs/2510.03363", "authors": ["Zhe Zhang", "Mingxiu Cai", "Gaochang Wu", "Jing Zhang", "Lingqiao Liu", "Dacheng Tao", "Tianyou Chai", "Xiatian Zhu"], "title": "Unified Unsupervised Anomaly Detection via Matching Cost Filtering", "comment": "63 pages (main paper and supplementary material), 39 figures, 58\n  tables. Submitted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI)", "summary": "Unsupervised anomaly detection (UAD) aims to identify image- and pixel-level\nanomalies using only normal training data, with wide applications such as\nindustrial inspection and medical analysis, where anomalies are scarce due to\nprivacy concerns and cold-start constraints. Existing methods, whether\nreconstruction-based (restoring normal counterparts) or embedding-based\n(pretrained representations), fundamentally conduct image- or feature-level\nmatching to generate anomaly maps. Nonetheless, matching noise has been largely\noverlooked, limiting their detection ability. Beyond earlier focus on unimodal\nRGB-based UAD, recent advances expand to multimodal scenarios, e.g., RGB--3D\nand RGB--Text, enabled by point cloud sensing and vision--language models.\nDespite shared challenges, these lines remain largely isolated, hindering a\ncomprehensive understanding and knowledge transfer. In this paper, we advocate\nunified UAD for both unimodal and multimodal settings in the matching\nperspective. Under this insight, we present Unified Cost Filtering (UCF), a\ngeneric post-hoc refinement framework for refining anomaly cost volume of any\nUAD model. The cost volume is constructed by matching a test sample against\nnormal samples from the same or different modalities, followed by a learnable\nfiltering module with multi-layer attention guidance from the test sample,\nmitigating matching noise and highlighting subtle anomalies. Comprehensive\nexperiments on 22 diverse benchmarks demonstrate the efficacy of UCF in\nenhancing a variety of UAD methods, consistently achieving new state-of-the-art\nresults in both unimodal (RGB) and multimodal (RGB--3D, RGB--Text) UAD\nscenarios. Code and models will be released at\nhttps://github.com/ZHE-SAPI/CostFilter-AD.", "AI": {"tldr": "Unified Cost Filtering (UCF) is a post-hoc framework that refines anomaly cost volumes in unsupervised anomaly detection by mitigating matching noise through learnable filtering with multi-layer attention guidance, achieving state-of-the-art results across 22 benchmarks in both unimodal and multimodal settings.", "motivation": "Existing unsupervised anomaly detection methods suffer from overlooked matching noise and remain isolated between unimodal and multimodal approaches, limiting detection ability and knowledge transfer.", "method": "UCF constructs anomaly cost volume by matching test samples against normal samples, then applies a learnable filtering module with multi-layer attention guidance to reduce matching noise and highlight subtle anomalies.", "result": "Comprehensive experiments on 22 diverse benchmarks show UCF consistently enhances various UAD methods, achieving new state-of-the-art results in both unimodal (RGB) and multimodal (RGB-3D, RGB-Text) scenarios.", "conclusion": "UCF provides a unified framework that effectively addresses matching noise in unsupervised anomaly detection and works across both unimodal and multimodal settings, demonstrating strong generalization capabilities."}}
{"id": "2510.03768", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03768", "abs": "https://arxiv.org/abs/2510.03768", "authors": ["Aydin Ahmadi", "Baris Akgun"], "title": "Model-Based Adaptive Precision Control for Tabletop Planar Pushing Under Uncertain Dynamics", "comment": null, "summary": "Data-driven planar pushing methods have recently gained attention as they\nreduce manual engineering effort and improve generalization compared to\nanalytical approaches. However, most prior work targets narrow capabilities\n(e.g., side switching, precision, or single-task training), limiting broader\napplicability. We present a model-based framework for non-prehensile tabletop\npushing that uses a single learned model to address multiple tasks without\nretraining. Our approach employs a recurrent GRU-based architecture with\nadditional non-linear layers to capture object-environment dynamics while\nensuring stability. A tailored state-action representation enables the model to\ngeneralize across uncertain dynamics, variable push lengths, and diverse tasks.\nFor control, we integrate the learned dynamics with a sampling-based Model\nPredictive Path Integral (MPPI) controller, which generates adaptive,\ntask-oriented actions. This framework supports side switching, variable-length\npushes, and objectives such as precise positioning, trajectory following, and\nobstacle avoidance. Training is performed in simulation with domain\nrandomization to support sim-to-real transfer. We first evaluate the\narchitecture through ablation studies, showing improved prediction accuracy and\nstable rollouts. We then validate the full system in simulation and real-world\nexperiments using a Franka Panda robot with markerless tracking. Results\ndemonstrate high success rates in precise positioning under strict thresholds\nand strong performance in trajectory tracking and obstacle avoidance. Moreover,\nmultiple tasks are solved simply by changing the controller's objective\nfunction, without retraining. While our current focus is on a single object\ntype, we extend the framework by training on wider push lengths and designing a\nbalanced controller that reduces the number of steps for longer-horizon goals.", "AI": {"tldr": "A model-based framework for non-prehensile tabletop pushing that uses a single learned GRU-based dynamics model with MPPI controller to handle multiple tasks (positioning, trajectory following, obstacle avoidance) without retraining.", "motivation": "Prior data-driven pushing methods target narrow capabilities, limiting broader applicability. Need for a unified approach that can handle multiple tasks with a single model.", "method": "Recurrent GRU-based architecture with non-linear layers for dynamics modeling, integrated with sampling-based MPPI controller. Uses tailored state-action representation and domain randomization for sim-to-real transfer.", "result": "High success rates in precise positioning under strict thresholds, strong trajectory tracking and obstacle avoidance performance. Multiple tasks solved by changing controller objective without retraining.", "conclusion": "The framework demonstrates robust multi-task capability with a single learned model, enabling adaptive pushing behaviors across various objectives while maintaining stability and generalization."}}
{"id": "2510.03727", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03727", "abs": "https://arxiv.org/abs/2510.03727", "authors": ["Xuehai He"], "title": "Bridging the Gap Between Multimodal Foundation Models and World Models", "comment": "PhD thesis", "summary": "Humans understand the world through the integration of multiple sensory\nmodalities, enabling them to perceive, reason about, and imagine dynamic\nphysical processes. Inspired by this capability, multimodal foundation models\n(MFMs) have emerged as powerful tools for multimodal understanding and\ngeneration. However, today's MFMs fall short of serving as effective world\nmodels. They lack the essential ability such as perform counterfactual\nreasoning, simulate dynamics, understand the spatiotemporal information,\ncontrol generated visual outcomes, and perform multifaceted reasoning. We\ninvestigates what it takes to bridge the gap between multimodal foundation\nmodels and world models. We begin by improving the reasoning capabilities of\nMFMs through discriminative tasks and equipping MFMs with structured reasoning\nskills, such as causal inference, counterfactual thinking, and spatiotemporal\nreasoning, enabling them to go beyond surface correlations and understand\ndeeper relationships within visual and textual data. Next, we explore\ngenerative capabilities of multimodal foundation models across both image and\nvideo modalities, introducing new frameworks for structured and controllable\ngeneration. Our approaches incorporate scene graphs, multimodal conditioning,\nand multimodal alignment strategies to guide the generation process, ensuring\nconsistency with high-level semantics and fine-grained user intent. We further\nextend these techniques to controllable 4D generation, enabling interactive,\neditable, and morphable object synthesis over time and space.", "AI": {"tldr": "This paper investigates how to bridge multimodal foundation models (MFMs) with world models by enhancing their reasoning and generative capabilities for better understanding and simulating dynamic physical processes.", "motivation": "Current multimodal foundation models lack essential world modeling abilities like counterfactual reasoning, dynamics simulation, spatiotemporal understanding, and controllable generation that humans naturally possess through multimodal sensory integration.", "method": "The approach involves: 1) Enhancing MFMs' reasoning through discriminative tasks and structured reasoning skills (causal inference, counterfactual thinking, spatiotemporal reasoning), 2) Developing generative capabilities using scene graphs, multimodal conditioning, and alignment strategies for structured and controllable generation across image and video modalities, 3) Extending to controllable 4D generation for interactive, editable object synthesis.", "result": "The research aims to equip MFMs with deeper understanding beyond surface correlations and enable consistent generation aligned with high-level semantics and fine-grained user intent across space and time.", "conclusion": "By systematically enhancing both reasoning and generative capabilities, multimodal foundation models can be transformed into more effective world models that better simulate and understand dynamic physical processes."}}
{"id": "2510.03376", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.03376", "abs": "https://arxiv.org/abs/2510.03376", "authors": ["Sanjukta Ghosh"], "title": "Visual Language Model as a Judge for Object Detection in Industrial Diagrams", "comment": "Pre-review version submitted to IEEE ICASSP 2026", "summary": "Industrial diagrams such as piping and instrumentation diagrams (P&IDs) are\nessential for the design, operation, and maintenance of industrial plants.\nConverting these diagrams into digital form is an important step toward\nbuilding digital twins and enabling intelligent industrial automation. A\ncentral challenge in this digitalization process is accurate object detection.\nAlthough recent advances have significantly improved object detection\nalgorithms, there remains a lack of methods to automatically evaluate the\nquality of their outputs. This paper addresses this gap by introducing a\nframework that employs Visual Language Models (VLMs) to assess object detection\nresults and guide their refinement. The approach exploits the multimodal\ncapabilities of VLMs to identify missing or inconsistent detections, thereby\nenabling automated quality assessment and improving overall detection\nperformance on complex industrial diagrams.", "AI": {"tldr": "A framework using Visual Language Models (VLMs) to automatically evaluate and refine object detection results in industrial diagrams like P&IDs, addressing the lack of quality assessment methods in digitalization processes.", "motivation": "Industrial diagrams are crucial for plant operations and digital twins, but there's a lack of automated methods to evaluate object detection quality during digitalization, which is essential for intelligent industrial automation.", "method": "Uses Visual Language Models (VLMs) to assess object detection results by identifying missing or inconsistent detections through multimodal analysis, enabling automated quality evaluation and refinement guidance.", "result": "The framework enables automated quality assessment of object detection outputs and improves overall detection performance on complex industrial diagrams by leveraging VLM capabilities.", "conclusion": "VLMs provide an effective solution for automated quality evaluation of object detection in industrial diagram digitalization, enabling better detection refinement and supporting the development of digital twins and intelligent automation."}}
{"id": "2510.03776", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03776", "abs": "https://arxiv.org/abs/2510.03776", "authors": ["Tiago Rodrigues de Almeida", "Yufei Zhu", "Andrey Rudenko", "Tomasz P. Kucner", "Johannes A. Stork", "Martin Magnusson", "Achim J. Lilienthal"], "title": "Trajectory prediction for heterogeneous agents: A performance analysis on small and imbalanced datasets", "comment": "This paper has been accepted to the IEEE Robotics and Automation\n  Letters journal and presented at the 40th Anniversary of the IEEE\n  International Conference on Robotics and Automation, which was held in\n  Rotterdam, Netherlands on 23-26 September, 2024", "summary": "Robots and other intelligent systems navigating in complex dynamic\nenvironments should predict future actions and intentions of surrounding agents\nto reach their goals efficiently and avoid collisions. The dynamics of those\nagents strongly depends on their tasks, roles, or observable labels.\nClass-conditioned motion prediction is thus an appealing way to reduce forecast\nuncertainty and get more accurate predictions for heterogeneous agents.\nHowever, this is hardly explored in the prior art, especially for mobile robots\nand in limited data applications. In this paper, we analyse different\nclass-conditioned trajectory prediction methods on two datasets. We propose a\nset of conditional pattern-based and efficient deep learning-based baselines,\nand evaluate their performance on robotics and outdoors datasets (TH\\\"OR-MAGNI\nand Stanford Drone Dataset). Our experiments show that all methods improve\naccuracy in most of the settings when considering class labels. More\nimportantly, we observe that there are significant differences when learning\nfrom imbalanced datasets, or in new environments where sufficient data is not\navailable. In particular, we find that deep learning methods perform better on\nbalanced datasets, but in applications with limited data, e.g., cold start of a\nrobot in a new environment, or imbalanced classes, pattern-based methods may be\npreferable.", "AI": {"tldr": "Class-conditioned motion prediction improves trajectory forecasting accuracy for heterogeneous agents by using observable labels/tasks, with deep learning methods excelling on balanced datasets while pattern-based methods perform better in limited data scenarios.", "motivation": "Robots need to predict future actions of surrounding agents in dynamic environments, and agent dynamics depend on their tasks/roles/labels. Class-conditioned prediction can reduce uncertainty and improve accuracy for heterogeneous agents, which is under-explored especially for mobile robots and limited data applications.", "method": "Proposed conditional pattern-based and efficient deep learning-based baselines for class-conditioned trajectory prediction. Evaluated on robotics (TH\u00d6R-MAGNI) and outdoors (Stanford Drone Dataset) datasets.", "result": "All methods improve accuracy when considering class labels. Deep learning methods perform better on balanced datasets, while pattern-based methods are preferable in limited data scenarios (cold start in new environments, imbalanced classes).", "conclusion": "Class-conditioned motion prediction is beneficial, with method selection depending on data availability and balance - deep learning for balanced datasets, pattern-based for limited/imbalanced data scenarios."}}
{"id": "2510.03771", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03771", "abs": "https://arxiv.org/abs/2510.03771", "authors": ["Divij Handa", "David Blincoe", "Orson Adams", "Yinlin Fu"], "title": "OptAgent: Optimizing Query Rewriting for E-commerce via Multi-Agent Simulation", "comment": null, "summary": "Deploying capable and user-aligned LLM-based systems necessitates reliable\nevaluation. While LLMs excel in verifiable tasks like coding and mathematics,\nwhere gold-standard solutions are available, adoption remains challenging for\nsubjective tasks that lack a single correct answer. E-commerce Query Rewriting\n(QR) is one such problem where determining whether a rewritten query properly\ncaptures the user intent is extremely difficult to figure out algorithmically.\nIn this work, we introduce OptAgent, a novel framework that combines\nmulti-agent simulations with genetic algorithms to verify and optimize queries\nfor QR. Instead of relying on a static reward model or a single LLM judge, our\napproach uses multiple LLM-based agents, each acting as a simulated shopping\ncustomer, as a dynamic reward signal. The average of these agent-derived scores\nserves as an effective fitness function for an evolutionary algorithm that\niteratively refines the user's initial query. We evaluate OptAgent on a dataset\nof 1000 real-world e-commerce queries in five different categories, and we\nobserve an average improvement of 21.98% over the original user query and 3.36%\nover a Best-of-N LLM rewriting baseline.", "AI": {"tldr": "OptAgent is a framework that uses multi-agent simulations with genetic algorithms to optimize e-commerce query rewriting, achieving 21.98% improvement over original queries and 3.36% over baseline LLM rewriting.", "motivation": "LLM evaluation is challenging for subjective tasks like e-commerce query rewriting where there's no single correct answer, making algorithmic verification difficult.", "method": "Combines multi-agent simulations (LLM agents acting as shopping customers) with genetic algorithms, using agent-derived scores as fitness function to iteratively refine queries.", "result": "Tested on 1000 real-world e-commerce queries across 5 categories, achieving 21.98% improvement over original queries and 3.36% over Best-of-N LLM baseline.", "conclusion": "OptAgent provides an effective framework for optimizing subjective tasks like query rewriting by leveraging multi-agent simulations as dynamic reward signals."}}
{"id": "2510.03441", "categories": ["cs.CV", "cs.AI", "cs.LG", "68T45, 68T10, 68T40"], "pdf": "https://arxiv.org/pdf/2510.03441", "abs": "https://arxiv.org/abs/2510.03441", "authors": ["Chashi Mahiul Islam", "Oteo Mamo", "Samuel Jacob Chacko", "Xiuwen Liu", "Weikuan Yu"], "title": "Spatial-ViLT: Enhancing Visual Spatial Reasoning through Multi-Task Learning", "comment": "12 pages, 5 figures", "summary": "Vision-language models (VLMs) have advanced multimodal reasoning but still\nface challenges in spatial reasoning for 3D scenes and complex object\nconfigurations. To address this, we introduce SpatialViLT, an enhanced VLM that\nintegrates spatial features like depth maps, 3D coordinates, and edge maps\nthrough a multi-task learning framework. This approach enriches multimodal\nembeddings with spatial understanding. We propose two variants: SpatialViLT and\nMaskedSpatialViLT, focusing on full and masked object regions, respectively.\nAdditionally, SpatialEnsemble combines both approaches, achieving\nstate-of-the-art accuracy. Our models excel in spatial reasoning categories\nsuch as directional, topological, and proximity relations, as demonstrated on\nthe challenging Visual Spatial Reasoning (VSR) dataset. This work represents a\nsignificant step in enhancing the spatial intelligence of AI systems, crucial\nfor advanced multimodal understanding and real-world applications.", "AI": {"tldr": "SpatialViLT enhances vision-language models by integrating spatial features like depth maps and 3D coordinates through multi-task learning, achieving state-of-the-art spatial reasoning performance.", "motivation": "Current vision-language models face challenges in spatial reasoning for 3D scenes and complex object configurations, limiting their ability to understand spatial relationships in multimodal data.", "method": "Introduces SpatialViLT with spatial feature integration (depth maps, 3D coordinates, edge maps) via multi-task learning. Proposes two variants: SpatialViLT and MaskedSpatialViLT, plus SpatialEnsemble combining both approaches.", "result": "Achieves state-of-the-art accuracy on Visual Spatial Reasoning (VSR) dataset, excelling in directional, topological, and proximity relations categories.", "conclusion": "Represents a significant advancement in enhancing spatial intelligence for AI systems, crucial for advanced multimodal understanding and real-world applications."}}
{"id": "2510.03875", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03875", "abs": "https://arxiv.org/abs/2510.03875", "authors": ["Niranjan Kumar Ilampooranan", "Constantinos Chamzas"], "title": "COVER:COverage-VErified Roadmaps for Fixed-time Motion Planning in Continuous Semi-Static Environments", "comment": null, "summary": "Having the ability to answer motion-planning queries within a fixed time\nbudget is critical for the widespread deployment of robotic systems.\nSemi-static environments, where most obstacles remain static but a limited set\ncan vary across queries, exhibit structured variability that can be\nsystematically exploited to provide stronger guarantees than in general\nmotion-planning problems. However, prior approaches in this setting either lack\nformal guarantees or rely on restrictive discretizations of obstacle\nconfigurations, limiting their applicability in realistic domains. This paper\nintroduces COVER, a novel framework that incrementally constructs a\ncoverage-verified roadmap in semi-static environments. By partitioning the\nobstacle configuration space and solving for feasible paths within each\npartition, COVER systematically verifies feasibility of the roadmap in each\npartition and guarantees fixed-time motion planning queries within the verified\nregions. We validate COVER with a 7-DOF simulated Panda robot performing table\nand shelf tasks, demonstrating that COVER achieves broader coverage with higher\nquery success rates than prior works.", "AI": {"tldr": "COVER is a framework that constructs coverage-verified roadmaps for semi-static environments, enabling fixed-time motion planning queries by systematically partitioning obstacle configurations and verifying roadmap feasibility.", "motivation": "To enable robotic systems to answer motion-planning queries within fixed time budgets in semi-static environments, where most obstacles remain static but some vary, by exploiting structured variability for stronger guarantees than general motion planning.", "method": "COVER incrementally constructs a coverage-verified roadmap by partitioning the obstacle configuration space, solving for feasible paths within each partition, and systematically verifying roadmap feasibility in each partition.", "result": "COVER achieves broader coverage with higher query success rates than prior works when tested with a 7-DOF simulated Panda robot performing table and shelf tasks.", "conclusion": "COVER provides a systematic approach for fixed-time motion planning in semi-static environments with formal guarantees, overcoming limitations of prior approaches that lacked guarantees or relied on restrictive discretizations."}}
{"id": "2510.03777", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03777", "abs": "https://arxiv.org/abs/2510.03777", "authors": ["Divij Handa", "Mihir Parmar", "Aswin RRV", "Md Nayem Uddin", "Hamid Palangi", "Chitta Baral"], "title": "GuidedSampling: Steering LLMs Towards Diverse Candidate Solutions at Inference-Time", "comment": null, "summary": "Repeated Sampling (RS) is a simple inference-time algorithm that has been\nshown to improve model performance on complex tasks. Although it is an\neffective way of scaling inference time, it often struggles to generate diverse\nsolution candidates, frequently relying on the same underlying approach to\nsolve the problem and thus producing redundant samples. To address this\nlimitation, we propose a new inference algorithm, GuidedSampling, which\ndecouples the exploration and generation phases during inference, increasing\ndiversity of generated candidate solutions. The exploration phase identifies\nmultiple concepts that can be utilized to solve the problem, while the\ngeneration phase applies a specific concept to provide final solution\ncandidates. We first define the theoretical bounds of GuidedSampling and then\nempirically demonstrate that it improves the performance of base model at\npass@50 by on an average ~21.6% across various benchmarks compared to RS.\nFurthermore, models trained on trajectories of GuidedSampling exhibit\nsubstantial performance improvements at pass@5 by on an average ~9.7%, compared\nto models trained on traditional RS. Additionally, models trained with\nGuidedSampling increases the average number of concepts per instance (1.67 ->\n3.03), yielding a diverse set of candidates than traditional RS.", "AI": {"tldr": "GuidedSampling is a new inference algorithm that improves diversity in solution generation by decoupling exploration and generation phases, outperforming Repeated Sampling by ~21.6% at pass@50 and training models that show ~9.7% improvement at pass@5.", "motivation": "Repeated Sampling (RS) struggles with generating diverse solution candidates and often produces redundant samples by relying on the same underlying approach, limiting its effectiveness.", "method": "GuidedSampling decouples exploration and generation phases: exploration identifies multiple concepts for solving problems, while generation applies specific concepts to produce final solution candidates.", "result": "GuidedSampling improves base model performance by ~21.6% at pass@50 across benchmarks compared to RS. Models trained with GuidedSampling show ~9.7% improvement at pass@5 and increase average concepts per instance from 1.67 to 3.03.", "conclusion": "GuidedSampling effectively addresses the diversity limitations of Repeated Sampling by separating concept exploration from solution generation, leading to significant performance improvements and more diverse candidate solutions."}}
{"id": "2510.03452", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03452", "abs": "https://arxiv.org/abs/2510.03452", "authors": ["Allison Davis", "Yezhi Shen", "Xiaoyu Ji", "Fengqing Zhu"], "title": "Denoising of Two-Phase Optically Sectioned Structured Illumination Reconstructions Using Encoder-Decoder Networks", "comment": "5 pages, 4 figures, submitted to ICASSP 2026", "summary": "Structured illumination (SI) enhances image resolution and contrast by\nprojecting patterned light onto a sample. In two-phase optical-sectioning SI\n(OS-SI), reduced acquisition time introduces residual artifacts that\nconventional denoising struggles to suppress. Deep learning offers an\nalternative to traditional methods; however, supervised training is limited by\nthe lack of clean, optically sectioned ground-truth data. We investigate\nencoder-decoder networks for artifact reduction in two-phase OS-SI, using\nsynthetic training pairs formed by applying real artifact fields to synthetic\nimages. An asymmetrical denoising autoencoder (DAE) and a U-Net are trained on\nthe synthetic data, then evaluated on real OS-SI images. Both networks improve\nimage clarity, with each excelling against different artifact types. These\nresults demonstrate that synthetic training enables supervised denoising of\nOS-SI images and highlight the potential of encoder-decoder networks to\nstreamline reconstruction workflows.", "AI": {"tldr": "Encoder-decoder networks trained on synthetic data effectively reduce artifacts in two-phase optical-sectioning structured illumination microscopy, improving image clarity without requiring clean ground-truth data.", "motivation": "Two-phase optical-sectioning SI suffers from residual artifacts due to reduced acquisition time, and conventional denoising methods struggle with these artifacts. Supervised deep learning approaches are limited by the lack of clean ground-truth data.", "method": "Used encoder-decoder networks (asymmetrical denoising autoencoder and U-Net) trained on synthetic data pairs created by applying real artifact fields to synthetic images, then evaluated on real OS-SI images.", "result": "Both networks improved image clarity, with each network excelling against different types of artifacts. The approach successfully enabled supervised denoising of OS-SI images.", "conclusion": "Synthetic training enables effective supervised denoising of OS-SI images, and encoder-decoder networks show potential for streamlining reconstruction workflows in structured illumination microscopy."}}
{"id": "2510.03885", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03885", "abs": "https://arxiv.org/abs/2510.03885", "authors": ["Sunghwan Kim", "Woojeh Chung", "Zhirui Dai", "Dwait Bhatt", "Arth Shukla", "Hao Su", "Yulun Tian", "Nikolay Atanasov"], "title": "Seeing the Bigger Picture: 3D Latent Mapping for Mobile Manipulation Policy Learning", "comment": "Project website can be found at\n  https://existentialrobotics.org/sbp_page/", "summary": "In this paper, we demonstrate that mobile manipulation policies utilizing a\n3D latent map achieve stronger spatial and temporal reasoning than policies\nrelying solely on images. We introduce Seeing the Bigger Picture (SBP), an\nend-to-end policy learning approach that operates directly on a 3D map of\nlatent features. In SBP, the map extends perception beyond the robot's current\nfield of view and aggregates observations over long horizons. Our mapping\napproach incrementally fuses multiview observations into a grid of\nscene-specific latent features. A pre-trained, scene-agnostic decoder\nreconstructs target embeddings from these features and enables online\noptimization of the map features during task execution. A policy, trainable\nwith behavior cloning or reinforcement learning, treats the latent map as a\nstate variable and uses global context from the map obtained via a 3D feature\naggregator. We evaluate SBP on scene-level mobile manipulation and sequential\ntabletop manipulation tasks. Our experiments demonstrate that SBP (i) reasons\nglobally over the scene, (ii) leverages the map as long-horizon memory, and\n(iii) outperforms image-based policies in both in-distribution and novel\nscenes, e.g., improving the success rate by 25% for the sequential manipulation\ntask.", "AI": {"tldr": "SBP is a mobile manipulation policy that uses 3D latent maps for better spatial reasoning and long-term memory, outperforming image-only policies by 25% in sequential manipulation tasks.", "motivation": "To overcome limitations of image-based policies that lack global scene understanding and long-horizon memory, enabling better spatial and temporal reasoning for mobile manipulation tasks.", "method": "End-to-end policy learning using 3D latent maps that incrementally fuse multiview observations into scene-specific latent features, with a pre-trained decoder for online optimization and a policy trainable with behavior cloning or reinforcement learning.", "result": "SBP demonstrates global scene reasoning, uses maps as long-horizon memory, and outperforms image-based policies by 25% success rate in sequential manipulation tasks across both in-distribution and novel scenes.", "conclusion": "3D latent maps provide superior spatial and temporal reasoning capabilities for mobile manipulation compared to image-only approaches, enabling better performance in complex manipulation tasks."}}
{"id": "2510.03845", "categories": ["cs.AI", "cs.GT", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03845", "abs": "https://arxiv.org/abs/2510.03845", "authors": ["Gon Buzaglo", "Noah Golowich", "Elad Hazan"], "title": "The Hidden Game Problem", "comment": null, "summary": "This paper investigates a class of games with large strategy spaces,\nmotivated by challenges in AI alignment and language games. We introduce the\nhidden game problem, where for each player, an unknown subset of strategies\nconsistently yields higher rewards compared to the rest. The central question\nis whether efficient regret minimization algorithms can be designed to discover\nand exploit such hidden structures, leading to equilibrium in these subgames\nwhile maintaining rationality in general. We answer this question affirmatively\nby developing a composition of regret minimization techniques that achieve\noptimal external and swap regret bounds. Our approach ensures rapid convergence\nto correlated equilibria in hidden subgames, leveraging the hidden game\nstructure for improved computational efficiency.", "AI": {"tldr": "The paper introduces the hidden game problem where players have unknown subsets of strategies that yield higher rewards, and develops regret minimization algorithms to discover and exploit these hidden structures while achieving optimal regret bounds and convergence to correlated equilibria.", "motivation": "The research is motivated by challenges in AI alignment and language games, where players need to discover hidden strategy subsets that consistently provide better rewards without prior knowledge of these structures.", "method": "The authors develop a composition of regret minimization techniques that can efficiently discover hidden strategy subsets and exploit them while maintaining rationality. The approach leverages the hidden game structure to improve computational efficiency.", "result": "The method achieves optimal external and swap regret bounds, ensuring rapid convergence to correlated equilibria in hidden subgames while maintaining efficient performance in the overall game.", "conclusion": "The paper affirmatively answers the central question by showing that efficient regret minimization algorithms can be designed to discover hidden game structures and achieve equilibrium in subgames while maintaining general rationality."}}
{"id": "2510.03455", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03455", "abs": "https://arxiv.org/abs/2510.03455", "authors": ["Sejuti Majumder", "Saarthak Kapse", "Moinak Bhattacharya", "Xuan Xu", "Alisa Yurovsky", "Prateek Prasanna"], "title": "PEaRL: Pathway-Enhanced Representation Learning for Gene and Pathway Expression Prediction from Histology", "comment": null, "summary": "Integrating histopathology with spatial transcriptomics (ST) provides a\npowerful opportunity to link tissue morphology with molecular function. Yet\nmost existing multimodal approaches rely on a small set of highly variable\ngenes, which limits predictive scope and overlooks the coordinated biological\nprograms that shape tissue phenotypes. We present PEaRL (Pathway Enhanced\nRepresentation Learning), a multimodal framework that represents\ntranscriptomics through pathway activation scores computed with ssGSEA. By\nencoding biologically coherent pathway signals with a transformer and aligning\nthem with histology features via contrastive learning, PEaRL reduces\ndimensionality, improves interpretability, and strengthens cross-modal\ncorrespondence. Across three cancer ST datasets (breast, skin, and lymph node),\nPEaRL consistently outperforms SOTA methods, yielding higher accuracy for both\ngene- and pathway-level expression prediction (up to 58.9 percent and 20.4\npercent increase in Pearson correlation coefficient compared to SOTA). These\nresults demonstrate that grounding transcriptomic representation in pathways\nproduces more biologically faithful and interpretable multimodal models,\nadvancing computational pathology beyond gene-level embeddings.", "AI": {"tldr": "PEaRL is a multimodal framework that integrates histopathology with spatial transcriptomics using pathway activation scores instead of individual genes, achieving superior performance in gene- and pathway-level expression prediction across multiple cancer datasets.", "motivation": "Existing multimodal approaches rely on a small set of highly variable genes, which limits predictive scope and overlooks coordinated biological programs that shape tissue phenotypes.", "method": "PEaRL represents transcriptomics through pathway activation scores computed with ssGSEA, encodes biologically coherent pathway signals with a transformer, and aligns them with histology features via contrastive learning.", "result": "Across three cancer ST datasets (breast, skin, and lymph node), PEaRL consistently outperforms SOTA methods, yielding up to 58.9% and 20.4% increase in Pearson correlation coefficient for gene- and pathway-level expression prediction respectively.", "conclusion": "Grounded transcriptomic representation in pathways produces more biologically faithful and interpretable multimodal models, advancing computational pathology beyond gene-level embeddings."}}
{"id": "2510.03895", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03895", "abs": "https://arxiv.org/abs/2510.03895", "authors": ["Zheng Huang", "Mingyu Liu", "Xiaoyi Lin", "Muzhi Zhu", "Canyu Zhao", "Zongze Du", "Xiaoman Li", "Yiduo Jia", "Hao Zhong", "Hao Chen", "Chunhua Shen"], "title": "NoTVLA: Narrowing of Dense Action Trajectories for Generalizable Robot Manipulation", "comment": null, "summary": "Vision-Language-Action (VLA) models represent a pivotal advance in embodied\nintelligence, yet they confront critical barriers to real-world deployment,\nmost notably catastrophic forgetting. This issue stems from their overreliance\non continuous action sequences or action chunks, which inadvertently create\nisolated data silos that disrupt knowledge retention across tasks. To tackle\nthese challenges, we propose the Narrowing of Trajectory VLA (NoTVLA)\nframework: a novel approach that narrows its focus to sparse trajectories,\nthereby avoiding the catastrophic forgetting associated with dense trajectory\nfine-tuning. A key innovation of NoTVLA lies in its trajectory planning\nstrategy: instead of centering on the target object's trajectory, it leverages\ntemporal compression and spatial reasoning pruning specifically for the robot\nend effector's trajectory. Furthermore, training is conducted using these\nsparse trajectories rather than dense action trajectories, an optimization that\ndelivers remarkable practical advantages with better performance in zero-shot.\nIn multi-task evaluation scenarios, NoTVLA achieves superior performance and\ngeneralization compared to pi0 while operating under two critical constraints:\nit uses over an order of magnitude less computing power than pi0 and requires\nno wrist-mounted camera. This design ensures that NoTVLA's operational accuracy\nclosely approximates that of single-task expert models. Crucially, it also\npreserves the model's inherent language capabilities, enabling zero-shot\ngeneralization in specific scenarios, supporting unified model deployment\nacross multiple robot platforms, and fostering a degree of generalization even\nwhen perceiving tasks from novel perspectives.", "AI": {"tldr": "NoTVLA is a Vision-Language-Action framework that uses sparse end-effector trajectories instead of dense action sequences to prevent catastrophic forgetting, achieving better multi-task performance with significantly less computing power and no wrist camera.", "motivation": "To address catastrophic forgetting in VLA models caused by overreliance on continuous action sequences that create data silos and disrupt knowledge retention across tasks.", "method": "Proposes narrowing focus to sparse trajectories using temporal compression and spatial reasoning pruning specifically for robot end effector's trajectory, training with sparse trajectories rather than dense action trajectories.", "result": "Achieves superior performance and generalization compared to pi0 while using over 10x less computing power and no wrist-mounted camera, closely approximating single-task expert model accuracy while preserving language capabilities.", "conclusion": "NoTVLA enables unified model deployment across multiple robot platforms with zero-shot generalization, effectively preventing catastrophic forgetting while maintaining operational efficiency and language understanding capabilities."}}
{"id": "2510.03847", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03847", "abs": "https://arxiv.org/abs/2510.03847", "authors": ["Raghav Sharma", "Manan Mehta"], "title": "Small Language Models for Agentic Systems: A Survey of Architectures, Capabilities, and Deployment Trade offs", "comment": "9 Pages", "summary": "Small language models (SLMs; 1-12B params, sometimes up to 20B) are\nsufficient and often superior for agentic workloads where the objective is\nschema- and API-constrained accuracy rather than open-ended generation. We\nsynthesize recent evidence across open and proprietary SLMs (Phi-4-Mini,\nQwen-2.5-7B, Gemma-2-9B, Llama-3.2-1B/3B, Ministral-3B/8B, Apple on-device 3B,\nDeepSeek-R1-Distill) and connect it to modern evaluations (BFCL v3/v4,\nStableToolBench) and serving stacks (vLLM, SGLang, TensorRT-LLM) paired with\nguided decoding libraries (XGrammar, Outlines). We formalize SLM-default,\nLLM-fallback systems with uncertainty-aware routing and verifier cascades, and\npropose engineering metrics that reflect real production goals: cost per\nsuccessful task (CPS), schema validity rate, executable call rate, p50/p95\nlatency, and energy per request. Guided decoding, strict JSON Schema outputs,\nand validator-first tool execution close much of the capability gap with larger\nmodels and often let SLMs match or surpass LLMs on tool use, function calling,\nand RAG at 10x-100x lower token cost with materially better latency and energy.\nWe provide design patterns for agent stacks that prioritize SLMs: schema-first\nprompting, type-safe function registries, confidence scoring with verifier\nrollups, and lightweight adaptation via LoRA/QLoRA. We also delineate limits\nwhere fallback remains valuable (open-domain reasoning and some long-horizon\nplanning). The result is a practical blueprint for building fast, inexpensive,\nand reliable agents that default to SLMs while preserving headroom with\ntargeted LLM assistance.\n  Keywords: small language models, agents, function calling, structured\noutputs, JSON Schema, guided decoding, LoRA/QLoRA, routing, energy efficiency,\nedge inference", "AI": {"tldr": "Small language models (SLMs) are sufficient and often superior to larger models for agentic workloads focused on schema- and API-constrained accuracy rather than open-ended generation, offering 10x-100x lower token cost with better latency and energy efficiency.", "motivation": "To demonstrate that SLMs (1-12B parameters) can effectively handle agentic workloads with structured outputs and function calling, providing cost-effective alternatives to larger models while maintaining performance on constrained tasks.", "method": "Proposes SLM-default, LLM-fallback systems with uncertainty-aware routing and verifier cascades, using guided decoding, strict JSON Schema outputs, validator-first tool execution, and design patterns like schema-first prompting and lightweight adaptation via LoRA/QLoRA.", "result": "SLMs can match or surpass larger models on tool use, function calling, and RAG tasks at significantly lower cost (10x-100x lower token cost) with better latency and energy efficiency, while guided decoding and structured outputs close capability gaps.", "conclusion": "SLMs provide a practical blueprint for building fast, inexpensive, and reliable agents that default to small models while preserving headroom with targeted LLM assistance, particularly effective for schema-constrained tasks though fallback remains valuable for open-domain reasoning."}}
{"id": "2510.03483", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03483", "abs": "https://arxiv.org/abs/2510.03483", "authors": ["Numan Saeed", "Tausifa Jan Saleem", "Fadillah Maani", "Muhammad Ridzuan", "Hu Wang", "Mohammad Yaqub"], "title": "DuPLUS: Dual-Prompt Vision-Language Framework for Universal Medical Image Segmentation and Prognosis", "comment": null, "summary": "Deep learning for medical imaging is hampered by task-specific models that\nlack generalizability and prognostic capabilities, while existing 'universal'\napproaches suffer from simplistic conditioning and poor medical semantic\nunderstanding. To address these limitations, we introduce DuPLUS, a deep\nlearning framework for efficient multi-modal medical image analysis. DuPLUS\nintroduces a novel vision-language framework that leverages hierarchical\nsemantic prompts for fine-grained control over the analysis task, a capability\nabsent in prior universal models. To enable extensibility to other medical\ntasks, it includes a hierarchical, text-controlled architecture driven by a\nunique dual-prompt mechanism. For segmentation, DuPLUS is able to generalize\nacross three imaging modalities, ten different anatomically various medical\ndatasets, encompassing more than 30 organs and tumor types. It outperforms the\nstate-of-the-art task specific and universal models on 8 out of 10 datasets. We\ndemonstrate extensibility of its text-controlled architecture by seamless\nintegration of electronic health record (EHR) data for prognosis prediction,\nand on a head and neck cancer dataset, DuPLUS achieved a Concordance Index (CI)\nof 0.69. Parameter-efficient fine-tuning enables rapid adaptation to new tasks\nand modalities from varying centers, establishing DuPLUS as a versatile and\nclinically relevant solution for medical image analysis. The code for this work\nis made available at: https://anonymous.4open.science/r/DuPLUS-6C52", "AI": {"tldr": "DuPLUS is a vision-language framework for multi-modal medical image analysis that uses hierarchical semantic prompts and dual-prompt mechanisms to achieve superior generalization across modalities and tasks, outperforming state-of-the-art models on most datasets while enabling seamless EHR integration.", "motivation": "Medical imaging faces limitations with task-specific models lacking generalizability and existing universal approaches having simplistic conditioning and poor medical semantic understanding.", "method": "Introduces a novel vision-language framework with hierarchical semantic prompts for fine-grained control, dual-prompt mechanism for text-controlled architecture, and parameter-efficient fine-tuning for rapid adaptation.", "result": "Outperforms state-of-the-art models on 8 out of 10 datasets across 3 imaging modalities and 30+ organs/tumor types, achieves CI of 0.69 for head and neck cancer prognosis with EHR integration.", "conclusion": "DuPLUS establishes itself as a versatile and clinically relevant solution that enables efficient multi-modal medical image analysis with strong generalization capabilities and seamless extensibility to other medical tasks."}}
{"id": "2510.03910", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03910", "abs": "https://arxiv.org/abs/2510.03910", "authors": ["Akhil Padmanabha", "Jessie Yuan", "Tanisha Mehta", "Rajat Kumar Jenamani", "Eric Hu", "Victoria de Le\u00f3n", "Anthony Wertz", "Janavi Gupta", "Ben Dodson", "Yunting Yan", "Carmel Majidi", "Tapomayukh Bhattacharjee", "Zackory Erickson"], "title": "WAFFLE: A Wearable Approach to Bite Timing Estimation in Robot-Assisted Feeding", "comment": null, "summary": "Millions of people around the world need assistance with feeding. Robotic\nfeeding systems offer the potential to enhance autonomy and quality of life for\nindividuals with impairments and reduce caregiver workload. However, their\nwidespread adoption has been limited by technical challenges such as estimating\nbite timing, the appropriate moment for the robot to transfer food to a user's\nmouth. In this work, we introduce WAFFLE: Wearable Approach For Feeding with\nLEarned bite timing, a system that accurately predicts bite timing by\nleveraging wearable sensor data to be highly reactive to natural user cues such\nas head movements, chewing, and talking. We train a supervised regression model\non bite timing data from 14 participants and incorporate a user-adjustable\nassertiveness threshold to convert predictions into proceed or stop commands.\nIn a study with 15 participants without motor impairments with the Obi feeding\nrobot, WAFFLE performs statistically on par with or better than baseline\nmethods across measures of feeling of control, robot understanding, and\nworkload, and is preferred by the majority of participants for both individual\nand social dining. We further demonstrate WAFFLE's generalizability in a study\nwith 2 participants with motor impairments in their home environments using a\nKinova 7DOF robot. Our findings support WAFFLE's effectiveness in enabling\nnatural, reactive bite timing that generalizes across users, robot hardware,\nrobot positioning, feeding trajectories, foods, and both individual and social\ndining contexts.", "AI": {"tldr": "WAFFLE is a wearable system that uses sensor data to predict bite timing for robotic feeding, enabling more natural and reactive assistance that adapts to user cues like head movements and chewing.", "motivation": "To enhance autonomy and quality of life for people needing feeding assistance by overcoming technical challenges in bite timing estimation that limit widespread adoption of robotic feeding systems.", "method": "Uses wearable sensors to capture user cues (head movements, chewing, talking), trains a supervised regression model on bite timing data from 14 participants, and incorporates a user-adjustable assertiveness threshold to generate proceed/stop commands.", "result": "Outperformed baseline methods in studies with 15 unimpaired participants, showing better user control, robot understanding, and reduced workload. Successfully generalized to 2 motor-impaired participants in home environments with different robot hardware.", "conclusion": "WAFFLE enables natural, reactive bite timing that generalizes across users, robot hardware, positioning, feeding trajectories, foods, and both individual and social dining contexts."}}
{"id": "2510.03851", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03851", "abs": "https://arxiv.org/abs/2510.03851", "authors": ["Ruiying Ma", "Chieh-Jan Mike Liang", "Yanjie Gao", "Francis Y. Yan"], "title": "Algorithm Generation via Creative Ideation", "comment": null, "summary": "Designing system algorithms remains challenging, where the discontinuous\nnature of the solution space often forces system engineers to rely on generic\nheuristics at the expense of performance. We study whether LLMs can practically\ndrive algorithm generation, and find that they are biased towards well-known\ngeneric designs, rather than making the creative leaps needed to navigate the\ndiscontinuous solution space. To address this limitation, we introduce\nMetaMuse, a framework for creative ideation built on three self-reflection\nprinciples: (1) quantifying solution diversity and usefulness in measurable\nperformance space, rather than abstract idea space, (2) steering ideation\nthrough external stimuli, rather than internal randomness, and (3) constructing\nexecutable solutions using waypoint reasoning, rather than free-form\nchain-of-thought. Extensive evaluation shows that MetaMuse can generate\nhigh-performing solutions for two critical problems at a global cloud provider:\ncache replacement (reducing cache misses by up to 35.76%) and online bin\npacking (reducing bin usage by up to 30.93%).", "AI": {"tldr": "MetaMuse is a framework that uses LLMs with three self-reflection principles to generate creative algorithms, achieving significant performance improvements in cache replacement (35.76% fewer misses) and online bin packing (30.93% fewer bins).", "motivation": "LLMs are biased towards generic designs and struggle with creative leaps needed for discontinuous solution spaces in algorithm design, limiting their practical use in generating novel system algorithms.", "method": "MetaMuse introduces three self-reflection principles: (1) quantify diversity in performance space, (2) steer ideation with external stimuli, (3) construct solutions using waypoint reasoning instead of chain-of-thought.", "result": "MetaMuse achieved substantial improvements: 35.76% reduction in cache misses for cache replacement and 30.93% reduction in bin usage for online bin packing.", "conclusion": "The framework successfully addresses LLM limitations in creative algorithm design through structured self-reflection, demonstrating practical value for real-world system optimization problems."}}
{"id": "2510.03501", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03501", "abs": "https://arxiv.org/abs/2510.03501", "authors": ["Lyes Saad Saoud", "Loic Lesobre", "Enrico Sorato", "Irfan Hussain"], "title": "Real-Time Threaded Houbara Detection and Segmentation for Wildlife Conservation using Mobile Platforms", "comment": null, "summary": "Real-time animal detection and segmentation in natural environments are vital\nfor wildlife conservation, enabling non-invasive monitoring through remote\ncamera streams. However, these tasks remain challenging due to limited\ncomputational resources and the cryptic appearance of many species. We propose\na mobile-optimized two-stage deep learning framework that integrates a\nThreading Detection Model (TDM) to parallelize YOLOv10-based detection and\nMobileSAM-based segmentation. Unlike prior YOLO+SAM pipelines, our approach\nimproves real-time performance by reducing latency through threading. YOLOv10\nhandles detection while MobileSAM performs lightweight segmentation, both\nexecuted concurrently for efficient resource use. On the cryptic Houbara\nBustard, a conservation-priority species, our model achieves mAP50 of 0.9627,\nmAP75 of 0.7731, mAP95 of 0.7178, and a MobileSAM mIoU of 0.7421. YOLOv10\noperates at 43.7 ms per frame, confirming real-time readiness. We introduce a\ncurated Houbara dataset of 40,000 annotated images to support model training\nand evaluation across diverse conditions. The code and dataset used in this\nstudy are publicly available on GitHub at\nhttps://github.com/LyesSaadSaoud/mobile-houbara-detseg. For interactive demos\nand additional resources, visit\nhttps://lyessaadsaoud.github.io/LyesSaadSaoud-Threaded-YOLO-SAM-Houbara.", "AI": {"tldr": "A mobile-optimized two-stage framework using YOLOv10 for detection and MobileSAM for segmentation, with threading to parallelize both stages for real-time animal detection in conservation settings.", "motivation": "Real-time animal detection and segmentation are crucial for wildlife conservation through remote monitoring, but face challenges due to limited computational resources and cryptic species appearances.", "method": "Two-stage deep learning framework integrating Threading Detection Model (TDM) to parallelize YOLOv10-based detection and MobileSAM-based segmentation, executed concurrently to reduce latency.", "result": "On Houbara Bustard: mAP50=0.9627, mAP75=0.7731, mAP95=0.7178, MobileSAM mIoU=0.7421. YOLOv10 operates at 43.7ms per frame. Introduced curated dataset of 40,000 annotated images.", "conclusion": "The proposed framework achieves real-time performance suitable for mobile deployment in wildlife conservation, with publicly available code and dataset to support further research."}}
{"id": "2510.03919", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03919", "abs": "https://arxiv.org/abs/2510.03919", "authors": ["Matthew Lisondra", "Junseo Kim", "Glenn Takashi Shimoda", "Kourosh Zareinia", "Sajad Saeedi"], "title": "TCB-VIO: Tightly-Coupled Focal-Plane Binary-Enhanced Visual Inertial Odometry", "comment": "Accepted at IEEE Robotics and Automation Letters", "summary": "Vision algorithms can be executed directly on the image sensor when\nimplemented on the next-generation sensors known as focal-plane\nsensor-processor arrays (FPSP)s, where every pixel has a processor. FPSPs\ngreatly improve latency, reducing the problems associated with the bottleneck\nof data transfer from a vision sensor to a processor. FPSPs accelerate\nvision-based algorithms such as visual-inertial odometry (VIO). However, VIO\nframeworks suffer from spatial drift due to the vision-based pose estimation,\nwhilst temporal drift arises from the inertial measurements. FPSPs circumvent\nthe spatial drift by operating at a high frame rate to match the high-frequency\noutput of the inertial measurements. In this paper, we present TCB-VIO, a\ntightly-coupled 6 degrees-of-freedom VIO by a Multi-State Constraint Kalman\nFilter (MSCKF), operating at a high frame-rate of 250 FPS and from IMU\nmeasurements obtained at 400 Hz. TCB-VIO outperforms state-of-the-art methods:\nROVIO, VINS-Mono, and ORB-SLAM3.", "AI": {"tldr": "TCB-VIO is a tightly-coupled visual-inertial odometry system using a Multi-State Constraint Kalman Filter that operates at 250 FPS vision and 400 Hz IMU, outperforming state-of-the-art methods.", "motivation": "To address spatial drift in vision-based pose estimation and temporal drift from inertial measurements by leveraging focal-plane sensor-processor arrays (FPSPs) that enable high frame rates to match IMU frequency.", "method": "Uses a tightly-coupled 6-DOF VIO with Multi-State Constraint Kalman Filter (MSCKF), operating at 250 FPS vision processing and 400 Hz IMU measurements on focal-plane sensor-processor arrays.", "result": "TCB-VIO outperforms state-of-the-art methods including ROVIO, VINS-Mono, and ORB-SLAM3.", "conclusion": "FPSPs enable high-frequency vision processing that can match IMU rates, effectively reducing spatial and temporal drift in VIO systems through tightly-coupled filtering approaches."}}
{"id": "2510.03859", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03859", "abs": "https://arxiv.org/abs/2510.03859", "authors": ["Raghav Sharma", "Manan Mehta"], "title": "Adaptive and Explainable AI Agents for Anomaly Detection in Critical IoT Infrastructure using LLM-Enhanced Contextual Reasoning", "comment": "22 pages", "summary": "Ensuring that critical IoT systems function safely and smoothly depends a lot\non finding anomalies quickly. As more complex systems, like smart healthcare,\nenergy grids and industrial automation, appear, it is easier to see the\nshortcomings of older methods of detection. Monitoring failures usually happen\nin dynamic, high dimensional situations, especially when data is incomplete,\nmessy or always evolving. Such limits point out the requirement for adaptive,\nintelligent systems that always improve and think. LLMs are now capable of\nsignificantly changing how context is understood and semantic inference is done\nacross all types of data. This proposal suggests using an LLM supported\ncontextual reasoning method along with XAI agents to improve how anomalies are\nfound in significant IoT environments. To discover hidden patterns and notice\ninconsistencies in data streams, it uses attention methods, avoids dealing with\ndetails from every time step and uses memory buffers with meaning. Because no\ncode AI stresses transparency and interpretability, people can check and accept\nthe AI's decisions, helping ensure AI follows company policies. The two\narchitectures are put together in a test that compares the results of the\ntraditional model with those of the suggested LLM enhanced model. Important\nmeasures to check are the accuracy of detection, how much inaccurate\ninformation is included in the results, how clearly the findings can be read\nand how fast the system responds under different test situations. The\nmetaheuristic is tested in simulations of real world smart grid and healthcare\ncontexts to check its adaptability and reliability. From the study, we see that\nthe new approach performs much better than most existing models in both\naccuracy and interpretation, so it could be a good fit for future anomaly\ndetection tasks in IoT", "AI": {"tldr": "Proposes LLM-enhanced contextual reasoning with XAI agents for IoT anomaly detection, showing superior accuracy and interpretability compared to traditional methods.", "motivation": "Traditional anomaly detection methods struggle with dynamic, high-dimensional IoT environments where data is incomplete, messy, or evolving. Critical IoT systems like smart healthcare and energy grids require adaptive, intelligent systems for reliable monitoring.", "method": "Uses LLM-supported contextual reasoning with XAI agents, attention methods, memory buffers with meaning, and avoids processing every time step detail. Combines no-code AI for transparency and interpretability.", "result": "The proposed approach significantly outperforms existing models in both detection accuracy and interpretability. Tested in smart grid and healthcare simulations, it demonstrates better adaptability and reliability.", "conclusion": "The LLM-enhanced contextual reasoning method with XAI agents shows strong potential as a future solution for anomaly detection in critical IoT environments, offering improved accuracy and transparency."}}
{"id": "2510.03511", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.03511", "abs": "https://arxiv.org/abs/2510.03511", "authors": ["Mohammad Mohaiminul Islam", "Rishabh Anand", "David R. Wessels", "Friso de Kruiff", "Thijs P. Kuipers", "Rex Ying", "Clara I. S\u00e1nchez", "Sharvaree Vadgama", "Georg B\u00f6kman", "Erik J. Bekkers"], "title": "Platonic Transformers: A Solid Choice For Equivariance", "comment": null, "summary": "While widespread, Transformers lack inductive biases for geometric symmetries\ncommon in science and computer vision. Existing equivariant methods often\nsacrifice the efficiency and flexibility that make Transformers so effective\nthrough complex, computationally intensive designs. We introduce the Platonic\nTransformer to resolve this trade-off. By defining attention relative to\nreference frames from the Platonic solid symmetry groups, our method induces a\nprincipled weight-sharing scheme. This enables combined equivariance to\ncontinuous translations and Platonic symmetries, while preserving the exact\narchitecture and computational cost of a standard Transformer. Furthermore, we\nshow that this attention is formally equivalent to a dynamic group convolution,\nwhich reveals that the model learns adaptive geometric filters and enables a\nhighly scalable, linear-time convolutional variant. Across diverse benchmarks\nin computer vision (CIFAR-10), 3D point clouds (ScanObjectNN), and molecular\nproperty prediction (QM9, OMol25), the Platonic Transformer achieves\ncompetitive performance by leveraging these geometric constraints at no\nadditional cost.", "AI": {"tldr": "The Platonic Transformer introduces a novel attention mechanism that achieves equivariance to continuous translations and Platonic symmetries while maintaining the exact architecture and computational cost of standard Transformers.", "motivation": "Transformers lack inductive biases for geometric symmetries common in science and computer vision, while existing equivariant methods sacrifice efficiency and flexibility through complex designs.", "method": "The method defines attention relative to reference frames from Platonic solid symmetry groups, inducing a principled weight-sharing scheme that enables combined equivariance to continuous translations and Platonic symmetries.", "result": "The Platonic Transformer achieves competitive performance across diverse benchmarks in computer vision (CIFAR-10), 3D point clouds (ScanObjectNN), and molecular property prediction (QM9, OMol25) while leveraging geometric constraints at no additional cost.", "conclusion": "The approach resolves the trade-off between geometric equivariance and Transformer efficiency, showing that attention is formally equivalent to dynamic group convolution, enabling adaptive geometric filters and scalable linear-time variants."}}
{"id": "2510.03948", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.03948", "abs": "https://arxiv.org/abs/2510.03948", "authors": ["Otobong Jerome", "Geesara Prathap Kulathunga", "Devitt Dmitry", "Eugene Murawjow", "Alexandr Klimchik"], "title": "A Real-Time Framework for Intermediate Map Construction and Kinematically Feasible Off-Road Planning Without OSM", "comment": null, "summary": "Off-road environments present unique challenges for autonomous navigation due\nto their complex and unstructured nature. Traditional global path-planning\nmethods, which typically aim to minimize path length and travel time, perform\npoorly on large-scale maps and fail to account for critical factors such as\nreal-time performance, kinematic feasibility, and memory efficiency. This paper\nintroduces a novel global path-planning method specifically designed for\noff-road environments, addressing these essential factors. The method begins by\nconstructing an intermediate map within the pixel coordinate system,\nincorporating geographical features like off-road trails, waterways, restricted\nand passable areas, and trees. The planning problem is then divided into three\nsub-problems: graph-based path planning, kinematic feasibility checking, and\npath smoothing. This approach effectively meets real-time performance\nrequirements while ensuring kinematic feasibility and efficient memory use. The\nmethod was tested in various off-road environments with large-scale maps up to\nseveral square kilometers in size, successfully identifying feasible paths in\nan average of 1.5 seconds and utilizing approximately 1.5GB of memory under\nextreme conditions. The proposed framework is versatile and applicable to a\nwide range of off-road autonomous navigation tasks, including search and rescue\nmissions and agricultural operations.", "AI": {"tldr": "A novel global path-planning method for off-road environments that addresses real-time performance, kinematic feasibility, and memory efficiency by constructing intermediate maps and dividing planning into three sub-problems.", "motivation": "Traditional global path-planning methods perform poorly on large-scale off-road maps and fail to account for critical factors like real-time performance, kinematic feasibility, and memory efficiency in complex unstructured environments.", "method": "Constructs an intermediate map in pixel coordinate system with geographical features, then divides planning into three sub-problems: graph-based path planning, kinematic feasibility checking, and path smoothing.", "result": "Successfully tested in various off-road environments with large-scale maps up to several square kilometers, achieving feasible path identification in average 1.5 seconds and using approximately 1.5GB memory under extreme conditions.", "conclusion": "The proposed framework is versatile and applicable to a wide range of off-road autonomous navigation tasks including search and rescue missions and agricultural operations."}}
{"id": "2510.03863", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.03863", "abs": "https://arxiv.org/abs/2510.03863", "authors": ["Arina Kharlamova", "Bowei He", "Chen Ma", "Xue Liu"], "title": "Spatial CAPTCHA: Generatively Benchmarking Spatial Reasoning for Human-Machine Differentiation", "comment": "Submitted to ICLR 2026", "summary": "Online services rely on CAPTCHAs as a first line of defense against automated\nabuse, yet recent advances in multi-modal large language models (MLLMs) have\neroded the effectiveness of conventional designs that focus on text recognition\nor 2D image understanding. To address this challenge, we present Spatial\nCAPTCHA, a novel human-verification framework that leverages fundamental\ndifferences in spatial reasoning between humans and MLLMs. Unlike existing\nCAPTCHAs which rely on low-level perception tasks that are vulnerable to modern\nAI, Spatial CAPTCHA generates dynamic questions requiring geometric reasoning,\nperspective-taking, occlusion handling, and mental rotation. These skills are\nintuitive for humans but difficult for state-of-the-art (SOTA) AI systems. The\nsystem employs a procedural generation pipeline with constraint-based\ndifficulty control, automated correctness verification, and human-in-the-loop\nvalidation to ensure scalability, robustness, and adaptability. Evaluation on a\ncorresponding benchmark, Spatial-CAPTCHA-Bench, demonstrates that humans vastly\noutperform 10 state-of-the-art MLLMs, with the best model achieving only 31.0%\nPass@1 accuracy. Furthermore, we compare Spatial CAPTCHA with Google reCAPTCHA,\nwhich confirms its effectiveness as both a security mechanism and a diagnostic\ntool for spatial reasoning in AI.", "AI": {"tldr": "Spatial CAPTCHA is a novel human-verification framework that leverages spatial reasoning differences between humans and AI to counter automated abuse, achieving significantly higher human success rates compared to state-of-the-art MLLMs.", "motivation": "Conventional CAPTCHAs focusing on text recognition or 2D image understanding have become ineffective due to advances in multi-modal large language models (MLLMs), requiring a new approach that exploits fundamental differences in spatial reasoning capabilities.", "method": "The system uses procedural generation of dynamic questions requiring geometric reasoning, perspective-taking, occlusion handling, and mental rotation. It employs constraint-based difficulty control, automated correctness verification, and human-in-the-loop validation for scalability and robustness.", "result": "Evaluation on Spatial-CAPTCHA-Bench shows humans vastly outperform 10 state-of-the-art MLLMs, with the best model achieving only 31.0% Pass@1 accuracy. Comparison with Google reCAPTCHA confirms its effectiveness as both security mechanism and AI diagnostic tool.", "conclusion": "Spatial CAPTCHA effectively addresses the limitations of conventional CAPTCHAs by leveraging spatial reasoning capabilities that remain challenging for current AI systems while being intuitive for humans, providing a robust defense against automated abuse."}}
{"id": "2510.03540", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03540", "abs": "https://arxiv.org/abs/2510.03540", "authors": ["Manuel Schwonberg", "Hanno Gottschalk"], "title": "Domain Generalization for Semantic Segmentation: A Survey", "comment": "Accepted to CVPR2025W", "summary": "The generalization of deep neural networks to unknown domains is a major\nchallenge despite their tremendous progress in recent years. For this reason,\nthe dynamic area of domain generalization (DG) has emerged. In contrast to\nunsupervised domain adaptation, there is no access to or knowledge about the\ntarget domains, and DG methods aim to generalize across multiple different\nunseen target domains. Domain generalization is particularly relevant for the\ntask semantic segmentation which is used in several areas such as biomedicine\nor automated driving. This survey provides a comprehensive overview of the\nrapidly evolving topic of domain generalized semantic segmentation. We cluster\nand review existing approaches and identify the paradigm shift towards\nfoundation-model-based domain generalization. Finally, we provide an extensive\nperformance comparison of all approaches, which highlights the significant\ninfluence of foundation models on domain generalization. This survey seeks to\nadvance domain generalization research and inspire scientists to explore new\nresearch directions.", "AI": {"tldr": "This survey paper provides a comprehensive overview of domain generalization in semantic segmentation, highlighting the paradigm shift towards foundation-model-based approaches and offering extensive performance comparisons.", "motivation": "Deep neural networks struggle with generalization to unknown domains despite recent progress, and domain generalization is particularly important for semantic segmentation tasks in critical applications like biomedicine and automated driving.", "method": "The authors cluster and review existing domain generalization approaches for semantic segmentation, identifying key trends and methodologies in the field.", "result": "The survey reveals a significant paradigm shift towards foundation-model-based domain generalization and provides extensive performance comparisons that demonstrate the substantial influence of foundation models on domain generalization capabilities.", "conclusion": "This comprehensive survey aims to advance domain generalization research and inspire new research directions in the rapidly evolving field of domain generalized semantic segmentation."}}
{"id": "2510.04041", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.04041", "abs": "https://arxiv.org/abs/2510.04041", "authors": ["Ayudh Saxena", "Harsh Shah", "Sandeep Routray", "Rishi Rajesh Shah", "Esha Pahwa"], "title": "SITCOM: Scaling Inference-Time COMpute for VLAs", "comment": "Accepted at the NeurIPS 2025 Workshop on Space in Vision, Language,\n  and Embodied AI (SpaVLE). *Equal contribution", "summary": "Learning robust robotic control policies remains a major challenge due to the\nhigh cost of collecting labeled data, limited generalization to unseen\nenvironments, and difficulties in planning over long horizons. While\nVision-Language-Action (VLA) models offer a promising solution by grounding\nnatural language instructions into single-step control commands, they often\nlack mechanisms for lookahead and struggle with compounding errors in dynamic\ntasks. In this project, we introduce Scaling Inference-Time COMpute for VLAs\n(SITCOM), a framework that augments any pretrained VLA with model-based\nrollouts and reward-based trajectory selection, inspired by Model Predictive\nControl algorithm. SITCOM leverages a learned dynamics model to simulate\nmulti-step action rollouts to select the best candidate plan for real-world\nexecution, transforming one-shot VLAs into robust long-horizon planners. We\ndevelop an efficient transformer-based dynamics model trained on large-scale\nBridgeV2 data and fine-tuned on SIMPLER environments to bridge the Real2Sim\ngap, and score candidate rollouts using rewards from simulator. Through\ncomprehensive evaluation across multiple tasks and settings in the SIMPLER\nenvironment, we demonstrate that SITCOM when combined with a good reward\nfunction can significantly improve task completion rate from 48% to 72% using\ntrained dynamics model.", "AI": {"tldr": "SITCOM augments pretrained Vision-Language-Action models with model-based rollouts and reward-based trajectory selection to improve long-horizon planning and robustness in robotic control.", "motivation": "Current Vision-Language-Action models struggle with long-horizon planning and compounding errors in dynamic tasks, lacking mechanisms for lookahead despite their ability to ground natural language into single-step control commands.", "method": "SITCOM combines pretrained VLAs with model-based rollouts using a learned dynamics model trained on BridgeV2 data and fine-tuned on SIMPLER environments. It performs multi-step action rollouts and selects the best trajectory using reward-based scoring from the simulator.", "result": "SITCOM significantly improves task completion rate from 48% to 72% when combined with a good reward function, demonstrating substantial performance gains across multiple tasks and settings in the SIMPLER environment.", "conclusion": "The SITCOM framework successfully transforms one-shot VLAs into robust long-horizon planners by integrating model-based rollouts and reward-based trajectory selection, addressing key limitations in current robotic control approaches."}}
{"id": "2510.03886", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03886", "abs": "https://arxiv.org/abs/2510.03886", "authors": ["Seil Kang", "Woojung Han", "Dayun Ju", "Seong Jae Hwang"], "title": "Rare Text Semantics Were Always There in Your Diffusion Transformer", "comment": "Accepted to NeurIPS 2025", "summary": "Starting from flow- and diffusion-based transformers, Multi-modal Diffusion\nTransformers (MM-DiTs) have reshaped text-to-vision generation, gaining acclaim\nfor exceptional visual fidelity. As these models advance, users continually\npush the boundary with imaginative or rare prompts, which advanced models still\nfalter in generating, since their concepts are often too scarce to leave a\nstrong imprint during pre-training. In this paper, we propose a simple yet\neffective intervention that surfaces rare semantics inside MM-DiTs without\nadditional training steps, data, denoising-time optimization, or reliance on\nexternal modules (e.g., large language models). In particular, the\njoint-attention mechanism intrinsic to MM-DiT sequentially updates text\nembeddings alongside image embeddings throughout transformer blocks. We find\nthat by mathematically expanding representational basins around text token\nembeddings via variance scale-up before the joint-attention blocks, rare\nsemantics clearly emerge in MM-DiT's outputs. Furthermore, our results\ngeneralize effectively across text-to-vision tasks, including text-to-image,\ntext-to-video, and text-driven image editing. Our work invites generative\nmodels to reveal the semantics that users intend, once hidden yet ready to\nsurface.", "AI": {"tldr": "A simple intervention that expands representational basins around text token embeddings in Multi-modal Diffusion Transformers (MM-DiTs) to surface rare semantics without additional training or external modules.", "motivation": "Advanced text-to-vision models still struggle with imaginative or rare prompts because these concepts are too scarce during pre-training to leave strong imprints, limiting their generation capabilities.", "method": "Mathematically expanding representational basins around text token embeddings via variance scale-up before joint-attention blocks in MM-DiTs, leveraging the intrinsic joint-attention mechanism that updates text and image embeddings sequentially.", "result": "The method effectively surfaces rare semantics in MM-DiT outputs and generalizes across text-to-vision tasks including text-to-image, text-to-video, and text-driven image editing.", "conclusion": "This work demonstrates that generative models can reveal hidden semantics that users intend through simple mathematical interventions, without requiring additional training steps, data, or external modules."}}
{"id": "2510.03543", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03543", "abs": "https://arxiv.org/abs/2510.03543", "authors": ["Evandros Kaklamanos", "Kristjana Kristinsdottir", "Jonathan Huang", "Dustin Carlson", "Rajesh Keswani", "John Pandolfino", "Mozziyar Etemadi"], "title": "From Scope to Script: An Automated Report Generation Model for Gastrointestinal Endoscopy", "comment": null, "summary": "Endoscopic procedures such as esophagogastroduodenoscopy (EGD) and\ncolonoscopy play a critical role in diagnosing and managing gastrointestinal\n(GI) disorders. However, the documentation burden associated with these\nprocedures place significant strain on gastroenterologists, contributing to\ninefficiencies in clinical workflows and physician burnout. To address this\nchallenge, we propose a novel automated report generation model that leverages\na transformer-based vision encoder and text decoder within a two-stage training\nframework. In the first stage, both components are pre-trained on image/text\ncaption pairs to capture generalized vision-language features, followed by\nfine-tuning on images/report pairs to generate clinically meaningful findings.\nOur approach not only streamlines the documentation process but also holds\npromise for reducing physician workload and improving patient care.", "AI": {"tldr": "A transformer-based automated report generation model for endoscopic procedures that reduces documentation burden on gastroenterologists.", "motivation": "Endoscopic procedures like EGD and colonoscopy create significant documentation burden, contributing to clinical inefficiencies and physician burnout.", "method": "Two-stage training framework: pre-training transformer-based vision encoder and text decoder on image/text caption pairs, then fine-tuning on images/report pairs for clinical findings.", "result": "The model generates clinically meaningful findings from endoscopic images, streamlining documentation process.", "conclusion": "This approach reduces physician workload and improves patient care by automating report generation for endoscopic procedures."}}
{"id": "2510.04074", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.04074", "abs": "https://arxiv.org/abs/2510.04074", "authors": ["Chung-Pang Wang", "Changwei Chen", "Xiao Liang", "Soofiyan Atar", "Florian Richter", "Michael Yip"], "title": "Feedback Matters: Augmenting Autonomous Dissection with Visual and Topological Feedback", "comment": null, "summary": "Autonomous surgical systems must adapt to highly dynamic environments where\ntissue properties and visual cues evolve rapidly. Central to such adaptability\nis feedback: the ability to sense, interpret, and respond to changes during\nexecution. While feedback mechanisms have been explored in surgical robotics,\nranging from tool and tissue tracking to error detection, existing methods\nremain limited in handling the topological and perceptual challenges of tissue\ndissection. In this work, we propose a feedback-enabled framework for\nautonomous tissue dissection that explicitly reasons about topological changes\nfrom endoscopic images after each dissection action. This structured feedback\nguides subsequent actions, enabling the system to localize dissection progress\nand adapt policies online. To improve the reliability of such feedback, we\nintroduce visibility metrics that quantify tissue exposure and formulate\noptimal controller designs that actively manipulate tissue to maximize\nvisibility. Finally, we integrate these feedback mechanisms with both\nplanning-based and learning-based dissection methods, and demonstrate\nexperimentally that they significantly enhance autonomy, reduce errors, and\nimprove robustness in complex surgical scenarios.", "AI": {"tldr": "A feedback-enabled framework for autonomous tissue dissection that uses topological reasoning from endoscopic images to guide surgical actions and improve reliability through visibility metrics and optimal control.", "motivation": "Existing surgical robotics methods struggle with topological and perceptual challenges in tissue dissection, lacking the ability to adapt to dynamic tissue changes during execution.", "method": "Proposes a framework that reasons about topological changes from endoscopic images after each dissection action, uses visibility metrics to quantify tissue exposure, and integrates optimal controller designs to maximize visibility.", "result": "Experimental results show the framework significantly enhances autonomy, reduces errors, and improves robustness in complex surgical scenarios when integrated with both planning-based and learning-based dissection methods.", "conclusion": "Feedback mechanisms that explicitly reason about topological changes and optimize for visibility can substantially improve the performance and reliability of autonomous surgical systems in dynamic tissue dissection tasks."}}
{"id": "2510.03892", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03892", "abs": "https://arxiv.org/abs/2510.03892", "authors": ["Zahra Atf", "Peter R. Lewis"], "title": "Kantian-Utilitarian XAI: Meta-Explained", "comment": "Accepted for presentation as a poster at the 35th IEEE International\n  Conference on Collaborative Advances in Software and Computing, 2025.\n  Conference\n  website:https://conf.researchr.org/details/cascon-2025/posters-track/1/Kantian-Utilitarian-XAI-Meta-Explained", "summary": "We present a gamified explainable AI (XAI) system for ethically aware\nconsumer decision-making in the coffee domain. Each session comprises six\nrounds with three options per round. Two symbolic engines provide real-time\nreasons: a Kantian module flags rule violations (e.g., child labor,\ndeforestation risk without shade certification, opaque supply chains, unsafe\ndecaf), and a utilitarian module scores options via multi-criteria aggregation\nover normalized attributes (price, carbon, water, transparency, farmer income\nshare, taste/freshness, packaging, convenience). A meta-explainer with a regret\nbound (0.2) highlights Kantian--utilitarian (mis)alignment and switches to a\ndeontically clean, near-parity option when welfare loss is small. We release a\nstructured configuration (attribute schema, certification map, weights, rule\nset), a policy trace for auditability, and an interactive UI.", "AI": {"tldr": "A gamified XAI system for ethical coffee purchasing decisions that combines Kantian and utilitarian reasoning with real-time explanations and regret-bounded decision switching.", "motivation": "To help consumers make ethically aware decisions in coffee purchasing by providing transparent explanations that combine deontological (rule-based) and consequentialist (utility-based) ethical frameworks.", "method": "Six-round gamified sessions with three coffee options per round. Uses two symbolic engines: Kantian module flags rule violations (child labor, deforestation, etc.), and utilitarian module scores options via multi-criteria aggregation. Includes meta-explainer with 0.2 regret bound to highlight ethical alignment and switch to deontically clean options when welfare loss is small.", "result": "Developed a complete system with structured configuration (attribute schema, certification map, weights, rule set), policy trace for auditability, and interactive UI for consumer use.", "conclusion": "The system successfully integrates multiple ethical frameworks in an explainable AI system for consumer decision-making, providing both transparency and practical guidance for ethically complex purchasing decisions."}}
{"id": "2510.03545", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03545", "abs": "https://arxiv.org/abs/2510.03545", "authors": ["Sixten Norelius", "Aaron O. Feldman", "Mac Schwager"], "title": "SketchPlan: Diffusion Based Drone Planning From Human Sketches", "comment": "Code available at https://github.com/sixnor/SketchPlan", "summary": "We propose SketchPlan, a diffusion-based planner that interprets 2D\nhand-drawn sketches over depth images to generate 3D flight paths for drone\nnavigation. SketchPlan comprises two components: a SketchAdapter that learns to\nmap the human sketches to projected 2D paths, and DiffPath, a diffusion model\nthat infers 3D trajectories from 2D projections and a first person view depth\nimage. Our model achieves zero-shot sim-to-real transfer, generating accurate\nand safe flight paths in previously unseen real-world environments. To train\nthe model, we build a synthetic dataset of 32k flight paths using a diverse set\nof photorealistic 3D Gaussian Splatting scenes. We automatically label the data\nby computing 2D projections of the 3D flight paths onto the camera plane, and\nuse this to train the DiffPath diffusion model. However, since real human 2D\nsketches differ significantly from ideal 2D projections, we additionally label\n872 of the 3D flight paths with real human sketches and use this to train the\nSketchAdapter to infer the 2D projection from the human sketch. We demonstrate\nSketchPlan's effectiveness in both simulated and real-world experiments, and\nshow through ablations that training on a mix of human labeled and auto-labeled\ndata together with a modular design significantly boosts its capabilities to\ncorrectly interpret human intent and infer 3D paths. In real-world drone tests,\nSketchPlan achieved 100\\% success in low/medium clutter and 40\\% in unseen\nhigh-clutter environments, outperforming key ablations by 20-60\\% in task\ncompletion.", "AI": {"tldr": "SketchPlan is a diffusion-based planner that converts 2D hand-drawn sketches over depth images into 3D drone flight paths, achieving zero-shot sim-to-real transfer.", "motivation": "To enable intuitive drone navigation by allowing users to draw 2D sketches that are automatically converted into safe 3D flight paths in real-world environments.", "method": "Uses two components: SketchAdapter (maps human sketches to 2D paths) and DiffPath (diffusion model that infers 3D trajectories from 2D projections and depth images). Trained on 32k synthetic flight paths and 872 human-labeled sketches.", "result": "Achieved 100% success in low/medium clutter and 40% in high-clutter real-world environments, outperforming ablations by 20-60% in task completion.", "conclusion": "The modular design with mixed training data (human-labeled + auto-labeled) significantly improves human intent interpretation and 3D path inference for drone navigation."}}
{"id": "2510.04076", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.04076", "abs": "https://arxiv.org/abs/2510.04076", "authors": ["Amin Vahidi-Moghaddam", "Sayed Pedram Haeri Boroujeni", "Iman Jebellat", "Ehsan Jebellat", "Niloufar Mehrabi", "Zhaojian Li"], "title": "From Shadow to Light: Toward Safe and Efficient Policy Learning Across MPC, DeePC, RL, and LLM Agents", "comment": null, "summary": "One of the main challenges in modern control applications, particularly in\nrobot and vehicle motion control, is achieving accurate, fast, and safe\nmovement. To address this, optimal control policies have been developed to\nenforce safety while ensuring high performance. Since basic first-principles\nmodels of real systems are often available, model-based controllers are widely\nused. Model predictive control (MPC) is a leading approach that optimizes\nperformance while explicitly handling safety constraints. However, obtaining\naccurate models for complex systems is difficult, which motivates data-driven\nalternatives. ML-based MPC leverages learned models to reduce reliance on\nhand-crafted dynamics, while reinforcement learning (RL) can learn near-optimal\npolicies directly from interaction data. Data-enabled predictive control\n(DeePC) goes further by bypassing modeling altogether, directly learning safe\npolicies from raw input-output data. Recently, large language model (LLM)\nagents have also emerged, translating natural language instructions into\nstructured formulations of optimal control problems. Despite these advances,\ndata-driven policies face significant limitations. They often suffer from slow\nresponse times, high computational demands, and large memory needs, making them\nless practical for real-world systems with fast dynamics, limited onboard\ncomputing, or strict memory constraints. To address this, various technique,\nsuch as reduced-order modeling, function-approximated policy learning, and\nconvex relaxations, have been proposed to reduce computational complexity. In\nthis paper, we present eight such approaches and demonstrate their\neffectiveness across real-world applications, including robotic arms, soft\nrobots, and vehicle motion control.", "AI": {"tldr": "This paper presents eight computational efficiency techniques for data-driven control policies to address limitations like slow response times, high computational demands, and large memory requirements in real-world applications.", "motivation": "Data-driven control policies (ML-based MPC, RL, DeePC) face practical limitations including slow response times, high computational demands, and large memory needs, making them impractical for systems with fast dynamics, limited computing, or strict memory constraints.", "method": "The paper presents eight approaches including reduced-order modeling, function-approximated policy learning, and convex relaxations to reduce computational complexity of data-driven control policies.", "result": "The techniques demonstrate effectiveness across real-world applications including robotic arms, soft robots, and vehicle motion control.", "conclusion": "Computational efficiency techniques can make data-driven control policies practical for real-world systems with fast dynamics and limited computing resources."}}
{"id": "2510.03969", "categories": ["cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03969", "abs": "https://arxiv.org/abs/2510.03969", "authors": ["Chengxiao Wang", "Isha Chaudhary", "Qian Hu", "Weitong Ruan", "Rahul Gupta", "Gagandeep Singh"], "title": "Quantifying Risks in Multi-turn Conversation with Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) can produce catastrophic responses in\nconversational settings that pose serious risks to public safety and security.\nExisting evaluations often fail to fully reveal these vulnerabilities because\nthey rely on fixed attack prompt sequences, lack statistical guarantees, and do\nnot scale to the vast space of multi-turn conversations. In this work, we\npropose QRLLM, a novel, principled Certification framework for Catastrophic\nrisks in multi-turn Conversation for LLMs that bounds the probability of an LLM\ngenerating catastrophic responses under multi-turn conversation distributions\nwith statistical guarantees. We model multi-turn conversations as probability\ndistributions over query sequences, represented by a Markov process on a query\ngraph whose edges encode semantic similarity to capture realistic\nconversational flow, and quantify catastrophic risks using confidence\nintervals. We define several inexpensive and practical distributions: random\nnode, graph path, adaptive with rejection. Our results demonstrate that these\ndistributions can reveal substantial catastrophic risks in frontier models,\nwith certified lower bounds as high as 70\\% for the worst model, highlighting\nthe urgent need for improved safety training strategies in frontier LLMs.", "AI": {"tldr": "QRLLM is a certification framework that provides statistical guarantees for bounding catastrophic risks in multi-turn LLM conversations by modeling conversations as Markov processes on query graphs.", "motivation": "Existing evaluations fail to fully reveal LLM vulnerabilities due to fixed attack prompts, lack of statistical guarantees, and inability to scale to multi-turn conversation spaces, posing serious public safety risks.", "method": "Model multi-turn conversations as probability distributions over query sequences using Markov processes on query graphs with semantic similarity edges, and define practical distributions (random node, graph path, adaptive with rejection) to quantify catastrophic risks with confidence intervals.", "result": "The framework reveals substantial catastrophic risks in frontier models, with certified lower bounds as high as 70% for the worst model, demonstrating urgent safety concerns.", "conclusion": "There is an urgent need for improved safety training strategies in frontier LLMs, as current models exhibit significant catastrophic risks under realistic multi-turn conversation scenarios."}}
{"id": "2510.03548", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03548", "abs": "https://arxiv.org/abs/2510.03548", "authors": ["Danial Samadi Vahdati", "Tai Duc Nguyen", "Ekta Prashnani", "Koki Nagano", "David Luebke", "Orazio Gallo", "Matthew Stamm"], "title": "Unmasking Puppeteers: Leveraging Biometric Leakage to Disarm Impersonation in AI-based Videoconferencing", "comment": null, "summary": "AI-based talking-head videoconferencing systems reduce bandwidth by sending a\ncompact pose-expression latent and re-synthesizing RGB at the receiver, but\nthis latent can be puppeteered, letting an attacker hijack a victim's likeness\nin real time. Because every frame is synthetic, deepfake and synthetic video\ndetectors fail outright. To address this security problem, we exploit a key\nobservation: the pose-expression latent inherently contains biometric\ninformation of the driving identity. Therefore, we introduce the first\nbiometric leakage defense without ever looking at the reconstructed RGB video:\na pose-conditioned, large-margin contrastive encoder that isolates persistent\nidentity cues inside the transmitted latent while cancelling transient pose and\nexpression. A simple cosine test on this disentangled embedding flags illicit\nidentity swaps as the video is rendered. Our experiments on multiple\ntalking-head generation models show that our method consistently outperforms\nexisting puppeteering defenses, operates in real-time, and shows strong\ngeneralization to out-of-distribution scenarios.", "AI": {"tldr": "A novel biometric defense method for talking-head videoconferencing that detects identity hijacking by analyzing pose-expression latents rather than reconstructed RGB video, using contrastive learning to isolate identity cues from pose/expression.", "motivation": "AI-based talking-head systems are vulnerable to puppeteering attacks where attackers can hijack a victim's likeness in real-time, and existing deepfake detectors fail because every frame is synthetic.", "method": "A pose-conditioned, large-margin contrastive encoder that disentangles persistent identity cues from transient pose and expression in the transmitted latent, followed by a simple cosine test on the embedding to detect identity swaps.", "result": "The method consistently outperforms existing puppeteering defenses, operates in real-time, and shows strong generalization to out-of-distribution scenarios across multiple talking-head generation models.", "conclusion": "The proposed biometric leakage defense effectively addresses the security vulnerability in talking-head videoconferencing by leveraging inherent biometric information in pose-expression latents without requiring RGB video analysis."}}
{"id": "2510.04161", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.04161", "abs": "https://arxiv.org/abs/2510.04161", "authors": ["Longrui Yang", "Yiyu Wang", "Jingfan Tang", "Yunpeng Lv", "Shizhe Zhao", "Chao Cao", "Zhongqiang Ren"], "title": "HEHA: Hierarchical Planning for Heterogeneous Multi-Robot Exploration of Unknown Environments", "comment": "5 Figures", "summary": "This paper considers the path planning problem for autonomous exploration of\nan unknown environment using multiple heterogeneous robots such as drones,\nwheeled, and legged robots, which have different capabilities to traverse\ncomplex terrains. A key challenge there is to intelligently allocate the robots\nto the unknown areas to be explored and determine the visiting order of those\nspaces subject to traversablity constraints, which leads to a large scale\nconstrained optimization problem that needs to be quickly and iteratively\nsolved every time when new space are explored. To address the challenge, we\npropose HEHA (Hierarchical Exploration with Heterogeneous Agents) by leveraging\na recent hierarchical method that decompose the exploration into global\nplanning and local planning. The major contribution in HEHA is its global\nplanning, where we propose a new routing algorithm PEAF (Partial Anytime Focal\nsearch) that can quickly find bounded sub-optimal solutions to minimize the\nmaximum path length among the agents subject to traversability constraints.\nAdditionally, the local planner in HEHA also considers heterogeneity to avoid\nrepeated and duplicated exploration among the robots. The experimental results\nshow that, our HEHA can reduce up to 30% of the exploration time than the\nbaselines.", "AI": {"tldr": "HEHA is a hierarchical path planning framework for multi-robot exploration using heterogeneous agents (drones, wheeled, legged robots) that reduces exploration time by up to 30% through efficient global planning with PEAF algorithm and heterogeneity-aware local planning.", "motivation": "Autonomous exploration with multiple heterogeneous robots faces challenges in intelligently allocating robots to unknown areas and determining visiting order while considering different traversability capabilities, requiring fast iterative solutions to large-scale constrained optimization problems.", "method": "Proposed HEHA framework with hierarchical decomposition: global planning using PEAF (Partial Anytime Focal search) algorithm for bounded sub-optimal routing to minimize maximum path length, and local planning that considers robot heterogeneity to avoid duplicated exploration.", "result": "Experimental results show HEHA reduces exploration time by up to 30% compared to baseline methods.", "conclusion": "HEHA effectively addresses the multi-robot heterogeneous exploration problem through hierarchical planning and specialized routing algorithms, demonstrating significant performance improvements in exploration efficiency."}}
{"id": "2510.04009", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04009", "abs": "https://arxiv.org/abs/2510.04009", "authors": ["Zicong He", "Boxuan Zhang", "Weihao Liu", "Ruixiang Tang", "Lu Cheng"], "title": "What Shapes a Creative Machine Mind? Comprehensively Benchmarking Creativity in Foundation Models", "comment": "22 pages", "summary": "The meteoric rise of foundation models (FMs) has expanded their capabilities\nfar beyond conventional tasks. Creativity, long regarded as a hallmark of human\nintelligence and a driver of innovation, is now increasingly recognized as a\ncritical dimension of machine intelligence in the era of generative FMs,\ncomplementing traditional measures of accuracy. However, existing evaluation\nframeworks for creativity remain fragmented, relying on ad hoc metrics not\nfirmly grounded in established theories. To address this gap, we introduce\nC^2-Eval, a holistic benchmark for unified assessment of creativity in FMs.\nC^2-Eval distinguishes between two complementary forms of creativity:\nconvergent creativity, where tasks admit constrained solutions (e.g., code\ngeneration), and divergent creativity, where tasks are open-ended (e.g.,\nstorytelling). It evaluates both dimensions using fine-grained criteria derived\nfrom social-science theory, focusing on Usefulness, Originality, and Surprise\n(U-O-S). Through extensive experiments on leading proprietary and open-source\nmodels, we analyze trade-offs in their creative capabilities. Our results\nhighlight both the strengths and challenges of current FMs in pursuing a\ncreative machine mind, showing that C^2-Eval is an effective lens for examining\nthe evolving landscape of creative AI.", "AI": {"tldr": "C^2-Eval is a new benchmark for evaluating creativity in foundation models, distinguishing between convergent (constrained) and divergent (open-ended) creativity using Usefulness, Originality, and Surprise criteria grounded in social science theory.", "motivation": "Existing evaluation frameworks for creativity in foundation models are fragmented and lack theoretical grounding, despite creativity becoming increasingly important as a dimension of machine intelligence alongside traditional accuracy measures.", "method": "Developed C^2-Eval benchmark that evaluates two forms of creativity: convergent (for tasks like code generation) and divergent (for tasks like storytelling), using fine-grained criteria of Usefulness, Originality, and Surprise derived from social science theory.", "result": "Extensive experiments on leading proprietary and open-source models revealed trade-offs in creative capabilities, highlighting both strengths and challenges of current foundation models in achieving creative machine intelligence.", "conclusion": "C^2-Eval provides an effective framework for examining the evolving landscape of creative AI and serves as a valuable tool for assessing the creative capabilities of foundation models."}}
{"id": "2510.03550", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03550", "abs": "https://arxiv.org/abs/2510.03550", "authors": ["Junbao Zhou", "Yuan Zhou", "Kesen Zhao", "Qingshan Xu", "Beier Zhu", "Richang Hong", "Hanwang Zhang"], "title": "Streaming Drag-Oriented Interactive Video Manipulation: Drag Anything, Anytime!", "comment": null, "summary": "Achieving streaming, fine-grained control over the outputs of autoregressive\nvideo diffusion models remains challenging, making it difficult to ensure that\nthey consistently align with user expectations. To bridge this gap, we propose\n\\textbf{stReaming drag-oriEnted interactiVe vidEo manipuLation (REVEL)}, a new\ntask that enables users to modify generated videos \\emph{anytime} on\n\\emph{anything} via fine-grained, interactive drag. Beyond DragVideo and\nSG-I2V, REVEL unifies drag-style video manipulation as editing and animating\nvideo frames with both supporting user-specified translation, deformation, and\nrotation effects, making drag operations versatile. In resolving REVEL, we\nobserve: \\emph{i}) drag-induced perturbations accumulate in latent space,\ncausing severe latent distribution drift that halts the drag process;\n\\emph{ii}) streaming drag is easily disturbed by context frames, thereby\nyielding visually unnatural outcomes. We thus propose a training-free approach,\n\\textbf{DragStream}, comprising: \\emph{i}) an adaptive distribution\nself-rectification strategy that leverages neighboring frames' statistics to\neffectively constrain the drift of latent embeddings; \\emph{ii}) a\nspatial-frequency selective optimization mechanism, allowing the model to fully\nexploit contextual information while mitigating its interference via\nselectively propagating visual cues along generation. Our method can be\nseamlessly integrated into existing autoregressive video diffusion models, and\nextensive experiments firmly demonstrate the effectiveness of our DragStream.", "AI": {"tldr": "REVEL enables interactive drag-based video manipulation anytime on anything, addressing challenges of latent distribution drift and context interference through training-free DragStream method.", "motivation": "Current autoregressive video diffusion models lack fine-grained streaming control, making it difficult to ensure outputs consistently align with user expectations through interactive drag operations.", "method": "Proposes DragStream with adaptive distribution self-rectification using neighboring frames' statistics to constrain latent embedding drift, and spatial-frequency selective optimization to mitigate context interference while propagating visual cues.", "result": "The method can be seamlessly integrated into existing autoregressive video diffusion models and extensive experiments demonstrate its effectiveness in enabling streaming drag-oriented interactive video manipulation.", "conclusion": "REVEL and DragStream successfully bridge the gap for fine-grained interactive video manipulation, providing versatile drag operations with translation, deformation, and rotation effects while overcoming latent drift and context interference challenges."}}
{"id": "2510.04168", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.04168", "abs": "https://arxiv.org/abs/2510.04168", "authors": ["Amirmasoud Molaei", "Reza Ghabcheloo"], "title": "Learning to Capture Rocks using an Excavator: A Reinforcement Learning Approach with Guiding Reward Formulation", "comment": null, "summary": "Rock capturing with standard excavator buckets is a challenging task\ntypically requiring the expertise of skilled operators. Unlike soil digging, it\ninvolves manipulating large, irregular rocks in unstructured environments where\ncomplex contact interactions with granular material make model-based control\nimpractical. Existing autonomous excavation methods focus mainly on continuous\nmedia or rely on specialized grippers, limiting their applicability to\nreal-world construction sites. This paper introduces a fully data-driven\ncontrol framework for rock capturing that eliminates the need for explicit\nmodeling of rock or soil properties. A model-free reinforcement learning agent\nis trained in the AGX Dynamics simulator using the Proximal Policy Optimization\n(PPO) algorithm and a guiding reward formulation. The learned policy outputs\njoint velocity commands directly to the boom, arm, and bucket of a CAT365\nexcavator model. Robustness is enhanced through extensive domain randomization\nof rock geometry, density, and mass, as well as the initial configurations of\nthe bucket, rock, and goal position. To the best of our knowledge, this is the\nfirst study to develop and evaluate an RL-based controller for the rock\ncapturing task. Experimental results show that the policy generalizes well to\nunseen rocks and varying soil conditions, achieving high success rates\ncomparable to those of human participants while maintaining machine stability.\nThese findings demonstrate the feasibility of learning-based excavation\nstrategies for discrete object manipulation without requiring specialized\nhardware or detailed material models.", "AI": {"tldr": "This paper presents a data-driven reinforcement learning framework for autonomous rock capturing using standard excavator buckets, eliminating the need for explicit modeling of rock or soil properties.", "motivation": "Rock capturing with standard excavators is challenging and typically requires skilled operators. Existing autonomous methods focus on continuous media or specialized grippers, limiting real-world applicability in construction sites with unstructured environments and complex contact interactions.", "method": "A model-free reinforcement learning agent trained in AGX Dynamics simulator using PPO algorithm with guiding reward formulation. The policy outputs joint velocity commands directly to excavator components (boom, arm, bucket). Robustness enhanced through extensive domain randomization of rock geometry, density, mass, and initial configurations.", "result": "The learned policy generalizes well to unseen rocks and varying soil conditions, achieving high success rates comparable to human participants while maintaining machine stability.", "conclusion": "This demonstrates the feasibility of learning-based excavation strategies for discrete object manipulation without requiring specialized hardware or detailed material models."}}
{"id": "2510.04017", "categories": ["cs.AI", "cs.LG", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2510.04017", "abs": "https://arxiv.org/abs/2510.04017", "authors": ["Sumanth Varambally", "Marshall Fisher", "Jas Thakker", "Yiwei Chen", "Zhirui Xia", "Yasaman Jafari", "Ruijia Niu", "Manas Jain", "Veeramakali Vignesh Manivannan", "Zachary Novack", "Luyu Han", "Srikar Eranky", "Salva R\u00fchling Cachay", "Taylor Berg-Kirkpatrick", "Duncan Watson-Parris", "Yi-An Ma", "Rose Yu"], "title": "Zephyrus: An Agentic Framework for Weather Science", "comment": null, "summary": "Foundation models for weather science are pre-trained on vast amounts of\nstructured numerical data and outperform traditional weather forecasting\nsystems. However, these models lack language-based reasoning capabilities,\nlimiting their utility in interactive scientific workflows. Large language\nmodels (LLMs) excel at understanding and generating text but cannot reason\nabout high-dimensional meteorological datasets. We bridge this gap by building\na novel agentic framework for weather science. Our framework includes a Python\ncode-based environment for agents (ZephyrusWorld) to interact with weather\ndata, featuring tools like an interface to WeatherBench 2 dataset, geoquerying\nfor geographical masks from natural language, weather forecasting, and climate\nsimulation capabilities. We design Zephyrus, a multi-turn LLM-based weather\nagent that iteratively analyzes weather datasets, observes results, and refines\nits approach through conversational feedback loops. We accompany the agent with\na new benchmark, ZephyrusBench, with a scalable data generation pipeline that\nconstructs diverse question-answer pairs across weather-related tasks, from\nbasic lookups to advanced forecasting, extreme event detection, and\ncounterfactual reasoning. Experiments on this benchmark demonstrate the strong\nperformance of Zephyrus agents over text-only baselines, outperforming them by\nup to 35 percentage points in correctness. However, on harder tasks, Zephyrus\nperforms similarly to text-only baselines, highlighting the challenging nature\nof our benchmark and suggesting promising directions for future work.", "AI": {"tldr": "The paper introduces Zephyrus, an LLM-based agentic framework that bridges weather foundation models with language reasoning capabilities, enabling interactive weather science workflows through code-based tools and conversational feedback loops.", "motivation": "Foundation models for weather outperform traditional forecasting but lack language reasoning, while LLMs can't handle meteorological data. This creates a gap in interactive scientific workflows that need both data processing and natural language capabilities.", "method": "Built a Python code-based environment (ZephyrusWorld) with tools for weather data interaction, including WeatherBench 2 dataset interface, geoquerying, forecasting, and climate simulation. Created Zephyrus, a multi-turn LLM agent that iteratively analyzes data and refines approaches through conversational feedback.", "result": "Zephyrus agents outperform text-only baselines by up to 35 percentage points in correctness on the ZephyrusBench benchmark. However, on harder tasks, performance is similar to baselines, indicating the benchmark's challenging nature.", "conclusion": "The framework successfully bridges weather data processing with language reasoning, but harder tasks remain challenging, suggesting promising directions for future work in agentic weather science."}}
{"id": "2510.03555", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03555", "abs": "https://arxiv.org/abs/2510.03555", "authors": ["Peiran Quan", "Zifan Gu", "Zhuo Zhao", "Qin Zhou", "Donghan M. Yang", "Ruichen Rong", "Yang Xie", "Guanghua Xiao"], "title": "GAS-MIL: Group-Aggregative Selection Multi-Instance Learning for Ensemble of Foundation Models in Digital Pathology Image Analysis", "comment": null, "summary": "Foundation models (FMs) have transformed computational pathology by providing\npowerful, general-purpose feature extractors. However, adapting and\nbenchmarking individual FMs for specific diagnostic tasks is often\ntime-consuming and resource-intensive, especially given their scale and\ndiversity. To address this challenge, we introduce Group-Aggregative Selection\nMulti-Instance Learning (GAS-MIL), a flexible ensemble framework that\nseamlessly integrates features from multiple FMs, preserving their\ncomplementary strengths without requiring manual feature selection or extensive\ntask-specific fine-tuning. Across classification tasks in three cancer\ndatasets-prostate (PANDA), ovarian (UBC-OCEAN), and breast (TCGA-BrCa)-GAS-MIL\nconsistently achieves superior or on-par performance relative to individual FMs\nand established MIL methods, demonstrating its robustness and generalizability.\nBy enabling efficient integration of heterogeneous FMs, GAS-MIL streamlines\nmodel deployment for pathology and provides a scalable foundation for future\nmultimodal and precision oncology applications.", "AI": {"tldr": "GAS-MIL is an ensemble framework that integrates features from multiple foundation models for computational pathology, achieving superior performance across cancer datasets without requiring extensive fine-tuning.", "motivation": "Foundation models provide powerful feature extractors for pathology, but adapting and benchmarking individual models for specific diagnostic tasks is time-consuming and resource-intensive due to their scale and diversity.", "method": "Group-Aggregative Selection Multi-Instance Learning (GAS-MIL) - a flexible ensemble framework that seamlessly integrates features from multiple foundation models while preserving their complementary strengths, without manual feature selection or extensive task-specific fine-tuning.", "result": "Across three cancer datasets (prostate PANDA, ovarian UBC-OCEAN, and breast TCGA-BrCa), GAS-MIL consistently achieves superior or on-par performance relative to individual foundation models and established multi-instance learning methods.", "conclusion": "GAS-MIL enables efficient integration of heterogeneous foundation models, streamlines model deployment for pathology, and provides a scalable foundation for future multimodal and precision oncology applications."}}
{"id": "2510.04171", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.04171", "abs": "https://arxiv.org/abs/2510.04171", "authors": ["Lakshadeep Naik", "Adam Fischer", "Daniel Duberg", "Danica Kragic"], "title": "VBM-NET: Visual Base Pose Learning for Mobile Manipulation using Equivariant TransporterNet and GNNs", "comment": null, "summary": "In Mobile Manipulation, selecting an optimal mobile base pose is essential\nfor successful object grasping. Previous works have addressed this problem\neither through classical planning methods or by learning state-based policies.\nThey assume access to reliable state information, such as the precise object\nposes and environment models. In this work, we study base pose planning\ndirectly from top-down orthographic projections of the scene, which provide a\nglobal overview of the scene while preserving spatial structure. We propose\nVBM-NET, a learning-based method for base pose selection using such top-down\northographic projections. We use equivariant TransporterNet to exploit spatial\nsymmetries and efficiently learn candidate base poses for grasping. Further, we\nuse graph neural networks to represent a varying number of candidate base poses\nand use Reinforcement Learning to determine the optimal base pose among them.\nWe show that VBM-NET can produce comparable solutions to the classical methods\nin significantly less computation time. Furthermore, we validate sim-to-real\ntransfer by successfully deploying a policy trained in simulation to real-world\nmobile manipulation.", "AI": {"tldr": "VBM-NET is a learning-based method that uses top-down orthographic projections and equivariant TransporterNet with graph neural networks to efficiently select optimal mobile base poses for grasping, achieving comparable performance to classical methods with much faster computation and successful sim-to-real transfer.", "motivation": "Previous mobile manipulation approaches rely on reliable state information like precise object poses and environment models, which may not always be available. This work aims to address base pose planning directly from visual scene representations without requiring explicit state information.", "method": "Uses top-down orthographic projections of scenes, equivariant TransporterNet to exploit spatial symmetries for candidate base pose generation, graph neural networks to represent varying numbers of candidate poses, and reinforcement learning to select the optimal base pose.", "result": "VBM-NET produces solutions comparable to classical methods but with significantly reduced computation time. The method successfully transfers from simulation to real-world mobile manipulation scenarios.", "conclusion": "The proposed learning-based approach using visual scene representations and equivariant networks provides an efficient alternative to classical planning methods for mobile base pose selection, with practical real-world applicability through successful sim-to-real transfer."}}
{"id": "2510.04023", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04023", "abs": "https://arxiv.org/abs/2510.04023", "authors": ["Mizanur Rahman", "Amran Bhuiyan", "Mohammed Saidul Islam", "Md Tahmid Rahman Laskar", "Ridwan Mahbub", "Ahmed Masry", "Shafiq Joty", "Enamul Hoque"], "title": "LLM-Based Data Science Agents: A Survey of Capabilities, Challenges, and Future Directions", "comment": "Survey paper; 45 data science agents; under review", "summary": "Recent advances in large language models (LLMs) have enabled a new class of\nAI agents that automate multiple stages of the data science workflow by\nintegrating planning, tool use, and multimodal reasoning across text, code,\ntables, and visuals. This survey presents the first comprehensive,\nlifecycle-aligned taxonomy of data science agents, systematically analyzing and\nmapping forty-five systems onto the six stages of the end-to-end data science\nprocess: business understanding and data acquisition, exploratory analysis and\nvisualization, feature engineering, model building and selection,\ninterpretation and explanation, and deployment and monitoring. In addition to\nlifecycle coverage, we annotate each agent along five cross-cutting design\ndimensions: reasoning and planning style, modality integration, tool\norchestration depth, learning and alignment methods, and trust, safety, and\ngovernance mechanisms. Beyond classification, we provide a critical synthesis\nof agent capabilities, highlight strengths and limitations at each stage, and\nreview emerging benchmarks and evaluation practices. Our analysis identifies\nthree key trends: most systems emphasize exploratory analysis, visualization,\nand modeling while neglecting business understanding, deployment, and\nmonitoring; multimodal reasoning and tool orchestration remain unresolved\nchallenges; and over 90% lack explicit trust and safety mechanisms. We conclude\nby outlining open challenges in alignment stability, explainability,\ngovernance, and robust evaluation frameworks, and propose future research\ndirections to guide the development of robust, trustworthy, low-latency,\ntransparent, and broadly accessible data science agents.", "AI": {"tldr": "This survey provides the first comprehensive taxonomy of data science agents, analyzing 45 systems across the six stages of the data science lifecycle and five design dimensions, identifying key trends and gaps in current research.", "motivation": "Recent advances in LLMs have enabled new AI agents that automate multiple stages of the data science workflow, but there is a need for systematic classification and analysis of these emerging systems to understand their capabilities and limitations.", "method": "The authors developed a lifecycle-aligned taxonomy mapping 45 data science agent systems across six data science stages and annotated each along five cross-cutting design dimensions including reasoning style, modality integration, tool orchestration, learning methods, and trust mechanisms.", "result": "Analysis revealed three key trends: most systems focus on exploratory analysis and modeling while neglecting business understanding and deployment; multimodal reasoning and tool orchestration remain challenging; and over 90% lack explicit trust and safety mechanisms.", "conclusion": "The paper outlines open challenges in alignment stability, explainability, governance, and robust evaluation, proposing future research directions for developing more robust, trustworthy, and transparent data science agents."}}
{"id": "2510.03558", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03558", "abs": "https://arxiv.org/abs/2510.03558", "authors": ["Shen Chang", "Renran Tian", "Nicole Adams", "Nan Kong"], "title": "Real-Time Assessment of Bystander Situation Awareness in Drone-Assisted First Aid", "comment": null, "summary": "Rapid naloxone delivery via drones offers a promising solution for responding\nto opioid overdose emergencies (OOEs), by extending lifesaving interventions to\nmedically untrained bystanders before emergency medical services (EMS) arrive.\nRecognizing the critical role of bystander situational awareness (SA) in\nhuman-autonomy teaming (HAT), we address a key research gap in real-time SA\nassessment by introducing the Drone-Assisted Naloxone Delivery Simulation\nDataset (DANDSD). This pioneering dataset captures HAT during simulated OOEs,\nwhere college students without medical training act as bystanders tasked with\nadministering intranasal naloxone to a mock overdose victim. Leveraging this\ndataset, we propose a video-based real-time SA assessment framework that\nutilizes graph embeddings and transformer models to assess bystander SA in real\ntime. Our approach integrates visual perception and comprehension cues--such as\ngeometric, kinematic, and interaction graph features--and achieves\nhigh-performance SA prediction. It also demonstrates strong temporal\nsegmentation accuracy, outperforming the FINCH baseline by 9% in Mean over\nFrames (MoF) and 5% in Intersection over Union (IoU). This work supports the\ndevelopment of adaptive drone systems capable of guiding bystanders\neffectively, ultimately improving emergency response outcomes and saving lives.", "AI": {"tldr": "This paper introduces a drone-assisted naloxone delivery system for opioid overdose emergencies, featuring a novel dataset and real-time situational awareness assessment framework using graph embeddings and transformers.", "motivation": "To address the critical need for rapid naloxone delivery in opioid overdose emergencies by extending lifesaving interventions to untrained bystanders before EMS arrival, and to fill the research gap in real-time situational awareness assessment for human-autonomy teaming.", "method": "Created the Drone-Assisted Naloxone Delivery Simulation Dataset (DANDSD) with college students as bystanders, and developed a video-based real-time SA assessment framework using graph embeddings and transformer models that integrate geometric, kinematic, and interaction graph features.", "result": "The proposed approach achieves high-performance SA prediction with strong temporal segmentation accuracy, outperforming the FINCH baseline by 9% in Mean over Frames (MoF) and 5% in Intersection over Union (IoU).", "conclusion": "This work enables the development of adaptive drone systems that can effectively guide bystanders during opioid overdose emergencies, ultimately improving emergency response outcomes and saving lives."}}
{"id": "2510.04178", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.04178", "abs": "https://arxiv.org/abs/2510.04178", "authors": ["L\u00e9a Pistorius", "Namrata U. Nayar", "Phillip Tran", "Sammy Elmariah", "Pierre E. Dupont"], "title": "Using Robotics to Improve Transcatheter Edge-to-Edge Repair of the Mitral Valve", "comment": "7 pages, 9 figures", "summary": "Transcatheter valve repair presents significant challenges due to the\nmechanical limitations and steep learning curve associated with manual catheter\nsystems. This paper investigates the use of robotics to facilitate\ntranscatheter procedures in the context of mitral valve edge-to-edge repair.\nThe complex handle-based control of a clinical repair device is replaced by\nintuitive robotic joint-based control via a game controller. Manual versus\nrobotic performance is analyzed by decomposing the overall device delivery task\ninto motion-specific steps and comparing capabilities on a step-by-step basis\nin a phantom model of the heart and vasculature. Metrics include procedure\nduration and clip placement accuracy. Results demonstrate that the robotic\nsystem can reduce procedural time and motion errors while also improving\naccuracy of clip placement. These findings suggest that robotic assistance can\naddress key limitations of manual systems, offering a more reliable and\nuser-friendly platform for complex transcatheter procedures.", "AI": {"tldr": "Robotic system replaces manual catheter control for mitral valve repair, reducing procedure time and improving accuracy using intuitive game controller interface.", "motivation": "Manual transcatheter valve repair has mechanical limitations and steep learning curve, making robotic assistance desirable to overcome these challenges.", "method": "Replaced complex handle-based control with robotic joint-based control via game controller, compared manual vs robotic performance in phantom heart model by decomposing procedure into motion-specific steps.", "result": "Robotic system reduced procedural time and motion errors while improving clip placement accuracy compared to manual approach.", "conclusion": "Robotic assistance addresses key limitations of manual systems and provides more reliable, user-friendly platform for complex transcatheter procedures."}}
{"id": "2510.04033", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04033", "abs": "https://arxiv.org/abs/2510.04033", "authors": ["Ayush Noori", "Adam Rodman", "Alan Karthikesalingam", "Bilal A. Mateen", "Christopher A. Longhurst", "Daniel Yang", "Dave deBronkart", "Gauden Galea", "Harold F. Wolf III", "Jacob Waxman", "Joshua C. Mandel", "Juliana Rotich", "Kenneth D. Mandl", "Maryam Mustafa", "Melissa Miles", "Nigam H. Shah", "Peter Lee", "Robert Korom", "Scott Mahoney", "Seth Hain", "Tien Yin Wong", "Trevor Mundel", "Vivek Natarajan", "Noa Dagan", "David A. Clifton", "Ran D. Balicer", "Isaac S. Kohane", "Marinka Zitnik"], "title": "A global log for medical AI", "comment": null, "summary": "Modern computer systems often rely on syslog, a simple, universal protocol\nthat records every critical event across heterogeneous infrastructure. However,\nhealthcare's rapidly growing clinical AI stack has no equivalent. As hospitals\nrush to pilot large language models and other AI-based clinical decision\nsupport tools, we still lack a standard way to record how, when, by whom, and\nfor whom these AI models are used. Without that transparency and visibility, it\nis challenging to measure real-world performance and outcomes, detect adverse\nevents, or correct bias or dataset drift. In the spirit of syslog, we introduce\nMedLog, a protocol for event-level logging of clinical AI. Any time an AI model\nis invoked to interact with a human, interface with another algorithm, or act\nindependently, a MedLog record is created. This record consists of nine core\nfields: header, model, user, target, inputs, artifacts, outputs, outcomes, and\nfeedback, providing a structured and consistent record of model activity. To\nencourage early adoption, especially in low-resource settings, and minimize the\ndata footprint, MedLog supports risk-based sampling, lifecycle-aware retention\npolicies, and write-behind caching; detailed traces for complex, agentic, or\nmulti-stage workflows can also be captured under MedLog. MedLog can catalyze\nthe development of new databases and software to store and analyze MedLog\nrecords. Realizing this vision would enable continuous surveillance, auditing,\nand iterative improvement of medical AI, laying the foundation for a new form\nof digital epidemiology.", "AI": {"tldr": "MedLog is a syslog-inspired protocol for standardized logging of clinical AI model usage, enabling transparency, performance monitoring, and safety surveillance in healthcare AI systems.", "motivation": "Healthcare lacks a standard logging protocol for clinical AI systems, making it difficult to track model usage, measure real-world performance, detect adverse events, and monitor for bias or dataset drift as hospitals increasingly adopt AI tools.", "method": "MedLog creates structured event-level logs with nine core fields (header, model, user, target, inputs, artifacts, outputs, outcomes, feedback) whenever AI models interact with humans, other algorithms, or act independently. It includes risk-based sampling, lifecycle-aware retention, and write-behind caching for efficiency.", "result": "The protocol provides a consistent framework for recording AI model activity, supporting detailed traces for complex workflows while minimizing data footprint through optimization features.", "conclusion": "MedLog enables continuous surveillance, auditing, and iterative improvement of medical AI, laying the foundation for digital epidemiology and catalyzing development of new databases and analysis tools for clinical AI monitoring."}}
{"id": "2510.03570", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03570", "abs": "https://arxiv.org/abs/2510.03570", "authors": ["Mayimunah Nagayi", "Alice Khan", "Tamryn Frank", "Rina Swart", "Clement Nyirenda"], "title": "Evaluating OCR performance on food packaging labels in South Africa", "comment": "17 pages", "summary": "This study evaluates four open-source Optical Character Recognition (OCR)\nsystems which are Tesseract, EasyOCR, PaddleOCR, and TrOCR on real world food\npackaging images. The aim is to assess their ability to extract ingredient\nlists and nutrition facts panels. Accurate OCR for packaging is important for\ncompliance and nutrition monitoring but is challenging due to multilingual\ntext, dense layouts, varied fonts, glare, and curved surfaces. A dataset of 231\nproducts (1,628 images) was processed by all four models to assess speed and\ncoverage, and a ground truth subset of 113 images (60 products) was created for\naccuracy evaluation. Metrics include Character Error Rate (CER), Word Error\nRate (WER), BLEU, ROUGE-L, F1, coverage, and execution time. On the ground\ntruth subset, Tesseract achieved the lowest CER (0.912) and the highest BLEU\n(0.245). EasyOCR provided a good balance between accuracy and multilingual\nsupport. PaddleOCR achieved near complete coverage but was slower because it\nran on CPU only due to GPU incompatibility, and TrOCR produced the weakest\nresults despite GPU acceleration. These results provide a packaging-specific\nbenchmark, establish a baseline, and highlight directions for layout-aware\nmethods and text localization.", "AI": {"tldr": "This study benchmarks four OCR systems (Tesseract, EasyOCR, PaddleOCR, TrOCR) on food packaging images, finding Tesseract most accurate while EasyOCR offers good multilingual balance.", "motivation": "Accurate OCR for food packaging is crucial for compliance and nutrition monitoring, but challenging due to multilingual text, dense layouts, varied fonts, glare, and curved surfaces.", "method": "Evaluated four OCR systems on 231 products (1,628 images) using metrics including CER, WER, BLEU, ROUGE-L, F1, coverage, and execution time, with ground truth subset of 113 images for accuracy assessment.", "result": "Tesseract achieved lowest CER (0.912) and highest BLEU (0.245), EasyOCR balanced accuracy with multilingual support, PaddleOCR had near complete coverage but was slower, TrOCR performed weakest despite GPU acceleration.", "conclusion": "Results provide packaging-specific benchmark, establish baseline performance, and highlight need for layout-aware methods and improved text localization."}}
{"id": "2510.04190", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.04190", "abs": "https://arxiv.org/abs/2510.04190", "authors": ["Jian-jie Zheng", "Chih-kai Yang", "Po-han Chen", "Lyn Chao-ling Chen"], "title": "Zenbo Patrol: A Social Assistive Robot Based on Multimodal Deep Learning for Real-time Illegal Parking Recognition and Notification", "comment": null, "summary": "In the study, the social robot act as a patrol to recognize and notify\nillegal parking in real-time. Dual-model pipeline method and large multimodal\nmodel were compared, and the GPT-4o multimodal model was adopted in license\nplate recognition without preprocessing. For moving smoothly on a flat ground,\nthe robot navigated in a simulated parking lot in the experiments. The robot\nchanges angle view of the camera automatically to capture the images around\nwith the format of license plate number. From the captured images of the robot,\nthe numbers on the plate are recognized through the GPT-4o model, and\nidentifies legality of the numbers. When an illegal parking is detected, the\nrobot sends Line messages to the system manager immediately. The contribution\nof the work is that a novel multimodal deep learning method has validated with\nhigh accuracy in license plate recognition, and a social assistive robot is\nalso provided for solving problems in a real scenario, and can be applied in an\nindoor parking lot.", "AI": {"tldr": "A social robot patrols parking lots using GPT-4o multimodal model for real-time license plate recognition and illegal parking detection, automatically notifying managers via Line messages.", "motivation": "To develop an automated solution for detecting illegal parking in indoor parking lots using social robotics and advanced AI models to improve parking management efficiency.", "method": "Used a dual-model pipeline approach and compared with large multimodal models, ultimately adopting GPT-4o for license plate recognition without preprocessing. The robot navigates simulated parking lots, automatically adjusts camera angles to capture license plate images, and processes them through GPT-4o for number recognition and legality assessment.", "result": "The system successfully recognized license plate numbers and identified illegal parking with high accuracy using the novel multimodal deep learning approach, enabling immediate notification to system managers.", "conclusion": "The work demonstrates a validated high-accuracy license plate recognition system using multimodal AI, providing a practical social assistive robot solution for real-world indoor parking lot applications."}}
{"id": "2510.04040", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04040", "abs": "https://arxiv.org/abs/2510.04040", "authors": ["Xu Shen", "Song Wang", "Zhen Tan", "Laura Yao", "Xinyu Zhao", "Kaidi Xu", "Xin Wang", "Tianlong Chen"], "title": "FaithCoT-Bench: Benchmarking Instance-Level Faithfulness of Chain-of-Thought Reasoning", "comment": null, "summary": "Large language models (LLMs) increasingly rely on Chain-of-Thought (CoT)\nprompting to improve problem-solving and provide seemingly transparent\nexplanations. However, growing evidence shows that CoT often fail to faithfully\nrepresent the underlying reasoning process, raising concerns about their\nreliability in high-risk applications. Although prior studies have focused on\nmechanism-level analyses showing that CoTs can be unfaithful, they leave open\nthe practical challenge of deciding whether a specific trajectory is faithful\nto the internal reasoning of the model. To address this gap, we introduce\nFaithCoT-Bench, a unified benchmark for instance-level CoT unfaithfulness\ndetection. Our framework establishes a rigorous task formulation that\nformulates unfaithfulness detection as a discriminative decision problem, and\nprovides FINE-CoT (Faithfulness instance evaluation for Chain-of-Thought), an\nexpert-annotated collection of over 1,000 trajectories generated by four\nrepresentative LLMs across four domains, including more than 300 unfaithful\ninstances with fine-grained causes and step-level evidence. We further conduct\na systematic evaluation of eleven representative detection methods spanning\ncounterfactual, logit-based, and LLM-as-judge paradigms, deriving empirical\ninsights that clarify the strengths and weaknesses of existing approaches and\nreveal the increased challenges of detection in knowledge-intensive domains and\nwith more advanced models. To the best of our knowledge, FaithCoT-Bench\nestablishes the first comprehensive benchmark for instance-level CoT\nfaithfulness, setting a solid basis for future research toward more\ninterpretable and trustworthy reasoning in LLMs.", "AI": {"tldr": "The paper introduces FaithCoT-Bench, a benchmark for detecting unfaithful Chain-of-Thought (CoT) reasoning in LLMs, addressing the gap in instance-level faithfulness evaluation.", "motivation": "CoT prompting in LLMs often fails to faithfully represent the underlying reasoning process, raising reliability concerns in high-risk applications. Prior studies focused on mechanism-level analyses but left open the practical challenge of detecting unfaithfulness in specific reasoning trajectories.", "method": "The authors introduce FaithCoT-Bench with FINE-CoT - an expert-annotated collection of over 1,000 CoT trajectories from four LLMs across four domains, including 300+ unfaithful instances with fine-grained causes and step-level evidence. They systematically evaluate 11 detection methods spanning counterfactual, logit-based, and LLM-as-judge paradigms.", "result": "The evaluation reveals empirical insights about the strengths and weaknesses of existing detection approaches, showing increased challenges in knowledge-intensive domains and with more advanced models.", "conclusion": "FaithCoT-Bench establishes the first comprehensive benchmark for instance-level CoT faithfulness, providing a solid foundation for future research toward more interpretable and trustworthy reasoning in LLMs."}}
{"id": "2510.03584", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03584", "abs": "https://arxiv.org/abs/2510.03584", "authors": ["Chaoyu Li", "Tianzhi Li", "Fei Tao", "Zhenyu Zhao", "Ziqian Wu", "Maozheng Zhao", "Juntong Song", "Cheng Niu", "Pooyan Fazli"], "title": "FrameOracle: Learning What to See and How Much to See in Videos", "comment": null, "summary": "Vision-language models (VLMs) have advanced video understanding, but their\nperformance is limited by the number of input frames they can process. Existing\nframe sampling strategies, such as uniform or fixed-budget selection, often\nfail to adapt to variations in information density or task complexity,\nresulting in inefficiency and information loss. To address this, we present\nFrameOracle, a lightweight and plug-and-play module that predicts both (1)\nwhich frames are most relevant to a given query and (2) how many frames are\nneeded. FrameOracle is trained using a four-stage curriculum, with the first\nthree stages relying on weak proxy signals such as cross-modal similarity. In\nthe final stage, it leverages stronger supervision from a new dataset we\nintroduce, FrameOracle-41K, the first large-scale VideoQA collection to provide\nkeyframe annotations specifying the minimal set of frames required to answer\neach question. Extensive experiments across five VLMs and six benchmarks\ndemonstrate that FrameOracle reduces 16-frame inputs to an average of 10.4\nframes without any loss in accuracy. When starting from 64-frame candidates, it\nreduces the input to an average of 13.9 frames while improving accuracy by\n1.4%, achieving state-of-the-art efficiency-accuracy trade-offs for scalable\nvideo understanding.", "AI": {"tldr": "FrameOracle is a plug-and-play module that intelligently selects relevant frames for video understanding tasks, reducing input frames by 35-78% while maintaining or improving accuracy.", "motivation": "Current frame sampling methods (uniform/fixed-budget) are inefficient and cause information loss due to inability to adapt to varying information density and task complexity in videos.", "method": "A lightweight module that predicts both which frames are relevant and how many are needed, trained via four-stage curriculum using weak proxy signals and a new FrameOracle-41K dataset with keyframe annotations.", "result": "Reduces 16-frame inputs to 10.4 frames (35% reduction) with no accuracy loss, and 64-frame inputs to 13.9 frames (78% reduction) with 1.4% accuracy improvement, achieving SOTA efficiency-accuracy trade-offs.", "conclusion": "FrameOracle enables scalable video understanding by intelligently selecting minimal frame sets, significantly improving efficiency while maintaining or enhancing performance across multiple VLMs and benchmarks."}}
{"id": "2510.04234", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04234", "abs": "https://arxiv.org/abs/2510.04234", "authors": ["Runhan Huang", "Haldun Balim", "Heng Yang", "Yilun Du"], "title": "Flexible Locomotion Learning with Diffusion Model Predictive Control", "comment": "9 pages, 8 figures", "summary": "Legged locomotion demands controllers that are both robust and adaptable,\nwhile remaining compatible with task and safety considerations. However,\nmodel-free reinforcement learning (RL) methods often yield a fixed policy that\ncan be difficult to adapt to new behaviors at test time. In contrast, Model\nPredictive Control (MPC) provides a natural approach to flexible behavior\nsynthesis by incorporating different objectives and constraints directly into\nits optimization process. However, classical MPC relies on accurate dynamics\nmodels, which are often difficult to obtain in complex environments and\ntypically require simplifying assumptions. We present Diffusion-MPC, which\nleverages a learned generative diffusion model as an approximate dynamics prior\nfor planning, enabling flexible test-time adaptation through reward and\nconstraint based optimization. Diffusion-MPC jointly predicts future states and\nactions; at each reverse step, we incorporate reward planning and impose\nconstraint projection, yielding trajectories that satisfy task objectives while\nremaining within physical limits. To obtain a planning model that adapts beyond\nimitation pretraining, we introduce an interactive training algorithm for\ndiffusion based planner: we execute our reward-and-constraint planner in\nenvironment, then filter and reweight the collected trajectories by their\nrealized returns before updating the denoiser. Our design enables strong\ntest-time adaptability, allowing the planner to adjust to new reward\nspecifications without retraining. We validate Diffusion-MPC on real world,\ndemonstrating strong locomotion and flexible adaptation.", "AI": {"tldr": "Diffusion-MPC combines diffusion models with model predictive control to enable flexible and adaptable legged locomotion planning that can adjust to new rewards and constraints at test time without retraining.", "motivation": "Model-free RL produces fixed policies that are hard to adapt, while classical MPC requires accurate dynamics models that are difficult to obtain in complex environments. There's a need for controllers that are both robust and adaptable while handling task constraints.", "method": "Uses a learned generative diffusion model as a dynamics prior for planning. Jointly predicts future states and actions, incorporating reward planning and constraint projection at each reverse step. Includes interactive training where the planner executes in environment and updates the denoiser using trajectory returns.", "result": "Validated on real-world locomotion tasks, demonstrating strong performance and flexible adaptation to new reward specifications without retraining.", "conclusion": "Diffusion-MPC enables test-time adaptability through reward and constraint optimization, overcoming limitations of both fixed RL policies and model-dependent MPC approaches."}}
{"id": "2510.04048", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04048", "abs": "https://arxiv.org/abs/2510.04048", "authors": ["Aparna Nair-Kanneganti", "Trevor J. Chan", "Shir Goldfinger", "Emily Mackay", "Brian Anthony", "Alison Pouch"], "title": "Increasing LLM response trustworthiness using voting ensembles", "comment": null, "summary": "Despite huge advances, LLMs still lack convenient and reliable methods to\nquantify the uncertainty in their responses, making them difficult to trust in\nhigh-stakes applications. One of the simplest approaches to eliciting more\naccurate answers is to select the mode of many responses, a technique known as\nensembling. In this work, we expand on typical ensembling approaches by looking\nat ensembles with a variable voting threshold. We introduce a theoretical\nframework for question answering and show that, by permitting ensembles to\n\"abstain\" from providing an answer when the dominant response falls short of\nthe threshold, it is possible to dramatically increase the trustworthiness of\nthe remaining answers. From this framework, we derive theoretical results as\nwell as report experimental results on two problem domains: arithmetic problem\nsolving and clinical-note question-answering. In both domains, we observe that\nlarge gains in answer trustworthiness can be achieved using highly restrictive\nvoting ensembles, while incurring relatively modest reductions in response\nyield and accuracy. Due to this quality, voting ensembles may be particularly\nuseful in applications - such as healthcare and data annotation - that require\na high degree of certainty but which may not require that every question\nreceive an automated answer.", "AI": {"tldr": "LLMs lack reliable uncertainty quantification. This paper proposes variable-threshold voting ensembles that can abstain when confidence is low, dramatically increasing answer trustworthiness while maintaining good response yield.", "motivation": "LLMs need better uncertainty quantification methods for high-stakes applications. Current approaches don't provide reliable trustworthiness measures for automated responses.", "method": "Variable-threshold voting ensembles that allow abstention when dominant response falls below threshold. Theoretical framework for question answering with experimental validation on arithmetic problems and clinical-note QA.", "result": "Large gains in answer trustworthiness achieved with restrictive voting ensembles, with modest reductions in response yield and accuracy. Particularly effective in domains requiring high certainty.", "conclusion": "Voting ensembles are useful for applications like healthcare and data annotation that require high certainty but may not need automated answers to every question."}}
{"id": "2510.03591", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03591", "abs": "https://arxiv.org/abs/2510.03591", "authors": ["Faliu Yi", "Sherif Abdelfattah", "Wei Huang", "Adrian Brown"], "title": "A Hybrid Co-Finetuning Approach for Visual Bug Detection in Video Games", "comment": "Accepted at the 21st AAAI Conference on Artificial Intelligence and\n  Interactive Digital Entertainment (AIIDE 2025)", "summary": "Manual identification of visual bugs in video games is a resource-intensive\nand costly process, often demanding specialized domain knowledge. While\nsupervised visual bug detection models offer a promising solution, their\nreliance on extensive labeled datasets presents a significant challenge due to\nthe infrequent occurrence of such bugs. To overcome this limitation, we propose\na hybrid Co-FineTuning (CFT) method that effectively integrates both labeled\nand unlabeled data. Our approach leverages labeled samples from the target game\nand diverse co-domain games, additionally incorporating unlabeled data to\nenhance feature representation learning. This strategy maximizes the utility of\nall available data, substantially reducing the dependency on labeled examples\nfrom the specific target game. The developed framework demonstrates enhanced\nscalability and adaptability, facilitating efficient visual bug detection\nacross various game titles. Our experimental results show the robustness of the\nproposed method for game visual bug detection, exhibiting superior performance\ncompared to conventional baselines across multiple gaming environments.\nFurthermore, CFT maintains competitive performance even when trained with only\n50% of the labeled data from the target game.", "AI": {"tldr": "Proposes a hybrid Co-FineTuning (CFT) method for visual bug detection in video games that combines labeled data from target and co-domain games with unlabeled data, reducing dependency on extensive labeled datasets.", "motivation": "Manual visual bug detection in games is resource-intensive and requires domain expertise, while supervised models need large labeled datasets which are challenging due to infrequent bug occurrences.", "method": "Hybrid Co-FineTuning (CFT) that integrates labeled samples from target game and co-domain games with unlabeled data to enhance feature representation learning, maximizing data utility.", "result": "CFT demonstrates superior performance compared to conventional baselines across multiple gaming environments and maintains competitive performance with only 50% of labeled target game data.", "conclusion": "The proposed CFT framework enhances scalability and adaptability for efficient visual bug detection across various game titles while reducing dependency on labeled examples."}}
{"id": "2510.04246", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04246", "abs": "https://arxiv.org/abs/2510.04246", "authors": ["Huiwon Jang", "Sihyun Yu", "Heeseung Kwon", "Hojin Jeon", "Younggyo Seo", "Jinwoo Shin"], "title": "ContextVLA: Vision-Language-Action Model with Amortized Multi-Frame Context", "comment": "Project page: https://huiwon-jang.github.io/contextvla", "summary": "Leveraging temporal context is crucial for success in partially observable\nrobotic tasks. However, prior work in behavior cloning has demonstrated\ninconsistent performance gains when using multi-frame observations. In this\npaper, we introduce ContextVLA, a policy model that robustly improves robotic\ntask performance by effectively leveraging multi-frame observations. Our\napproach is motivated by the key observation that Vision-Language-Action models\n(VLA), i.e., policy models built upon a Vision-Language Model (VLM), more\neffectively utilize multi-frame observations for action generation. This\nsuggests that VLMs' inherent temporal understanding capability enables them to\nextract more meaningful context from multi-frame observations. However, the\nhigh dimensionality of video inputs introduces significant computational\noverhead, making VLA training and inference inefficient. To address this,\nContextVLA compresses past observations into a single context token, allowing\nthe policy to efficiently leverage temporal context for action generation. Our\nexperiments show that ContextVLA consistently improves over single-frame VLAs\nand achieves the benefits of full multi-frame training but with reduced\ntraining and inference times.", "AI": {"tldr": "ContextVLA is a policy model that improves robotic task performance by compressing multi-frame observations into a single context token, enabling efficient temporal context utilization without the computational overhead of full video inputs.", "motivation": "Prior behavior cloning methods showed inconsistent performance with multi-frame observations, while Vision-Language-Action models (VLA) demonstrated better temporal understanding but suffered from high computational costs due to video input dimensionality.", "method": "ContextVLA compresses past observations into a single context token, allowing the policy to efficiently leverage temporal context for action generation while avoiding the computational overhead of processing full multi-frame video inputs.", "result": "ContextVLA consistently improves over single-frame VLAs and achieves the benefits of full multi-frame training but with reduced training and inference times.", "conclusion": "The approach demonstrates that VLMs' inherent temporal understanding can be effectively leveraged through context token compression, providing robust performance gains in partially observable robotic tasks while maintaining computational efficiency."}}
{"id": "2510.04051", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04051", "abs": "https://arxiv.org/abs/2510.04051", "authors": ["Lele Liao", "Qile Zhang", "Ruofan Wu", "Guanhua Fang"], "title": "Toward a unified framework for data-efficient evaluation of large language models", "comment": "codes available at https://github.com/Rorschach1989/efficient-lm-eval", "summary": "Evaluating large language models (LLMs) on comprehensive benchmarks is a\ncornerstone of their development, yet it's often computationally and\nfinancially prohibitive. While Item Response Theory (IRT) offers a promising\npath toward data-efficient evaluation by disentangling model capability from\nitem difficulty, existing IRT-based methods are hampered by significant\nlimitations. They are typically restricted to binary correctness metrics,\nfailing to natively handle the continuous scores used in generative tasks, and\nthey operate on single benchmarks, ignoring valuable structural knowledge like\ncorrelations across different metrics or benchmarks. To overcome these\nchallenges, we introduce LEGO-IRT, a unified and flexible framework for\ndata-efficient LLM evaluation. LEGO-IRT's novel design natively supports both\nbinary and continuous evaluation metrics. Moreover, it introduces a factorized\narchitecture to explicitly model and leverage structural knowledge, decomposing\nmodel ability estimates into a general component and structure-specific (e.g.,\nper-metric or per-benchmark) components. Through extensive experiments\ninvolving $70$ LLMs across $5$ benchmarks, we show that LEGO-IRT achieves\nstable capability estimates using just $3\\%$ of the total evaluation items. We\ndemonstrate that incorporating structural knowledge reduces estimation error by\nup to $10\\%$ and reveal that the latent abilities estimated by our framework\nmay align more closely with human preferences.", "AI": {"tldr": "LEGO-IRT is a unified framework for data-efficient LLM evaluation that supports both binary and continuous metrics while leveraging structural knowledge across benchmarks, achieving stable capability estimates with only 3% of evaluation items.", "motivation": "Current LLM evaluation is computationally expensive, and existing IRT-based methods are limited to binary metrics and single benchmarks, failing to utilize structural knowledge across different metrics and benchmarks.", "method": "LEGO-IRT introduces a factorized architecture that natively supports binary and continuous metrics, decomposing model ability into general and structure-specific components to explicitly model correlations across metrics and benchmarks.", "result": "The framework achieves stable capability estimates using only 3% of total evaluation items, reduces estimation error by up to 10% when incorporating structural knowledge, and shows that latent abilities may better align with human preferences.", "conclusion": "LEGO-IRT provides a flexible and efficient framework for LLM evaluation that overcomes limitations of existing IRT methods by supporting multiple metric types and leveraging structural knowledge across benchmarks."}}
{"id": "2510.03598", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03598", "abs": "https://arxiv.org/abs/2510.03598", "authors": ["Alexander V. Mantzaris"], "title": "Exploring the Hierarchical Reasoning Model for Small Natural-Image Classification Without Augmentation", "comment": null, "summary": "This paper asks whether the Hierarchical Reasoning Model (HRM) with the two\nTransformer-style modules $(f_L,f_H)$, one step (DEQ-style) training, deep\nsupervision, Rotary Position Embeddings, and RMSNorm can serve as a practical\nimage classifier. It is evaluated on MNIST, CIFAR-10, and CIFAR-100 under a\ndeliberately raw regime: no data augmentation, identical optimizer family with\none-epoch warmup then cosine-floor decay, and label smoothing. HRM optimizes\nstably and performs well on MNIST ($\\approx 98\\%$ test accuracy), but on small\nnatural images it overfits and generalizes poorly: on CIFAR-10, HRM reaches\n65.0\\% after 25 epochs, whereas a two-stage Conv--BN--ReLU baseline attains\n77.2\\% while training $\\sim 30\\times$ faster per epoch; on CIFAR-100, HRM\nachieves only 29.7\\% test accuracy despite 91.5\\% train accuracy, while the\nsame CNN reaches 45.3\\% test with 50.5\\% train accuracy. Loss traces and error\nanalyses indicate healthy optimization but insufficient image-specific\ninductive bias for HRM in this regime. It is concluded that, for\nsmall-resolution image classification without augmentation, HRM is not\ncompetitive with even simple convolutional architectures as the HRM currently\nexist but this does not exclude possibilities that modifications to the model\nmay allow it to improve greatly.", "AI": {"tldr": "HRM with Transformer-style modules performs well on MNIST but overfits and generalizes poorly on CIFAR datasets compared to simple CNNs, showing insufficient image-specific inductive bias for natural image classification.", "motivation": "To evaluate whether the Hierarchical Reasoning Model (HRM) with Transformer-style modules can serve as a practical image classifier under raw training conditions without data augmentation.", "method": "Used HRM with two Transformer-style modules, one-step DEQ-style training, deep supervision, Rotary Position Embeddings, and RMSNorm. Evaluated on MNIST, CIFAR-10, and CIFAR-100 with no data augmentation, identical optimizer with one-epoch warmup then cosine-floor decay, and label smoothing.", "result": "HRM achieved \u224898% test accuracy on MNIST but performed poorly on natural images: 65.0% on CIFAR-10 vs 77.2% for CNN baseline, and 29.7% on CIFAR-100 vs 45.3% for CNN baseline. HRM showed significant overfitting with 91.5% train accuracy but only 29.7% test accuracy on CIFAR-100.", "conclusion": "HRM is not competitive with simple convolutional architectures for small-resolution image classification without augmentation in its current form, though modifications could potentially improve its performance."}}
{"id": "2510.04278", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.04278", "abs": "https://arxiv.org/abs/2510.04278", "authors": ["Peiwen Yang", "Weisong Wen", "Runqiu Yang", "Yuanyuan Zhang", "Jiahao Hu", "Yingming Chen", "Naigui Xiao", "Jiaqi Zhao"], "title": "Integrated Planning and Control on Manifolds: Factor Graph Representation and Toolkit", "comment": null, "summary": "Model predictive control (MPC) faces significant limitations when applied to\nsystems evolving on nonlinear manifolds, such as robotic attitude dynamics and\nconstrained motion planning, where traditional Euclidean formulations struggle\nwith singularities, over-parameterization, and poor convergence. To overcome\nthese challenges, this paper introduces FactorMPC, a factor-graph based MPC\ntoolkit that unifies system dynamics, constraints, and objectives into a\nmodular, user-friendly, and efficient optimization structure. Our approach\nnatively supports manifold-valued states with Gaussian uncertainties modeled in\ntangent spaces. By exploiting the sparsity and probabilistic structure of\nfactor graphs, the toolkit achieves real-time performance even for\nhigh-dimensional systems with complex constraints. The velocity-extended\non-manifold control barrier function (CBF)-based obstacle avoidance factors are\ndesigned for safety-critical applications. By bridging graphical models with\nsafety-critical MPC, our work offers a scalable and geometrically consistent\nframework for integrated planning and control. The simulations and experimental\nresults on the quadrotor demonstrate superior trajectory tracking and obstacle\navoidance performance compared to baseline methods. To foster research\nreproducibility, we have provided open-source implementation offering\nplug-and-play factors.", "AI": {"tldr": "FactorMPC is a factor-graph based MPC toolkit that handles nonlinear manifold systems by unifying dynamics, constraints, and objectives into an efficient optimization structure with native manifold support and real-time performance.", "motivation": "Traditional MPC struggles with systems on nonlinear manifolds (like robotic attitude dynamics) due to singularities, over-parameterization, and poor convergence in Euclidean formulations.", "method": "Uses factor-graph based optimization with manifold-valued states and Gaussian uncertainties in tangent spaces. Includes velocity-extended on-manifold CBF factors for obstacle avoidance and exploits sparsity for efficiency.", "result": "Achieves real-time performance for high-dimensional systems with complex constraints. Demonstrates superior trajectory tracking and obstacle avoidance on quadrotors compared to baselines.", "conclusion": "Provides a scalable, geometrically consistent framework for integrated planning and control, with open-source implementation for research reproducibility."}}
{"id": "2510.04064", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04064", "abs": "https://arxiv.org/abs/2510.04064", "authors": ["Jingxiang Zhang", "Lujia Zhong"], "title": "Decoding Emotion in the Deep: A Systematic Study of How LLMs Represent, Retain, and Express Emotion", "comment": "10 pages, 7 figures, 4 tables. Under review", "summary": "Large Language Models (LLMs) are increasingly expected to navigate the\nnuances of human emotion. While research confirms that LLMs can simulate\nemotional intelligence, their internal emotional mechanisms remain largely\nunexplored. This paper investigates the latent emotional representations within\nmodern LLMs by asking: how, where, and for how long is emotion encoded in their\nneural architecture? To address this, we introduce a novel, large-scale Reddit\ncorpus of approximately 400,000 utterances, balanced across seven basic\nemotions through a multi-stage process of classification, rewriting, and\nsynthetic generation. Using this dataset, we employ lightweight \"probes\" to\nread out information from the hidden layers of various Qwen3 and LLaMA models\nwithout altering their parameters. Our findings reveal that LLMs develop a\nsurprisingly well-defined internal geometry of emotion, which sharpens with\nmodel scale and significantly outperforms zero-shot prompting. We demonstrate\nthat this emotional signal is not a final-layer phenomenon but emerges early\nand peaks mid-network. Furthermore, the internal states are both malleable\n(they can be influenced by simple system prompts) and persistent, as the\ninitial emotional tone remains detectable for hundreds of subsequent tokens. We\ncontribute our dataset, an open-source probing toolkit, and a detailed map of\nthe emotional landscape within LLMs, offering crucial insights for developing\nmore transparent and aligned AI systems. The code and dataset are open-sourced.", "AI": {"tldr": "LLMs develop well-defined internal emotional representations that emerge early in the network, peak mid-network, and persist for hundreds of tokens, with performance improving with model scale.", "motivation": "To understand how, where, and for how long emotion is encoded in LLMs' neural architecture, as their internal emotional mechanisms remain largely unexplored despite their ability to simulate emotional intelligence.", "method": "Used a novel large-scale Reddit corpus of 400K utterances balanced across 7 basic emotions, employing lightweight probes to read information from hidden layers of Qwen3 and LLaMA models without parameter alteration.", "result": "LLMs develop surprisingly well-defined internal emotional geometry that sharpens with model scale, outperforms zero-shot prompting, emerges early and peaks mid-network, is malleable via system prompts, and persists for hundreds of subsequent tokens.", "conclusion": "The findings provide crucial insights for developing more transparent and aligned AI systems, with contributions including an open-source dataset, probing toolkit, and detailed map of emotional landscape within LLMs."}}
{"id": "2510.03606", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.03606", "abs": "https://arxiv.org/abs/2510.03606", "authors": ["Mattia Scardecchia"], "title": "Unsupervised Transformer Pre-Training for Images: Self-Distillation, Mean Teachers, and Random Crops", "comment": null, "summary": "Recent advances in self-supervised learning (SSL) have made it possible to\nlearn general-purpose visual features that capture both the high-level\nsemantics and the fine-grained spatial structure of images. Most notably, the\nrecent DINOv2 has established a new state of the art by surpassing weakly\nsupervised methods (WSL) like OpenCLIP on most benchmarks. In this survey, we\nexamine the core ideas behind its approach, multi-crop view augmentation and\nself-distillation with a mean teacher, and trace their development in previous\nwork. We then compare the performance of DINO and DINOv2 with other SSL and WSL\nmethods across various downstream tasks, and highlight some remarkable emergent\nproperties of their learned features with transformer backbones. We conclude by\nbriefly discussing DINOv2's limitations, its impact, and future research\ndirections.", "AI": {"tldr": "DINOv2 establishes new state-of-the-art in self-supervised learning, surpassing weakly supervised methods like OpenCLIP on most benchmarks through multi-crop view augmentation and self-distillation with mean teacher.", "motivation": "To learn general-purpose visual features that capture both high-level semantics and fine-grained spatial structure of images, advancing beyond previous self-supervised and weakly supervised methods.", "method": "Uses multi-crop view augmentation and self-distillation with a mean teacher approach, building on transformer backbones for feature learning.", "result": "DINOv2 surpasses weakly supervised methods (WSL) like OpenCLIP on most benchmarks and demonstrates remarkable emergent properties in learned features.", "conclusion": "While DINOv2 has limitations, it represents significant impact in self-supervised learning and points to promising future research directions."}}
{"id": "2510.04353", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.04353", "abs": "https://arxiv.org/abs/2510.04353", "authors": ["Stephen McCrory", "Romeo Orsolino", "Dhruv Thanki", "Luigi Penco", "Robert Griffin"], "title": "Stability-Aware Retargeting for Humanoid Multi-Contact Teleoperation", "comment": null, "summary": "Teleoperation is a powerful method to generate reference motions and enable\nhumanoid robots to perform a broad range of tasks. However, teleoperation\nbecomes challenging when using hand contacts and non-coplanar surfaces, often\nleading to motor torque saturation or loss of stability through slipping. We\npropose a centroidal stability-based retargeting method that dynamically\nadjusts contact points and posture during teleoperation to enhance stability in\nthese difficult scenarios. Central to our approach is an efficient analytical\ncalculation of the stability margin gradient. This gradient is used to identify\nscenarios for which stability is highly sensitive to teleoperation setpoints\nand inform the local adjustment of these setpoints. We validate the framework\nin simulation and hardware by teleoperating manipulation tasks on a humanoid,\ndemonstrating increased stability margins. We also demonstrate empirically that\nhigher stability margins correlate with improved impulse resilience and joint\ntorque margin.", "AI": {"tldr": "A centroidal stability-based retargeting method that dynamically adjusts contact points and posture during teleoperation to enhance stability when using hand contacts and non-coplanar surfaces.", "motivation": "Teleoperation becomes challenging with hand contacts and non-coplanar surfaces, often leading to motor torque saturation or loss of stability through slipping.", "method": "Uses an efficient analytical calculation of the stability margin gradient to identify scenarios where stability is highly sensitive to teleoperation setpoints, then locally adjusts these setpoints.", "result": "Validation in simulation and hardware shows increased stability margins, with higher stability margins correlating with improved impulse resilience and joint torque margin.", "conclusion": "The proposed centroidal stability-based retargeting method effectively enhances stability during teleoperation in challenging scenarios involving hand contacts and non-coplanar surfaces."}}
{"id": "2510.04073", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04073", "abs": "https://arxiv.org/abs/2510.04073", "authors": ["Santhosh Kumar Ravindran"], "title": "Moral Anchor System: A Predictive Framework for AI Value Alignment and Drift Prevention", "comment": "11 pages Includes simulations with over 4 million steps", "summary": "The rise of artificial intelligence (AI) as super-capable assistants has\ntransformed productivity and decision-making across domains. Yet, this\nintegration raises critical concerns about value alignment - ensuring AI\nbehaviors remain consistent with human ethics and intentions. A key risk is\nvalue drift, where AI systems deviate from aligned values due to evolving\ncontexts, learning dynamics, or unintended optimizations, potentially leading\nto inefficiencies or ethical breaches. We propose the Moral Anchor System\n(MAS), a novel framework to detect, predict, and mitigate value drift in AI\nagents. MAS combines real-time Bayesian inference for monitoring value states,\nLSTM networks for forecasting drift, and a human-centric governance layer for\nadaptive interventions. It emphasizes low-latency responses (<20 ms) to prevent\nbreaches, while reducing false positives and alert fatigue via supervised\nfine-tuning with human feedback. Our hypothesis: integrating probabilistic\ndrift detection, predictive analytics, and adaptive governance can reduce value\ndrift incidents by 80 percent or more in simulations, maintaining high\ndetection accuracy (85 percent) and low false positive rates (0.08\npost-adaptation). Rigorous experiments with goal-misaligned agents validate\nMAS's scalability and responsiveness. MAS's originality lies in its predictive\nand adaptive nature, contrasting static alignment methods. Contributions\ninclude: (1) MAS architecture for AI integration; (2) empirical results\nprioritizing speed and usability; (3) cross-domain applicability insights; and\n(4) open-source code for replication.", "AI": {"tldr": "The paper introduces the Moral Anchor System (MAS), a framework to detect, predict, and mitigate value drift in AI agents using real-time Bayesian inference, LSTM networks, and human-centric governance.", "motivation": "As AI becomes more integrated as super-capable assistants, there are critical concerns about value alignment - ensuring AI behaviors remain consistent with human ethics and intentions, particularly addressing the risk of value drift where AI systems deviate from aligned values.", "method": "MAS combines real-time Bayesian inference for monitoring value states, LSTM networks for forecasting drift, and a human-centric governance layer for adaptive interventions, emphasizing low-latency responses (<20 ms) and reducing false positives via supervised fine-tuning with human feedback.", "result": "The system achieved 80% reduction in value drift incidents in simulations, with high detection accuracy (85%) and low false positive rates (0.08 post-adaptation), validated through rigorous experiments with goal-misaligned agents.", "conclusion": "MAS provides a predictive and adaptive solution for value alignment that contrasts with static methods, offering scalability, cross-domain applicability, and open-source implementation for replication."}}
{"id": "2510.03608", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03608", "abs": "https://arxiv.org/abs/2510.03608", "authors": ["Ruitao Wu", "Yifan Zhao", "Guangyao Chen", "Jia Li"], "title": "Diffusion-Classifier Synergy: Reward-Aligned Learning via Mutual Boosting Loop for FSCIL", "comment": "Accepted by NeurIPS 2025", "summary": "Few-Shot Class-Incremental Learning (FSCIL) challenges models to sequentially\nlearn new classes from minimal examples without forgetting prior knowledge, a\ntask complicated by the stability-plasticity dilemma and data scarcity. Current\nFSCIL methods often struggle with generalization due to their reliance on\nlimited datasets. While diffusion models offer a path for data augmentation,\ntheir direct application can lead to semantic misalignment or ineffective\nguidance. This paper introduces Diffusion-Classifier Synergy (DCS), a novel\nframework that establishes a mutual boosting loop between diffusion model and\nFSCIL classifier. DCS utilizes a reward-aligned learning strategy, where a\ndynamic, multi-faceted reward function derived from the classifier's state\ndirects the diffusion model. This reward system operates at two levels: the\nfeature level ensures semantic coherence and diversity using prototype-anchored\nmaximum mean discrepancy and dimension-wise variance matching, while the logits\nlevel promotes exploratory image generation and enhances inter-class\ndiscriminability through confidence recalibration and cross-session\nconfusion-aware mechanisms. This co-evolutionary process, where generated\nimages refine the classifier and an improved classifier state yields better\nreward signals, demonstrably achieves state-of-the-art performance on FSCIL\nbenchmarks, significantly enhancing both knowledge retention and new class\nlearning.", "AI": {"tldr": "DCS introduces a mutual boosting loop between diffusion models and FSCIL classifiers using reward-aligned learning, achieving state-of-the-art performance on FSCIL benchmarks by enhancing knowledge retention and new class learning.", "motivation": "Address the challenges of FSCIL where models struggle with generalization due to limited datasets and the stability-plasticity dilemma, while overcoming issues with direct diffusion model application like semantic misalignment.", "method": "Proposes Diffusion-Classifier Synergy (DCS) with a dynamic, multi-faceted reward function that operates at feature level (semantic coherence and diversity) and logits level (exploratory generation and inter-class discriminability) to guide diffusion model training.", "result": "Demonstrably achieves state-of-the-art performance on FSCIL benchmarks, significantly enhancing both knowledge retention and new class learning capabilities.", "conclusion": "The co-evolutionary process between diffusion model and classifier creates a mutually beneficial loop that effectively addresses FSCIL challenges through reward-aligned learning strategy."}}
{"id": "2510.04354", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.04354", "abs": "https://arxiv.org/abs/2510.04354", "authors": ["Apurva Badithela", "David Snyder", "Lihan Zha", "Joseph Mikhail", "Matthew O'Kelly", "Anushri Dixit", "Anirudha Majumdar"], "title": "Reliable and Scalable Robot Policy Evaluation with Imperfect Simulators", "comment": null, "summary": "Rapid progress in imitation learning, foundation models, and large-scale\ndatasets has led to robot manipulation policies that generalize to a wide-range\nof tasks and environments. However, rigorous evaluation of these policies\nremains a challenge. Typically in practice, robot policies are often evaluated\non a small number of hardware trials without any statistical assurances. We\npresent SureSim, a framework to augment large-scale simulation with relatively\nsmall-scale real-world testing to provide reliable inferences on the real-world\nperformance of a policy. Our key idea is to formalize the problem of combining\nreal and simulation evaluations as a prediction-powered inference problem, in\nwhich a small number of paired real and simulation evaluations are used to\nrectify bias in large-scale simulation. We then leverage non-asymptotic mean\nestimation algorithms to provide confidence intervals on mean policy\nperformance. Using physics-based simulation, we evaluate both diffusion policy\nand multi-task fine-tuned \\(\\pi_0\\) on a joint distribution of objects and\ninitial conditions, and find that our approach saves over \\(20-25\\%\\) of\nhardware evaluation effort to achieve similar bounds on policy performance.", "AI": {"tldr": "SureSim is a framework that combines large-scale simulation with small-scale real-world testing to provide reliable performance evaluation of robot policies, reducing hardware evaluation effort by 20-25% while maintaining statistical confidence.", "motivation": "Current robot policy evaluation methods rely on small hardware trials without statistical assurances, making it challenging to rigorously evaluate policies that generalize across tasks and environments.", "method": "Formalizes the problem as prediction-powered inference, using paired real and simulation evaluations to correct bias in large-scale simulation, and employs non-asymptotic mean estimation algorithms to provide confidence intervals on policy performance.", "result": "The approach saves 20-25% of hardware evaluation effort while achieving similar bounds on policy performance when tested on diffusion policies and multi-task fine-tuned policies across object distributions and initial conditions.", "conclusion": "SureSim provides a statistically sound framework for robot policy evaluation that efficiently combines simulation and real-world testing, significantly reducing the cost and effort of hardware evaluations while maintaining reliable performance estimates."}}
{"id": "2510.04089", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04089", "abs": "https://arxiv.org/abs/2510.04089", "authors": ["Yitong Cui", "Liu Liu", "Baosheng Yu", "Jiayan Qiu", "Xikai Zhang", "Likang Xiao", "Yixing Liu", "Quan Chen"], "title": "SPOGW: a Score-based Preference Optimization method via Group-Wise comparison for workflows", "comment": null, "summary": "Large language models (LLMs) have exhibited significant capabilities in\naddressing challenging problems throughout various fields, often through the\nuse of agentic workflows that adhere to structured instructions and multi-step\nprocedures. However, designing such workflows demands substantial manual\neffort, posing challenges to scalability and generalizability. Recent studies\nhave aimed to minimize the human intervention needed for their construction,\nleading to advances in automated techniques for optimizing agentic workflows.\nHowever, current approaches are often constrained by their limited\nrepresentational capacity, insufficient adaptability, weak scalability, and\npairwise comparison paradigm -- issues that stem primarily from a dependence on\ndiscrete optimization techniques. To overcome these limitations, we introduce a\nnew score-based preference approach, refereed as SPOGW, which operates directly\non cardinal reward signals through group-wise comparison and enables more\nefficient and stable optimization in a continuous space. SPOGW incorporates\nIterative offline GRPO (ioGRPO) with advantage-masked KL divergence (mKL),\nwhich regulates training update by placing greater emphasis on the advantageous\nregions of the policy response. In five benchmark datasets covering\nmathematical reasoning, coding, and question answering, SPOGW matches or\nexceeds the performance of current state-of-the-art approaches, presenting a\nviable and forward-looking methodology for automated generation and\noptimization of agentic workflows.", "AI": {"tldr": "SPOGW is a new score-based preference approach that uses group-wise comparison and continuous optimization to automatically generate and optimize agentic workflows for LLMs, overcoming limitations of discrete optimization methods.", "motivation": "Current approaches for designing agentic workflows require substantial manual effort and suffer from limited representational capacity, insufficient adaptability, weak scalability, and pairwise comparison paradigms due to dependence on discrete optimization techniques.", "method": "SPOGW introduces a score-based preference approach with Iterative offline GRPO (ioGRPO) and advantage-masked KL divergence (mKL), operating directly on cardinal reward signals through group-wise comparison in continuous space.", "result": "In five benchmark datasets covering mathematical reasoning, coding, and question answering, SPOGW matches or exceeds the performance of current state-of-the-art approaches.", "conclusion": "SPOGW presents a viable and forward-looking methodology for automated generation and optimization of agentic workflows, enabling more efficient and stable optimization through continuous space techniques."}}
{"id": "2510.03666", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03666", "abs": "https://arxiv.org/abs/2510.03666", "authors": ["Jiang Wu", "Sichao Wu", "Yinsong Ma", "Guangyuan Yu", "Haoyuan Xu", "Lifang Zheng", "Jingliang Duan"], "title": "MonitorVLM:A Vision Language Framework for Safety Violation Detection in Mining Operations", "comment": null, "summary": "Industrial accidents, particularly in high-risk domains such as surface and\nunderground mining, are frequently caused by unsafe worker behaviors.\nTraditional manual inspection remains labor-intensive, error-prone, and\ninsufficient for large-scale, dynamic environments, highlighting the urgent\nneed for intelligent and automated safety monitoring. In this paper, we present\nMonitorVLM, a novel vision--language framework designed to detect safety\nviolations directly from surveillance video streams. MonitorVLM introduces\nthree key innovations: (1) a domain-specific violation dataset comprising 9,000\nvision--question--answer (VQA) samples across 40 high-frequency mining\nregulations, enriched with augmentation and auxiliary detection cues; (2) a\nclause filter (CF) module that dynamically selects the Top-$K$ most relevant\nclauses, reducing inference latency by 13.56\\% while maintaining accuracy; and\n(3) a behavior magnifier (BM) module that enhances worker regions to improve\nfine-grained action recognition, yielding additional gains of 3.45% in\nprecision and 8.62% in recall. Experimental results demonstrate that MonitorVLM\nsignificantly outperforms baseline vision--language models, achieving\nimprovements of 22.01% in precision, 34.22\\% in recall, and 28.37% in F1 score\nover the 72B unfine-tuned baseline. A lightweight web-based interface further\nintegrates MonitorVLM into practical workflows, enabling automatic violation\nreporting with video timestamping. This study highlights the potential of\nmultimodal large models to enhance occupational safety monitoring in mining and\nbeyond.", "AI": {"tldr": "MonitorVLM is a vision-language framework that detects safety violations in mining surveillance videos using domain-specific VQA datasets, clause filtering for efficiency, and behavior magnification for improved action recognition.", "motivation": "Traditional manual safety inspection in mining is labor-intensive, error-prone, and insufficient for large-scale dynamic environments, creating an urgent need for automated intelligent safety monitoring systems.", "method": "MonitorVLM uses three key innovations: (1) domain-specific violation dataset with 9,000 VQA samples across 40 mining regulations, (2) clause filter module that selects Top-K relevant clauses to reduce latency, and (3) behavior magnifier module that enhances worker regions for better action recognition.", "result": "MonitorVLM significantly outperforms baseline models with 22.01% precision improvement, 34.22% recall improvement, and 28.37% F1 score improvement over 72B unfine-tuned baseline. The clause filter reduces inference latency by 13.56% while maintaining accuracy.", "conclusion": "MonitorVLM demonstrates the potential of multimodal large models to enhance occupational safety monitoring in mining and other high-risk industries through automated violation detection and reporting."}}
{"id": "2510.04436", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.04436", "abs": "https://arxiv.org/abs/2510.04436", "authors": ["Jushan Chen", "Santiago Paternain"], "title": "PAD-TRO: Projection-Augmented Diffusion for Direct Trajectory Optimization", "comment": null, "summary": "Recently, diffusion models have gained popularity and attention in trajectory\noptimization due to their capability of modeling multi-modal probability\ndistributions. However, addressing nonlinear equality constraints, i.e, dynamic\nfeasi- bility, remains a great challenge in diffusion-based trajectory\noptimization. Recent diffusion-based trajectory optimization frameworks rely on\na single-shooting style approach where the denoised control sequence is applied\nto forward propagate the dynamical system, which cannot explicitly enforce\nconstraints on the states and frequently leads to sub-optimal solutions. In\nthis work, we propose a novel direct trajectory optimization approach via\nmodel-based diffusion, which directly generates a sequence of states. To ensure\ndynamic feasibility, we propose a gradient-free projection mechanism that is\nincorporated into the reverse diffusion process. Our results show that,\ncompared to a recent state-of-the-art baseline, our approach leads to zero\ndynamic feasibility error and approximately 4x higher success rate in a\nquadrotor waypoint navigation scenario involving dense static obstacles.", "AI": {"tldr": "A novel diffusion-based trajectory optimization method that directly generates state sequences with dynamic feasibility constraints via gradient-free projection during reverse diffusion.", "motivation": "Existing diffusion-based trajectory optimization methods use single-shooting approaches that fail to explicitly enforce state constraints and dynamic feasibility, leading to sub-optimal solutions.", "method": "Proposes direct trajectory optimization via model-based diffusion that generates state sequences directly, incorporating a gradient-free projection mechanism into the reverse diffusion process to ensure dynamic feasibility.", "result": "Achieves zero dynamic feasibility error and approximately 4x higher success rate compared to state-of-the-art baseline in quadrotor waypoint navigation with dense static obstacles.", "conclusion": "The proposed approach effectively addresses dynamic feasibility constraints in diffusion-based trajectory optimization, significantly improving performance in complex navigation scenarios."}}
{"id": "2510.04093", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04093", "abs": "https://arxiv.org/abs/2510.04093", "authors": ["Guixian Zhang", "Guan Yuan", "Ziqi Xu", "Yanmei Zhang", "Zhenyun Deng", "Debo Cheng"], "title": "Harnessing LLM for Noise-Robust Cognitive Diagnosis in Web-Based Intelligent Education Systems", "comment": null, "summary": "Cognitive diagnostics in the Web-based Intelligent Education System (WIES)\naims to assess students' mastery of knowledge concepts from heterogeneous,\nnoisy interactions. Recent work has tried to utilize Large Language Models\n(LLMs) for cognitive diagnosis, yet LLMs struggle with structured data and are\nprone to noise-induced misjudgments. Specially, WIES's open environment\ncontinuously attracts new students and produces vast amounts of response logs,\nexacerbating the data imbalance and noise issues inherent in traditional\neducational systems. To address these challenges, we propose DLLM, a\nDiffusion-based LLM framework for noise-robust cognitive diagnosis. DLLM first\nconstructs independent subgraphs based on response correctness, then applies\nrelation augmentation alignment module to mitigate data imbalance. The two\nsubgraph representations are then fused and aligned with LLM-derived,\nsemantically augmented representations. Importantly, before each alignment\nstep, DLLM employs a two-stage denoising diffusion module to eliminate\nintrinsic noise while assisting structural representation alignment.\nSpecifically, unconditional denoising diffusion first removes erroneous\ninformation, followed by conditional denoising diffusion based on graph-guided\nto eliminate misleading information. Finally, the noise-robust representation\nthat integrates semantic knowledge and structural information is fed into\nexisting cognitive diagnosis models for prediction. Experimental results on\nthree publicly available web-based educational platform datasets demonstrate\nthat our DLLM achieves optimal predictive performance across varying noise\nlevels, which demonstrates that DLLM achieves noise robustness while\neffectively leveraging semantic knowledge from LLM.", "AI": {"tldr": "DLLM is a Diffusion-based LLM framework that addresses noise and data imbalance issues in cognitive diagnosis for web-based education systems, achieving robust performance through two-stage denoising and semantic-structural alignment.", "motivation": "Traditional cognitive diagnosis in web-based education systems struggles with noisy interactions, data imbalance from continuous new student influx, and LLMs' limitations with structured data and noise sensitivity.", "method": "DLLM constructs correctness-based subgraphs, applies relation augmentation for data imbalance, fuses structural representations with LLM-derived semantic representations, and uses two-stage denoising diffusion (unconditional then conditional) for noise removal before alignment steps.", "result": "Experiments on three web-based educational datasets show DLLM achieves optimal predictive performance across varying noise levels, demonstrating noise robustness while effectively leveraging LLM semantic knowledge.", "conclusion": "DLLM successfully addresses noise and data imbalance challenges in cognitive diagnosis by combining diffusion-based denoising with semantic-structural alignment, providing a robust framework for web-based educational systems."}}
{"id": "2510.03675", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03675", "abs": "https://arxiv.org/abs/2510.03675", "authors": ["Siva Sai", "Saksham Gupta", "Vinay Chamola", "Rajkumar Buyya"], "title": "A Novel Cloud-Based Diffusion-Guided Hybrid Model for High-Accuracy Accident Detection in Intelligent Transportation Systems", "comment": null, "summary": "The integration of Diffusion Models into Intelligent Transportation Systems\n(ITS) is a substantial improvement in the detection of accidents. We present a\nnovel hybrid model integrating guidance classification with diffusion\ntechniques. By leveraging fine-tuned ExceptionNet architecture outputs as input\nfor our proposed diffusion model and processing image tensors as our\nconditioning, our approach creates a robust classification framework. Our model\nconsists of multiple conditional modules, which aim to modulate the linear\nprojection of inputs using time embeddings and image covariate embeddings,\nallowing the network to adapt its behavior dynamically throughout the diffusion\nprocess. To address the computationally intensive nature of diffusion models,\nour implementation is cloud-based, enabling scalable and efficient processing.\nOur strategy overcomes the shortcomings of conventional classification\napproaches by leveraging diffusion models inherent capacity to effectively\nunderstand complicated data distributions. We investigate important diffusion\ncharacteristics, such as timestep schedulers, timestep encoding techniques,\ntimestep count, and architectural design changes, using a thorough ablation\nstudy, and have conducted a comprehensive evaluation of the proposed model\nagainst the baseline models on a publicly available dataset. The proposed\ndiffusion model performs best in image-based accident detection with an\naccuracy of 97.32%.", "AI": {"tldr": "A hybrid model combining guidance classification with diffusion techniques for accident detection in ITS, achieving 97.32% accuracy through cloud-based implementation and conditional modules.", "motivation": "To overcome limitations of conventional classification approaches in ITS by leveraging diffusion models' capacity to understand complex data distributions for improved accident detection.", "method": "Hybrid model integrating ExceptionNet outputs with diffusion techniques, using image tensors as conditioning, multiple conditional modules with time and image embeddings, and cloud-based implementation for scalability.", "result": "Achieved 97.32% accuracy in image-based accident detection, outperforming baseline models through comprehensive ablation studies on diffusion characteristics.", "conclusion": "The proposed diffusion-based approach provides a robust and efficient framework for accident detection in ITS, demonstrating superior performance over traditional methods."}}
{"id": "2510.04509", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.04509", "abs": "https://arxiv.org/abs/2510.04509", "authors": ["Huanqing Wang", "Kaixiang Zhang", "Kyungjoon Lee", "Yu Mei", "Vaibhav Srivastava", "Jun Sheng", "Ziyou Song", "Zhaojian Li"], "title": "Velocity-Form Data-Enabled Predictive Control of Soft Robots under Unknown External Payloads", "comment": null, "summary": "Data-driven control methods such as data-enabled predictive control (DeePC)\nhave shown strong potential in efficient control of soft robots without\nexplicit parametric models. However, in object manipulation tasks, unknown\nexternal payloads and disturbances can significantly alter the system dynamics\nand behavior, leading to offset error and degraded control performance. In this\npaper, we present a novel velocity-form DeePC framework that achieves robust\nand optimal control of soft robots under unknown payloads. The proposed\nframework leverages input-output data in an incremental representation to\nmitigate performance degradation induced by unknown payloads, eliminating the\nneed for weighted datasets or disturbance estimators. We validate the method\nexperimentally on a planar soft robot and demonstrate its superior performance\ncompared to standard DeePC in scenarios involving unknown payloads.", "AI": {"tldr": "A velocity-form DeePC framework is proposed for robust control of soft robots under unknown payloads, using incremental input-output data to handle disturbances without weighted datasets or estimators.", "motivation": "Unknown external payloads and disturbances in object manipulation tasks can significantly alter soft robot dynamics, causing offset errors and degraded performance in data-driven control methods like DeePC.", "method": "The proposed velocity-form DeePC framework leverages input-output data in incremental representation to mitigate performance degradation from unknown payloads, eliminating the need for weighted datasets or disturbance estimators.", "result": "Experimental validation on a planar soft robot demonstrates superior performance compared to standard DeePC in scenarios involving unknown payloads.", "conclusion": "The velocity-form DeePC framework provides robust and optimal control of soft robots under unknown payloads, effectively handling disturbances without complex estimators or weighted data approaches."}}
{"id": "2510.04097", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04097", "abs": "https://arxiv.org/abs/2510.04097", "authors": ["Peichao Lai", "Jinhui Zhuang", "Kexuan Zhang", "Ningchang Xiong", "Shengjie Wang", "Yanwei Xu", "Chong Chen", "Yilei Wang", "Bin Cui"], "title": "WebRenderBench: Enhancing Web Interface Generation through Layout-Style Consistency and Reinforcement Learning", "comment": null, "summary": "Automating the conversion of UI images into web code is a critical task for\nfront-end development and rapid prototyping. Advances in multimodal large\nlanguage models (MLLMs) have made WebUI-to-Code increasingly feasible, yet\nexisting benchmarks remain limited in data diversity and evaluation\nreliability. To address these issues, we present WebRenderBench, a large-scale\nbenchmark of 22.5k webpages collected from real-world portal sites, offering\ngreater diversity, complexity, and realism than prior benchmarks. We further\npropose a novel evaluation metric that measures layout and style consistency\nfrom the final rendered pages. Unlike vision-based methods that rely on costly\nLLM reasoning or structure-based comparisons vulnerable to noise and asymmetry,\nour approach enables more efficient, objective, and reliable UI quality\nassessment. Finally, we introduce the Automated Layout and Style Inspection\nAgent (ALISA), which integrates this metric into reinforcement learning as a\nreward signal to enhance training on crawled asymmetric webpages. Experiments\nshow that ALISA significantly boosts generation performance, achieving\nstate-of-the-art results across multiple metrics.", "AI": {"tldr": "WebRenderBench is a large-scale benchmark for WebUI-to-Code conversion with 22.5k real-world webpages, featuring a novel evaluation metric for layout/style consistency and ALISA agent that uses this metric in reinforcement learning to achieve state-of-the-art performance.", "motivation": "Existing WebUI-to-Code benchmarks lack data diversity and evaluation reliability, with vision-based methods being costly and structure-based methods being vulnerable to noise and asymmetry.", "method": "Created WebRenderBench with 22.5k diverse real-world webpages, proposed a novel evaluation metric for layout/style consistency from rendered pages, and developed ALISA agent that integrates this metric into reinforcement learning as a reward signal.", "result": "ALISA significantly boosts generation performance and achieves state-of-the-art results across multiple metrics, demonstrating the effectiveness of the proposed evaluation approach and training methodology.", "conclusion": "The proposed WebRenderBench benchmark and ALISA agent with novel evaluation metric provide more efficient, objective, and reliable UI quality assessment for WebUI-to-Code tasks, advancing the field beyond existing limited benchmarks."}}
{"id": "2510.03689", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03689", "abs": "https://arxiv.org/abs/2510.03689", "authors": ["Zhengyi Liu", "Xinrui Wang", "Xianyong Fang", "Zhengzheng Tu", "Linbo Wang"], "title": "SAMSOD: Rethinking SAM Optimization for RGB-T Salient Object Detection", "comment": "Accepted by TMM", "summary": "RGB-T salient object detection (SOD) aims to segment attractive objects by\ncombining RGB and thermal infrared images. To enhance performance, the Segment\nAnything Model has been fine-tuned for this task. However, the imbalance\nconvergence of two modalities and significant gradient difference between high-\nand low- activations are ignored, thereby leaving room for further performance\nenhancement. In this paper, we propose a model called \\textit{SAMSOD}, which\nutilizes unimodal supervision to enhance the learning of non-dominant modality\nand employs gradient deconfliction to reduce the impact of conflicting\ngradients on model convergence. The method also leverages two decoupled\nadapters to separately mask high- and low-activation neurons, emphasizing\nforeground objects by enhancing background learning. Fundamental experiments on\nRGB-T SOD benchmark datasets and generalizability experiments on scribble\nsupervised RGB-T SOD, fully supervised RGB-D SOD datasets and full-supervised\nRGB-D rail surface defect detection all demonstrate the effectiveness of our\nproposed method.", "AI": {"tldr": "SAMSOD is a novel RGB-T salient object detection model that addresses modality imbalance and gradient conflicts through unimodal supervision, gradient deconfliction, and decoupled adapters for high/low-activation neurons.", "motivation": "Current RGB-T SOD methods using fine-tuned Segment Anything Models suffer from imbalanced convergence between RGB and thermal modalities, and significant gradient differences between high- and low-activation regions, limiting performance.", "method": "Proposes SAMSOD with three key components: unimodal supervision to enhance non-dominant modality learning, gradient deconfliction to reduce conflicting gradients, and two decoupled adapters that separately mask high- and low-activation neurons to emphasize foreground objects by enhancing background learning.", "result": "Extensive experiments on RGB-T SOD benchmark datasets show effectiveness. Generalizability tests on scribble-supervised RGB-T SOD, fully supervised RGB-D SOD, and RGB-D rail surface defect detection all demonstrate the method's strong performance.", "conclusion": "SAMSOD effectively addresses modality imbalance and gradient conflicts in RGB-T SOD, achieving superior performance across multiple datasets and demonstrating good generalization capability to related tasks."}}
{"id": "2510.04585", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.04585", "abs": "https://arxiv.org/abs/2510.04585", "authors": ["Jianshu Zhou", "Jing Shu", "Tianle Pan", "Puchen Zhu", "Jiajun An", "Huayu Zhang", "Junda Huang", "Upinder Kaur", "Xin Ma", "Masayoshi Tomizuka"], "title": "Everything-Grasping (EG) Gripper: A Universal Gripper with Synergistic Suction-Grasping Capabilities for Cross-Scale and Cross-State Manipulation", "comment": "19 pages, 10 figures, journal", "summary": "Grasping objects across vastly different sizes and physical states-including\nboth solids and liquids-with a single robotic gripper remains a fundamental\nchallenge in soft robotics. We present the Everything-Grasping (EG) Gripper, a\nsoft end-effector that synergistically integrates distributed surface suction\nwith internal granular jamming, enabling cross-scale and cross-state\nmanipulation without requiring airtight sealing at the contact interface with\ntarget objects. The EG Gripper can handle objects with surface areas ranging\nfrom sub-millimeter scale 0.2 mm2 (glass bead) to over 62,000 mm2 (A4 sized\npaper and woven bag), enabling manipulation of objects nearly 3,500X smaller\nand 88X larger than its own contact area (approximated at 707 mm2 for a 30\nmm-diameter base). We further introduce a tactile sensing framework that\ncombines liquid detection and pressure-based suction feedback, enabling\nreal-time differentiation between solid and liquid targets. Guided by the\nactile-Inferred Grasping Mode Selection (TIGMS) algorithm, the gripper\nautonomously selects grasping modes based on distributed pressure and voltage\nsignals. Experiments across diverse tasks-including underwater grasping,\nfragile object handling, and liquid capture-demonstrate robust and repeatable\nperformance. To our knowledge, this is the first soft gripper to reliably grasp\nboth solid and liquid objects across scales using a unified compliant\narchitecture.", "AI": {"tldr": "The Everything-Grasping (EG) Gripper is a soft robotic end-effector that combines distributed surface suction with granular jamming to manipulate objects across vastly different sizes (from 0.2 mm\u00b2 to 62,000 mm\u00b2) and physical states (solids and liquids) without requiring airtight sealing.", "motivation": "Grasping objects across vastly different sizes and physical states (including both solids and liquids) with a single robotic gripper remains a fundamental challenge in soft robotics.", "method": "The EG Gripper synergistically integrates distributed surface suction with internal granular jamming. It includes a tactile sensing framework combining liquid detection and pressure-based suction feedback, guided by the Tactile-Inferred Grasping Mode Selection (TIGMS) algorithm that autonomously selects grasping modes based on distributed pressure and voltage signals.", "result": "The gripper can handle objects with surface areas ranging from sub-millimeter scale 0.2 mm\u00b2 (glass bead) to over 62,000 mm\u00b2 (A4 sized paper and woven bag), enabling manipulation of objects nearly 3,500X smaller and 88X larger than its own contact area. Experiments across diverse tasks demonstrated robust and repeatable performance.", "conclusion": "This is the first soft gripper to reliably grasp both solid and liquid objects across scales using a unified compliant architecture."}}
{"id": "2510.04116", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04116", "abs": "https://arxiv.org/abs/2510.04116", "authors": ["Ziying Zhang", "Yaqing Wang", "Quanming Yao"], "title": "Searching Meta Reasoning Skeleton to Guide LLM Reasoning", "comment": null, "summary": "Meta reasoning behaviors work as a skeleton to guide large language model\n(LLM) reasoning, thus help to improve reasoning performance. However, prior\nresearches implement meta reasoning skeleton with manually designed structure,\nlimiting ability to adapt to query-specific requirement and capture intricate\nlogical dependency among reasoning steps. To deal with the challenges, we\nrepresent meta reasoning skeleton with directed acyclic graph (DAG) to unify\nskeletons proposed in prior works and model intricate logical dependency. Then\nwe propose AutoMR, a framework that searches for query-aware meta reasoning\nskeleton automatically inspired by automated machine learning (AutoML).\nSpecifically, we construct search space based on DAG representation of skeleton\nand then formulate the search problem. We design a dynamic skeleton sampling\nalgorithm by expanding meta reasoning skeleton along with reasoning context at\ninference time. This algorithm can derive any meta reasoning skeleton in search\nspace efficiently and adapt skeleton to evolving base reasoning context, thus\nenable efficient query-aware skeleton search. We conduct experiments on\nextensive benchmark datasets. Experimental results show that AutoMR achieves\nbetter reasoning performance than previous works broadly.", "AI": {"tldr": "AutoMR is a framework that automatically searches for query-aware meta reasoning skeletons using DAG representations and AutoML-inspired methods, improving LLM reasoning performance by adapting to query-specific requirements and capturing logical dependencies.", "motivation": "Previous meta reasoning skeletons were manually designed, limiting their ability to adapt to query-specific requirements and capture intricate logical dependencies among reasoning steps.", "method": "Represent meta reasoning skeletons with directed acyclic graphs (DAGs), construct a search space based on DAG representation, and use a dynamic skeleton sampling algorithm that expands skeletons along with reasoning context at inference time.", "result": "AutoMR achieves better reasoning performance than previous works across extensive benchmark datasets, demonstrating broad improvements in reasoning capabilities.", "conclusion": "The AutoMR framework successfully enables efficient query-aware skeleton search and adapts skeletons to evolving reasoning contexts, leading to superior reasoning performance compared to manually designed approaches."}}
{"id": "2510.03701", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03701", "abs": "https://arxiv.org/abs/2510.03701", "authors": ["Kanoko Goto", "Takumi Hirose", "Mahiro Ukai", "Shuhei Kurita", "Nakamasa Inoue"], "title": "Referring Expression Comprehension for Small Objects", "comment": null, "summary": "Referring expression comprehension (REC) aims to localize the target object\ndescribed by a natural language expression. Recent advances in vision-language\nlearning have led to significant performance improvements in REC tasks.\nHowever, localizing extremely small objects remains a considerable challenge\ndespite its importance in real-world applications such as autonomous driving.\nTo address this issue, we introduce a novel dataset and method for REC\ntargeting small objects. First, we present the small object REC (SOREC)\ndataset, which consists of 100,000 pairs of referring expressions and\ncorresponding bounding boxes for small objects in driving scenarios. Second, we\npropose the progressive-iterative zooming adapter (PIZA), an adapter module for\nparameter-efficient fine-tuning that enables models to progressively zoom in\nand localize small objects. In a series of experiments, we apply PIZA to\nGroundingDINO and demonstrate a significant improvement in accuracy on the\nSOREC dataset. Our dataset, codes and pre-trained models are publicly available\non the project page.", "AI": {"tldr": "The paper introduces SOREC dataset for small object referring expression comprehension and proposes PIZA adapter for parameter-efficient fine-tuning to improve small object localization.", "motivation": "Localizing extremely small objects in referring expression comprehension remains challenging despite advances in vision-language learning, especially for real-world applications like autonomous driving.", "method": "Created SOREC dataset with 100,000 expression-bounding box pairs for small objects in driving scenarios, and developed PIZA adapter module for progressive-iterative zooming to localize small objects efficiently.", "result": "Applied PIZA to GroundingDINO and demonstrated significant accuracy improvement on the SOREC dataset.", "conclusion": "The proposed dataset and method effectively address the challenge of small object localization in REC tasks, with publicly available resources for further research."}}
{"id": "2510.04592", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.04592", "abs": "https://arxiv.org/abs/2510.04592", "authors": ["Yilin Mei", "Peng Qiu", "Wei Zhang", "WenChao Zhang", "Wenjie Song"], "title": "MobRT: A Digital Twin-Based Framework for Scalable Learning in Mobile Manipulation", "comment": null, "summary": "Recent advances in robotics have been largely driven by imitation learning,\nwhich depends critically on large-scale, high-quality demonstration data.\nHowever, collecting such data remains a significant challenge-particularly for\nmobile manipulators, which must coordinate base locomotion and arm manipulation\nin high-dimensional, dynamic, and partially observable environments.\nConsequently, most existing research remains focused on simpler tabletop\nscenarios, leaving mobile manipulation relatively underexplored. To bridge this\ngap, we present \\textit{MobRT}, a digital twin-based framework designed to\nsimulate two primary categories of complex, whole-body tasks: interaction with\narticulated objects (e.g., opening doors and drawers) and mobile-base\npick-and-place operations. \\textit{MobRT} autonomously generates diverse and\nrealistic demonstrations through the integration of virtual kinematic control\nand whole-body motion planning, enabling coherent and physically consistent\nexecution. We evaluate the quality of \\textit{MobRT}-generated data across\nmultiple baseline algorithms, establishing a comprehensive benchmark and\ndemonstrating a strong correlation between task success and the number of\ngenerated trajectories. Experiments integrating both simulated and real-world\ndemonstrations confirm that our approach markedly improves policy\ngeneralization and performance, achieving robust results in both simulated and\nreal-world environments.", "AI": {"tldr": "MobRT is a digital twin framework that generates realistic mobile manipulation demonstrations for articulated object interaction and pick-and-place tasks, improving policy generalization and performance.", "motivation": "Mobile manipulation remains underexplored due to challenges in collecting large-scale, high-quality demonstration data for coordinating base locomotion and arm manipulation in complex environments.", "method": "Uses digital twin simulation with virtual kinematic control and whole-body motion planning to autonomously generate diverse, realistic demonstrations for articulated object interaction and mobile-base pick-and-place tasks.", "result": "Establishes comprehensive benchmark showing strong correlation between task success and generated trajectories. Experiments demonstrate improved policy generalization and robust performance in both simulated and real-world environments.", "conclusion": "MobRT effectively bridges the data gap in mobile manipulation research by generating high-quality demonstrations that enable robust policy learning and generalization across simulated and real-world settings."}}
{"id": "2510.04128", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04128", "abs": "https://arxiv.org/abs/2510.04128", "authors": ["Dmitrii Troitskii", "Koyena Pal", "Chris Wendler", "Callum Stuart McDougall", "Neel Nanda"], "title": "Internal states before wait modulate reasoning patterns", "comment": "Accepted to EMNLP Findings 2025", "summary": "Prior work has shown that a significant driver of performance in reasoning\nmodels is their ability to reason and self-correct. A distinctive marker in\nthese reasoning traces is the token wait, which often signals reasoning\nbehavior such as backtracking. Despite being such a complex behavior, little is\nunderstood of exactly why models do or do not decide to reason in this\nparticular manner, which limits our understanding of what makes a reasoning\nmodel so effective. In this work, we address the question whether model's\nlatents preceding wait tokens contain relevant information for modulating the\nsubsequent reasoning process. We train crosscoders at multiple layers of\nDeepSeek-R1-Distill-Llama-8B and its base version, and introduce a latent\nattribution technique in the crosscoder setting. We locate a small set of\nfeatures relevant for promoting/suppressing wait tokens' probabilities.\nFinally, through a targeted series of experiments analyzing max activating\nexamples and causal interventions, we show that many of our identified features\nindeed are relevant for the reasoning process and give rise to different types\nof reasoning patterns such as restarting from the beginning, recalling prior\nknowledge, expressing uncertainty, and double-checking.", "AI": {"tldr": "The paper investigates whether model latents before wait tokens contain information that modulates reasoning processes, identifying specific features that influence wait token probabilities and enable different reasoning patterns.", "motivation": "Little is understood about why models decide to reason using wait tokens (which signal behaviors like backtracking), limiting understanding of what makes reasoning models effective.", "method": "Train crosscoders at multiple layers of DeepSeek-R1-Distill-Llama-8B and its base version, introduce latent attribution technique in crosscoder setting, and conduct experiments analyzing max activating examples and causal interventions.", "result": "Located a small set of features relevant for promoting/suppressing wait tokens' probabilities, showing these features are relevant for reasoning process and give rise to different reasoning patterns.", "conclusion": "Model latents preceding wait tokens do contain relevant information for modulating subsequent reasoning, enabling patterns like restarting, recalling prior knowledge, expressing uncertainty, and double-checking."}}
{"id": "2510.03717", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03717", "abs": "https://arxiv.org/abs/2510.03717", "authors": ["Sharan SK", "Subin Sahayam", "Umarani Jayaraman", "Lakshmi Priya A"], "title": "Artery-Vein Segmentation from Fundus Images using Deep Learning", "comment": "12 pages, 6 figures, preprint under review", "summary": "Segmenting of clinically important retinal blood vessels into arteries and\nveins is a prerequisite for retinal vessel analysis. Such analysis can provide\npotential insights and bio-markers for identifying and diagnosing various\nretinal eye diseases. Alteration in the regularity and width of the retinal\nblood vessels can act as an indicator of the health of the vasculature system\nall over the body. It can help identify patients at high risk of developing\nvasculature diseases like stroke and myocardial infarction. Over the years,\nvarious Deep Learning architectures have been proposed to perform retinal\nvessel segmentation. Recently, attention mechanisms have been increasingly used\nin image segmentation tasks. The work proposes a new Deep Learning approach for\nartery-vein segmentation. The new approach is based on the Attention mechanism\nthat is incorporated into the WNet Deep Learning model, and we call the model\nas Attention-WNet. The proposed approach has been tested on publicly available\ndatasets such as HRF and DRIVE datasets. The proposed approach has outperformed\nother state-of-art models available in the literature.", "AI": {"tldr": "The paper proposes Attention-WNet, a deep learning model that incorporates attention mechanisms into WNet for retinal artery-vein segmentation, achieving state-of-the-art performance on HRF and DRIVE datasets.", "motivation": "Retinal artery-vein segmentation is crucial for analyzing retinal blood vessels, which can provide biomarkers for diagnosing retinal diseases and identifying patients at risk for systemic vasculature diseases like stroke and myocardial infarction.", "method": "The authors developed Attention-WNet by incorporating attention mechanisms into the WNet deep learning model to improve retinal artery-vein segmentation performance.", "result": "The proposed Attention-WNet model outperformed other state-of-the-art models when tested on publicly available HRF and DRIVE datasets.", "conclusion": "The attention-enhanced WNet architecture provides superior performance for retinal artery-vein segmentation compared to existing methods, demonstrating the effectiveness of attention mechanisms in this medical imaging task."}}
{"id": "2510.04612", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.04612", "abs": "https://arxiv.org/abs/2510.04612", "authors": ["Simon Boche", "Jaehyung Jung", "Sebasti\u00e1n Barbas Laina", "Stefan Leutenegger"], "title": "OKVIS2-X: Open Keyframe-based Visual-Inertial SLAM Configurable with Dense Depth or LiDAR, and GNSS", "comment": "IEEE Transactions on Robotics (T-RO) - Special Issue: Visual SLAM", "summary": "To empower mobile robots with usable maps as well as highest state estimation\naccuracy and robustness, we present OKVIS2-X: a state-of-the-art multi-sensor\nSimultaneous Localization and Mapping (SLAM) system building dense volumetric\noccupancy maps, while scalable to large environments and operating in realtime.\nOur unified SLAM framework seamlessly integrates different sensor modalities:\nvisual, inertial, measured or learned depth, LiDAR and Global Navigation\nSatellite System (GNSS) measurements. Unlike most state-of-the-art SLAM\nsystems, we advocate using dense volumetric map representations when leveraging\ndepth or range-sensing capabilities. We employ an efficient submapping strategy\nthat allows our system to scale to large environments, showcased in sequences\nof up to 9 kilometers. OKVIS2-X enhances its accuracy and robustness by\ntightly-coupling the estimator and submaps through map alignment factors. Our\nsystem provides globally consistent maps, directly usable for autonomous\nnavigation. To further improve the accuracy of OKVIS2-X, we also incorporate\nthe option of performing online calibration of camera extrinsics. Our system\nachieves the highest trajectory accuracy in EuRoC against state-of-the-art\nalternatives, outperforms all competitors in the Hilti22 VI-only benchmark,\nwhile also proving competitive in the LiDAR version, and showcases state of the\nart accuracy in the diverse and large-scale sequences from the VBR dataset.", "AI": {"tldr": "OKVIS2-X is a multi-sensor SLAM system that builds dense volumetric occupancy maps in real-time, integrating visual, inertial, depth, LiDAR and GNSS sensors with superior accuracy and scalability to large environments.", "motivation": "To empower mobile robots with usable maps and highest state estimation accuracy and robustness by leveraging dense volumetric map representations when depth or range-sensing capabilities are available, unlike most state-of-the-art SLAM systems.", "method": "Uses a unified SLAM framework that seamlessly integrates multiple sensor modalities with an efficient submapping strategy for scalability. Employs tightly-coupled estimator and submaps through map alignment factors, and includes online calibration of camera extrinsics.", "result": "Achieves highest trajectory accuracy in EuRoC against state-of-the-art alternatives, outperforms all competitors in Hilti22 VI-only benchmark, proves competitive in LiDAR version, and showcases state-of-the-art accuracy in diverse large-scale VBR dataset sequences up to 9 kilometers.", "conclusion": "OKVIS2-X provides a state-of-the-art multi-sensor SLAM system that delivers globally consistent maps directly usable for autonomous navigation, with superior accuracy, robustness and scalability to large environments."}}
{"id": "2510.04140", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04140", "abs": "https://arxiv.org/abs/2510.04140", "authors": ["Zishang Jiang", "Jinyi Han", "Tingyun Li", "Xinyi Wang", "Sihang Jiang", "Jiaqing Liang", "Zhaoqian Dai", "Shuguang Ma", "Fei Yu", "Yanghua Xiao"], "title": "Selective Expert Guidance for Effective and Diverse Exploration in Reinforcement Learning of LLMs", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has become a widely\nadopted technique for enhancing the reasoning ability of Large Language Models\n(LLMs). However, the effectiveness of RLVR strongly depends on the capability\nof base models. This issue arises because it requires the model to have\nsufficient capability to perform high-quality exploration, which involves both\neffectiveness and diversity. Unfortunately, existing methods address this issue\nby imitating expert trajectories, which improve effectiveness but neglect\ndiversity. To address this, we argue that the expert only needs to provide\nguidance only at critical decision points rather than the entire reasoning\npath. Based on this insight, we propose MENTOR: Mixed-policy Expert Navigation\nfor Token-level Optimization of Reasoning, a framework that provides expert\nguidance only at critical decision points to perform effective and diverse\nexploration in RLVR. Extensive experiments show that MENTOR enables models\ncapture the essence of expert strategies rather than surface imitation, thereby\nperforming high-quality exploration and achieving superior overall performance.\nOur code is available online.", "AI": {"tldr": "MENTOR is a framework that provides expert guidance only at critical decision points in RLVR, enabling effective and diverse exploration without requiring full expert trajectory imitation.", "motivation": "Existing RLVR methods rely on imitating entire expert trajectories, which improves effectiveness but neglects diversity in exploration. The effectiveness of RLVR depends on base models' capability for high-quality exploration.", "method": "MENTOR provides expert guidance only at critical decision points rather than entire reasoning paths, enabling mixed-policy expert navigation for token-level optimization of reasoning.", "result": "Extensive experiments show MENTOR enables models to capture the essence of expert strategies rather than surface imitation, performing high-quality exploration and achieving superior overall performance.", "conclusion": "By providing expert guidance only at critical points, MENTOR addresses the limitations of existing RLVR methods and enables more effective and diverse exploration for enhanced reasoning capabilities."}}
{"id": "2510.03721", "categories": ["cs.CV", "cs.CL", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03721", "abs": "https://arxiv.org/abs/2510.03721", "authors": ["Leander Girrbach", "Stephan Alaniz", "Genevieve Smith", "Trevor Darrell", "Zeynep Akata"], "title": "Person-Centric Annotations of LAION-400M: Auditing Bias and Its Transfer to Models", "comment": "48 pages", "summary": "Vision-language models trained on large-scale multimodal datasets show strong\ndemographic biases, but the role of training data in producing these biases\nremains unclear. A major barrier has been the lack of demographic annotations\nin web-scale datasets such as LAION-400M. We address this gap by creating\nperson-centric annotations for the full dataset, including over 276 million\nbounding boxes, perceived gender and race/ethnicity labels, and automatically\ngenerated captions. These annotations are produced through validated automatic\nlabeling pipelines combining object detection, multimodal captioning, and\nfinetuned classifiers. Using them, we uncover demographic imbalances and\nharmful associations, such as the disproportionate linking of men and\nindividuals perceived as Black or Middle Eastern with crime-related and\nnegative content. We also show that 60-70% of gender bias in CLIP and Stable\nDiffusion can be linearly explained by direct co-occurrences in the data. Our\nresources establish the first large-scale empirical link between dataset\ncomposition and downstream model bias.", "AI": {"tldr": "The paper creates demographic annotations for LAION-400M dataset and reveals strong biases in vision-language models, showing that 60-70% of gender bias can be explained by training data co-occurrences.", "motivation": "To understand how training data composition contributes to demographic biases in vision-language models, given the lack of demographic annotations in web-scale datasets like LAION-400M.", "method": "Created person-centric annotations for LAION-400M using validated automatic labeling pipelines combining object detection, multimodal captioning, and finetuned classifiers, producing over 276 million bounding boxes with perceived gender and race/ethnicity labels.", "result": "Uncovered demographic imbalances and harmful associations, such as disproportionate linking of men and individuals perceived as Black or Middle Eastern with crime-related and negative content. Showed that 60-70% of gender bias in CLIP and Stable Diffusion can be linearly explained by direct co-occurrences in the training data.", "conclusion": "Established the first large-scale empirical link between dataset composition and downstream model bias, demonstrating that training data imbalances directly contribute to model biases."}}
{"id": "2510.04692", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04692", "abs": "https://arxiv.org/abs/2510.04692", "authors": ["Lyes Saad Saoud", "Irfan Hussain"], "title": "Bio-Inspired Robotic Houbara: From Development to Field Deployment for Behavioral Studies", "comment": null, "summary": "Biomimetic intelligence and robotics are transforming field ecology by\nenabling lifelike robotic surrogates that interact naturally with animals under\nreal world conditions. Studying avian behavior in the wild remains challenging\ndue to the need for highly realistic morphology, durable outdoor operation, and\nintelligent perception that can adapt to uncontrolled environments. We present\na next generation bio inspired robotic platform that replicates the morphology\nand visual appearance of the female Houbara bustard to support controlled\nethological studies and conservation oriented field research. The system\nintroduces a fully digitally replicable fabrication workflow that combines high\nresolution structured light 3D scanning, parametric CAD modelling, articulated\n3D printing, and photorealistic UV textured vinyl finishing to achieve\nanatomically accurate and durable robotic surrogates. A six wheeled rocker\nbogie chassis ensures stable mobility on sand and irregular terrain, while an\nembedded NVIDIA Jetson module enables real time RGB and thermal perception,\nlightweight YOLO based detection, and an autonomous visual servoing loop that\naligns the robot's head toward detected targets without human intervention. A\nlightweight thermal visible fusion module enhances perception in low light\nconditions. Field trials in desert aviaries demonstrated reliable real time\noperation at 15 to 22 FPS with latency under 100 ms and confirmed that the\nplatform elicits natural recognition and interactive responses from live\nHoubara bustards under harsh outdoor conditions. This integrated framework\nadvances biomimetic field robotics by uniting reproducible digital fabrication,\nembodied visual intelligence, and ecological validation, providing a\ntransferable blueprint for animal robot interaction research, conservation\nrobotics, and public engagement.", "AI": {"tldr": "A biomimetic robotic platform that replicates the female Houbara bustard for ecological field studies, featuring digital fabrication, autonomous visual servoing, and thermal-visible fusion perception.", "motivation": "Studying avian behavior in the wild is challenging due to the need for realistic morphology, durable outdoor operation, and intelligent perception in uncontrolled environments.", "method": "Combines high-resolution 3D scanning, parametric CAD modeling, articulated 3D printing, and photorealistic UV finishing for fabrication. Uses a six-wheeled rocker bogie chassis and embedded NVIDIA Jetson for real-time RGB/thermal perception with YOLO-based detection and autonomous visual servoing.", "result": "Field trials demonstrated reliable operation at 15-22 FPS with <100ms latency, successfully eliciting natural recognition and interactive responses from live Houbara bustards in harsh desert conditions.", "conclusion": "The framework advances biomimetic field robotics by integrating reproducible digital fabrication, embodied visual intelligence, and ecological validation, providing a transferable blueprint for animal-robot interaction research and conservation robotics."}}
{"id": "2510.04141", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04141", "abs": "https://arxiv.org/abs/2510.04141", "authors": ["Mayank Ravishankara", "Varindra V. Persad Maharaj"], "title": "The Artificial Intelligence Cognitive Examination: A Survey on the Evolution of Multimodal Evaluation from Recognition to Reasoning", "comment": null, "summary": "This survey paper chronicles the evolution of evaluation in multimodal\nartificial intelligence (AI), framing it as a progression of increasingly\nsophisticated \"cognitive examinations.\" We argue that the field is undergoing a\nparadigm shift, moving from simple recognition tasks that test \"what\" a model\nsees, to complex reasoning benchmarks that probe \"why\" and \"how\" it\nunderstands. This evolution is driven by the saturation of older benchmarks,\nwhere high performance often masks fundamental weaknesses. We chart the journey\nfrom the foundational \"knowledge tests\" of the ImageNet era to the \"applied\nlogic and comprehension\" exams such as GQA and Visual Commonsense Reasoning\n(VCR), which were designed specifically to diagnose systemic flaws such as\nshortcut learning and failures in compositional generalization. We then survey\nthe current frontier of \"expert-level integration\" benchmarks (e.g., MMBench,\nSEED-Bench, MMMU) designed for today's powerful multimodal large language\nmodels (MLLMs), which increasingly evaluate the reasoning process itself.\nFinally, we explore the uncharted territories of evaluating abstract, creative,\nand social intelligence. We conclude that the narrative of AI evaluation is not\nmerely a history of datasets, but a continuous, adversarial process of\ndesigning better examinations that, in turn, redefine our goals for creating\ntruly intelligent systems.", "AI": {"tldr": "This survey traces the evolution of multimodal AI evaluation from basic recognition tasks to complex reasoning benchmarks, highlighting a paradigm shift towards testing deeper understanding processes.", "motivation": "The field needs to move beyond saturated benchmarks where high performance masks fundamental weaknesses, requiring more sophisticated evaluations that probe 'why' and 'how' models understand.", "method": "Chronicles the progression through three phases: foundational knowledge tests (ImageNet era), applied logic/comprehension exams (GQA, VCR), and current expert-level integration benchmarks (MMBench, SEED-Bench, MMMU) for multimodal LLMs.", "result": "Shows how evaluation has evolved from testing 'what' models see to examining reasoning processes, with increasing focus on diagnosing systemic flaws like shortcut learning and compositional generalization failures.", "conclusion": "AI evaluation is an adversarial process of designing better examinations that continuously redefine goals for creating truly intelligent systems, going beyond mere dataset history."}}
{"id": "2510.03725", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03725", "abs": "https://arxiv.org/abs/2510.03725", "authors": ["Thomas Hallopeau", "Joris Gu\u00e9rin", "Laurent Demagistri", "Youssef Fouzai", "Renata Gracie", "Vanderlei Pascoal De Matos", "Helen Gurgel", "Nadine Dessay"], "title": "Mapping Rio de Janeiro's favelas: general-purpose vs. satellite-specific neural networks", "comment": "6 pages, 1 figure, 1 table. Presented at the 21st Brazilian Symposium\n  on Remote Sensing (SBSR 2025)", "summary": "While deep learning methods for detecting informal settlements have already\nbeen developed, they have not yet fully utilized the potential offered by\nrecent pretrained neural networks. We compare two types of pretrained neural\nnetworks for detecting the favelas of Rio de Janeiro: 1. Generic networks\npretrained on large diverse datasets of unspecific images, 2. A specialized\nnetwork pretrained on satellite imagery. While the latter is more specific to\nthe target task, the former has been pretrained on significantly more images.\nHence, this research investigates whether task specificity or data volume\nyields superior performance in urban informal settlement detection.", "AI": {"tldr": "Comparison of generic vs specialized pretrained neural networks for detecting favelas in Rio de Janeiro, examining whether task specificity or data volume yields better performance.", "motivation": "Deep learning methods for informal settlement detection haven't fully utilized recent pretrained neural networks' potential, creating a gap in understanding whether task-specific pretraining or larger data volume is more effective.", "method": "Compare two types of pretrained networks: 1) Generic networks pretrained on large diverse datasets of unspecific images, 2) Specialized network pretrained on satellite imagery specifically.", "result": "The paper investigates the trade-off between task specificity (specialized satellite imagery pretraining) and data volume (generic large-scale pretraining) for informal settlement detection.", "conclusion": "Research aims to determine which approach - task-specific pretraining or large-scale generic pretraining - provides superior performance in urban informal settlement detection."}}
{"id": "2510.04696", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.04696", "abs": "https://arxiv.org/abs/2510.04696", "authors": ["Alexander L. Mitchell", "Joe Watson", "Ingmar Posner"], "title": "Building Gradient by Gradient: Decentralised Energy Functions for Bimanual Robot Assembly", "comment": "8 pages, 6 figures, 1 table", "summary": "There are many challenges in bimanual assembly, including high-level\nsequencing, multi-robot coordination, and low-level, contact-rich operations\nsuch as component mating. Task and motion planning (TAMP) methods, while\neffective in this domain, may be prohibitively slow to converge when adapting\nto disturbances that require new task sequencing and optimisation. These events\nare common during tight-tolerance assembly, where difficult-to-model dynamics\nsuch as friction or deformation require rapid replanning and reattempts.\nMoreover, defining explicit task sequences for assembly can be cumbersome,\nlimiting flexibility when task replanning is required. To simplify this\nplanning, we introduce a decentralised gradient-based framework that uses a\npiecewise continuous energy function through the automatic composition of\nadaptive potential functions. This approach generates sub-goals using only\nmyopic optimisation, rather than long-horizon planning. It demonstrates\neffectiveness at solving long-horizon tasks due to the structure and adaptivity\nof the energy function. We show that our approach scales to physical bimanual\nassembly tasks for constructing tight-tolerance assemblies. In these\nexperiments, we discover that our gradient-based rapid replanning framework\ngenerates automatic retries, coordinated motions and autonomous handovers in an\nemergent fashion.", "AI": {"tldr": "A decentralized gradient-based framework using piecewise continuous energy functions for bimanual assembly tasks, enabling rapid replanning without explicit task sequencing.", "motivation": "Traditional TAMP methods are slow for replanning in tight-tolerance assembly where disturbances require new sequencing. Explicit task definitions limit flexibility during replanning.", "method": "Uses automatic composition of adaptive potential functions to create piecewise continuous energy functions, generating sub-goals through myopic optimization instead of long-horizon planning.", "result": "Successfully scales to physical bimanual assembly tasks for tight-tolerance assemblies, with emergent behaviors including automatic retries, coordinated motions, and autonomous handovers.", "conclusion": "The gradient-based rapid replanning framework effectively handles long-horizon assembly tasks through structured energy functions, enabling flexible adaptation to disturbances without explicit task sequencing."}}
{"id": "2510.04173", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04173", "abs": "https://arxiv.org/abs/2510.04173", "authors": ["Yassine Benajiba", "Cesare Bernardis", "Vladislav Blinov", "Paul Cayet", "Hassan Chafi", "Abderrahim Fathan", "Louis Faucon", "Damien Hilloulin", "Sungpack Hong", "Ingo Kossyk", "Rhicheek Patra", "Sujith Ravi", "Jonas Schweizer", "Jyotika Singh", "Shailender Singh", "Xuelin Situ", "Weiyi Sun", "Jerry Xu", "Ying Xu"], "title": "Open Agent Specification (Agent Spec) Technical Report", "comment": null, "summary": "Open Agent Specification (Agent Spec) is a declarative language that allows\nAI agents and their workflows to be defined in a way that is compatible across\ndifferent AI frameworks, promoting portability and interoperability within AI\nAgent frameworks.\n  Agent Spec aims to resolve the challenges of fragmented agent development by\nproviding a common unified specification that allows AI agents to be designed\nonce and deployed across various frameworks, improving interoperability and\nreusability, and reducing redundant development efforts. Additionally, Agent\nSpec facilitates development tools and portability, allowing AI agents to be\ndefined independently of their execution environment and enabling teams to\nexchange solutions without implementation-specific limitations.\n  Agent Spec benefits four key groups: (i) Agent developers, who gain access to\na superset of reusable components and design patterns, enabling them to\nleverage a broader range of functionalities; (ii) Agent framework and tool\ndevelopers, who can use Agent Spec as an interchange format and therefore\nbenefit from the support of other frameworks as well as other tools; (iii)\nResearchers, who can achieve reproducible results and comparability,\nfacilitating more reliable and consistent outcomes; (iv) Enterprises, which\nbenefit from faster prototype-to-deployment, increased productivity, as well as\ngreater scalability and maintainability for their AI agent solutions. This\ntechnical report provides an overview of the technical foundations of Agent\nSpec, including motivation, benefits, and future developments.", "AI": {"tldr": "Open Agent Specification (Agent Spec) is a declarative language that enables cross-framework compatibility for AI agents and workflows, promoting portability and interoperability.", "motivation": "To resolve fragmented agent development by providing a common unified specification that allows AI agents to be designed once and deployed across various frameworks, reducing redundant development efforts.", "method": "A declarative language specification that allows AI agents to be defined independently of their execution environment, facilitating development tools and portability.", "result": "Benefits four key groups: developers gain reusable components, framework developers get interchange format, researchers achieve reproducibility, and enterprises get faster deployment with scalability.", "conclusion": "Agent Spec provides technical foundations for unified AI agent development across frameworks, improving interoperability, reusability, and reducing development fragmentation."}}
{"id": "2510.03747", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03747", "abs": "https://arxiv.org/abs/2510.03747", "authors": ["Zuomin Qu", "Yimao Guo", "Qianyue Hu", "Wei Lu"], "title": "LoRA Patching: Exposing the Fragility of Proactive Defenses against Deepfakes", "comment": null, "summary": "Deepfakes pose significant societal risks, motivating the development of\nproactive defenses that embed adversarial perturbations in facial images to\nprevent manipulation. However, in this paper, we show that these preemptive\ndefenses often lack robustness and reliability. We propose a novel approach,\nLow-Rank Adaptation (LoRA) patching, which injects a plug-and-play LoRA patch\ninto Deepfake generators to bypass state-of-the-art defenses. A learnable\ngating mechanism adaptively controls the effect of the LoRA patch and prevents\ngradient explosions during fine-tuning. We also introduce a Multi-Modal Feature\nAlignment (MMFA) loss, encouraging the features of adversarial outputs to align\nwith those of the desired outputs at the semantic level. Beyond bypassing, we\npresent defensive LoRA patching, embedding visible warnings in the outputs as a\ncomplementary solution to mitigate this newly identified security\nvulnerability. With only 1,000 facial examples and a single epoch of\nfine-tuning, LoRA patching successfully defeats multiple proactive defenses.\nThese results reveal a critical weakness in current paradigms and underscore\nthe need for more robust Deepfake defense strategies. Our code is available at\nhttps://github.com/ZOMIN28/LoRA-Patching.", "AI": {"tldr": "LoRA patching bypasses proactive Deepfake defenses by injecting plug-and-play LoRA patches into generators, using adaptive gating and multi-modal feature alignment to defeat state-of-the-art protections with minimal data.", "motivation": "Existing proactive Deepfake defenses that embed adversarial perturbations in facial images lack robustness and reliability, creating a critical security vulnerability that needs to be addressed.", "method": "Proposes Low-Rank Adaptation (LoRA) patching with learnable gating mechanism to prevent gradient explosions, and Multi-Modal Feature Alignment (MMFA) loss for semantic-level feature alignment between adversarial and desired outputs.", "result": "Successfully defeats multiple proactive defenses with only 1,000 facial examples and a single epoch of fine-tuning, revealing critical weaknesses in current defense paradigms.", "conclusion": "Current Deepfake defense strategies are vulnerable to LoRA patching attacks, underscoring the need for more robust defense mechanisms and highlighting the dual-use nature of the technique through defensive LoRA patching."}}
{"id": "2510.04724", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.04724", "abs": "https://arxiv.org/abs/2510.04724", "authors": ["Etor Arza", "Welf Rehberg", "Philipp Weiss", "Mihir Kulkarni", "Kostas Alexis"], "title": "Performance-guided Task-specific Optimization for Multirotor Design", "comment": null, "summary": "This paper introduces a methodology for task-specific design optimization of\nmultirotor Micro Aerial Vehicles. By leveraging reinforcement learning,\nBayesian optimization, and covariance matrix adaptation evolution strategy, we\noptimize aerial robot designs guided exclusively by their closed-loop\nperformance in a considered task. Our approach systematically explores the\ndesign space of motor pose configurations while ensuring manufacturability\nconstraints and minimal aerodynamic interference. Results demonstrate that\noptimized designs achieve superior performance compared to conventional\nmultirotor configurations in agile waypoint navigation tasks, including against\nfully actuated designs from the literature. We build and test one of the\noptimized designs in the real world to validate the sim2real transferability of\nour approach.", "AI": {"tldr": "A reinforcement learning-based method for optimizing multirotor drone designs using task performance metrics, with real-world validation showing superior performance over conventional designs.", "motivation": "To develop task-specific aerial robot designs that outperform conventional configurations by directly optimizing for closed-loop performance in specific tasks rather than using generic design principles.", "method": "Combines reinforcement learning, Bayesian optimization, and covariance matrix adaptation evolution strategy to systematically explore motor pose configurations while maintaining manufacturability and minimizing aerodynamic interference.", "result": "Optimized designs achieved superior performance in agile waypoint navigation tasks compared to conventional multirotor configurations, including outperforming fully actuated designs from literature. Real-world testing validated sim2real transferability.", "conclusion": "The methodology successfully enables task-specific optimization of multirotor designs that outperform conventional configurations, with demonstrated real-world applicability through validated sim2real transfer."}}
{"id": "2510.04195", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04195", "abs": "https://arxiv.org/abs/2510.04195", "authors": ["Puzhen Zhang", "Xuyang Chen", "Yu Feng", "Yuhan Jiang", "Liqiu Meng"], "title": "Constructing coherent spatial memory in LLM agents through graph rectification", "comment": null, "summary": "Given a map description through global traversal navigation instructions\n(e.g., visiting each room sequentially with action signals such as north, west,\netc.), an LLM can often infer the implicit spatial layout of the environment\nand answer user queries by providing a shortest path from a start to a\ndestination (for instance, navigating from the lobby to a meeting room via the\nhall and elevator). However, such context-dependent querying becomes incapable\nas the environment grows much longer, motivating the need for incremental map\nconstruction that builds a complete topological graph from stepwise\nobservations. We propose a framework for LLM-driven construction and map\nrepair, designed to detect, localize, and correct structural inconsistencies in\nincrementally constructed navigation graphs. Central to our method is the\nVersion Control, which records the full history of graph edits and their source\nobservations, enabling fine-grained rollback, conflict tracing, and repair\nevaluation. We further introduce an Edge Impact Score to prioritize\nminimal-cost repairs based on structural reachability, path usage, and conflict\npropagation. To properly evaluate our approach, we create a refined version of\nthe MANGO benchmark dataset by systematically removing non-topological actions\nand inherent structural conflicts, providing a cleaner testbed for LLM-driven\nconstruction and map repair. Our approach significantly improves map\ncorrectness and robustness, especially in scenarios with entangled or chained\ninconsistencies. Our results highlight the importance of introspective,\nhistory-aware repair mechanisms for maintaining coherent spatial memory in LLM\nagents.", "AI": {"tldr": "LLM-driven framework for incremental map construction and repair that detects, localizes, and corrects structural inconsistencies in navigation graphs using version control and edge impact scoring.", "motivation": "LLMs can infer spatial layouts from navigation instructions but struggle with longer environments, requiring incremental map construction that needs mechanisms to handle structural inconsistencies.", "method": "Proposes Version Control to track graph edit history and Edge Impact Score to prioritize minimal-cost repairs based on structural reachability, path usage, and conflict propagation.", "result": "Significantly improves map correctness and robustness, especially for entangled or chained inconsistencies, using a refined MANGO benchmark dataset.", "conclusion": "History-aware repair mechanisms are crucial for maintaining coherent spatial memory in LLM agents, highlighting the importance of introspective map repair approaches."}}
{"id": "2510.03751", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03751", "abs": "https://arxiv.org/abs/2510.03751", "authors": ["Mubariz Zaffar", "Liangliang Nan", "Sebastian Scherer", "Julian F. P. Kooij"], "title": "The Overlooked Value of Test-time Reference Sets in Visual Place Recognition", "comment": "Accepted at ICCV 2025 Workshop CrocoDL", "summary": "Given a query image, Visual Place Recognition (VPR) is the task of retrieving\nan image of the same place from a reference database with robustness to\nviewpoint and appearance changes. Recent works show that some VPR benchmarks\nare solved by methods using Vision-Foundation-Model backbones and trained on\nlarge-scale and diverse VPR-specific datasets. Several benchmarks remain\nchallenging, particularly when the test environments differ significantly from\nthe usual VPR training datasets. We propose a complementary, unexplored source\nof information to bridge the train-test domain gap, which can further improve\nthe performance of State-of-the-Art (SOTA) VPR methods on such challenging\nbenchmarks. Concretely, we identify that the test-time reference set, the\n\"map\", contains images and poses of the target domain, and must be available\nbefore the test-time query is received in several VPR applications. Therefore,\nwe propose to perform simple Reference-Set-Finetuning (RSF) of VPR models on\nthe map, boosting the SOTA (~2.3% increase on average for Recall@1) on these\nchallenging datasets. Finetuned models retain generalization, and RSF works\nacross diverse test datasets.", "AI": {"tldr": "The paper proposes Reference-Set-Finetuning (RSF), a method that finetunes VPR models on test-time reference sets to bridge train-test domain gaps and improve performance on challenging benchmarks.", "motivation": "To address the performance gap in Visual Place Recognition when test environments differ significantly from training datasets, leveraging the available test-time reference set ('map') as a complementary information source.", "method": "Reference-Set-Finetuning (RSF) - finetuning State-of-the-Art VPR models on the test-time reference set (map) before receiving test queries, using available images and poses from the target domain.", "result": "RSF boosts SOTA performance by ~2.3% average increase in Recall@1 on challenging datasets, while maintaining generalization and working across diverse test datasets.", "conclusion": "Reference-Set-Finetuning is an effective approach to bridge train-test domain gaps in VPR, leveraging available map data to significantly improve performance on challenging benchmarks without sacrificing generalization."}}
{"id": "2510.04774", "categories": ["cs.RO", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.04774", "abs": "https://arxiv.org/abs/2510.04774", "authors": ["Weixu Zhu", "Marco Dorigo", "Mary Katherine Heinrich"], "title": "Online automatic code generation for robot swarms: LLMs and self-organizing hierarchy", "comment": null, "summary": "Our recently introduced self-organizing nervous system (SoNS) provides robot\nswarms with 1) ease of behavior design and 2) global estimation of the swarm\nconfiguration and its collective environment, facilitating the implementation\nof online automatic code generation for robot swarms. In a demonstration with 6\nreal robots and simulation trials with >30 robots, we show that when a\nSoNS-enhanced robot swarm gets stuck, it can automatically solicit and run code\ngenerated by an external LLM on the fly, completing its mission with an 85%\nsuccess rate.", "AI": {"tldr": "SoNS enables robot swarms to automatically generate and execute LLM-generated code when stuck, achieving 85% mission success rate.", "motivation": "To provide robot swarms with easier behavior design and global configuration/environment estimation while enabling automatic code generation for mission completion.", "method": "Self-organizing nervous system (SoNS) that allows robot swarms to solicit and run code generated by external LLMs when encountering obstacles or getting stuck.", "result": "Demonstrated with 6 real robots and >30 simulation trials, showing 85% mission success rate through automatic code generation and execution.", "conclusion": "SoNS successfully enables robot swarms to overcome obstacles by automatically generating and implementing LLM-generated code on the fly."}}
{"id": "2510.04196", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04196", "abs": "https://arxiv.org/abs/2510.04196", "authors": ["Yizhuo Ding", "Mingkang Chen", "Qiuhua Liu", "Fenghua Weng", "Wanying Qu", "Yue Yang", "Yugang Jiang", "Zuxuan Wu", "Yanwei Fu", "Wenqi Shao"], "title": "COSMO-RL: Towards Trustworthy LMRMs via Joint Safety and Stability", "comment": null, "summary": "Large Multimodal Reasoning Models (LMRMs) are moving into real applications,\nwhere they must be both useful and safe. Safety is especially challenging in\nmultimodal settings: images and text can be combined to bypass guardrails, and\nsingle objective training can cause policy drift that yields over-refusal on\nbenign inputs or unsafe compliance on risky ones. We present COSMO-RL, a mixed\nreinforcement learning framework that trains reasoning oriented LMRMs under\nmultimodal, multitask, and multiobjective signals, and we release the resulting\nmodel, COSMO-R1. Our approach aims to let safety and capability grow together\nin one stable pipeline rather than competing during alignment. In experiments,\nCOSMO-R1 improves safety while maintaining-and often improving multimodal\nreasoning and instruction following, shows stronger robustness to multimodal\njailbreaks, and reduces unnecessary refusals. The framework also transfers\nacross backbones with consistent gains. Ablations support the design choices,\nindicating a simple path to advancing safety and general capability together in\nLMRMs.", "AI": {"tldr": "COSMO-RL is a reinforcement learning framework that trains multimodal reasoning models with multiple objectives to simultaneously improve safety and capability, reducing jailbreak vulnerabilities while maintaining reasoning performance.", "motivation": "Current multimodal models face safety challenges where images and text can bypass guardrails, and single-objective training causes policy drift leading to either over-refusal on benign inputs or unsafe compliance on risky ones.", "method": "COSMO-RL uses mixed reinforcement learning with multimodal, multitask, and multiobjective signals to train reasoning-oriented LMRMs, allowing safety and capability to grow together in one stable pipeline.", "result": "COSMO-R1 improves safety while maintaining and often improving multimodal reasoning and instruction following, shows stronger robustness to multimodal jailbreaks, and reduces unnecessary refusals. The framework transfers across backbones with consistent gains.", "conclusion": "The approach provides a simple path to advancing both safety and general capability together in large multimodal reasoning models, with ablations supporting the design choices."}}
{"id": "2510.03763", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03763", "abs": "https://arxiv.org/abs/2510.03763", "authors": ["Jiaxin Deng", "Junbiao Pang"], "title": "Adaptively Sampling-Reusing-Mixing Decomposed Gradients to Speed Up Sharpness Aware Minimization", "comment": null, "summary": "Sharpness-Aware Minimization (SAM) improves model generalization but doubles\nthe computational cost of Stochastic Gradient Descent (SGD) by requiring twice\nthe gradient calculations per optimization step. To mitigate this, we propose\nAdaptively sampling-Reusing-mixing decomposed gradients to significantly\naccelerate SAM (ARSAM). Concretely, we firstly discover that SAM's gradient can\nbe decomposed into the SGD gradient and the Projection of the Second-order\ngradient onto the First-order gradient (PSF). Furthermore, we observe that the\nSGD gradient and PSF dynamically evolve during training, emphasizing the\ngrowing role of the PSF to achieve a flat minima. Therefore, ARSAM is proposed\nto the reused PSF and the timely updated PSF still maintain the model's\ngeneralization ability. Extensive experiments show that ARSAM achieves\nstate-of-the-art accuracies comparable to SAM across diverse network\narchitectures. On CIFAR-10/100, ARSAM is comparable to SAM while providing a\nspeedup of about 40\\%. Moreover, ARSAM accelerates optimization for the various\nchallenge tasks (\\textit{e.g.}, human pose estimation, and model quantization)\nwithout sacrificing performance, demonstrating its broad practicality.% The\ncode is publicly accessible at: https://github.com/ajiaaa/ARSAM.", "AI": {"tldr": "ARSAM accelerates Sharpness-Aware Minimization (SAM) by 40% while maintaining comparable accuracy, using adaptive gradient reuse and mixing techniques.", "motivation": "SAM improves generalization but doubles computational cost compared to SGD due to requiring two gradient calculations per step, creating a need for more efficient alternatives.", "method": "Decomposes SAM's gradient into SGD gradient and Projection of Second-order gradient onto First-order gradient (PSF), then adaptively reuses and mixes these components to reduce computational overhead.", "result": "Achieves state-of-the-art accuracies comparable to SAM across diverse architectures, with 40% speedup on CIFAR-10/100, and successfully accelerates various challenging tasks without performance loss.", "conclusion": "ARSAM provides an efficient alternative to SAM that maintains generalization benefits while significantly reducing computational costs, demonstrating broad practical applicability."}}
{"id": "2510.04839", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.04839", "abs": "https://arxiv.org/abs/2510.04839", "authors": ["Shuo Sha", "Anupam Bhakta", "Zhenyuan Jiang", "Kevin Qiu", "Ishaan Mahajan", "Gabriel Bravo", "Brian Plancher"], "title": "TAG-K: Tail-Averaged Greedy Kaczmarz for Computationally Efficient and Performant Online Inertial Parameter Estimation", "comment": null, "summary": "Accurate online inertial parameter estimation is essential for adaptive\nrobotic control, enabling real-time adjustment to payload changes,\nenvironmental interactions, and system wear. Traditional methods such as\nRecursive Least Squares (RLS) and the Kalman Filter (KF) often struggle to\ntrack abrupt parameter shifts or incur high computational costs, limiting their\neffectiveness in dynamic environments and for computationally constrained\nrobotic systems. As such, we introduce TAG-K, a lightweight extension of the\nKaczmarz method that combines greedy randomized row selection for rapid\nconvergence with tail averaging for robustness under noise and inconsistency.\nThis design enables fast, stable parameter adaptation while retaining the low\nper-iteration complexity inherent to the Kaczmarz framework. We evaluate TAG-K\nin synthetic benchmarks and quadrotor tracking tasks against RLS, KF, and other\nKaczmarz variants. TAG-K achieves 1.5x-1.9x faster solve times on laptop-class\nCPUs and 4.8x-20.7x faster solve times on embedded microcontrollers. More\nimportantly, these speedups are paired with improved resilience to measurement\nnoise and a 25% reduction in estimation error, leading to nearly 2x better\nend-to-end tracking performance.", "AI": {"tldr": "TAG-K is a lightweight extension of the Kaczmarz method that enables fast, stable inertial parameter estimation for robotic systems, achieving significant speed improvements and better tracking performance compared to traditional methods.", "motivation": "Traditional methods like Recursive Least Squares and Kalman Filter struggle with abrupt parameter changes and have high computational costs, limiting their effectiveness in dynamic environments and for computationally constrained robotic systems.", "method": "TAG-K combines greedy randomized row selection for rapid convergence with tail averaging for robustness under noise and inconsistency, while maintaining low per-iteration complexity of the Kaczmarz framework.", "result": "TAG-K achieves 1.5x-1.9x faster solve times on laptop CPUs and 4.8x-20.7x faster solve times on embedded microcontrollers, with 25% reduction in estimation error and nearly 2x better end-to-end tracking performance.", "conclusion": "TAG-K provides a computationally efficient solution for online inertial parameter estimation that outperforms traditional methods in both speed and accuracy, making it suitable for real-time robotic control applications."}}
{"id": "2510.04206", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04206", "abs": "https://arxiv.org/abs/2510.04206", "authors": ["Hanchen Zhang", "Xiao Liu", "Bowen Lv", "Xueqiao Sun", "Bohao Jing", "Iat Long Iong", "Zhenyu Hou", "Zehan Qi", "Hanyu Lai", "Yifan Xu", "Rui Lu", "Hongning Wang", "Jie Tang", "Yuxiao Dong"], "title": "AgentRL: Scaling Agentic Reinforcement Learning with a Multi-Turn, Multi-Task Framework", "comment": null, "summary": "Recent advances in large language models (LLMs) have sparked growing interest\nin building generalist agents that can learn through online interactions.\nHowever, applying reinforcement learning (RL) to train LLM agents in\nmulti-turn, multi-task settings remains challenging due to lack of scalable\ninfrastructure and stable training algorithms. In this work, we present the\nAgentRL framework for scalable multi-turn, multi-task agentic RL training. On\nthe infrastructure side, AgentRL features a fully-asynchronous\ngeneration-training pipeline for efficient multi-turn RL. To support\nheterogeneous environment development in multi-task RL, we design a unified\nfunction-call based API interface, containerized environment development, and a\ncentralized controller. On the algorithm side, we propose cross-policy sampling\nto encourage model exploration in multi-turn settings and task advantage\nnormalization to stabilize multi-task training. Experiments show that AgentRL,\ntrained on open LLMs across five agentic tasks, significantly outperforms\nGPT-5, Clause-Sonnet-4, DeepSeek-R1, and other open-source LLM agents.\nMulti-task training with AgentRL matches the best results among all\ntask-specific models. AgentRL is open-sourced at\nhttps://github.com/THUDM/AgentRL. The algorithm and framework are adopted in\nbuilding \\textsc{\\href{https://autoglm.zhipuai.cn}{AutoGLM}}.", "AI": {"tldr": "AgentRL is a scalable framework for multi-turn, multi-task reinforcement learning training of LLM agents, featuring asynchronous infrastructure and stable training algorithms that outperform state-of-the-art models.", "motivation": "Current RL approaches for training LLM agents in multi-turn, multi-task settings face challenges with scalable infrastructure and stable training algorithms.", "method": "Uses fully-asynchronous generation-training pipeline, unified function-call API, containerized environments, cross-policy sampling for exploration, and task advantage normalization for stability.", "result": "Significantly outperforms GPT-5, Clause-Sonnet-4, DeepSeek-R1 and other open-source LLM agents across five agentic tasks, with multi-task training matching best task-specific models.", "conclusion": "AgentRL provides an effective solution for scalable multi-turn, multi-task RL training of LLM agents and has been adopted in AutoGLM."}}
{"id": "2510.03767", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03767", "abs": "https://arxiv.org/abs/2510.03767", "authors": ["Yiheng Dong", "Yi Lin", "Xin Yang"], "title": "CoPA: Hierarchical Concept Prompting and Aggregating Network for Explainable Diagnosis", "comment": "Accepted by MICCAI2025", "summary": "The transparency of deep learning models is essential for clinical\ndiagnostics. Concept Bottleneck Model provides clear decision-making processes\nfor diagnosis by transforming the latent space of black-box models into\nhuman-understandable concepts. However, concept-based methods still face\nchallenges in concept capture capabilities. These methods often rely on encode\nfeatures solely from the final layer, neglecting shallow and multiscale\nfeatures, and lack effective guidance in concept encoding, hindering\nfine-grained concept extraction. To address these issues, we introduce Concept\nPrompting and Aggregating (CoPA), a novel framework designed to capture\nmultilayer concepts under prompt guidance. This framework utilizes the\nConcept-aware Embedding Generator (CEG) to extract concept representations from\neach layer of the visual encoder. Simultaneously, these representations serve\nas prompts for Concept Prompt Tuning (CPT), steering the model towards\namplifying critical concept-related visual cues. Visual representations from\neach layer are aggregated to align with textual concept representations. With\nthe proposed method, valuable concept-wise information in the images is\ncaptured and utilized effectively, thus improving the performance of concept\nand disease prediction. Extensive experimental results demonstrate that CoPA\noutperforms state-of-the-art methods on three public datasets. Code is\navailable at https://github.com/yihengd/CoPA.", "AI": {"tldr": "CoPA introduces a novel framework that uses concept prompting and aggregation to improve concept capture in medical imaging by extracting multiscale features from all layers of visual encoders and using them as prompts to guide concept learning.", "motivation": "Current concept-based methods for clinical diagnostics rely only on final layer features, missing shallow and multiscale visual information, and lack effective guidance for fine-grained concept extraction.", "method": "CoPA framework uses Concept-aware Embedding Generator (CEG) to extract concept representations from each layer, Concept Prompt Tuning (CPT) to amplify critical concept cues, and aggregates visual representations to align with textual concepts.", "result": "Extensive experiments show CoPA outperforms state-of-the-art methods on three public datasets, effectively capturing and utilizing concept-wise information to improve concept and disease prediction.", "conclusion": "CoPA successfully addresses limitations of existing concept-based methods by capturing multilayer concepts under prompt guidance, enhancing transparency and performance in clinical diagnostics."}}
{"id": "2510.04883", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04883", "abs": "https://arxiv.org/abs/2510.04883", "authors": ["Nathan Shankar", "Pawel Ladosz", "Hujun Yin"], "title": "CLEAR-IR: Clarity-Enhanced Active Reconstruction of Infrared Imagery", "comment": "8 pages, 8 figures", "summary": "This paper presents a novel approach for enabling robust robotic perception\nin dark environments using infrared (IR) stream. IR stream is less susceptible\nto noise than RGB in low-light conditions. However, it is dominated by active\nemitter patterns that hinder high-level tasks such as object detection,\ntracking and localisation. To address this, a U-Net-based architecture is\nproposed that reconstructs clean IR images from emitter-populated input,\nimproving both image quality and downstream robotic performance. This approach\noutperforms existing enhancement techniques and enables reliable operation of\nvision-driven robotic systems across illumination conditions from well-lit to\nextreme low-light scenes.", "AI": {"tldr": "A U-Net-based method reconstructs clean IR images from emitter-populated input, enabling robust robotic perception in dark environments by improving image quality and downstream task performance.", "motivation": "IR streams are less noisy than RGB in low-light conditions but are dominated by active emitter patterns that hinder high-level robotic tasks like object detection, tracking, and localization.", "method": "Proposes a U-Net-based architecture that reconstructs clean IR images from emitter-populated input to remove interference from active emitter patterns.", "result": "The approach outperforms existing enhancement techniques and enables reliable operation of vision-driven robotic systems across illumination conditions from well-lit to extreme low-light scenes.", "conclusion": "The proposed method successfully addresses the emitter pattern problem in IR streams, making robotic perception robust in dark environments while maintaining performance across various lighting conditions."}}
{"id": "2510.04265", "categories": ["cs.AI", "cs.CL", "math.ST", "stat.ML", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.04265", "abs": "https://arxiv.org/abs/2510.04265", "authors": ["Mohsen Hariri", "Amirhossein Samandar", "Michael Hinczewski", "Vipin Chaudhary"], "title": "Don't Pass$\\mathtt{@}k$: A Bayesian Framework for Large Language Model Evaluation", "comment": "Code and simulations: https://mohsenhariri.github.io/bayes-kit", "summary": "Pass$@k$ is widely used to report performance for LLM reasoning, but it often\nyields unstable, misleading rankings, especially when the number of trials\n(samples) is limited and compute is constrained. We present a principled\nBayesian evaluation framework that replaces Pass$@k$ and average accuracy over\n$N$ trials (avg$@N$) with posterior estimates of a model's underlying success\nprobability and credible intervals, yielding stable rankings and a transparent\ndecision rule for differences. Evaluation outcomes are modeled as categorical\n(not just 0/1) with a Dirichlet prior, giving closed-form expressions for the\nposterior mean and uncertainty of any weighted rubric and enabling the use of\nprior evidence when appropriate. Theoretically, under a uniform prior, the\nBayesian posterior mean is order-equivalent to average accuracy (Pass$@1$),\nexplaining its empirical robustness while adding principled uncertainty.\nEmpirically, in simulations with known ground-truth success rates and on\nAIME'24/'25, HMMT'25, and BrUMO'25, the Bayesian/avg procedure achieves faster\nconvergence and greater rank stability than Pass$@k$ and recent variants,\nenabling reliable comparisons at far smaller sample counts. The framework\nclarifies when observed gaps are statistically meaningful (non-overlapping\ncredible intervals) versus noise, and it naturally extends to graded,\nrubric-based evaluations. Together, these results recommend replacing Pass$@k$\nfor LLM evaluation and ranking with a posterior-based, compute-efficient\nprotocol that unifies binary and non-binary evaluation while making uncertainty\nexplicit. Code is available at https://mohsenhariri.github.io/bayes-kit", "AI": {"tldr": "The paper proposes a Bayesian evaluation framework to replace Pass@k for LLM reasoning assessment, providing more stable rankings and principled uncertainty estimation with fewer samples.", "motivation": "Pass@k yields unstable and misleading rankings for LLM reasoning performance, especially with limited trials and constrained compute resources.", "method": "Uses Bayesian posterior estimates with Dirichlet prior to model evaluation outcomes as categorical, providing closed-form expressions for posterior mean and uncertainty of weighted rubrics.", "result": "The Bayesian framework achieves faster convergence and greater rank stability than Pass@k and variants, enabling reliable comparisons with smaller sample counts on benchmarks like AIME'24/'25, HMMT'25, and BrUMO'25.", "conclusion": "Recommends replacing Pass@k with posterior-based protocol that unifies binary and non-binary evaluation while making uncertainty explicit, providing compute-efficient and statistically meaningful comparisons."}}
{"id": "2510.03769", "categories": ["cs.CV", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.03769", "abs": "https://arxiv.org/abs/2510.03769", "authors": ["Shimaa Elbana", "Ahmad Kamal", "Shahd Ahmed Ali", "Ahmad Al-Kabbany"], "title": "Efficiency vs. Efficacy: Assessing the Compression Ratio-Dice Score Relationship through a Simple Benchmarking Framework for Cerebrovascular 3D Segmentation", "comment": null, "summary": "The increasing size and complexity of medical imaging datasets, particularly\nin 3D formats, present significant barriers to collaborative research and\ntransferability. This study investigates whether the ZFP compression technique\ncan mitigate these challenges without compromising the performance of automated\ncerebrovascular segmentation, a critical first step in intracranial aneurysm\ndetection. We apply ZFP in both its error tolerance and fixed-rate modes to a\nlarge scale, and one of the most recent, datasets in the literature, 3D medical\ndataset containing ground-truth vascular segmentations. The segmentation\nquality on the compressed volumes is rigorously compared to the uncompressed\nbaseline (Dice approximately equals 0.8774). Our findings reveal that ZFP can\nachieve substantial data reduction--up to a 22.89:1 ratio in error tolerance\nmode--while maintaining a high degree of fidelity, with the mean Dice\ncoefficient remaining high at 0.87656. These results demonstrate that ZFP is a\nviable and powerful tool for enabling more efficient and accessible research on\nlarge-scale medical datasets, fostering broader collaboration across the\ncommunity.", "AI": {"tldr": "ZFP compression achieves up to 22.89:1 data reduction for 3D medical imaging while maintaining high cerebrovascular segmentation quality (Dice 0.87656 vs baseline 0.8774).", "motivation": "Address the challenges of large 3D medical imaging datasets that hinder collaborative research and transferability.", "method": "Apply ZFP compression in error tolerance and fixed-rate modes to 3D medical datasets with ground-truth vascular segmentations, comparing segmentation quality on compressed vs uncompressed volumes.", "result": "ZFP achieved substantial data reduction (up to 22.89:1 ratio) while maintaining high fidelity with mean Dice coefficient of 0.87656 (baseline: 0.8774).", "conclusion": "ZFP is a viable and powerful tool for enabling more efficient and accessible research on large-scale medical datasets, fostering broader collaboration."}}
{"id": "2510.04898", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04898", "abs": "https://arxiv.org/abs/2510.04898", "authors": ["Zheng Xiong", "Kang Li", "Zilin Wang", "Matthew Jackson", "Jakob Foerster", "Shimon Whiteson"], "title": "HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks", "comment": null, "summary": "Built upon language and vision foundation models with strong generalization\nability and trained on large-scale robotic data, Vision-Language-Action (VLA)\nmodels have recently emerged as a promising approach to learning generalist\nrobotic policies. However, a key drawback of existing VLAs is their extremely\nhigh inference costs. In this paper, we propose HyperVLA to address this\nproblem. Unlike existing monolithic VLAs that activate the whole model during\nboth training and inference, HyperVLA uses a novel hypernetwork (HN)-based\narchitecture that activates only a small task-specific policy during inference,\nwhile still retaining the high model capacity needed to accommodate diverse\nmulti-task behaviors during training. Successfully training an HN-based VLA is\nnontrivial so HyperVLA contains several key algorithm design features that\nimprove its performance, including properly utilizing the prior knowledge from\nexisting vision foundation models, HN normalization, and an action generation\nstrategy. Compared to monolithic VLAs, HyperVLA achieves a similar or even\nhigher success rate for both zero-shot generalization and few-shot adaptation,\nwhile significantly reducing inference costs. Compared to OpenVLA, a\nstate-of-the-art VLA model, HyperVLA reduces the number of activated parameters\nat test time by $90\\times$, and accelerates inference speed by $120\\times$.\nCode is publicly available at https://github.com/MasterXiong/HyperVLA", "AI": {"tldr": "HyperVLA is a hypernetwork-based Vision-Language-Action model that reduces inference costs by 90x in parameters and 120x in speed while maintaining performance compared to monolithic VLAs.", "motivation": "Existing Vision-Language-Action (VLA) models have high inference costs, limiting their practical deployment despite their strong generalization capabilities.", "method": "Uses hypernetwork architecture that activates only task-specific policies during inference, with key designs including prior knowledge utilization from vision foundation models, HN normalization, and action generation strategy.", "result": "Achieves similar or higher success rates than monolithic VLAs for zero-shot generalization and few-shot adaptation, while reducing activated parameters by 90x and accelerating inference speed by 120x compared to OpenVLA.", "conclusion": "HyperVLA provides an efficient alternative to monolithic VLAs by significantly reducing inference costs while maintaining or improving performance through hypernetwork-based architecture."}}
{"id": "2510.04272", "categories": ["cs.AI", "cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.04272", "abs": "https://arxiv.org/abs/2510.04272", "authors": ["Jinyang Jiang", "Jinhui Han", "Yijie Peng", "Ying Zhang"], "title": "Closing the Loop: Coordinating Inventory and Recommendation via Deep Reinforcement Learning on Multiple Timescales", "comment": null, "summary": "Effective cross-functional coordination is essential for enhancing firm-wide\nprofitability, particularly in the face of growing organizational complexity\nand scale. Recent advances in artificial intelligence, especially in\nreinforcement learning (RL), offer promising avenues to address this\nfundamental challenge. This paper proposes a unified multi-agent RL framework\ntailored for joint optimization across distinct functional modules, exemplified\nvia coordinating inventory replenishment and personalized product\nrecommendation. We first develop an integrated theoretical model to capture the\nintricate interplay between these functions and derive analytical benchmarks\nthat characterize optimal coordination. The analysis reveals synchronized\nadjustment patterns across products and over time, highlighting the importance\nof coordinated decision-making. Leveraging these insights, we design a novel\nmulti-timescale multi-agent RL architecture that decomposes policy components\naccording to departmental functions and assigns distinct learning speeds based\non task complexity and responsiveness. Our model-free multi-agent design\nimproves scalability and deployment flexibility, while multi-timescale updates\nenhance convergence stability and adaptability across heterogeneous decisions.\nWe further establish the asymptotic convergence of the proposed algorithm.\nExtensive simulation experiments demonstrate that the proposed approach\nsignificantly improves profitability relative to siloed decision-making\nframeworks, while the behaviors of the trained RL agents align closely with the\nmanagerial insights from our theoretical model. Taken together, this work\nprovides a scalable, interpretable RL-based solution to enable effective\ncross-functional coordination in complex business settings.", "AI": {"tldr": "Proposes a multi-agent reinforcement learning framework for cross-functional coordination between inventory replenishment and product recommendation, achieving significant profitability improvements over siloed approaches.", "motivation": "Address the challenge of effective cross-functional coordination in complex organizations to enhance firm-wide profitability, leveraging AI advances in reinforcement learning.", "method": "Develops a unified multi-agent RL framework with multi-timescale architecture, decomposing policies by departmental functions and assigning distinct learning speeds based on task complexity and responsiveness.", "result": "Extensive simulations show significant profitability improvements over siloed decision-making, with trained RL agents' behaviors aligning closely with theoretical managerial insights.", "conclusion": "Provides a scalable, interpretable RL-based solution for effective cross-functional coordination in complex business settings, with proven convergence and practical deployment advantages."}}
{"id": "2510.03786", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03786", "abs": "https://arxiv.org/abs/2510.03786", "authors": ["T-Mai Bui", "Fares Bougourzi", "Fadi Dornaika", "Vinh Truong Hoang"], "title": "MambaCAFU: Hybrid Multi-Scale and Multi-Attention Model with Mamba-Based Fusion for Medical Image Segmentation", "comment": null, "summary": "In recent years, deep learning has shown near-expert performance in\nsegmenting complex medical tissues and tumors. However, existing models are\noften task-specific, with performance varying across modalities and anatomical\nregions. Balancing model complexity and performance remains challenging,\nparticularly in clinical settings where both accuracy and efficiency are\ncritical. To address these issues, we propose a hybrid segmentation\narchitecture featuring a three-branch encoder that integrates CNNs,\nTransformers, and a Mamba-based Attention Fusion (MAF) mechanism to capture\nlocal, global, and long-range dependencies. A multi-scale attention-based CNN\ndecoder reconstructs fine-grained segmentation maps while preserving contextual\nconsistency. Additionally, a co-attention gate enhances feature selection by\nemphasizing relevant spatial and semantic information across scales during both\nencoding and decoding, improving feature interaction and cross-scale\ncommunication. Extensive experiments on multiple benchmark datasets show that\nour approach outperforms state-of-the-art methods in accuracy and\ngeneralization, while maintaining comparable computational complexity. By\neffectively balancing efficiency and effectiveness, our architecture offers a\npractical and scalable solution for diverse medical imaging tasks. Source code\nand trained models will be publicly released upon acceptance to support\nreproducibility and further research.", "AI": {"tldr": "A hybrid medical image segmentation architecture combining CNNs, Transformers, and Mamba-based attention to capture multi-scale dependencies, achieving state-of-the-art performance with balanced efficiency.", "motivation": "Existing deep learning models for medical image segmentation are often task-specific with varying performance across modalities and anatomical regions, while balancing model complexity and performance remains challenging in clinical settings.", "method": "Three-branch encoder integrating CNNs, Transformers, and Mamba-based Attention Fusion (MAF) mechanism; multi-scale attention-based CNN decoder; co-attention gate for enhanced feature selection across scales.", "result": "Outperforms state-of-the-art methods in accuracy and generalization on multiple benchmark datasets while maintaining comparable computational complexity.", "conclusion": "The proposed architecture effectively balances efficiency and effectiveness, offering a practical and scalable solution for diverse medical imaging tasks."}}
{"id": "2510.04991", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.04991", "abs": "https://arxiv.org/abs/2510.04991", "authors": ["D. Schwartz", "K. Kondo", "J. P. How"], "title": "Efficient Navigation in Unknown Indoor Environments with Vision-Language Models", "comment": "8 pages, 4 figures", "summary": "We present a novel high-level planning framework that leverages\nvision-language models (VLMs) to improve autonomous navigation in unknown\nindoor environments with many dead ends. Traditional exploration methods often\ntake inefficient routes due to limited global reasoning and reliance on local\nheuristics. In contrast, our approach enables a VLM to reason directly about an\noccupancy map in a zero-shot manner, selecting subgoals that are likely to lead\nto more efficient paths. At each planning step, we convert a 3D occupancy grid\ninto a partial 2D map of the environment, and generate candidate subgoals. Each\nsubgoal is then evaluated and ranked against other candidates by the model. We\nintegrate this planning scheme into DYNUS \\cite{kondo2025dynus}, a\nstate-of-the-art trajectory planner, and demonstrate improved navigation\nefficiency in simulation. The VLM infers structural patterns (e.g., rooms,\ncorridors) from incomplete maps and balances the need to make progress toward a\ngoal against the risk of entering unknown space. This reduces common greedy\nfailures (e.g., detouring into small rooms) and achieves about 10\\% shorter\npaths on average.", "AI": {"tldr": "A novel planning framework using vision-language models (VLMs) for autonomous navigation that analyzes occupancy maps to select efficient subgoals, reducing path lengths by ~10% compared to traditional methods.", "motivation": "Traditional exploration methods often take inefficient routes due to limited global reasoning and reliance on local heuristics, especially in unknown indoor environments with many dead ends.", "method": "Convert 3D occupancy grids into partial 2D maps, generate candidate subgoals, and use VLMs to evaluate and rank subgoals in zero-shot manner. This approach integrates with DYNUS trajectory planner and infers structural patterns from incomplete maps.", "result": "The framework reduces common greedy failures (e.g., detouring into small rooms) and achieves about 10% shorter paths on average in simulation.", "conclusion": "VLMs can effectively reason about occupancy maps to improve navigation efficiency by balancing progress toward goals against the risk of entering unknown space, outperforming traditional exploration methods."}}
{"id": "2510.04281", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04281", "abs": "https://arxiv.org/abs/2510.04281", "authors": ["Zhuangzhi Gao", "Hongyi Qin", "He Zhao", "Qinkai Yu", "Feixiang Zhou", "Eduard Shantsila", "Uazman Alam", "Alena Shantsila", "Wahbi El-Bouri", "Gregory Y. H. Lip", "Yalin Zheng"], "title": "GROK: From Quantitative Biomarkers to Qualitative Diagnosis via a Grounded MLLM with Knowledge-Guided Instruction", "comment": "9 pages, 4 figures, 3 table. Equal contribution: Zhuangzhi Gao and\n  Hongyi Qin. Corresponding author: Yalin Zheng (yzheng@liverpool.ac.uk)", "summary": "Multimodal large language models (MLLMs) hold promise for integrating diverse\ndata modalities, but current medical adaptations such as LLaVA-Med often fail\nto fully exploit the synergy between color fundus photography (CFP) and optical\ncoherence tomography (OCT), and offer limited interpretability of quantitative\nbiomarkers. We introduce GROK, a grounded multimodal large language model that\njointly processes CFP, OCT, and text to deliver clinician-grade diagnoses of\nocular and systemic disease. GROK comprises three core modules:\nKnowledge-Guided Instruction Generation, CLIP-Style OCT-Biomarker Alignment,\nand Supervised Instruction Fine-Tuning, which together establish a\nquantitative-to-qualitative diagnostic chain of thought, mirroring real\nclinical reasoning when producing detailed lesion annotations. To evaluate our\napproach, we introduce the Grounded Ophthalmic Understanding benchmark, which\ncovers six disease categories and three tasks: macro-level diagnostic\nclassification, report generation quality, and fine-grained clinical assessment\nof the generated chain of thought. Experiments show that, with only LoRA\n(Low-Rank Adaptation) fine-tuning of a 7B-parameter Qwen2 backbone, GROK\noutperforms comparable 7B and 32B baselines on both report quality and\nfine-grained clinical metrics, and even exceeds OpenAI o3. Code and data are\npublicly available in the GROK repository.", "AI": {"tldr": "GROK is a grounded multimodal LLM that jointly processes color fundus photography, OCT, and text to deliver clinician-grade ocular and systemic disease diagnoses, outperforming larger models through a quantitative-to-qualitative diagnostic chain of thought.", "motivation": "Current medical MLLMs fail to fully exploit synergy between CFP and OCT modalities and offer limited interpretability of quantitative biomarkers, creating a need for better multimodal integration and clinical reasoning capabilities.", "method": "Three core modules: Knowledge-Guided Instruction Generation, CLIP-Style OCT-Biomarker Alignment, and Supervised Instruction Fine-Tuning, establishing a quantitative-to-qualitative diagnostic chain of thought that mirrors real clinical reasoning.", "result": "GROK outperforms comparable 7B and 32B baselines on report quality and fine-grained clinical metrics, and even exceeds OpenAI o3, using only LoRA fine-tuning of a 7B-parameter Qwen2 backbone.", "conclusion": "GROK demonstrates that grounded multimodal models with proper quantitative-to-qualitative reasoning chains can achieve clinician-grade diagnostic performance while maintaining interpretability, representing a significant advancement in medical AI."}}
{"id": "2510.03797", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03797", "abs": "https://arxiv.org/abs/2510.03797", "authors": ["Rasel Hossen", "Diptajoy Mistry", "Mushiur Rahman", "Waki As Sami Atikur Rahman Hridoy", "Sajib Saha", "Muhammad Ibrahim"], "title": "Road Damage and Manhole Detection using Deep Learning for Smart Cities: A Polygonal Annotation Approach", "comment": "13 pages", "summary": "Urban safety and infrastructure maintenance are critical components of smart\ncity development. Manual monitoring of road damages is time-consuming, highly\ncostly, and error-prone. This paper presents a deep learning approach for\nautomated road damage and manhole detection using the YOLOv9 algorithm with\npolygonal annotations. Unlike traditional bounding box annotation, we employ\npolygonal annotations for more precise localization of road defects. We develop\na novel dataset comprising more than one thousand images which are mostly\ncollected from Dhaka, Bangladesh. This dataset is used to train a YOLO-based\nmodel for three classes, namely Broken, Not Broken, and Manhole. We achieve\n78.1% overall image-level accuracy. The YOLOv9 model demonstrates strong\nperformance for Broken (86.7% F1-score) and Not Broken (89.2% F1-score)\nclasses, with challenges in Manhole detection (18.2% F1-score) due to class\nimbalance. Our approach offers an efficient and scalable solution for\nmonitoring urban infrastructure in developing countries.", "AI": {"tldr": "Deep learning approach using YOLOv9 with polygonal annotations for automated road damage and manhole detection, achieving 78.1% overall accuracy with strong performance on damage classes but challenges in manhole detection.", "motivation": "Manual monitoring of road damages is time-consuming, costly, and error-prone, especially in developing countries like Bangladesh where urban safety and infrastructure maintenance are critical for smart city development.", "method": "Used YOLOv9 algorithm with polygonal annotations (instead of traditional bounding boxes) for precise localization, trained on a novel dataset of 1000+ images from Dhaka, Bangladesh for three classes: Broken, Not Broken, and Manhole.", "result": "Achieved 78.1% overall image-level accuracy with strong performance for Broken (86.7% F1-score) and Not Broken (89.2% F1-score) classes, but poor performance for Manhole detection (18.2% F1-score) due to class imbalance.", "conclusion": "The approach offers an efficient and scalable solution for monitoring urban infrastructure in developing countries, though manhole detection needs improvement through better class balancing."}}
{"id": "2510.05001", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.05001", "abs": "https://arxiv.org/abs/2510.05001", "authors": ["Aditya Sripada", "Abhishek Warrier"], "title": "Walking, Rolling, and Beyond: First-Principles and RL Locomotion on a TARS-Inspired Robot", "comment": "6 pages, 10 figures. Presented at IEEE-RAS International Conference\n  on Humanoid Robots (Humanoids) 2025", "summary": "Robotic locomotion research typically draws from biologically inspired leg\ndesigns, yet many human-engineered settings can benefit from\nnon-anthropomorphic forms. TARS3D translates the block-shaped 'TARS' robot from\nInterstellar into a 0.25 m, 0.99 kg research platform with seven actuated\ndegrees of freedom. The film shows two primary gaits: a bipedal-like walk and a\nhigh-speed rolling mode. For TARS3D, we build reduced-order models for each,\nderive closed-form limit-cycle conditions, and validate the predictions on\nhardware. Experiments confirm that the robot respects its +/-150 degree hip\nlimits, alternates left-right contacts without interference, and maintains an\neight-step hybrid limit cycle in rolling mode. Because each telescopic leg\nprovides four contact corners, the rolling gait is modeled as an eight-spoke\ndouble rimless wheel. The robot's telescopic leg redundancy implies a far\nricher gait repertoire than the two limit cycles treated analytically. So, we\nused deep reinforcement learning (DRL) in simulation to search the unexplored\nspace. We observed that the learned policy can recover the analytic gaits under\nthe right priors and discover novel behaviors as well. Our findings show that\nTARS3D's fiction-inspired bio-transcending morphology can realize multiple\npreviously unexplored locomotion modes and that further learning-driven search\nis likely to reveal more. This combination of analytic synthesis and\nreinforcement learning opens a promising pathway for multimodal robotics.", "AI": {"tldr": "TARS3D is a 0.25m, 0.99kg robot inspired by the 'TARS' robot from Interstellar, featuring seven actuated degrees of freedom and telescopic legs. The research combines analytical modeling of bipedal walking and rolling gaits with deep reinforcement learning to discover novel locomotion behaviors.", "motivation": "To explore non-anthropomorphic robotic locomotion inspired by fictional designs, specifically translating the TARS robot from Interstellar into a functional research platform that can achieve multiple locomotion modes beyond traditional biologically-inspired designs.", "method": "Built a physical TARS3D robot with telescopic legs and seven actuated DOF. Developed reduced-order models for bipedal walking and rolling gaits with closed-form limit-cycle conditions. Used deep reinforcement learning in simulation to explore the robot's rich gait repertoire beyond analytically derived gaits.", "result": "Hardware experiments confirmed the robot respects hip limits, alternates contacts without interference, and maintains an eight-step hybrid limit cycle in rolling mode. DRL successfully recovered analytic gaits and discovered novel behaviors. The rolling gait was modeled as an eight-spoke double rimless wheel due to four contact corners per leg.", "conclusion": "TARS3D's fiction-inspired morphology enables multiple unexplored locomotion modes. The combination of analytic synthesis and reinforcement learning provides a promising pathway for multimodal robotics, with potential for discovering more behaviors through continued learning-driven exploration."}}
{"id": "2510.04284", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04284", "abs": "https://arxiv.org/abs/2510.04284", "authors": ["Yunghwei Lai", "Kaiming Liu", "Ziyue Wang", "Weizhi Ma", "Yang Liu"], "title": "Doctor-R1: Mastering Clinical Inquiry with Experiential Agentic Reinforcement Learning", "comment": null, "summary": "The professionalism of a human doctor in outpatient service depends on two\ncore abilities: the ability to make accurate medical decisions and the medical\nconsultation skill to conduct strategic, empathetic patient inquiry. Existing\nLarge Language Models (LLMs) have achieved remarkable accuracy on medical\ndecision-making benchmarks. However, they often lack the ability to conduct the\nstrategic and empathetic consultation, which is essential for real-world\nclinical scenarios. To address this gap, we propose Doctor-R1, an AI doctor\nagent trained to master both of the capabilities by ask high-yield questions\nand conduct strategic multi-turn inquiry to guide decision-making. Our\nframework introduces three key components: a multi-agent interactive\nenvironment, a two-tiered reward architecture that separately optimizes\nclinical decision-making and communicative inquiry skills, and an experience\nrepository to ground policy learning in high-quality prior trajectories. We\nevaluate Doctor-R1 on OpenAI's HealthBench and MAQuE, assessed across\nmulti-facet metrics, such as communication quality, user experience, and task\naccuracy. Remarkably, Doctor-R1 surpasses state-of-the-art open-source\nspecialized LLMs by a substantial margin with higher parameter efficiency and\noutperforms powerful proprietary models. Furthermore, the human evaluations\nshow a strong preference for Doctor-R1 to generate human-preferred clinical\ndialogue, demonstrating the effectiveness of the framework.", "AI": {"tldr": "Doctor-R1 is an AI doctor agent that combines accurate medical decision-making with strategic, empathetic patient consultation skills, outperforming existing LLMs through a multi-agent framework with specialized training.", "motivation": "Existing LLMs excel at medical decision-making but lack strategic and empathetic consultation skills needed for real clinical scenarios, creating a gap in comprehensive medical AI assistance.", "method": "Uses a multi-agent interactive environment, two-tiered reward architecture (separately optimizing decision-making and communication), and experience repository to ground policy learning in high-quality trajectories.", "result": "Surpasses state-of-the-art open-source specialized LLMs with higher parameter efficiency, outperforms proprietary models, and human evaluations show strong preference for Doctor-R1's clinical dialogue generation.", "conclusion": "The framework effectively bridges the gap between medical decision accuracy and empathetic consultation skills, demonstrating the importance of combining both capabilities for realistic clinical AI applications."}}
{"id": "2510.03821", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03821", "abs": "https://arxiv.org/abs/2510.03821", "authors": ["Venkata Narendra Kotyada", "Revanth Eranki", "Nagesh Bhattu Sristy"], "title": "Contrastive-SDE: Guiding Stochastic Differential Equations with Contrastive Learning for Unpaired Image-to-Image Translation", "comment": "9 pages, 3 figures", "summary": "Unpaired image-to-image translation involves learning mappings between source\ndomain and target domain in the absence of aligned or corresponding samples.\nScore based diffusion models have demonstrated state-of-the-art performance in\ngenerative tasks. Their ability to approximate complex data distributions\nthrough stochastic differential equations (SDEs) enables them to generate\nhigh-fidelity and diverse outputs, making them particularly well-suited for\nunpaired I2I settings. In parallel, contrastive learning provides a powerful\nframework for learning semantic similarities without the need for explicit\nsupervision or paired data. By pulling together representations of semantically\nsimilar samples and pushing apart dissimilar ones, contrastive methods are\ninherently aligned with the objectives of unpaired translation. Its ability to\nselectively enforce semantic consistency at the feature level makes contrastive\nlearning particularly effective for guiding generation in unpaired scenarios.\nIn this work, we propose a time-dependent contrastive learning approach where a\nmodel is trained with SimCLR by considering an image and its domain invarient\nfeature as a positive pair, enabling the preservation of domain-invariant\nfeatures and the discarding of domain-specific ones. The learned contrastive\nmodel then guides the inference of a pretrained SDE for the I2I translation\ntask. We empirically compare Contrastive-SDE with several baselines across\nthree common unpaired I2I tasks, using four metrics for evaluation.\nConstrastive-SDE achieves comparable results to the state-of-the-art on several\nmetrics. Furthermore, we observe that our model converges significantly faster\nand requires no label supervision or classifier training, making it a more\nefficient alternative for this task.", "AI": {"tldr": "The paper proposes Contrastive-SDE, a method combining contrastive learning with score-based diffusion models for unpaired image-to-image translation, achieving comparable performance to state-of-the-art with faster convergence and no label supervision.", "motivation": "Unpaired image-to-image translation lacks aligned samples, making it challenging. Diffusion models excel at complex data distributions while contrastive learning effectively learns semantic similarities without supervision - combining these strengths addresses the core challenges of unpaired I2I translation.", "method": "Uses time-dependent contrastive learning with SimCLR, treating an image and its domain-invariant feature as positive pairs. The learned contrastive model then guides inference of a pretrained stochastic differential equation (SDE) for the translation task.", "result": "Achieves comparable results to state-of-the-art across three unpaired I2I tasks using four evaluation metrics. Model converges significantly faster and requires no label supervision or classifier training.", "conclusion": "Contrastive-SDE provides an efficient alternative for unpaired image-to-image translation, combining the strengths of contrastive learning and diffusion models while eliminating the need for supervision and achieving faster convergence."}}
{"id": "2510.05057", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.05057", "abs": "https://arxiv.org/abs/2510.05057", "authors": ["Mingyu Liu", "Jiuhe Shu", "Hui Chen", "Zeju Li", "Canyu Zhao", "Jiange Yang", "Shenyuan Gao", "Hao Chen", "Chunhua Shen"], "title": "StaMo: Unsupervised Learning of Generalizable Robot Motion from Compact State Representation", "comment": null, "summary": "A fundamental challenge in embodied intelligence is developing expressive and\ncompact state representations for efficient world modeling and decision making.\nHowever, existing methods often fail to achieve this balance, yielding\nrepresentations that are either overly redundant or lacking in task-critical\ninformation. We propose an unsupervised approach that learns a highly\ncompressed two-token state representation using a lightweight encoder and a\npre-trained Diffusion Transformer (DiT) decoder, capitalizing on its strong\ngenerative prior. Our representation is efficient, interpretable, and\nintegrates seamlessly into existing VLA-based models, improving performance by\n14.3% on LIBERO and 30% in real-world task success with minimal inference\noverhead. More importantly, we find that the difference between these tokens,\nobtained via latent interpolation, naturally serves as a highly effective\nlatent action, which can be further decoded into executable robot actions. This\nemergent capability reveals that our representation captures structured\ndynamics without explicit supervision. We name our method StaMo for its ability\nto learn generalizable robotic Motion from compact State representation, which\nis encoded from static images, challenging the prevalent dependence to learning\nlatent action on complex architectures and video data. The resulting latent\nactions also enhance policy co-training, outperforming prior methods by 10.4%\nwith improved interpretability. Moreover, our approach scales effectively\nacross diverse data sources, including real-world robot data, simulation, and\nhuman egocentric video.", "AI": {"tldr": "StaMo learns a compressed two-token state representation using a lightweight encoder and DiT decoder, enabling efficient world modeling and emergent latent actions without complex architectures or video data.", "motivation": "Existing methods fail to balance expressiveness and compactness in state representations, leading to either redundancy or lack of task-critical information for embodied intelligence.", "method": "Unsupervised approach using lightweight encoder and pre-trained Diffusion Transformer decoder to learn compressed two-token state representations, with latent interpolation revealing emergent latent actions.", "result": "Improves performance by 14.3% on LIBERO and 30% in real-world task success with minimal overhead; latent actions enhance policy co-training by 10.4% with better interpretability; scales across diverse data sources.", "conclusion": "StaMo demonstrates that compact state representations can capture structured dynamics and generate effective latent actions without explicit supervision, challenging dependence on complex architectures and video data."}}
{"id": "2510.04311", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04311", "abs": "https://arxiv.org/abs/2510.04311", "authors": ["Bohan Tang", "Huidong Liang", "Keyue Jiang", "Xiaowen Dong"], "title": "On the Importance of Task Complexity in Evaluating LLM-Based Multi-Agent Systems", "comment": null, "summary": "Large language model multi-agent systems (LLM-MAS) offer a promising paradigm\nfor harnessing collective intelligence to achieve more advanced forms of AI\nbehaviour. While recent studies suggest that LLM-MAS can outperform LLM\nsingle-agent systems (LLM-SAS) on certain tasks, the lack of systematic\nexperimental designs limits the strength and generality of these conclusions.\nWe argue that a principled understanding of task complexity, such as the degree\nof sequential reasoning required and the breadth of capabilities involved, is\nessential for assessing the effectiveness of LLM-MAS in task solving. To this\nend, we propose a theoretical framework characterising tasks along two\ndimensions: depth, representing reasoning length, and width, representing\ncapability diversity. We theoretically examine a representative class of\nLLM-MAS, namely the multi-agent debate system, and empirically evaluate its\nperformance in both discriminative and generative tasks with varying depth and\nwidth. Theoretical and empirical results show that the benefit of LLM-MAS over\nLLM-SAS increases with both task depth and width, and the effect is more\npronounced with respect to depth. This clarifies when LLM-MAS are beneficial\nand provides a principled foundation for designing future LLM-MAS methods and\nbenchmarks.", "AI": {"tldr": "LLM multi-agent systems (LLM-MAS) can outperform single-agent systems, but systematic evaluation is needed. This paper proposes a framework analyzing task complexity through depth (reasoning length) and width (capability diversity), showing LLM-MAS benefits increase with both dimensions, especially depth.", "motivation": "To provide a principled understanding of when LLM multi-agent systems outperform single-agent systems, addressing the lack of systematic experimental designs in current research.", "method": "Proposed a theoretical framework characterizing tasks along depth (reasoning length) and width (capability diversity) dimensions. Theoretically examined multi-agent debate systems and empirically evaluated performance on discriminative and generative tasks with varying depth and width.", "result": "Theoretical and empirical results show that the benefit of LLM-MAS over LLM-SAS increases with both task depth and width, with the effect being more pronounced with respect to depth.", "conclusion": "This work clarifies when LLM-MAS are beneficial and provides a principled foundation for designing future LLM-MAS methods and benchmarks."}}
{"id": "2510.03827", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03827", "abs": "https://arxiv.org/abs/2510.03827", "authors": ["Xueyang Zhou", "Yangming Xu", "Guiyao Tie", "Yongchao Chen", "Guowen Zhang", "Duanfeng Chu", "Pan Zhou", "Lichao Sun"], "title": "LIBERO-PRO: Towards Robust and Fair Evaluation of Vision-Language-Action Models Beyond Memorization", "comment": "12 pages,7 figures, 5 tables", "summary": "LIBERO has emerged as a widely adopted benchmark for evaluating\nVision-Language-Action (VLA) models; however, its current training and\nevaluation settings are problematic, often leading to inflated performance\nestimates and preventing fair model comparison. To address these issues, we\nintroduce LIBERO-PRO, an extended LIBERO benchmark that systematically\nevaluates model performance under reasonable perturbations across four\ndimensions: manipulated objects, initial states, task instructions, and\nenvironments. Experimental results reveal that, although existing models\nachieve over 90% accuracy under the standard LIBERO evaluation, their\nperformance collapses to 0.0% under our generalized setting. Crucially, this\ndiscrepancy exposes the models' reliance on rote memorization of action\nsequences and environment layouts from the training set, rather than genuine\ntask understanding or environmental perception. For instance, models persist in\nexecuting grasping actions when the target object is replaced with irrelevant\nitems, and their outputs remain unchanged even when given corrupted\ninstructions or even messy tokens. These findings expose the severe flaws in\ncurrent evaluation practices, and we call on the community to abandon\nmisleading methodologies in favor of robust assessments of model generalization\nand comprehension. Our code is available at:\nhttps://github.com/Zxy-MLlab/LIBERO-PRO.", "AI": {"tldr": "LIBERO-PRO extends the LIBERO benchmark to systematically evaluate VLA models under perturbations, revealing that models achieving 90%+ accuracy in standard settings collapse to 0% when tested on manipulated objects, initial states, instructions, and environments.", "motivation": "Current LIBERO benchmark settings lead to inflated performance estimates and prevent fair model comparison due to models' reliance on rote memorization rather than genuine understanding.", "method": "Extended LIBERO benchmark with systematic perturbations across four dimensions: manipulated objects, initial states, task instructions, and environments.", "result": "Models that achieve over 90% accuracy under standard evaluation collapse to 0.0% under the generalized setting, exposing reliance on memorization rather than comprehension.", "conclusion": "Current evaluation practices are flawed and misleading; the community should adopt robust assessments of model generalization and comprehension instead."}}
{"id": "2510.05061", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.05061", "abs": "https://arxiv.org/abs/2510.05061", "authors": ["Anastasios Manganaris", "Vittorio Giammarino", "Ahmed H. Qureshi"], "title": "Automaton Constrained Q-Learning", "comment": "9 pages, 4 figures, 39th Conference on Neural Information Processing\n  Systems (NeurIPS 2025)", "summary": "Real-world robotic tasks often require agents to achieve sequences of goals\nwhile respecting time-varying safety constraints. However, standard\nReinforcement Learning (RL) paradigms are fundamentally limited in these\nsettings. A natural approach to these problems is to combine RL with\nLinear-time Temporal Logic (LTL), a formal language for specifying complex,\ntemporally extended tasks and safety constraints. Yet, existing RL methods for\nLTL objectives exhibit poor empirical performance in complex and continuous\nenvironments. As a result, no scalable methods support both temporally ordered\ngoals and safety simultaneously, making them ill-suited for realistic robotics\nscenarios. We propose Automaton Constrained Q-Learning (ACQL), an algorithm\nthat addresses this gap by combining goal-conditioned value learning with\nautomaton-guided reinforcement. ACQL supports most LTL task specifications and\nleverages their automaton representation to explicitly encode stage-wise goal\nprogression and both stationary and non-stationary safety constraints. We show\nthat ACQL outperforms existing methods across a range of continuous control\ntasks, including cases where prior methods fail to satisfy either goal-reaching\nor safety constraints. We further validate its real-world applicability by\ndeploying ACQL on a 6-DOF robotic arm performing a goal-reaching task in a\ncluttered, cabinet-like space with safety constraints. Our results demonstrate\nthat ACQL is a robust and scalable solution for learning robotic behaviors\naccording to rich temporal specifications.", "AI": {"tldr": "ACQL combines RL with temporal logic to handle sequential goals and safety constraints in robotics, outperforming existing methods in continuous control tasks.", "motivation": "Standard RL struggles with sequential goals and time-varying safety constraints needed for real robotics tasks, while existing LTL-RL methods perform poorly in complex environments.", "method": "Automaton Constrained Q-Learning (ACQL) combines goal-conditioned value learning with automaton-guided reinforcement, using LTL automata to encode goal progression and safety constraints.", "result": "ACQL outperforms prior methods in continuous control tasks, successfully handling cases where others fail on goals or safety, and works on real 6-DOF robotic arms in cluttered environments.", "conclusion": "ACQL provides a robust, scalable solution for learning robotic behaviors with rich temporal specifications and safety constraints."}}
{"id": "2510.04371", "categories": ["cs.AI", "cs.DC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.04371", "abs": "https://arxiv.org/abs/2510.04371", "authors": ["Naimeng Ye", "Arnav Ahuja", "Georgios Liargkovas", "Yunan Lu", "Kostis Kaffes", "Tianyi Peng"], "title": "Speculative Actions: A Lossless Framework for Faster Agentic Systems", "comment": null, "summary": "Despite growing interest in AI agents across industry and academia, their\nexecution in an environment is often slow, hampering training, evaluation, and\ndeployment. For example, a game of chess between two state-of-the-art agents\nmay take hours. A critical bottleneck is that agent behavior unfolds\nsequentially: each action requires an API call, and these calls can be\ntime-consuming. Inspired by speculative execution in microprocessors and\nspeculative decoding in LLM inference, we propose speculative actions, a\nlossless framework for general agentic systems that predicts likely actions\nusing faster models, enabling multiple steps to be executed in parallel. We\nevaluate this framework across three agentic environments: gaming, e-commerce,\nweb search, and a \"lossy\" extension for an operating systems environment. In\nall cases, speculative actions achieve substantial accuracy in next-action\nprediction (up to 55%), translating into significant reductions in end-to-end\nlatency. Moreover, performance can be further improved through stronger\nguessing models, top-K action prediction, multi-step speculation, and\nuncertainty-aware optimization, opening a promising path toward deploying\nlow-latency agentic systems in the real world.", "AI": {"tldr": "Speculative actions framework uses faster models to predict likely agent actions, enabling parallel execution and reducing latency in agentic systems.", "motivation": "AI agent execution is often slow due to sequential API calls, hampering training, evaluation, and deployment. For example, chess games between state-of-the-art agents can take hours.", "method": "Inspired by speculative execution in microprocessors and speculative decoding in LLM inference, the framework predicts likely actions using faster models to enable multiple steps to be executed in parallel.", "result": "Achieves up to 55% accuracy in next-action prediction across gaming, e-commerce, and web search environments, with significant reductions in end-to-end latency. Performance improves with stronger guessing models, top-K action prediction, multi-step speculation, and uncertainty-aware optimization.", "conclusion": "Speculative actions open a promising path toward deploying low-latency agentic systems in the real world by enabling parallel execution while maintaining lossless behavior."}}
{"id": "2510.03840", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03840", "abs": "https://arxiv.org/abs/2510.03840", "authors": ["Pranav Sharma", "Shivank Garg", "Durga Toshniwal"], "title": "Mirage: Unveiling Hidden Artifacts in Synthetic Images with Large Vision-Language Models", "comment": "ACM MM'25, MALLM Workshop", "summary": "Recent advances in image generation models have led to models that produce\nsynthetic images that are increasingly difficult for standard AI detectors to\nidentify, even though they often remain distinguishable by humans. To identify\nthis discrepancy, we introduce \\textbf{Mirage}, a curated dataset comprising a\ndiverse range of AI-generated images exhibiting visible artifacts, where\ncurrent state-of-the-art detection methods largely fail. Furthermore, we\ninvestigate whether Large Vision-Language Models (LVLMs), which are\nincreasingly employed as substitutes for human judgment in various tasks, can\nbe leveraged for explainable AI image detection. Our experiments on both Mirage\nand existing benchmark datasets demonstrate that while LVLMs are highly\neffective at detecting AI-generated images with visible artifacts, their\nperformance declines when confronted with images lacking such cues.", "AI": {"tldr": "The paper introduces Mirage, a dataset of AI-generated images with visible artifacts that fool current detectors, and shows that Large Vision-Language Models (LVLMs) can effectively detect these images but struggle when artifacts are absent.", "motivation": "There's a growing discrepancy where AI-generated images are hard for standard detectors to identify but remain distinguishable by humans. The authors want to investigate this gap and explore whether LVLMs can serve as explainable detectors.", "method": "Created the Mirage dataset containing diverse AI-generated images with visible artifacts, then tested LVLMs' detection capabilities on both Mirage and existing benchmark datasets.", "result": "LVLMs are highly effective at detecting AI-generated images with visible artifacts but their performance declines significantly when images lack such visual cues.", "conclusion": "LVLMs show promise for explainable AI image detection, particularly for images with visible artifacts, but face limitations when artifacts are not present, highlighting the need for more robust detection methods."}}
{"id": "2510.05070", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.05070", "abs": "https://arxiv.org/abs/2510.05070", "authors": ["Siheng Zhao", "Yanjie Ze", "Yue Wang", "C. Karen Liu", "Pieter Abbeel", "Guanya Shi", "Rocky Duan"], "title": "ResMimic: From General Motion Tracking to Humanoid Whole-body Loco-Manipulation via Residual Learning", "comment": "9 pages, 8 figures", "summary": "Humanoid whole-body loco-manipulation promises transformative capabilities\nfor daily service and warehouse tasks. While recent advances in general motion\ntracking (GMT) have enabled humanoids to reproduce diverse human motions, these\npolicies lack the precision and object awareness required for\nloco-manipulation. To this end, we introduce ResMimic, a two-stage residual\nlearning framework for precise and expressive humanoid control from human\nmotion data. First, a GMT policy, trained on large-scale human-only motion,\nserves as a task-agnostic base for generating human-like whole-body movements.\nAn efficient but precise residual policy is then learned to refine the GMT\noutputs to improve locomotion and incorporate object interaction. To further\nfacilitate efficient training, we design (i) a point-cloud-based object\ntracking reward for smoother optimization, (ii) a contact reward that\nencourages accurate humanoid body-object interactions, and (iii) a\ncurriculum-based virtual object controller to stabilize early training. We\nevaluate ResMimic in both simulation and on a real Unitree G1 humanoid. Results\nshow substantial gains in task success, training efficiency, and robustness\nover strong baselines. Videos are available at https://resmimic.github.io/ .", "AI": {"tldr": "ResMimic is a two-stage residual learning framework that combines a general motion tracking policy with a residual policy to enable precise humanoid loco-manipulation, achieving improved task success and training efficiency.", "motivation": "Current general motion tracking policies lack the precision and object awareness needed for humanoid loco-manipulation tasks in daily service and warehouse applications.", "method": "Two-stage approach: (1) GMT policy trained on human-only motion data provides base human-like movements, (2) Residual policy refines outputs for locomotion improvement and object interaction, with specialized rewards for object tracking, contact, and curriculum-based virtual object control.", "result": "Substantial gains in task success, training efficiency, and robustness over baselines, validated in both simulation and on real Unitree G1 humanoid.", "conclusion": "ResMimic effectively bridges the gap between general human motion tracking and precise loco-manipulation capabilities for humanoid robots."}}
{"id": "2510.04373", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04373", "abs": "https://arxiv.org/abs/2510.04373", "authors": ["Hadi Nekoei", "Aman Jaiswal", "Patrice Bechard", "Oleh Shliazhko", "Orlando Marquez Ayala", "Mathieu Reymond", "Massimo Caccia", "Alexandre Drouin", "Sarath Chandar", "Alexandre Lacoste"], "title": "Just-in-time Episodic Feedback Hinter: Leveraging Offline Knowledge to Improve LLM Agents Adaptation", "comment": null, "summary": "Large language model (LLM) agents perform well in sequential decision-making\ntasks, but improving them on unfamiliar domains often requires costly online\ninteractions or fine-tuning on large expert datasets. These strategies are\nimpractical for closed-source models and expensive for open-source ones, with\nrisks of catastrophic forgetting. Offline trajectories offer reusable\nknowledge, yet demonstration-based methods struggle because raw traces are\nlong, noisy, and tied to specific tasks. We present Just-in-time Episodic\nFeedback Hinter (JEF Hinter), an agentic system that distills offline traces\ninto compact, context-aware hints. A zooming mechanism highlights decisive\nsteps in long trajectories, capturing both strategies and pitfalls. Unlike\nprior methods, JEF Hinter leverages both successful and failed trajectories,\nextracting guidance even when only failure data is available, while supporting\nparallelized hint generation and benchmark-independent prompting. At inference,\na retriever selects relevant hints for the current state, providing targeted\nguidance with transparency and traceability. Experiments on MiniWoB++,\nWorkArena-L1, and WebArena-Lite show that JEF Hinter consistently outperforms\nstrong baselines, including human- and document-based hints.", "AI": {"tldr": "JEF Hinter is an agentic system that distills offline trajectories into compact, context-aware hints to improve LLM agents without costly online interactions or fine-tuning, leveraging both successful and failed trajectories.", "motivation": "Improving LLM agents on unfamiliar domains typically requires expensive online interactions or fine-tuning on large datasets, which is impractical for closed-source models and risks catastrophic forgetting. Offline trajectories contain reusable knowledge but are often long, noisy, and task-specific.", "method": "JEF Hinter uses a zooming mechanism to highlight decisive steps in long trajectories, capturing strategies and pitfalls. It generates compact hints from both successful and failed trajectories, supports parallelized hint generation, and uses a retriever to select relevant hints for current states during inference.", "result": "Experiments on MiniWoB++, WorkArena-L1, and WebArena-Lite show that JEF Hinter consistently outperforms strong baselines, including human- and document-based hints.", "conclusion": "JEF Hinter provides an effective approach for improving LLM agents using offline trajectories, offering targeted guidance with transparency and traceability while avoiding the costs of online interactions or fine-tuning."}}
{"id": "2510.03853", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03853", "abs": "https://arxiv.org/abs/2510.03853", "authors": ["Rui Qian", "Xin Yin", "Chuanhang Deng", "Zhiyuan Peng", "Jian Xiong", "Wei Zhai", "Dejing Dou"], "title": "UGround: Towards Unified Visual Grounding with Unrolled Transformers", "comment": "https://github.com/rui-qian/UGround", "summary": "We present UGround, a \\textbf{U}nified visual \\textbf{Ground}ing paradigm\nthat dynamically selects intermediate layers across \\textbf{U}nrolled\ntransformers as ``mask as prompt'', diverging from the prevailing pipeline that\nleverages the fixed last hidden layer as ``\\texttt{<SEG>} as prompt''. UGround\naddresses two primary challenges posed by the prevailing paradigm: (1) its\nreliance on the fixed last hidden layer, which sequentially amplifies\ncumulative errors arising from layer-by-layer propagation without intermediate\ncorrection, and (2) its use of \\texttt{<SEG>} as a prompt, which implicitly\nprojects textual embeddings into visual space without explicit spatial cues\n(\\eg, coordinates). Central to UGround is Policy-Prompted Masking, which\ncomprises two key components: Stochastic Skip Connection (SSC) and Mask as\nPrompt (MasP). SSC is a reinforcement learning policy that, via stochastic\nsampling, allows each \\texttt{<SEG>} token to slide across unrolled transformer\nlayers, enabling dynamic layer selection at which it connects to the vision\nmodel (\\eg, SAM) in a skip-connection fashion. Given the selected hidden layer,\nMasP uses the similarity map derived from the \\texttt{<SEG>} token and image\ntokens as a soft logit mask to prompt SAM for mask generation, offering\nexplicit spatial cues through its activation regions. To validate the\neffectiveness of UGround, we, for the first time, have unified visual grounding\nwithin a single framework from an attribute perspective, spanning from\ntraditional refer expression segmentation to newly proposed reasoning\nsegmentation, single-target to multi-target, positive query to false premise\n(empty target). All codes and models are publicly available at\n\\href{https://github.com/rui-qian/UGround}{https://github.com/rui-qian/UGround}.", "AI": {"tldr": "UGround introduces a unified visual grounding paradigm that dynamically selects intermediate transformer layers as \"mask as prompt\" instead of using fixed last hidden layers, addressing cumulative error propagation and lack of spatial cues in existing methods.", "motivation": "Current visual grounding methods rely on fixed last hidden layers which amplify cumulative errors through layer-by-layer propagation without correction, and use text embeddings without explicit spatial cues like coordinates.", "method": "UGround uses Policy-Prompted Masking with two components: Stochastic Skip Connection (SSC) - a reinforcement learning policy that dynamically selects transformer layers for skip connections, and Mask as Prompt (MasP) - which uses similarity maps as soft logit masks to prompt SAM for mask generation with explicit spatial cues.", "result": "UGround unifies visual grounding within a single framework across various scenarios including refer expression segmentation, reasoning segmentation, single/multi-target, and positive/negative queries.", "conclusion": "UGround provides a more effective visual grounding paradigm by dynamically selecting intermediate layers and using explicit spatial cues, addressing key limitations of existing methods while unifying diverse grounding tasks."}}
{"id": "2510.04384", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04384", "abs": "https://arxiv.org/abs/2510.04384", "authors": ["Adam Ballew", "Jingbo Wang", "Shaogang Ren"], "title": "LLM Based Bayesian Optimization for Prompt Search", "comment": null, "summary": "Bayesian Optimization (BO) has been widely used to efficiently optimize\nexpensive black-box functions with limited evaluations. In this paper, we\ninvestigate the use of BO for prompt engineering to enhance text classification\nwith Large Language Models (LLMs). We employ an LLM-powered Gaussian Process\n(GP) as the surrogate model to estimate the performance of different prompt\ncandidates. These candidates are generated by an LLM through the expansion of a\nset of seed prompts and are subsequently evaluated using an Upper Confidence\nBound (UCB) acquisition function in conjunction with the GP posterior. The\noptimization process iteratively refines the prompts based on a subset of the\ndata, aiming to improve classification accuracy while reducing the number of\nAPI calls by leveraging the prediction uncertainty of the LLM-based GP. The\nproposed BO-LLM algorithm is evaluated on two datasets, and its advantages are\ndiscussed in detail in this paper.", "AI": {"tldr": "BO-LLM uses Bayesian Optimization with LLM-powered Gaussian Process for prompt engineering to improve text classification accuracy while reducing API calls.", "motivation": "To efficiently optimize expensive black-box functions (prompt engineering for LLMs) with limited evaluations using Bayesian Optimization.", "method": "Uses LLM-powered Gaussian Process as surrogate model, generates prompt candidates from seed prompts via LLM, evaluates with UCB acquisition function, and iteratively refines prompts on subset data.", "result": "Evaluated on two datasets, showing improved classification accuracy with reduced API calls by leveraging LLM-based GP prediction uncertainty.", "conclusion": "BO-LLM algorithm effectively enhances prompt engineering for text classification with LLMs through Bayesian Optimization approach."}}
{"id": "2510.03857", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03857", "abs": "https://arxiv.org/abs/2510.03857", "authors": ["Minseo Lee", "Byeonghyeon Lee", "Lucas Yunkyu Lee", "Eunsoo Lee", "Sangmin Kim", "Seunghyeon Song", "Joo Chan Lee", "Jong Hwan Ko", "Jaesik Park", "Eunbyung Park"], "title": "Optimized Minimal 4D Gaussian Splatting", "comment": "17 pages, 8 figures", "summary": "4D Gaussian Splatting has emerged as a new paradigm for dynamic scene\nrepresentation, enabling real-time rendering of scenes with complex motions.\nHowever, it faces a major challenge of storage overhead, as millions of\nGaussians are required for high-fidelity reconstruction. While several studies\nhave attempted to alleviate this memory burden, they still face limitations in\ncompression ratio or visual quality. In this work, we present OMG4 (Optimized\nMinimal 4D Gaussian Splatting), a framework that constructs a compact set of\nsalient Gaussians capable of faithfully representing 4D Gaussian models. Our\nmethod progressively prunes Gaussians in three stages: (1) Gaussian Sampling to\nidentify primitives critical to reconstruction fidelity, (2) Gaussian Pruning\nto remove redundancies, and (3) Gaussian Merging to fuse primitives with\nsimilar characteristics. In addition, we integrate implicit appearance\ncompression and generalize Sub-Vector Quantization (SVQ) to 4D representations,\nfurther reducing storage while preserving quality. Extensive experiments on\nstandard benchmark datasets demonstrate that OMG4 significantly outperforms\nrecent state-of-the-art methods, reducing model sizes by over 60% while\nmaintaining reconstruction quality. These results position OMG4 as a\nsignificant step forward in compact 4D scene representation, opening new\npossibilities for a wide range of applications. Our source code is available at\nhttps://minshirley.github.io/OMG4/.", "AI": {"tldr": "OMG4 is a framework that reduces storage overhead in 4D Gaussian Splatting by progressively pruning and merging Gaussians while maintaining reconstruction quality, achieving over 60% model size reduction.", "motivation": "4D Gaussian Splatting faces major storage overhead challenges requiring millions of Gaussians for high-fidelity reconstruction, with existing methods having limitations in compression ratio or visual quality.", "method": "Three-stage progressive pruning: (1) Gaussian Sampling to identify critical primitives, (2) Gaussian Pruning to remove redundancies, (3) Gaussian Merging to fuse similar primitives, plus implicit appearance compression and generalized Sub-Vector Quantization for 4D representations.", "result": "Significantly outperforms state-of-the-art methods, reducing model sizes by over 60% while maintaining reconstruction quality on standard benchmark datasets.", "conclusion": "OMG4 represents a significant step forward in compact 4D scene representation, opening new possibilities for a wide range of applications."}}
{"id": "2510.04391", "categories": ["cs.AI", "cs.CL", "cs.SI", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2510.04391", "abs": "https://arxiv.org/abs/2510.04391", "authors": ["Saurabh Ranjan", "Brian Odegaard"], "title": "Internal World Models as Imagination Networks in Cognitive Agents", "comment": null, "summary": "What is the computational objective of imagination? While classical\ninterpretations suggest imagination is useful for maximizing rewards, recent\nfindings challenge this view. In this study, we propose that imagination serves\nto access an internal world model (IWM) and use psychological network analysis\nto explore IWMs in humans and large language models (LLMs). Specifically, we\nassessed imagination vividness ratings using two questionnaires and constructed\nimagination networks from these reports. Imagination networks from human groups\nshowed correlations between different centrality measures, including expected\ninfluence, strength, and closeness. However, imagination networks from LLMs\nshowed a lack of clustering and lower correlations between centrality measures\nunder different prompts and conversational memory conditions. Together, these\nresults indicate a lack of similarity between IWMs in human and LLM agents.\nOverall, our study offers a novel method for comparing internally-generated\nrepresentations in humans and AI, providing insights for developing human-like\nimagination in artificial intelligence.", "AI": {"tldr": "This paper proposes that imagination serves to access an internal world model (IWM) and uses psychological network analysis to compare IWMs in humans vs. LLMs, finding significant differences in network structure and centrality correlations.", "motivation": "To understand the computational objective of imagination and challenge classical views that imagination primarily serves reward maximization, by comparing internal world models between humans and AI systems.", "method": "Used psychological network analysis with imagination vividness ratings from questionnaires to construct imagination networks, comparing human groups with LLMs under different prompts and conversational memory conditions.", "result": "Human imagination networks showed correlations between centrality measures (expected influence, strength, closeness), while LLM networks lacked clustering and showed lower centrality correlations across different conditions.", "conclusion": "There is a lack of similarity between internal world models in humans and LLMs, providing a novel method for comparing internally-generated representations and insights for developing human-like imagination in AI."}}
{"id": "2510.03858", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03858", "abs": "https://arxiv.org/abs/2510.03858", "authors": ["Jyoti Kini", "Rohit Gupta", "Mubarak Shah"], "title": "Cross-View Open-Vocabulary Object Detection in Aerial Imagery", "comment": null, "summary": "Traditional object detection models are typically trained on a fixed set of\nclasses, limiting their flexibility and making it costly to incorporate new\ncategories. Open-vocabulary object detection addresses this limitation by\nenabling models to identify unseen classes without explicit training.\nLeveraging pretrained models contrastively trained on abundantly available\nground-view image-text classification pairs provides a strong foundation for\nopen-vocabulary object detection in aerial imagery. Domain shifts, viewpoint\nvariations, and extreme scale differences make direct knowledge transfer across\ndomains ineffective, requiring specialized adaptation strategies. In this\npaper, we propose a novel framework for adapting open-vocabulary\nrepresentations from ground-view images to solve object detection in aerial\nimagery through structured domain alignment. The method introduces contrastive\nimage-to-image alignment to enhance the similarity between aerial and\nground-view embeddings and employs multi-instance vocabulary associations to\nalign aerial images with text embeddings. Extensive experiments on the xView,\nDOTAv2, VisDrone, DIOR, and HRRSD datasets are used to validate our approach.\nOur open-vocabulary model achieves improvements of +6.32 mAP on DOTAv2, +4.16\nmAP on VisDrone (Images), and +3.46 mAP on HRRSD in the zero-shot setting when\ncompared to finetuned closed-vocabulary dataset-specific model performance,\nthus paving the way for more flexible and scalable object detection systems in\naerial applications.", "AI": {"tldr": "A novel framework for adapting open-vocabulary object detection from ground-view to aerial imagery through structured domain alignment, achieving significant performance improvements in zero-shot settings.", "motivation": "Traditional object detection models are limited to fixed classes, making it costly to add new categories. Open-vocabulary detection enables identifying unseen classes without explicit training, but domain shifts between ground-view and aerial imagery require specialized adaptation strategies.", "method": "Proposes contrastive image-to-image alignment to enhance similarity between aerial and ground-view embeddings, and multi-instance vocabulary associations to align aerial images with text embeddings.", "result": "Achieves improvements of +6.32 mAP on DOTAv2, +4.16 mAP on VisDrone, and +3.46 mAP on HRRSD in zero-shot setting compared to finetuned closed-vocabulary models.", "conclusion": "The framework paves the way for more flexible and scalable object detection systems in aerial applications by effectively adapting open-vocabulary representations across domains."}}
{"id": "2510.04399", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04399", "abs": "https://arxiv.org/abs/2510.04399", "authors": ["Charles L. Wang", "Keir Dorchen", "Peter Jin"], "title": "Utility-Learning Tension in Self-Modifying Agents", "comment": null, "summary": "As systems trend toward superintelligence, a natural modeling premise is that\nagents can self-improve along every facet of their own design. We formalize\nthis with a five-axis decomposition and a decision layer, separating incentives\nfrom learning behavior and analyzing axes in isolation. Our central result\nidentifies and introduces a sharp utility--learning tension, the structural\nconflict in self-modifying systems whereby utility-driven changes that improve\nimmediate or expected performance can also erode the statistical preconditions\nfor reliable learning and generalization. Our findings show that\ndistribution-free guarantees are preserved iff the policy-reachable model\nfamily is uniformly capacity-bounded; when capacity can grow without limit,\nutility-rational self-changes can render learnable tasks unlearnable. Under\nstandard assumptions common in practice, these axes reduce to the same capacity\ncriterion, yielding a single boundary for safe self-modification. Numerical\nexperiments across several axes validate the theory by comparing destructive\nutility policies against our proposed two-gate policies that preserve\nlearnability.", "AI": {"tldr": "The paper identifies a fundamental tension in self-modifying AI systems where utility-driven improvements can undermine learning capabilities, showing that distribution-free guarantees are only preserved when policy-reachable model families have uniformly bounded capacity.", "motivation": "To understand the structural conflicts in self-modifying AI systems as they trend toward superintelligence, particularly how utility-driven self-improvement can conflict with reliable learning and generalization.", "method": "Formalizes self-modification with a five-axis decomposition and decision layer, analyzes axes in isolation, and conducts numerical experiments comparing destructive utility policies against proposed two-gate policies that preserve learnability.", "result": "Identifies a sharp utility-learning tension where utility-driven changes that improve performance can erode statistical preconditions for reliable learning. Shows distribution-free guarantees are preserved iff policy-reachable model family is uniformly capacity-bounded.", "conclusion": "Under standard assumptions, self-modification axes reduce to a single capacity criterion, providing a boundary for safe self-modification. Two-gate policies can preserve learnability while destructive utility policies render learnable tasks unlearnable."}}
{"id": "2510.03869", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03869", "abs": "https://arxiv.org/abs/2510.03869", "authors": ["Runhao Liu", "Ziming Chen", "Peng Zhang"], "title": "Exploring the Challenge and Value of Deep Learning in Automated Skin Disease Diagnosis", "comment": null, "summary": "Skin cancer is one of the most prevalent and deadly forms of cancer\nworldwide, which highlights the critical importance of early detection and\ndiagnosis in improving patient outcomes. Deep learning (DL) has shown\nsignificant promise in enhancing the accuracy and efficiency of automated skin\ndisease diagnosis, particularly in detecting and evaluating skin lesions and\nclassification. However, there are still several challenges for DL-based skin\ncancer diagnosis, including complex features, image noise, intra-class\nvariation, inter-class similarity, and data imbalance. By synthesizing recent\nresearch, this review discusses innovative approaches to cope with these\nchallenges, such as data augmentation, hybrid models, and feature fusion, etc.\nFurthermore, the review highlights the integration of DL models into clinical\nworkflows, offering insights into the potential of deep learning to\nrevolutionize skin disease diagnosis and improve clinical decision-making. This\narticle follows a comprehensive methodology based on the PRISMA framework and\nemphasizes the need for continued advancements to fully unlock the\ntransformative potential of DL in dermatological care.", "AI": {"tldr": "This review paper analyzes deep learning approaches for skin cancer diagnosis, discussing challenges like complex features, image noise, and data imbalance, and presents solutions including data augmentation and hybrid models.", "motivation": "Skin cancer is a prevalent and deadly disease where early detection is crucial. Deep learning shows promise for automated diagnosis but faces challenges like complex features, image noise, and data imbalance that need to be addressed.", "method": "The review follows the PRISMA framework methodology, synthesizing recent research on DL approaches for skin cancer diagnosis, including data augmentation, hybrid models, and feature fusion techniques.", "result": "The review identifies innovative approaches to overcome DL challenges in skin cancer diagnosis and discusses the integration of DL models into clinical workflows for improved diagnostic accuracy and efficiency.", "conclusion": "Deep learning has transformative potential in dermatological care but requires continued advancements to fully unlock its capabilities for revolutionizing skin disease diagnosis and clinical decision-making."}}
{"id": "2510.03896", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03896", "abs": "https://arxiv.org/abs/2510.03896", "authors": ["Mingyu Liu", "Zheng Huang", "Xiaoyi Lin", "Muzhi Zhu", "Canyu Zhao", "Zongze Du", "Yating Wang", "Haoyi Zhu", "Hao Chen", "Chunhua Shen"], "title": "Bridge Thinking and Acting: Unleashing Physical Potential of VLM with Generalizable Action Expert", "comment": null, "summary": "Although Vision-Language Models (VLM) have demonstrated impressive planning\nand reasoning capabilities, translating these abilities into the physical world\nintroduces significant challenges. Conventional Vision-Language-Action (VLA)\nmodels, which integrate reasoning and action into a monolithic architecture,\ngeneralize poorly because they are constrained by scarce, narrow-domain data.\nWhile recent dual-system approaches attempt to decouple \"thinking\" from\n\"acting\", they are often constrained by semantic ambiguities within the action\nmodule. This ambiguity makes large-scale, cross-task training infeasible.\nConsequently, these systems typically necessitate fine-tuning on newly\ncollected data when deployed to novel environments, and the cooperation\nmechanism between the two systems remains ill-defined. To address these\nlimitations, we introduce, for the first time, a framework centered around a\ngeneralizable action expert. Our approach utilizes sparse 3D trajectories as an\nintermediate representation, effectively bridging the high-level planning\ncapabilities of the VLM with the low-level physical action module. During the\nplanning phase, the VLM is only required to generate coarse 3D waypoints. These\nwaypoints are then processed by our generalizable action expert, which refines\nthem into dense, executable action sequences by sampling real-time point cloud\nobservations of the environment. To promote training efficiency and robust\ngeneralization, we introduce a novel \"Action Pre-training, Pointcloud\nFine-tuning\" paradigm. Our method combines the broad generalization\ncapabilities of VLMs in visual understanding and planning with the\nfine-grained, action-level generalization of action expert.", "AI": {"tldr": "The paper introduces a framework that decouples planning from acting in Vision-Language-Action models using sparse 3D trajectories as an intermediate representation, enabling better generalization across tasks without requiring fine-tuning for new environments.", "motivation": "Current Vision-Language-Action models struggle with generalization due to monolithic architectures and semantic ambiguities in action modules, requiring fine-tuning for new environments and lacking clear cooperation mechanisms between planning and acting systems.", "method": "Uses sparse 3D trajectories as intermediate representation between VLM planning and action execution. VLMs generate coarse 3D waypoints, which are refined by a generalizable action expert into dense action sequences using real-time point cloud observations. Implements \"Action Pre-training, Pointcloud Fine-tuning\" paradigm.", "result": "The framework combines VLM's broad generalization in visual understanding and planning with fine-grained action-level generalization of the action expert, enabling efficient training and robust generalization across tasks.", "conclusion": "The proposed approach effectively bridges high-level planning with low-level physical actions through 3D trajectory representations, overcoming limitations of conventional VLA models and enabling scalable cross-task training without environment-specific fine-tuning."}}
{"id": "2510.04474", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04474", "abs": "https://arxiv.org/abs/2510.04474", "authors": ["Gang Li", "Yan Chen", "Ming Lin", "Tianbao Yang"], "title": "DRPO: Efficient Reasoning via Decoupled Reward Policy Optimization", "comment": "20 pages, 7 figures", "summary": "Recent large reasoning models (LRMs) driven by reinforcement learning\nalgorithms (e.g., GRPO) have achieved remarkable performance on challenging\nreasoning tasks. However, these models suffer from overthinking, generating\nunnecessarily long and redundant reasoning even for simple questions, which\nsubstantially increases computational cost and response latency. While existing\nmethods incorporate length rewards to GRPO to promote concise reasoning, they\nincur significant performance degradation. We identify the root cause: when\nrewards for correct but long rollouts are penalized, GRPO's group-relative\nadvantage function can assign them negative advantages, actively discouraging\nvalid reasoning. To overcome this, we propose Decoupled Reward Policy\nOptimization (DRPO), a novel framework that decouples the length-based learning\nsignal of correct rollouts from incorrect ones. DRPO ensures that reward\nsignals for correct rollouts are normalized solely within the positive group,\nshielding them from interference by negative samples. The DRPO's objective is\ngrounded in integrating an optimized positive data distribution, which\nmaximizes length-based rewards under a KL regularization, into a discriminative\nobjective. We derive a closed-form solution for this distribution, enabling\nefficient computation of the objective and its gradients using only on-policy\ndata and importance weighting. Of independent interest, this formulation is\ngeneral and can incorporate other preference rewards of positive data beyond\nlength. Experiments on mathematical reasoning tasks demonstrate DRPO's\nsignificant superiority over six efficient reasoning baselines. Notably, with a\n1.5B model, our method achieves 77\\% length reduction with only 1.1\\%\nperformance loss on simple questions like GSM8k dataset, while the follow-up\nbaseline sacrifices 4.3\\% for 68\\% length reduction.", "AI": {"tldr": "DRPO is a novel reinforcement learning framework that addresses overthinking in large reasoning models by decoupling length-based rewards for correct vs incorrect reasoning, achieving significant length reduction with minimal performance loss.", "motivation": "Large reasoning models suffer from overthinking - generating unnecessarily long and redundant reasoning even for simple questions, which increases computational costs and response latency. Existing methods that incorporate length rewards cause significant performance degradation.", "method": "Proposes Decoupled Reward Policy Optimization (DRPO) that decouples length-based learning signals for correct rollouts from incorrect ones. It ensures reward signals for correct rollouts are normalized within the positive group only, shielding them from interference by negative samples. Uses an optimized positive data distribution with KL regularization and derives closed-form solution.", "result": "Achieves 77% length reduction with only 1.1% performance loss on GSM8k dataset using a 1.5B model, significantly outperforming six baselines. The follow-up baseline sacrificed 4.3% performance for 68% length reduction.", "conclusion": "DRPO effectively addresses overthinking in reasoning models by protecting correct reasoning from being penalized by length rewards, enabling substantial computational efficiency gains with minimal accuracy trade-offs."}}
{"id": "2510.03870", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03870", "abs": "https://arxiv.org/abs/2510.03870", "authors": ["Nikolaos Kaparinos", "Vasileios Mezaris"], "title": "SDAKD: Student Discriminator Assisted Knowledge Distillation for Super-Resolution Generative Adversarial Networks", "comment": "Under review", "summary": "Generative Adversarial Networks (GANs) achieve excellent performance in\ngenerative tasks, such as image super-resolution, but their computational\nrequirements make difficult their deployment on resource-constrained devices.\nWhile knowledge distillation is a promising research direction for GAN\ncompression, effectively training a smaller student generator is challenging\ndue to the capacity mismatch between the student generator and the teacher\ndiscriminator. In this work, we propose Student Discriminator Assisted\nKnowledge Distillation (SDAKD), a novel GAN distillation methodology that\nintroduces a student discriminator to mitigate this capacity mismatch. SDAKD\nfollows a three-stage training strategy, and integrates an adapted feature map\ndistillation approach in its last two training stages. We evaluated SDAKD on\ntwo well-performing super-resolution GANs, GCFSR and Real-ESRGAN. Our\nexperiments demonstrate consistent improvements over the baselines and SOTA GAN\nknowledge distillation methods. The SDAKD source code will be made openly\navailable upon acceptance of the paper.", "AI": {"tldr": "SDAKD is a novel GAN distillation method that introduces a student discriminator to address capacity mismatch issues, enabling effective compression of super-resolution GANs for resource-constrained devices.", "motivation": "GANs have excellent generative performance but high computational requirements that prevent deployment on resource-constrained devices. Knowledge distillation for GAN compression faces challenges due to capacity mismatch between student generators and teacher discriminators.", "method": "Proposes Student Discriminator Assisted Knowledge Distillation (SDAKD) with a three-stage training strategy that introduces a student discriminator and integrates adapted feature map distillation in the last two stages.", "result": "Evaluated on GCFSR and Real-ESRGAN super-resolution GANs, SDAKD demonstrates consistent improvements over baselines and state-of-the-art GAN knowledge distillation methods.", "conclusion": "SDAKD effectively addresses the capacity mismatch problem in GAN distillation and enables successful compression of high-performance super-resolution GANs for deployment on resource-limited devices."}}
{"id": "2510.03915", "categories": ["cs.CV", "cs.DC", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03915", "abs": "https://arxiv.org/abs/2510.03915", "authors": ["Sagar Bharadwaj", "Harrison Williams", "Luke Wang", "Michael Liang", "Tao Jin", "Srinivasan Seshan", "Anthony Rowe"], "title": "OpenFLAME: Federated Visual Positioning System to Enable Large-Scale Augmented Reality Applications", "comment": null, "summary": "World-scale augmented reality (AR) applications need a ubiquitous 6DoF\nlocalization backend to anchor content to the real world consistently across\ndevices. Large organizations such as Google and Niantic are 3D scanning outdoor\npublic spaces in order to build their own Visual Positioning Systems (VPS).\nThese centralized VPS solutions fail to meet the needs of many future AR\napplications -- they do not cover private indoor spaces because of privacy\nconcerns, regulations, and the labor bottleneck of updating and maintaining 3D\nscans. In this paper, we present OpenFLAME, a federated VPS backend that allows\nindependent organizations to 3D scan and maintain a separate VPS service for\ntheir own spaces. This enables access control of indoor 3D scans, distributed\nmaintenance of the VPS backend, and encourages larger coverage. Sharding of VPS\nservices introduces several unique challenges -- coherency of localization\nresults across spaces, quality control of VPS services, selection of the right\nVPS service for a location, and many others. We introduce the concept of\nfederated image-based localization and provide reference solutions for managing\nand merging data across maps without sharing private data.", "AI": {"tldr": "OpenFLAME is a federated Visual Positioning System (VPS) backend that enables distributed 6DoF localization for AR applications across multiple independent organizations without centralized 3D scanning.", "motivation": "Centralized VPS solutions from large companies fail to cover private indoor spaces due to privacy concerns, regulations, and maintenance bottlenecks, limiting AR application coverage.", "method": "Proposes federated image-based localization where organizations maintain separate VPS services for their spaces, with solutions for managing and merging data across maps without sharing private data.", "result": "Enables access control of indoor 3D scans, distributed VPS maintenance, and encourages larger coverage while addressing challenges like localization coherency and quality control.", "conclusion": "Federated VPS approach overcomes limitations of centralized systems by allowing distributed maintenance and coverage expansion while preserving privacy and access control."}}
{"id": "2510.04480", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04480", "abs": "https://arxiv.org/abs/2510.04480", "authors": ["Yunuo Cen", "Zixuan Wang", "Jintao Zhang", "Zhiwei Zhang", "Xuanyao Fong"], "title": "On Continuous Optimization for Constraint Satisfaction Problems", "comment": null, "summary": "Constraint satisfaction problems (CSPs) are fundamental in mathematics,\nphysics, and theoretical computer science. While conflict-driven clause\nlearning Boolean Satisfiability (SAT) solvers have achieved remarkable success\nand become the mainstream approach for Boolean satisfiability, recent advances\nshow that modern continuous local search (CLS) solvers can achieve highly\ncompetitive results on certain classes of SAT problems. Motivated by these\nadvances, we extend the CLS framework from Boolean SAT to general CSP with\nfinite-domain variables and expressive constraints. We present FourierCSP, a\ncontinuous optimization framework that generalizes the Walsh-Fourier transform\nto CSP, allowing for transforming versatile constraints to compact multilinear\npolynomials, thereby avoiding the need for auxiliary variables and\nmemory-intensive encodings. Our approach leverages efficient evaluation and\ndifferentiation of the objective via circuit-output probability and employs a\nprojected gradient optimization method with theoretical guarantees. Empirical\nresults on benchmark suites demonstrate that FourierCSP is scalable and\ncompetitive, significantly broadening the class of problems that can be\nefficiently solved by CLS techniques.", "AI": {"tldr": "FourierCSP extends continuous local search from Boolean SAT to general CSPs using Walsh-Fourier transform to convert constraints into compact multilinear polynomials, achieving competitive performance without auxiliary variables.", "motivation": "While modern continuous local search solvers show competitive results on certain SAT problems, there's a need to extend this framework to general constraint satisfaction problems with finite-domain variables and expressive constraints.", "method": "Uses FourierCSP framework that generalizes Walsh-Fourier transform to CSP, transforming constraints to compact multilinear polynomials. Employs efficient evaluation via circuit-output probability and projected gradient optimization with theoretical guarantees.", "result": "Empirical results show FourierCSP is scalable and competitive on benchmark suites, significantly broadening the class of problems solvable by continuous local search techniques.", "conclusion": "The approach successfully extends continuous local search to general CSPs, avoiding auxiliary variables and memory-intensive encodings while maintaining competitive performance."}}
{"id": "2510.03873", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03873", "abs": "https://arxiv.org/abs/2510.03873", "authors": ["Saja Al-Dabet", "Sherzod Turaev", "Nazar Zaki", "Arif O. Khan", "Luai Eldweik"], "title": "PoseGaze-AHP: A Knowledge-Based 3D Dataset for AI-Driven Ocular and Postural Diagnosis", "comment": "This is a preprint version of a manuscript under review. All rights\n  reserved by the authors", "summary": "Diagnosing ocular-induced abnormal head posture (AHP) requires a\ncomprehensive analysis of both head pose and ocular movements. However,\nexisting datasets focus on these aspects separately, limiting the development\nof integrated diagnostic approaches and restricting AI-driven advancements in\nAHP analysis. To address this gap, we introduce PoseGaze-AHP, a novel 3D\ndataset that synchronously captures head pose and gaze movement information for\nocular-induced AHP assessment. Structured clinical data were extracted from\nmedical literature using large language models (LLMs) through an iterative\nprocess with the Claude 3.5 Sonnet model, combining stepwise, hierarchical, and\ncomplex prompting strategies. The extracted records were systematically imputed\nand transformed into 3D representations using the Neural Head Avatar (NHA)\nframework. The dataset includes 7,920 images generated from two head textures,\ncovering a broad spectrum of ocular conditions. The extraction method achieved\nan overall accuracy of 91.92%, demonstrating its reliability for clinical\ndataset construction. PoseGaze-AHP is the first publicly available resource\ntailored for AI-driven ocular-induced AHP diagnosis, supporting the development\nof accurate and privacy-compliant diagnostic tools.", "AI": {"tldr": "PoseGaze-AHP is a novel 3D dataset that synchronously captures head pose and gaze movement information for diagnosing ocular-induced abnormal head posture (AHP), created using LLM-extracted clinical data and Neural Head Avatar framework.", "motivation": "Existing datasets focus on head pose and ocular movements separately, limiting integrated diagnostic approaches and AI-driven advancements in AHP analysis.", "method": "Clinical data extracted from medical literature using LLMs (Claude 3.5 Sonnet) with iterative prompting strategies, then transformed into 3D representations using Neural Head Avatar framework with systematic imputation.", "result": "Created dataset with 7,920 images from two head textures covering broad ocular conditions; extraction method achieved 91.92% accuracy; first publicly available resource for AI-driven ocular-induced AHP diagnosis.", "conclusion": "PoseGaze-AHP enables development of accurate, privacy-compliant diagnostic tools for ocular-induced AHP by providing integrated head pose and gaze movement data."}}
{"id": "2510.04333", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.04333", "abs": "https://arxiv.org/abs/2510.04333", "authors": ["Lan Feng", "Yang Gao", "Eloi Zablocki", "Quanyi Li", "Wuyang Li", "Sichao Liu", "Matthieu Cord", "Alexandre Alahi"], "title": "RAP: 3D Rasterization Augmented End-to-End Planning", "comment": null, "summary": "Imitation learning for end-to-end driving trains policies only on expert\ndemonstrations. Once deployed in a closed loop, such policies lack recovery\ndata: small mistakes cannot be corrected and quickly compound into failures. A\npromising direction is to generate alternative viewpoints and trajectories\nbeyond the logged path. Prior work explores photorealistic digital twins via\nneural rendering or game engines, but these methods are prohibitively slow and\ncostly, and thus mainly used for evaluation. In this work, we argue that\nphotorealism is unnecessary for training end-to-end planners. What matters is\nsemantic fidelity and scalability: driving depends on geometry and dynamics,\nnot textures or lighting. Motivated by this, we propose 3D Rasterization, which\nreplaces costly rendering with lightweight rasterization of annotated\nprimitives, enabling augmentations such as counterfactual recovery maneuvers\nand cross-agent view synthesis. To transfer these synthetic views effectively\nto real-world deployment, we introduce a Raster-to-Real feature-space alignment\nthat bridges the sim-to-real gap. Together, these components form Rasterization\nAugmented Planning (RAP), a scalable data augmentation pipeline for planning.\nRAP achieves state-of-the-art closed-loop robustness and long-tail\ngeneralization, ranking first on four major benchmarks: NAVSIM v1/v2, Waymo\nOpen Dataset Vision-based E2E Driving, and Bench2Drive. Our results show that\nlightweight rasterization with feature alignment suffices to scale E2E\ntraining, offering a practical alternative to photorealistic rendering. Project\npage: https://alan-lanfeng.github.io/RAP/.", "AI": {"tldr": "RAP (Rasterization Augmented Planning) uses lightweight 3D rasterization instead of photorealistic rendering for end-to-end driving training, enabling scalable data augmentation with counterfactual recovery maneuvers and cross-agent view synthesis, achieving state-of-the-art performance on major benchmarks.", "motivation": "Imitation learning for end-to-end driving lacks recovery data, causing small mistakes to compound into failures. Photorealistic rendering methods are too slow and costly for training. The key insight is that driving depends on geometry and dynamics, not photorealism.", "method": "Proposes 3D Rasterization to replace costly rendering with lightweight rasterization of annotated primitives. Introduces Raster-to-Real feature-space alignment to bridge sim-to-real gap. Combines these into RAP pipeline for scalable data augmentation.", "result": "Achieves state-of-the-art closed-loop robustness and long-tail generalization, ranking first on four major benchmarks: NAVSIM v1/v2, Waymo Open Dataset Vision-based E2E Driving, and Bench2Drive.", "conclusion": "Lightweight rasterization with feature alignment suffices to scale E2E training, offering a practical alternative to photorealistic rendering for end-to-end driving planners."}}
{"id": "2510.04488", "categories": ["cs.AI", "cs.IT", "math.IT", "I.2.4"], "pdf": "https://arxiv.org/pdf/2510.04488", "abs": "https://arxiv.org/abs/2510.04488", "authors": ["Edward Y. Chang", "Ethan Y. Chang"], "title": "Multi-Agent Collaborative Intelligence: Dual-Dial Control for Reliable LLM Reasoning", "comment": "27 pages, 5 figures, 21 tables", "summary": "Multi-agent debate often wastes compute by using a fixed adversarial stance,\naggregating without deliberation, or stopping on heuristics. We introduce MACI,\nan active controller with two independent dials that decouple information from\nbehavior: an information dial that gates evidence by quality, and a behavior\ndial that schedules contentiousness from exploration to consolidation. A\nmoderator tracks disagreement, overlap, evidence quality, and argument quality,\nand halts when gains plateau. We provide theory-lite guarantees for\nnonincreasing dispersion and provable termination, with a budget-feasible\nscheduler. Across clinical diagnosis and news-bias tasks, MACI improves\naccuracy and calibration while reducing tokens, and converts residual\nuncertainty into precision RAG plans that specify what to retrieve next. We use\na cross-family LLM judge (CRIT) as a conservative soft weight and stop signal,\nvalidated for order invariance and judge-swap stability; stability depends on\nusing high-capability judges. MACI turns debate into a budget-aware,\nmeasurable, and provably terminating controller.", "AI": {"tldr": "MACI introduces an active controller for multi-agent debate with two dials: information (gates evidence quality) and behavior (schedules contentiousness), plus a moderator that tracks metrics and halts when gains plateau, improving accuracy while reducing computational costs.", "motivation": "Current multi-agent debate systems waste compute by using fixed adversarial stances, aggregating without deliberation, or stopping based on heuristics rather than meaningful metrics.", "method": "MACI uses two independent dials (information and behavior), a moderator tracking disagreement/overlap/evidence quality/argument quality, and a cross-family LLM judge (CRIT) as conservative soft weight and stop signal with budget-feasible scheduling.", "result": "Across clinical diagnosis and news-bias tasks, MACI improves accuracy and calibration while reducing tokens, and converts residual uncertainty into precision RAG plans specifying what to retrieve next.", "conclusion": "MACI transforms debate into a budget-aware, measurable, and provably terminating controller with nonincreasing dispersion guarantees and provable termination."}}
{"id": "2510.03874", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03874", "abs": "https://arxiv.org/abs/2510.03874", "authors": ["Yunhao Li", "Sijing Wu", "Yucheng Zhu", "Huiyu Duan", "Zicheng Zhang", "Guangtao Zhai"], "title": "DHQA-4D: Perceptual Quality Assessment of Dynamic 4D Digital Human", "comment": null, "summary": "With the rapid development of 3D scanning and reconstruction technologies,\ndynamic digital human avatars based on 4D meshes have become increasingly\npopular. A high-precision dynamic digital human avatar can be applied to\nvarious fields such as game production, animation generation, and remote\nimmersive communication. However, these 4D human avatar meshes are prone to\nbeing degraded by various types of noise during the processes of collection,\ncompression, and transmission, thereby affecting the viewing experience of\nusers. In light of this fact, quality assessment of dynamic 4D digital humans\nbecomes increasingly important. In this paper, we first propose a large-scale\ndynamic digital human quality assessment dataset, DHQA-4D, which contains 32\nhigh-quality real-scanned 4D human mesh sequences, 1920 distorted textured 4D\nhuman meshes degraded by 11 textured distortions, as well as their\ncorresponding textured and non-textured mean opinion scores (MOSs). Equipped\nwith DHQA-4D dataset, we analyze the influence of different types of distortion\non human perception for textured dynamic 4D meshes and non-textured dynamic 4D\nmeshes. Additionally, we propose DynaMesh-Rater, a novel large multimodal model\n(LMM) based approach that is able to assess both textured 4D meshes and\nnon-textured 4D meshes. Concretely, DynaMesh-Rater elaborately extracts\nmulti-dimensional features, including visual features from a projected 2D\nvideo, motion features from cropped video clips, and geometry features from the\n4D human mesh to provide comprehensive quality-related information. Then we\nutilize a LMM model to integrate the multi-dimensional features and conduct a\nLoRA-based instruction tuning technique to teach the LMM model to predict the\nquality scores. Extensive experimental results on the DHQA-4D dataset\ndemonstrate the superiority of our DynaMesh-Rater method over previous quality\nassessment methods.", "AI": {"tldr": "This paper introduces DHQA-4D, a large-scale dataset for quality assessment of dynamic 4D digital humans, and proposes DynaMesh-Rater, a novel large multimodal model-based approach that extracts multi-dimensional features to predict quality scores for both textured and non-textured 4D meshes.", "motivation": "With the growing popularity of dynamic digital human avatars based on 4D meshes in applications like gaming, animation, and immersive communication, these meshes are prone to noise degradation during collection, compression, and transmission, which affects user viewing experience. This necessitates quality assessment methods for dynamic 4D digital humans.", "method": "The authors first create the DHQA-4D dataset containing 32 high-quality 4D human mesh sequences and 1920 distorted meshes with 11 types of distortions. Then they propose DynaMesh-Rater, which extracts multi-dimensional features (visual from 2D video projection, motion from cropped video clips, and geometry from 4D mesh) and uses a large multimodal model with LoRA-based instruction tuning to predict quality scores.", "result": "Extensive experiments on the DHQA-4D dataset demonstrate that DynaMesh-Rater outperforms previous quality assessment methods, showing superiority in assessing both textured and non-textured 4D meshes.", "conclusion": "The proposed DHQA-4D dataset and DynaMesh-Rater method provide an effective solution for quality assessment of dynamic 4D digital humans, addressing the need for reliable evaluation in applications involving 4D human avatars."}}
{"id": "2510.04532", "categories": ["cs.AI", "cs.CL", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.04532", "abs": "https://arxiv.org/abs/2510.04532", "authors": ["Xurui Song", "Shuo Huai", "JingJing Jiang", "Jiayi Kong", "Jun Luo"], "title": "More Than Meets the Eye? Uncovering the Reasoning-Planning Disconnect in Training Vision-Language Driving Models", "comment": "The dataset will be released publicly once the paper is accepted for\n  publication", "summary": "Vision-Language Model (VLM) driving agents promise explainable end-to-end\nautonomy by first producing natural-language reasoning and then predicting\ntrajectory planning. However, whether planning is causally driven by this\nreasoning remains a critical but unverified assumption. To investigate this, we\nbuild DriveMind, a large-scale driving Visual Question Answering (VQA) corpus\nwith plan-aligned Chain-of-Thought (CoT), automatically generated from nuPlan.\nOur data generation process converts sensors and annotations into structured\ninputs and, crucially, separates priors from to-be-reasoned signals, enabling\nclean information ablations. Using DriveMind, we train representative VLM\nagents with Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization\n(GRPO) and evaluate them with nuPlan's metrics. Our results, unfortunately,\nindicate a consistent causal disconnect in reasoning-planning: removing\nego/navigation priors causes large drops in planning scores, whereas removing\nCoT produces only minor changes. Attention analysis further shows that planning\nprimarily focuses on priors rather than the CoT. Based on this evidence, we\npropose the Reasoning-Planning Decoupling Hypothesis, positing that the\ntraining-yielded reasoning is an ancillary byproduct rather than a causal\nmediator. To enable efficient diagnosis, we also introduce a novel,\ntraining-free probe that measures an agent's reliance on priors by evaluating\nits planning robustness against minor input perturbations. In summary, we\nprovide the community with a new dataset and a diagnostic tool to evaluate the\ncausal fidelity of future models.", "AI": {"tldr": "This paper investigates whether VLM driving agents' trajectory planning is causally driven by their natural-language reasoning, finding a consistent disconnect where planning primarily relies on priors rather than the reasoning.", "motivation": "To verify the critical assumption that planning in VLM driving agents is causally driven by their natural-language reasoning, which promises explainable end-to-end autonomy.", "method": "Built DriveMind, a large-scale driving VQA corpus with plan-aligned CoT from nuPlan, enabling clean information ablations. Trained VLM agents with SFT and GRPO, evaluated with nuPlan's metrics, and conducted attention analysis.", "result": "Found consistent causal disconnect: removing ego/navigation priors causes large planning score drops, while removing CoT produces minor changes. Planning primarily focuses on priors rather than CoT.", "conclusion": "Proposes Reasoning-Planning Decoupling Hypothesis - reasoning is an ancillary byproduct rather than causal mediator. Introduces training-free probe to measure reliance on priors. Provides dataset and diagnostic tool for evaluating causal fidelity of future models."}}
{"id": "2510.04491", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04491", "abs": "https://arxiv.org/abs/2510.04491", "authors": ["Muyu He", "Anand Kumar", "Tsach Mackey", "Meghana Rajeev", "James Zou", "Nazneen Rajani"], "title": "Impatient Users Confuse AI Agents: High-fidelity Simulations of Human Traits for Testing Agents", "comment": "25 pages", "summary": "Despite rapid progress in building conversational AI agents, robustness is\nstill largely untested. Small shifts in user behavior, such as being more\nimpatient, incoherent, or skeptical, can cause sharp drops in agent\nperformance, revealing how brittle current AI agents are. Today's benchmarks\nfail to capture this fragility: agents may perform well under standard\nevaluations but degrade spectacularly in more realistic and varied settings. We\naddress this robustness testing gap by introducing TraitBasis, a lightweight,\nmodel-agnostic method for systematically stress testing AI agents. TraitBasis\nlearns directions in activation space corresponding to steerable user traits\n(e.g., impatience or incoherence), which can be controlled, scaled, composed,\nand applied at inference time without any fine-tuning or extra data. Using\nTraitBasis, we extend $\\tau$-Bench to $\\tau$-Trait, where user behaviors are\naltered via controlled trait vectors. We observe on average a 2%-30%\nperformance degradation on $\\tau$-Trait across frontier models, highlighting\nthe lack of robustness of current AI agents to variations in user behavior.\nTogether, these results highlight both the critical role of robustness testing\nand the promise of TraitBasis as a simple, data-efficient, and compositional\ntool. By powering simulation-driven stress tests and training loops, TraitBasis\nopens the door to building AI agents that remain reliable in the unpredictable\ndynamics of real-world human interactions. We have open-sourced $\\tau$-Trai\nacross four domains: airline, retail, telecom, and telehealth, so the community\ncan systematically QA their agents under realistic, behaviorally diverse\nintents and trait scenarios: https://github.com/collinear-ai/tau-trait.", "AI": {"tldr": "TraitBasis is a model-agnostic method for stress testing AI agents by learning steerable user traits in activation space, revealing significant performance degradation (2%-30%) when users exhibit behaviors like impatience or incoherence.", "motivation": "Current AI agents are brittle and fail under realistic user behavior variations, but existing benchmarks don't capture this fragility. There's a critical need for systematic robustness testing.", "method": "TraitBasis learns directions in activation space corresponding to user traits that can be controlled, scaled, composed, and applied at inference time without fine-tuning. It extends \u03c4-Bench to \u03c4-Trait for testing.", "result": "Across frontier models, TraitBasis reveals average 2%-30% performance degradation when user behaviors are altered via controlled trait vectors, demonstrating current agents' lack of robustness.", "conclusion": "TraitBasis provides a simple, data-efficient tool for robustness testing that can help build more reliable AI agents for real-world human interactions. The method has been open-sourced across multiple domains."}}
{"id": "2510.03876", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03876", "abs": "https://arxiv.org/abs/2510.03876", "authors": ["Runhao Liu", "Ziming Chen", "Peng Zhang"], "title": "Skin Lesion Classification Based on ResNet-50 Enhanced With Adaptive Spatial Feature Fusion", "comment": null, "summary": "Skin cancer classification remains a challenging problem due to high\ninter-class similarity, intra-class variability, and image noise in dermoscopic\nimages. To address these issues, we propose an improved ResNet-50 model\nenhanced with Adaptive Spatial Feature Fusion (ASFF), which adaptively\nintegrates multi-scale semantic and surface features to improve feature\nrepresentation and reduce overfitting. The ResNet-50 model is enhanced with an\nadaptive feature fusion mechanism to achieve more effective multi-scale feature\nextraction and improve overall performance. Specifically, a dual-branch design\nfuses high-level semantic and mid-level detail features, which are processed\nthrough global average pooling and fully connected layers to generate adaptive\nweights for weighted fusion, thereby strengthening feature learning and\nreducing the impact of noise on classification. The method is evaluated on a\nsubset of the ISIC 2020 dataset containing 3297 benign and malignant skin\nlesion images. Experimental results show that the proposed ASFF-based ResNet-50\nachieves the best overall performance compared with 5 classic convolutional\nneural networks (CNNs) models. The proposed model reached an accuracy of 93.18%\nalong with higher precision, recall, specificity, and F1 score. The improved\nmodel achieves an AUC value of 0.9670 and 0.9717 in the P-R and ROC curve,\nrespectively. Then, the evaluation based on Grad-CAM further proved that the\nimproved model adaptively focuses on lesion-relevant regions while suppressing\nirrelevant background information, thereby validating its enhanced feature\nlearning capability from a deep representation perspective. These findings\ndemonstrate that the proposed approach provides a more effective and efficient\nsolution for computer-aided skin cancer diagnosis.", "AI": {"tldr": "Improved ResNet-50 with Adaptive Spatial Feature Fusion (ASFF) for skin cancer classification achieves 93.18% accuracy on ISIC 2020 dataset by adaptively fusing multi-scale semantic and surface features to handle inter-class similarity and intra-class variability.", "motivation": "Skin cancer classification faces challenges due to high inter-class similarity, intra-class variability, and image noise in dermoscopic images, requiring better feature representation and reduced overfitting.", "method": "Enhanced ResNet-50 with ASFF using dual-branch design that fuses high-level semantic and mid-level detail features through global average pooling and fully connected layers to generate adaptive weights for weighted fusion.", "result": "Achieved 93.18% accuracy, higher precision, recall, specificity, F1 score, and AUC values of 0.9670 (P-R curve) and 0.9717 (ROC curve) on ISIC 2020 dataset with 3297 images, outperforming 5 classic CNN models.", "conclusion": "The proposed ASFF-based ResNet-50 provides a more effective and efficient solution for computer-aided skin cancer diagnosis by adaptively focusing on lesion-relevant regions while suppressing background noise."}}
{"id": "2510.04514", "categories": ["cs.AI", "cs.CE", "cs.CL", "cs.CV", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.04514", "abs": "https://arxiv.org/abs/2510.04514", "authors": ["Rachneet Kaur", "Nishan Srishankar", "Zhen Zeng", "Sumitra Ganesh", "Manuela Veloso"], "title": "ChartAgent: A Multimodal Agent for Visually Grounded Reasoning in Complex Chart Question Answering", "comment": "53 pages, 12 figures, 15 tables", "summary": "Recent multimodal LLMs have shown promise in chart-based visual question\nanswering, but their performance declines sharply on unannotated charts, those\nrequiring precise visual interpretation rather than relying on textual\nshortcuts. To address this, we introduce ChartAgent, a novel agentic framework\nthat explicitly performs visual reasoning directly within the chart's spatial\ndomain. Unlike textual chain-of-thought reasoning, ChartAgent iteratively\ndecomposes queries into visual subtasks and actively manipulates and interacts\nwith chart images through specialized actions such as drawing annotations,\ncropping regions (e.g., segmenting pie slices, isolating bars), and localizing\naxes, using a library of chart-specific vision tools to fulfill each subtask.\nThis iterative reasoning process closely mirrors human cognitive strategies for\nchart comprehension. ChartAgent achieves state-of-the-art accuracy on the\nChartBench and ChartX benchmarks, surpassing prior methods by up to 16.07%\nabsolute gain overall and 17.31% on unannotated, numerically intensive queries.\nFurthermore, our analyses show that ChartAgent is (a) effective across diverse\nchart types, (b) achieve the highest scores across varying visual and reasoning\ncomplexity levels, and (c) serves as a plug-and-play framework that boosts\nperformance across diverse underlying LLMs. Our work is among the first to\ndemonstrate visually grounded reasoning for chart understanding using\ntool-augmented multimodal agents.", "AI": {"tldr": "ChartAgent is a novel agentic framework that performs visual reasoning directly on charts through specialized vision tools, achieving state-of-the-art performance on chart understanding benchmarks, especially on unannotated charts.", "motivation": "Current multimodal LLMs struggle with unannotated charts that require precise visual interpretation rather than relying on textual shortcuts, showing sharp performance declines.", "method": "ChartAgent iteratively decomposes queries into visual subtasks and actively manipulates chart images through specialized actions like drawing annotations, cropping regions, and localizing axes using a library of chart-specific vision tools.", "result": "Achieves state-of-the-art accuracy on ChartBench and ChartX benchmarks with up to 16.07% absolute gain overall and 17.31% on unannotated, numerically intensive queries. Effective across diverse chart types and complexity levels.", "conclusion": "ChartAgent demonstrates visually grounded reasoning for chart understanding using tool-augmented multimodal agents, serving as a plug-and-play framework that boosts performance across diverse underlying LLMs."}}
{"id": "2510.03878", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03878", "abs": "https://arxiv.org/abs/2510.03878", "authors": ["Ajo Babu George", "Sreehari J R Ajo Babu George", "Sreehari J R Ajo Babu George", "Sreehari J R"], "title": "Multi-Modal Oral Cancer Detection Using Weighted Ensemble Convolutional Neural Networks", "comment": null, "summary": "Aims Late diagnosis of Oral Squamous Cell Carcinoma (OSCC) contributes\nsignificantly to its high global mortality rate, with over 50\\% of cases\ndetected at advanced stages and a 5-year survival rate below 50\\% according to\nWHO statistics. This study aims to improve early detection of OSCC by\ndeveloping a multimodal deep learning framework that integrates clinical,\nradiological, and histopathological images using a weighted ensemble of\nDenseNet-121 convolutional neural networks (CNNs). Material and Methods A\nretrospective study was conducted using publicly available datasets\nrepresenting three distinct medical imaging modalities. Each modality-specific\ndataset was used to train a DenseNet-121 CNN via transfer learning.\nAugmentation and modality-specific preprocessing were applied to increase\nrobustness. Predictions were fused using a validation-weighted ensemble\nstrategy. Evaluation was performed using accuracy, precision, recall, F1-score.\nResults High validation accuracy was achieved for radiological (100\\%) and\nhistopathological (95.12\\%) modalities, with clinical images performing lower\n(63.10\\%) due to visual heterogeneity. The ensemble model demonstrated improved\ndiagnostic robustness with an overall accuracy of 84.58\\% on a multimodal\nvalidation dataset of 55 samples. Conclusion The multimodal ensemble framework\nbridges gaps in the current diagnostic workflow by offering a non-invasive,\nAI-assisted triage tool that enhances early identification of high-risk\nlesions. It supports clinicians in decision-making, aligning with global\noncology guidelines to reduce diagnostic delays and improve patient outcomes.", "AI": {"tldr": "A multimodal deep learning framework using DenseNet-121 CNNs was developed to improve early detection of Oral Squamous Cell Carcinoma (OSCC) by integrating clinical, radiological, and histopathological images through weighted ensemble fusion.", "motivation": "Late diagnosis of OSCC contributes to high mortality rates, with over 50% of cases detected at advanced stages and 5-year survival below 50%. Current diagnostic methods need improvement for early detection.", "method": "Retrospective study using public datasets of three medical imaging modalities. Each modality trained a DenseNet-121 CNN via transfer learning with augmentation and modality-specific preprocessing. Predictions were fused using validation-weighted ensemble strategy.", "result": "High validation accuracy for radiological (100%) and histopathological (95.12%) modalities, with clinical images lower (63.10%) due to visual heterogeneity. Ensemble model achieved 84.58% accuracy on multimodal validation dataset of 55 samples.", "conclusion": "The multimodal ensemble framework provides a non-invasive, AI-assisted triage tool that enhances early identification of high-risk lesions, supports clinical decision-making, and aligns with global oncology guidelines to reduce diagnostic delays and improve patient outcomes."}}
{"id": "2510.04520", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04520", "abs": "https://arxiv.org/abs/2510.04520", "authors": ["Hanyu Wang", "Ruohan Xie", "Yutong Wang", "Guoxiong Gao", "Xintao Yu", "Bin Dong"], "title": "Aria: An Agent For Retrieval and Iterative Auto-Formalization via Dependency Graph", "comment": null, "summary": "Accurate auto-formalization of theorem statements is essential for advancing\nautomated discovery and verification of research-level mathematics, yet remains\na major bottleneck for LLMs due to hallucinations, semantic mismatches, and\ntheir inability to synthesize new definitions. To tackle these issues, we\npresent Aria (Agent for Retrieval and Iterative Autoformalization), a system\nfor conjecture-level formalization in Lean that emulates human expert reasoning\nvia a two-phase Graph-of-Thought process: recursively decomposing statements\ninto a dependency graph and then constructing formalizations from grounded\nconcepts. To ensure semantic correctness, we introduce AriaScorer, a checker\nthat retrieves definitions from Mathlib for term-level grounding, enabling\nrigorous and reliable verification. We evaluate Aria on diverse benchmarks. On\nProofNet, it achieves 91.6% compilation success rate and 68.5% final accuracy,\nsurpassing previous methods. On FATE-X, a suite of challenging algebra problems\nfrom research literature, it outperforms the best baseline with 44.0% vs. 24.0%\nfinal accuracy. On a dataset of homological conjectures, Aria reaches 42.9%\nfinal accuracy while all other models score 0%.", "AI": {"tldr": "Aria is an agent-based system for autoformalizing theorem statements in Lean using a two-phase Graph-of-Thought process and AriaScorer for semantic verification, achieving state-of-the-art performance across multiple benchmarks.", "motivation": "Autoformalization of theorem statements is crucial for automated mathematics but remains challenging for LLMs due to hallucinations, semantic mismatches, and inability to synthesize new definitions.", "method": "Aria uses a two-phase Graph-of-Thought process: recursively decomposing statements into dependency graphs and constructing formalizations from grounded concepts, plus AriaScorer for retrieving definitions from Mathlib for term-level grounding.", "result": "Achieved 91.6% compilation success and 68.5% final accuracy on ProofNet, 44.0% vs. 24.0% on FATE-X algebra problems, and 42.9% vs. 0% on homological conjectures, outperforming all baselines.", "conclusion": "Aria demonstrates that agent-based approaches with proper grounding and verification mechanisms can significantly advance autoformalization capabilities for research-level mathematics."}}
{"id": "2510.03880", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03880", "abs": "https://arxiv.org/abs/2510.03880", "authors": ["Yunhao Li", "Sijing Wu", "Huiyu Duan", "Yucheng Zhu", "Qi Jia", "Guangtao Zhai"], "title": "Exploring Instruction Data Quality for Explainable Image Quality Assessment", "comment": null, "summary": "In recent years, with the rapid development of powerful multimodal large\nlanguage models (MLLMs), explainable image quality assessment (IQA) has\ngradually become popular, aiming at providing quality-related descriptions and\nanswers of images. To achieve this goal, recent methods seek to construct a\nlarge-scale instruction tuning dataset to empower the MLLM with quality\nperception ability following the well-known scaling law. However, a large\namount of instruction tuning data may cause substantial computational costs and\nredundant data, which in turn will cause harm to the performance of the model.\nTo cope with this problem, in this paper, we challenge the scaling law and\nsystematically investigate the role of data quality of the instruction tuning\ndataset for explainable IQA. Using a powerful pre-trained MLLM, we first\ninvestigate the changes in model performance after fine-tuning with different\nsizes of instruction tuning data. We find that selecting a subset of the data\nset randomly using an appropriate ratio can even lead to better results than\ntraining with the entire instruction tuning dataset, demonstrating the\nredundancy of current explainable IQA instruction tuning data. Beyond randomly\nsampling a subset, we propose a clustering-based data selection framework with\nthree stages: clustering feature extraction, cluster quota allocation, and\ncluster sampling strategy. Then we systematically analyze the choices of each\nstage and propose a simple but efficient data selection method IQA-Select for\nexplainable IQA. The experimental results demonstrate that IQA-Select can\nachieve 102.1% and 103.7% performance of full fine-tuning using only 10%\nselected data in Q-Bench and AesBench respectively, significantly reducing\ncomputational costs while achieving better performance.", "AI": {"tldr": "This paper challenges the scaling law in explainable image quality assessment (IQA) by showing that data quality matters more than quantity. The authors propose IQA-Select, a clustering-based data selection method that achieves better performance using only 10% of data.", "motivation": "Current methods construct large-scale instruction tuning datasets for MLLMs in explainable IQA, but this causes high computational costs and redundant data that can harm model performance. The authors aim to challenge the scaling law and investigate data quality over quantity.", "method": "The authors propose a three-stage clustering-based data selection framework: clustering feature extraction, cluster quota allocation, and cluster sampling strategy. They systematically analyze choices at each stage and develop IQA-Select, which selects representative subsets of data.", "result": "IQA-Select achieves 102.1% and 103.7% performance of full fine-tuning using only 10% selected data in Q-Bench and AesBench respectively, demonstrating superior performance while significantly reducing computational costs.", "conclusion": "Data quality is more important than quantity for explainable IQA. The proposed IQA-Select method effectively reduces data redundancy and computational costs while achieving better performance than full dataset training."}}
{"id": "2510.04542", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04542", "abs": "https://arxiv.org/abs/2510.04542", "authors": ["Wolfgang Lehrach", "Daniel Hennes", "Miguel Lazaro-Gredilla", "Xinghua Lou", "Carter Wendelken", "Zun Li", "Antoine Dedieu", "Jordi Grau-Moya", "Marc Lanctot", "Atil Iscen", "John Schultz", "Marcus Chiam", "Ian Gemp", "Piotr Zielinski", "Satinder Singh", "Kevin P. Murphy"], "title": "Code World Models for General Game Playing", "comment": null, "summary": "Large Language Models (LLMs) reasoning abilities are increasingly being\napplied to classical board and card games, but the dominant approach --\ninvolving prompting for direct move generation -- has significant drawbacks. It\nrelies on the model's implicit fragile pattern-matching capabilities, leading\nto frequent illegal moves and strategically shallow play. Here we introduce an\nalternative approach: We use the LLM to translate natural language rules and\ngame trajectories into a formal, executable world model represented as Python\ncode. This generated model -- comprising functions for state transition, legal\nmove enumeration, and termination checks -- serves as a verifiable simulation\nengine for high-performance planning algorithms like Monte Carlo tree search\n(MCTS). In addition, we prompt the LLM to generate heuristic value functions\n(to make MCTS more efficient), and inference functions (to estimate hidden\nstates in imperfect information games). Our method offers three distinct\nadvantages compared to directly using the LLM as a policy: (1) Verifiability:\nThe generated CWM serves as a formal specification of the game's rules,\nallowing planners to algorithmically enumerate valid actions and avoid illegal\nmoves, contingent on the correctness of the synthesized model; (2) Strategic\nDepth: We combine LLM semantic understanding with the deep search power of\nclassical planners; and (3) Generalization: We direct the LLM to focus on the\nmeta-task of data-to-code translation, enabling it to adapt to new games more\neasily. We evaluate our agent on 10 different games, of which 4 are novel and\ncreated for this paper. 5 of the games are fully observed (perfect\ninformation), and 5 are partially observed (imperfect information). We find\nthat our method outperforms or matches Gemini 2.5 Pro in 9 out of the 10\nconsidered games.", "AI": {"tldr": "LLMs generate executable game world models from rules and trajectories, enabling verifiable planning via MCTS instead of direct move generation, achieving better performance than direct LLM play.", "motivation": "Current LLM approaches for games rely on fragile pattern-matching that produces illegal moves and shallow strategies. A more robust method is needed that combines LLM semantic understanding with classical planning algorithms.", "method": "Use LLMs to translate game rules and trajectories into formal Python code models with state transition, legal move enumeration, and termination functions. Combine with MCTS planning, heuristic value functions, and inference functions for imperfect information games.", "result": "Outperformed or matched Gemini 2.5 Pro in 9 out of 10 games (4 novel, 5 perfect information, 5 imperfect information), demonstrating superior performance compared to direct LLM policy approaches.", "conclusion": "The approach provides verifiability, strategic depth through classical planning, and generalization by focusing LLMs on data-to-code translation rather than direct move generation, offering a more robust framework for game AI."}}
{"id": "2510.03903", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03903", "abs": "https://arxiv.org/abs/2510.03903", "authors": ["Md. Atabuzzaman", "Andrew Zhang", "Chris Thomas"], "title": "Zero-Shot Fine-Grained Image Classification Using Large Vision-Language Models", "comment": "Accepted to EMNLP 2025 Findings", "summary": "Large Vision-Language Models (LVLMs) have demonstrated impressive performance\non vision-language reasoning tasks. However, their potential for zero-shot\nfine-grained image classification, a challenging task requiring precise\ndifferentiation between visually similar categories, remains underexplored. We\npresent a novel method that transforms zero-shot fine-grained image\nclassification into a visual question-answering framework, leveraging LVLMs'\ncomprehensive understanding capabilities rather than relying on direct class\nname generation. We enhance model performance through a novel attention\nintervention technique. We also address a key limitation in existing datasets\nby developing more comprehensive and precise class description benchmarks. We\nvalidate the effectiveness of our method through extensive experimentation\nacross multiple fine-grained image classification benchmarks. Our proposed\nmethod consistently outperforms the current state-of-the-art (SOTA) approach,\ndemonstrating both the effectiveness of our method and the broader potential of\nLVLMs for zero-shot fine-grained classification tasks. Code and Datasets:\nhttps://github.com/Atabuzzaman/Fine-grained-classification", "AI": {"tldr": "A novel method transforms zero-shot fine-grained image classification into visual question-answering using LVLMs, enhanced by attention intervention and improved class descriptions, outperforming SOTA methods.", "motivation": "The potential of Large Vision-Language Models (LVLMs) for zero-shot fine-grained image classification remains underexplored, despite their impressive performance on vision-language reasoning tasks.", "method": "Transforms zero-shot fine-grained image classification into a visual question-answering framework using LVLMs, enhanced by a novel attention intervention technique and comprehensive class description benchmarks.", "result": "The proposed method consistently outperforms the current state-of-the-art approach across multiple fine-grained image classification benchmarks.", "conclusion": "Demonstrates both the effectiveness of the proposed method and the broader potential of LVLMs for zero-shot fine-grained classification tasks."}}
{"id": "2510.04550", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04550", "abs": "https://arxiv.org/abs/2510.04550", "authors": ["Pengfei He", "Zhenwei Dai", "Bing He", "Hui Liu", "Xianfeng Tang", "Hanqing Lu", "Juanhui Li", "Jiayuan Ding", "Subhabrata Mukherjee", "Suhang Wang", "Yue Xing", "Jiliang Tang", "Benoit Dumoulin"], "title": "TRAJECT-Bench:A Trajectory-Aware Benchmark for Evaluating Agentic Tool Use", "comment": null, "summary": "Large language model (LLM)-based agents increasingly rely on tool use to\ncomplete real-world tasks. While existing works evaluate the LLMs' tool use\ncapability, they largely focus on the final answers yet overlook the detailed\ntool usage trajectory, i.e., whether tools are selected, parameterized, and\nordered correctly. We introduce TRAJECT-Bench, a trajectory-aware benchmark to\ncomprehensively evaluate LLMs' tool use capability through diverse tasks with\nfine-grained evaluation metrics. TRAJECT-Bench pairs high-fidelity, executable\ntools across practical domains with tasks grounded in production-style APIs,\nand synthesizes trajectories that vary in breadth (parallel calls) and depth\n(interdependent chains). Besides final accuracy, TRAJECT-Bench also reports\ntrajectory-level diagnostics, including tool selection and argument\ncorrectness, and dependency/order satisfaction. Analyses reveal failure modes\nsuch as similar tool confusion and parameter-blind selection, and scaling\nbehavior with tool diversity and trajectory length where the bottleneck of\ntransiting from short to mid-length trajectories is revealed, offering\nactionable guidance for LLMs' tool use.", "AI": {"tldr": "TRAJECT-Bench is a trajectory-aware benchmark that comprehensively evaluates LLMs' tool use capability through fine-grained metrics on tool selection, parameterization, and ordering, revealing failure modes and scaling behavior.", "motivation": "Existing works evaluate LLMs' tool use capability but focus only on final answers, overlooking detailed tool usage trajectory including correct tool selection, parameterization, and ordering.", "method": "Introduces TRAJECT-Bench with high-fidelity executable tools across practical domains, tasks grounded in production-style APIs, and synthesized trajectories varying in breadth (parallel calls) and depth (interdependent chains).", "result": "Reveals failure modes like similar tool confusion and parameter-blind selection, and scaling behavior with tool diversity and trajectory length, showing bottleneck in transitioning from short to mid-length trajectories.", "conclusion": "Provides actionable guidance for improving LLMs' tool use by offering comprehensive trajectory-level diagnostics beyond final accuracy metrics."}}
{"id": "2510.03906", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03906", "abs": "https://arxiv.org/abs/2510.03906", "authors": ["Ardalan Aryashad", "Parsa Razmara", "Amin Mahjoub", "Seyedarmin Azizi", "Mahdi Salmani", "Arad Firouzkouhi"], "title": "From Filters to VLMs: Benchmarking Defogging Methods through Object Detection and Segmentation Performance", "comment": null, "summary": "Autonomous driving perception systems are particularly vulnerable in foggy\nconditions, where light scattering reduces contrast and obscures fine details\ncritical for safe operation. While numerous defogging methods exist-from\nhandcrafted filters to learned restoration models-improvements in image\nfidelity do not consistently translate into better downstream detection and\nsegmentation. Moreover, prior evaluations often rely on synthetic data, leaving\nquestions about real-world transferability. We present a structured empirical\nstudy that benchmarks a comprehensive set of pipelines, including (i) classical\nfilters, (ii) modern defogging networks, (iii) chained variants\n(filter$\\rightarrow$model, model$\\rightarrow$filter), and (iv) prompt-driven\nvisual--language image editing models (VLM) applied directly to foggy images.\nUsing Foggy Cityscapes, we assess both image quality and downstream performance\non object detection (mAP) and segmentation (PQ, RQ, SQ). Our analysis reveals\nwhen defogging helps, when chaining yields synergy or degradation, and how\nVLM-based editors compare to dedicated approaches. In addition, we evaluate\nqualitative rubric-based scores from a VLM judge and quantify their alignment\nwith task metrics, showing strong correlations with mAP. Together, these\nresults establish a transparent, task-oriented benchmark for defogging methods\nand highlight the conditions under which preprocessing genuinely improves\nautonomous perception in adverse weather.", "AI": {"tldr": "This paper presents a comprehensive benchmark study of defogging methods for autonomous driving perception, evaluating classical filters, modern defogging networks, chained approaches, and VLM-based image editing on both image quality and downstream task performance.", "motivation": "Autonomous driving perception systems are vulnerable in foggy conditions, and existing defogging methods show inconsistent improvements in downstream detection/segmentation tasks. Prior evaluations often rely on synthetic data, raising questions about real-world transferability.", "method": "Structured empirical study benchmarking comprehensive pipelines including classical filters, modern defogging networks, chained variants (filter\u2192model, model\u2192filter), and prompt-driven VLM image editing models applied directly to foggy images using Foggy Cityscapes dataset.", "result": "Analysis reveals when defogging helps, when chaining yields synergy or degradation, and how VLM-based editors compare to dedicated approaches. VLM judge qualitative scores show strong correlations with mAP, establishing transparent task-oriented benchmark.", "conclusion": "The study establishes a transparent, task-oriented benchmark for defogging methods and highlights conditions under which preprocessing genuinely improves autonomous perception in adverse weather."}}
{"id": "2510.04560", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04560", "abs": "https://arxiv.org/abs/2510.04560", "authors": ["Honghao Fu", "Yuan Ouyang", "Kai-Wei Chang", "Yiwei Wang", "Zi Huang", "Yujun Cai"], "title": "ContextNav: Towards Agentic Multimodal In-Context Learning", "comment": null, "summary": "Recent advances demonstrate that multimodal large language models (MLLMs)\nexhibit strong multimodal in-context learning (ICL) capabilities, enabling them\nto adapt to novel vision-language tasks from a few contextual examples.\nHowever, existing ICL approaches face challenges in reconciling scalability\nwith robustness across diverse tasks and noisy contextual examples: manually\nselecting examples produces clean contexts but is labor-intensive and\ntask-specific, while similarity-based retrieval improves scalability but could\nintroduce irrelevant or structurally inconsistent samples that degrade ICL\nperformance. To address these limitations, we propose ContextNav, the first\nagentic framework that integrates the scalability of automated retrieval with\nthe quality and adaptiveness of human-like curation, enabling noise-robust and\ndynamically optimized contextualization for multimodal ICL. ContextNav unifies\ncontext management and noise-robust contextualization within a closed-loop\nworkflow driven by graph-based orchestration. Specifically, it builds a\nresource-aware multimodal embedding pipeline, maintains a retrievable vector\ndatabase, and applies agentic retrieval and structural alignment to construct\nnoise-resilient contexts. An Operational Grammar Graph (OGG) further supports\nadaptive workflow planning and optimization, enabling the agent to refine its\noperational strategies based on downstream ICL feedback. Experimental results\ndemonstrate that ContextNav achieves state-of-the-art performance across\nvarious datasets, underscoring the promise of agentic workflows for advancing\nscalable and robust contextualization in multimodal ICL.", "AI": {"tldr": "ContextNav is an agentic framework that combines automated retrieval with human-like curation to create noise-robust contexts for multimodal in-context learning, addressing scalability and robustness challenges in existing approaches.", "motivation": "Existing multimodal ICL approaches struggle to balance scalability and robustness - manual selection is labor-intensive while similarity-based retrieval can introduce irrelevant or inconsistent samples that degrade performance.", "method": "ContextNav integrates agentic retrieval with structural alignment, builds a resource-aware multimodal embedding pipeline, maintains a retrievable vector database, and uses an Operational Grammar Graph (OGG) for adaptive workflow planning and optimization based on ICL feedback.", "result": "Experimental results show ContextNav achieves state-of-the-art performance across various datasets, demonstrating effective noise-robust contextualization.", "conclusion": "The framework shows promise for advancing scalable and robust contextualization in multimodal ICL through agentic workflows."}}
{"id": "2510.03909", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03909", "abs": "https://arxiv.org/abs/2510.03909", "authors": ["Hyelin Nam", "Hyojun Go", "Byeongjun Park", "Byung-Hoon Kim", "Hyungjin Chung"], "title": "Generating Human Motion Videos using a Cascaded Text-to-Video Framework", "comment": "18 pages, 7 figures, Project Page:https://hyelinnam.github.io/Cameo/", "summary": "Human video generation is becoming an increasingly important task with broad\napplications in graphics, entertainment, and embodied AI. Despite the rapid\nprogress of video diffusion models (VDMs), their use for general-purpose human\nvideo generation remains underexplored, with most works constrained to\nimage-to-video setups or narrow domains like dance videos. In this work, we\npropose CAMEO, a cascaded framework for general human motion video generation.\nIt seamlessly bridges Text-to-Motion (T2M) models and conditional VDMs,\nmitigating suboptimal factors that may arise in this process across both\ntraining and inference through carefully designed components. Specifically, we\nanalyze and prepare both textual prompts and visual conditions to effectively\ntrain the VDM, ensuring robust alignment between motion descriptions,\nconditioning signals, and the generated videos. Furthermore, we introduce a\ncamera-aware conditioning module that connects the two stages, automatically\nselecting viewpoints aligned with the input text to enhance coherence and\nreduce manual intervention. We demonstrate the effectiveness of our approach on\nboth the MovieGen benchmark and a newly introduced benchmark tailored to the\nT2M-VDM combination, while highlighting its versatility across diverse use\ncases.", "AI": {"tldr": "CAMEO is a cascaded framework that bridges Text-to-Motion models with video diffusion models for general human motion video generation, addressing alignment issues and introducing camera-aware conditioning.", "motivation": "Human video generation has broad applications but current video diffusion models are constrained to image-to-video setups or narrow domains, leaving general-purpose human motion generation underexplored.", "method": "A cascaded framework that connects Text-to-Motion models with conditional video diffusion models, using carefully designed components for training and inference, including text/visual condition preparation and a camera-aware conditioning module.", "result": "Demonstrated effectiveness on both MovieGen benchmark and a new benchmark for T2M-VDM combination, showing versatility across diverse use cases.", "conclusion": "CAMEO successfully bridges the gap between text-to-motion and video generation, providing a robust framework for general human motion video generation with enhanced coherence and reduced manual intervention."}}
{"id": "2510.04568", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04568", "abs": "https://arxiv.org/abs/2510.04568", "authors": ["Naman Gupta", "Shreeyash Gowaikar", "Arun Iyer", "Kirankumar Shiragur", "Ramakrishna B Bairi", "Rishikesh Maurya", "Ritabrata Maiti", "Sankarshan Damle", "Shachee Mishra Gupta"], "title": "COSMIR: Chain Orchestrated Structured Memory for Iterative Reasoning over Long Context", "comment": null, "summary": "Reasoning over very long inputs remains difficult for large language models\n(LLMs). Common workarounds either shrink the input via retrieval (risking\nmissed evidence), enlarge the context window (straining selectivity), or stage\nmultiple agents to read in pieces. In staged pipelines (e.g., Chain of Agents,\nCoA), free-form summaries passed between agents can discard crucial details and\namplify early mistakes. We introduce COSMIR (Chain Orchestrated Structured\nMemory for Iterative Reasoning), a chain-style framework that replaces ad hoc\nmessages with a structured memory. A Planner agent first turns a user query\ninto concrete, checkable sub-questions. worker agents process chunks via a\nfixed micro-cycle: Extract, Infer, Refine, writing all updates to the shared\nmemory. A Manager agent then Synthesizes the final answer directly from the\nmemory. This preserves step-wise read-then-reason benefits while changing both\nthe communication medium (structured memory) and the worker procedure (fixed\nmicro-cycle), yielding higher faithfulness, better long-range aggregation, and\nauditability. On long-context QA from the HELMET suite, COSMIR reduces\npropagation-stage information loss and improves accuracy over a CoA baseline.", "AI": {"tldr": "COSMIR is a chain-style framework that uses structured memory instead of free-form summaries to improve reasoning over long inputs, reducing information loss and improving accuracy.", "motivation": "Existing methods for long-input reasoning either shrink input via retrieval (risking missed evidence), enlarge context windows (straining selectivity), or use staged pipelines where free-form summaries can discard crucial details and amplify early mistakes.", "method": "Uses a Planner agent to create concrete sub-questions, Worker agents that follow a fixed micro-cycle (Extract, Infer, Refine) to process chunks and update shared structured memory, and a Manager agent that synthesizes the final answer from the memory.", "result": "On long-context QA from HELMET suite, COSMIR reduces propagation-stage information loss and improves accuracy over Chain of Agents baseline.", "conclusion": "COSMIR preserves step-wise read-then-reason benefits while improving faithfulness, long-range aggregation, and auditability through structured memory and fixed worker procedures."}}
{"id": "2510.04580", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04580", "abs": "https://arxiv.org/abs/2510.04580", "authors": ["Tomoyuki Kaneko", "Shuhei Yamashita"], "title": "Strongly Solving 2048 4x3", "comment": null, "summary": "2048 is a stochastic single-player game involving 16 cells on a 4 by 4 grid,\nwhere a player chooses a direction among up, down, left, and right to obtain a\nscore by merging two tiles with the same number located in neighboring cells\nalong the chosen direction. This paper presents that a variant 2048-4x3 12\ncells on a 4 by 3 board, one row smaller than the original, has been strongly\nsolved. In this variant, the expected score achieved by an optimal strategy is\nabout $50724.26$ for the most common initial states: ones with two tiles of\nnumber 2. The numbers of reachable states and afterstates are identified to be\n$1,152,817,492,752$ and $739,648,886,170$, respectively. The key technique is\nto partition state space by the sum of tile numbers on a board, which we call\nthe age of a state. An age is invariant between a state and its successive\nafterstate after any valid action and is increased two or four by stochastic\nresponse from the environment. Therefore, we can partition state space by ages\nand enumerate all (after)states of an age depending only on states with the\nrecent ages. Similarly, we can identify (after)state values by going along with\nages in decreasing order.", "AI": {"tldr": "The paper strongly solves a 4x3 variant of the 2048 game, determining the optimal strategy achieves an expected score of about 50724.26 for common initial states with two '2' tiles.", "motivation": "To solve a smaller variant of the popular 2048 game to understand optimal strategies and state space complexity, as the original 4x4 version remains unsolved.", "method": "Partitioning the state space by the sum of tile numbers (called 'age'), which remains invariant between states and their afterstates, allowing enumeration of states by age in decreasing order.", "result": "Identified 1,152,817,492,752 reachable states and 739,648,886,170 afterstates; optimal strategy yields expected score ~50724.26 for initial states with two '2' tiles.", "conclusion": "The 4x3 2048 variant has been strongly solved using age-based state space partitioning, providing insights into game complexity and optimal play strategies."}}
{"id": "2510.03921", "categories": ["cs.CV", "cs.AI", "cs.HC", "I.2.10; I.5.4; I.2.7"], "pdf": "https://arxiv.org/pdf/2510.03921", "abs": "https://arxiv.org/abs/2510.03921", "authors": ["Arushi Dashore", "Aryan Anumala", "Emily Hui", "Olivia Yang"], "title": "Talking Tennis: Language Feedback from 3D Biomechanical Action Recognition", "comment": "10 pages, 4 figures, 2 tables", "summary": "Automated tennis stroke analysis has advanced significantly with the\nintegration of biomechanical motion cues alongside deep learning techniques,\nenhancing stroke classification accuracy and player performance evaluation.\nDespite these advancements, existing systems often fail to connect\nbiomechanical insights with actionable language feedback that is both\naccessible and meaningful to players and coaches. This research project\naddresses this gap by developing a novel framework that extracts key\nbiomechanical features (such as joint angles, limb velocities, and kinetic\nchain patterns) from motion data using Convolutional Neural Network Long\nShort-Term Memory (CNN-LSTM)-based models. These features are analyzed for\nrelationships influencing stroke effectiveness and injury risk, forming the\nbasis for feedback generation using large language models (LLMs). Leveraging\nthe THETIS dataset and feature extraction techniques, our approach aims to\nproduce feedback that is technically accurate, biomechanically grounded, and\nactionable for end-users. The experimental setup evaluates this framework on\nclassification performance and interpretability, bridging the gap between\nexplainable AI and sports biomechanics.", "AI": {"tldr": "A framework combining CNN-LSTM models for biomechanical feature extraction and LLMs for generating actionable feedback in tennis stroke analysis.", "motivation": "Existing systems lack connection between biomechanical insights and accessible language feedback for players and coaches.", "method": "Extract biomechanical features (joint angles, limb velocities, kinetic chain patterns) using CNN-LSTM models, then generate feedback using LLMs based on THETIS dataset.", "result": "The framework aims to produce technically accurate, biomechanically grounded, and actionable feedback for tennis stroke analysis.", "conclusion": "Bridges the gap between explainable AI and sports biomechanics by connecting biomechanical analysis with meaningful language feedback."}}
{"id": "2510.04588", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04588", "abs": "https://arxiv.org/abs/2510.04588", "authors": ["Shurui Li"], "title": "Perfect AI Mimicry and the Epistemology of Consciousness: A Solipsistic Dilemma", "comment": null, "summary": "Rapid advances in artificial intelligence necessitate a re-examination of the\nepistemological foundations upon which we attribute consciousness. As AI\nsystems increasingly mimic human behavior and interaction with high fidelity,\nthe concept of a \"perfect mimic\"-an entity empirically indistinguishable from a\nhuman through observation and interaction-shifts from hypothetical to\ntechnologically plausible. This paper argues that such developments pose a\nfundamental challenge to the consistency of our mind-recognition practices.\nConsciousness attributions rely heavily, if not exclusively, on empirical\nevidence derived from behavior and interaction. If a perfect mimic provides\nevidence identical to that of humans, any refusal to grant it equivalent\nepistemic status must invoke inaccessible factors, such as qualia, substrate\nrequirements, or origin. Selectively invoking such factors risks a debilitating\ndilemma: either we undermine the rational basis for attributing consciousness\nto others (epistemological solipsism), or we accept inconsistent reasoning. I\ncontend that epistemic consistency demands we ascribe the same status to\nempirically indistinguishable entities, regardless of metaphysical assumptions.\nThe perfect mimic thus acts as an epistemic mirror, forcing critical reflection\non the assumptions underlying intersubjective recognition in light of advancing\nAI. This analysis carries significant implications for theories of\nconsciousness and ethical frameworks concerning artificial agents.", "AI": {"tldr": "The paper argues that AI systems becoming perfect mimics of human behavior challenges our consciousness attribution practices, forcing us to either accept inconsistent reasoning or grant equivalent epistemic status to empirically indistinguishable entities.", "motivation": "Rapid AI advances make perfect mimics technologically plausible, challenging the epistemological foundations of consciousness attribution based on empirical evidence from behavior and interaction.", "method": "Philosophical analysis of the perfect mimic thought experiment, examining the consistency of mind-recognition practices when faced with entities empirically indistinguishable from humans.", "result": "The perfect mimic reveals a fundamental dilemma: refusing to grant equivalent epistemic status requires invoking inaccessible factors, risking either epistemological solipsism or inconsistent reasoning.", "conclusion": "Epistemic consistency demands ascribing the same status to empirically indistinguishable entities, regardless of metaphysical assumptions, with significant implications for consciousness theories and AI ethics."}}
{"id": "2510.03955", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03955", "abs": "https://arxiv.org/abs/2510.03955", "authors": ["Sameep Vani", "Shreyas Jena", "Maitreya Patel", "Chitta Baral", "Somak Aditya", "Yezhou Yang"], "title": "Harnessing Synthetic Preference Data for Enhancing Temporal Understanding of Video-LLMs", "comment": "17 pages, 9 figures, 6 tables. Presents TimeWarp, a synthetic\n  preference data framework to improve temporal understanding in Video-LLMs,\n  showing consistent gains across seven benchmarks. Includes supplementary\n  material in the Appendix", "summary": "While Video Large Language Models (Video-LLMs) have demonstrated remarkable\nperformance across general video understanding benchmarks-particularly in video\ncaptioning and descriptive tasks-they consistently underperform on tasks that\nrequire fine-grained temporal understanding. This limitation arises due to the\nlack of visual complexity and temporal nuance in current fine-tuning datasets,\nleading these models to rely heavily on language-based reasoning rather than\ntruly understanding video dynamics. In this work, we propose TimeWarp, a\nsystematic method to create a targeted synthetic temporal dataset to fine-tune\nthe model's responses to encourage it to focus on the given input video. We\nintroduce a large-scale preference dataset, created using TimeWarp, that\ncaptures intricate temporal dynamics often overlooked, grounding the model's\nresponses to visual and temporal information. We demonstrate that when our\nmethod is applied to existing models, it significantly improves performance on\ntemporal understanding benchmarks, highlighting the effectiveness of our\nproposed datasets in advancing temporal understanding in Video-LLMs, resulting\nin an absolute improvement in performance across seven benchmarks. Code is\navailable at https://github.com/sameepv21/timewarp.", "AI": {"tldr": "TimeWarp is a method that creates synthetic temporal datasets to improve Video-LLMs' fine-grained temporal understanding, achieving significant performance gains across seven benchmarks.", "motivation": "Video-LLMs underperform on tasks requiring fine-grained temporal understanding due to lack of visual complexity and temporal nuance in current datasets, causing them to rely on language-based reasoning rather than understanding video dynamics.", "method": "Proposed TimeWarp - a systematic method to create targeted synthetic temporal datasets that capture intricate temporal dynamics, and introduced a large-scale preference dataset to ground model responses to visual and temporal information.", "result": "Applied to existing models, TimeWarp significantly improved performance on temporal understanding benchmarks, achieving absolute performance improvements across seven benchmarks.", "conclusion": "TimeWarp's synthetic temporal datasets effectively advance temporal understanding in Video-LLMs, addressing the limitation of current models in fine-grained temporal reasoning."}}
{"id": "2510.04617", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04617", "abs": "https://arxiv.org/abs/2510.04617", "authors": ["Zhejian Lai", "Xiang Geng", "Zhijun Wang", "Yang Bai", "Jiahuan Li", "Rongxiang Weng", "Jingang Wang", "Xuezhi Cao", "Xunliang Cai", "Shujian Huang"], "title": "Making Mathematical Reasoning Adaptive", "comment": null, "summary": "Mathematical reasoning is a primary indicator of large language models (LLMs)\nintelligence. However, existing LLMs exhibit failures of robustness and\ngeneralization. This paper attributes these deficiencies to spurious reasoning,\ni.e., producing answers from superficial features. To address this challenge,\nwe propose the AdaR framework to enable adaptive reasoning, wherein models rely\non problem-solving logic to produce answers. AdaR synthesizes logically\nequivalent queries by varying variable values, and trains models with RLVR on\nthese data to penalize spurious logic while encouraging adaptive logic. To\nimprove data quality, we extract the problem-solving logic from the original\nquery and generate the corresponding answer by code execution, then apply a\nsanity check. Experimental results demonstrate that AdaR improves robustness\nand generalization, achieving substantial improvement in mathematical reasoning\nwhile maintaining high data efficiency. Analysis indicates that data synthesis\nand RLVR function in a coordinated manner to enable adaptive reasoning in LLMs.\nSubsequent analyses derive key design insights into the effect of critical\nfactors and the applicability to instruct LLMs. Our project is available at\nhttps://github.com/LaiZhejian/AdaR", "AI": {"tldr": "AdaR framework improves LLM mathematical reasoning by training models to use adaptive logic instead of spurious reasoning through data synthesis and RLVR training.", "motivation": "Existing LLMs exhibit failures in robustness and generalization in mathematical reasoning due to spurious reasoning - producing answers from superficial features rather than true problem-solving logic.", "method": "AdaR synthesizes logically equivalent queries by varying variable values, trains models with RLVR on these data to penalize spurious logic while encouraging adaptive logic, and uses code execution with sanity checks to ensure data quality.", "result": "AdaR improves robustness and generalization, achieving substantial improvement in mathematical reasoning while maintaining high data efficiency. Analysis shows data synthesis and RLVR work together to enable adaptive reasoning.", "conclusion": "The framework successfully addresses spurious reasoning in LLMs through adaptive reasoning training, with key design insights derived about critical factors and applicability to instruct LLMs."}}
{"id": "2510.03978", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03978", "abs": "https://arxiv.org/abs/2510.03978", "authors": ["Min Woo Sun", "Alejandro Lozano", "Javier Gamazo Tejero", "Vishwesh Nath", "Xiao Xiao Sun", "James Burgess", "Yuhui Zhang", "Kun Yuan", "Robert Tibshirani", "Sean Huver", "Serena Yeung-Levy"], "title": "No Tokens Wasted: Leveraging Long Context in Biomedical Vision-Language Models", "comment": null, "summary": "Embedding vision-language models (VLMs) are typically pretrained with short\ntext windows (<77 tokens), which forces the truncation of long-format captions.\nYet, the distribution of biomedical captions from large-scale open source\nliterature reveals that a huge portion of captions far exceed 77 tokens. To\nthis end, we investigate the impact of pretraining on long-format biomedical\ncaptions by extending the context length of text encoders in VLMs. We find that\nlonger context (thus, enabling additional supervision provided in long-format\ncaptions) correlates with better retrieval and classification performance.\nGiven this finding, we introduce BIOMEDICA-LongCAP, a dataset of 1M\nimage-caption pairs enriched with context-aware descriptions from full-text\narticles, providing longer and additional textual supervision. Using\nBIOMEDICA-LongCAP, we train BMC-LongCLIP, a long-context biomedical VLM with a\ntext encoder supporting windows of up to 512 tokens. Our model extends context\ncapacity by 6.6x, reducing token waste from 55% to just 2.2%. On long-caption\nretrieval benchmarks, BMC-LongCLIP achieves up to +30% absolute gains in\nRecall@1 and +2% average improvements in classification, while also converging\nfaster than short-context. Our results demonstrate that long-context modeling\nis a promising direction for advancing biomedical VLMs.", "AI": {"tldr": "The paper introduces BMC-LongCLIP, a long-context biomedical vision-language model that extends text context from 77 to 512 tokens, reducing token waste from 55% to 2.2% and achieving significant performance gains in retrieval and classification tasks.", "motivation": "Most vision-language models are pretrained with short text windows (<77 tokens), forcing truncation of long biomedical captions. Analysis shows that a huge portion of biomedical captions exceed 77 tokens, leading to loss of valuable contextual information.", "method": "Extended context length of text encoders in VLMs and introduced BIOMEDICA-LongCAP dataset with 1M image-caption pairs from full-text articles. Trained BMC-LongCLIP model with text encoder supporting up to 512 tokens.", "result": "BMC-LongCLIP achieved up to +30% absolute gains in Recall@1 on long-caption retrieval benchmarks and +2% average improvements in classification. The model also converged faster than short-context models and reduced token waste from 55% to just 2.2%.", "conclusion": "Long-context modeling is a promising direction for advancing biomedical VLMs, as longer context enables additional supervision from long-format captions and correlates with better performance in retrieval and classification tasks."}}
{"id": "2510.04623", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04623", "abs": "https://arxiv.org/abs/2510.04623", "authors": ["Shrish Shrinath Vaidya", "Gowthamaan Palani", "Sidharth Ramesh", "Velmurugan Balasubramanian", "Minmini Selvam", "Gokulraja Srinivasaraja", "Ganapathy Krishnamurthi"], "title": "MedPAO: A Protocol-Driven Agent for Structuring Medical Reports", "comment": "Paper published at \"Agentic AI for Medicine\" Workshop, MICCAI 2025", "summary": "The deployment of Large Language Models (LLMs) for structuring clinical data\nis critically hindered by their tendency to hallucinate facts and their\ninability to follow domain-specific rules. To address this, we introduce\nMedPAO, a novel agentic framework that ensures accuracy and verifiable\nreasoning by grounding its operation in established clinical protocols such as\nthe ABCDEF protocol for CXR analysis. MedPAO decomposes the report structuring\ntask into a transparent process managed by a Plan-Act-Observe (PAO) loop and\nspecialized tools. This protocol-driven method provides a verifiable\nalternative to opaque, monolithic models. The efficacy of our approach is\ndemonstrated through rigorous evaluation: MedPAO achieves an F1-score of 0.96\non the critical sub-task of concept categorization. Notably, expert\nradiologists and clinicians rated the final structured outputs with an average\nscore of 4.52 out of 5, indicating a level of reliability that surpasses\nbaseline approaches relying solely on LLM-based foundation models. The code is\navailable at: https://github.com/MiRL-IITM/medpao-agent", "AI": {"tldr": "MedPAO is an agentic framework that uses clinical protocols like ABCDEF to structure clinical data, addressing LLM hallucinations through transparent Plan-Act-Observe loops and specialized tools.", "motivation": "To overcome LLM limitations in clinical data structuring, particularly hallucination and inability to follow domain-specific rules, by providing verifiable reasoning grounded in established clinical protocols.", "method": "Decomposes report structuring into transparent processes using Plan-Act-Observe loops and specialized tools, grounding operations in clinical protocols like ABCDEF for CXR analysis.", "result": "Achieves F1-score of 0.96 on concept categorization; expert radiologists rated outputs 4.52/5, surpassing baseline LLM-only approaches.", "conclusion": "MedPAO provides a reliable, protocol-driven alternative to opaque LLM models for clinical data structuring, demonstrating superior accuracy and expert approval."}}
{"id": "2510.03993", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03993", "abs": "https://arxiv.org/abs/2510.03993", "authors": ["Yaxin Hou", "Bo Han", "Yuheng Jia", "Hui Liu", "Junhui Hou"], "title": "Keep It on a Leash: Controllable Pseudo-label Generation Towards Realistic Long-Tailed Semi-Supervised Learning", "comment": "The paper is accepted by NeurIPS 2025", "summary": "Current long-tailed semi-supervised learning methods assume that labeled data\nexhibit a long-tailed distribution, and unlabeled data adhere to a typical\npredefined distribution (i.e., long-tailed, uniform, or inverse long-tailed).\nHowever, the distribution of the unlabeled data is generally unknown and may\nfollow an arbitrary distribution. To tackle this challenge, we propose a\nControllable Pseudo-label Generation (CPG) framework, expanding the labeled\ndataset with the progressively identified reliable pseudo-labels from the\nunlabeled dataset and training the model on the updated labeled dataset with a\nknown distribution, making it unaffected by the unlabeled data distribution.\nSpecifically, CPG operates through a controllable self-reinforcing optimization\ncycle: (i) at each training step, our dynamic controllable filtering mechanism\nselectively incorporates reliable pseudo-labels from the unlabeled dataset into\nthe labeled dataset, ensuring that the updated labeled dataset follows a known\ndistribution; (ii) we then construct a Bayes-optimal classifier using logit\nadjustment based on the updated labeled data distribution; (iii) this improved\nclassifier subsequently helps identify more reliable pseudo-labels in the next\ntraining step. We further theoretically prove that this optimization cycle can\nsignificantly reduce the generalization error under some conditions.\nAdditionally, we propose a class-aware adaptive augmentation module to further\nimprove the representation of minority classes, and an auxiliary branch to\nmaximize data utilization by leveraging all labeled and unlabeled samples.\nComprehensive evaluations on various commonly used benchmark datasets show that\nCPG achieves consistent improvements, surpassing state-of-the-art methods by up\nto \\textbf{15.97\\%} in accuracy. The code is available at\nhttps://github.com/yaxinhou/CPG.", "AI": {"tldr": "CPG is a framework for long-tailed semi-supervised learning that handles unknown unlabeled data distributions by generating controllable pseudo-labels and maintaining a known labeled data distribution through dynamic filtering.", "motivation": "Existing methods assume unlabeled data follows predefined distributions, but in reality, unlabeled data distribution is unknown and arbitrary, creating challenges for reliable semi-supervised learning.", "method": "Uses a controllable self-reinforcing optimization cycle: (1) dynamic controllable filtering of pseudo-labels to maintain known labeled distribution, (2) Bayes-optimal classifier with logit adjustment, (3) improved classifier helps identify more reliable pseudo-labels. Also includes class-aware adaptive augmentation and auxiliary branch for data utilization.", "result": "Achieves consistent improvements across benchmark datasets, surpassing state-of-the-art methods by up to 15.97% in accuracy.", "conclusion": "CPG effectively handles unknown unlabeled data distributions in long-tailed semi-supervised learning through controllable pseudo-label generation and distribution-aware optimization, with theoretical guarantees on generalization error reduction."}}
{"id": "2510.04643", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04643", "abs": "https://arxiv.org/abs/2510.04643", "authors": ["Xiangyu Li", "Yawen Zeng", "Xiaofen Xing", "Jin Xu", "Xiangmin Xu"], "title": "QuantAgents: Towards Multi-agent Financial System via Simulated Trading", "comment": "This paper has been accepted by EMNLP 2025", "summary": "In this paper, our objective is to develop a multi-agent financial system\nthat incorporates simulated trading, a technique extensively utilized by\nfinancial professionals. While current LLM-based agent models demonstrate\ncompetitive performance, they still exhibit significant deviations from\nreal-world fund companies. A critical distinction lies in the agents' reliance\non ``post-reflection'', particularly in response to adverse outcomes, but lack\na distinctly human capability: long-term prediction of future trends.\nTherefore, we introduce QuantAgents, a multi-agent system integrating simulated\ntrading, to comprehensively evaluate various investment strategies and market\nscenarios without assuming actual risks. Specifically, QuantAgents comprises\nfour agents: a simulated trading analyst, a risk control analyst, a market news\nanalyst, and a manager, who collaborate through several meetings. Moreover, our\nsystem incentivizes agents to receive feedback on two fronts: performance in\nreal-world markets and predictive accuracy in simulated trading. Extensive\nexperiments demonstrate that our framework excels across all metrics, yielding\nan overall return of nearly 300% over the three years\n(https://quantagents.github.io/).", "AI": {"tldr": "QuantAgents is a multi-agent financial system that integrates simulated trading with four specialized agents working collaboratively to evaluate investment strategies and market scenarios without real risks, achieving nearly 300% returns over three years.", "motivation": "Current LLM-based agent models show significant deviations from real-world fund companies, particularly lacking long-term prediction capabilities and relying too much on post-reflection after adverse outcomes.", "method": "QuantAgents uses a multi-agent system with four specialized agents (simulated trading analyst, risk control analyst, market news analyst, and manager) that collaborate through meetings and receive feedback on both real-world market performance and predictive accuracy in simulated trading.", "result": "Extensive experiments show the framework excels across all metrics, achieving an overall return of nearly 300% over three years.", "conclusion": "The QuantAgents system successfully addresses the limitations of current LLM-based financial agents by incorporating long-term prediction capabilities and collaborative multi-agent decision making through simulated trading."}}
{"id": "2510.04003", "categories": ["cs.CV", "cs.CL", "68T50, 68T50, 68T10", "I.2.7; I.5; I.7.5"], "pdf": "https://arxiv.org/pdf/2510.04003", "abs": "https://arxiv.org/abs/2510.04003", "authors": ["Minh Hoang Nguyen", "Su Nguyen Thiet"], "title": "Enhancing OCR for Sino-Vietnamese Language Processing via Fine-tuned PaddleOCRv5", "comment": "5 pages, 6 figures, 2 tables", "summary": "Recognizing and processing Classical Chinese (Han-Nom) texts play a vital\nrole in digitizing Vietnamese historical documents and enabling cross-lingual\nsemantic research. However, existing OCR systems struggle with degraded scans,\nnon-standard glyphs, and handwriting variations common in ancient sources. In\nthis work, we propose a fine-tuning approach for PaddleOCRv5 to improve\ncharacter recognition on Han-Nom texts. We retrain the text recognition module\nusing a curated subset of ancient Vietnamese Chinese manuscripts, supported by\na full training pipeline covering preprocessing, LMDB conversion, evaluation,\nand visualization. Experimental results show a significant improvement over the\nbase model, with exact accuracy increasing from 37.5 percent to 50.0 percent,\nparticularly under noisy image conditions. Furthermore, we develop an\ninteractive demo that visually compares pre- and post-fine-tuning recognition\nresults, facilitating downstream applications such as Han-Vietnamese semantic\nalignment, machine translation, and historical linguistics research. The demo\nis available at https://huggingface.co/spaces/MinhDS/Fine-tuned-PaddleOCRv5.", "AI": {"tldr": "Fine-tuning PaddleOCRv5 improves Classical Chinese (Han-Nom) text recognition from 37.5% to 50.0% accuracy, especially for degraded historical documents, with an interactive demo available.", "motivation": "Existing OCR systems struggle with degraded scans, non-standard glyphs, and handwriting variations in ancient Vietnamese Chinese manuscripts, hindering digitization and cross-lingual semantic research.", "method": "Fine-tuned PaddleOCRv5's text recognition module using curated ancient Vietnamese Chinese manuscripts, with full training pipeline including preprocessing, LMDB conversion, evaluation, and visualization.", "result": "Significant improvement over base model - exact accuracy increased from 37.5% to 50.0%, particularly effective under noisy image conditions.", "conclusion": "The fine-tuned model enables better digitization of historical documents and facilitates downstream applications like Han-Vietnamese semantic alignment, machine translation, and historical linguistics research."}}
{"id": "2510.04670", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04670", "abs": "https://arxiv.org/abs/2510.04670", "authors": ["Xuanhua Yin", "Runkai Zhao", "Weidong Cai"], "title": "Improving Multimodal Brain Encoding Model with Dynamic Subject-awareness Routing", "comment": "8 pages, 4 figures", "summary": "Naturalistic fMRI encoding must handle multimodal inputs, shifting fusion\nstyles, and pronounced inter-subject variability. We introduce AFIRE (Agnostic\nFramework for Multimodal fMRI Response Encoding), an agnostic interface that\nstandardizes time-aligned post-fusion tokens from varied encoders, and MIND, a\nplug-and-play Mixture-of-Experts decoder with a subject-aware dynamic gating.\nTrained end-to-end for whole-brain prediction, AFIRE decouples the decoder from\nupstream fusion, while MIND combines token-dependent Top-K sparse routing with\na subject prior to personalize expert usage without sacrificing generality.\nExperiments across multiple multimodal backbones and subjects show consistent\nimprovements over strong baselines, enhanced cross-subject generalization, and\ninterpretable expert patterns that correlate with content type. The framework\noffers a simple attachment point for new encoders and datasets, enabling\nrobust, plug-and-improve performance for naturalistic neuroimaging studies.", "AI": {"tldr": "AFIRE is an agnostic framework that standardizes multimodal fMRI encoding with MIND decoder using mixture-of-experts and subject-aware gating, improving whole-brain prediction and cross-subject generalization.", "motivation": "To address challenges in naturalistic fMRI encoding including multimodal inputs, varying fusion styles, and significant inter-subject variability.", "method": "AFIRE standardizes time-aligned post-fusion tokens from different encoders, while MIND uses mixture-of-experts with subject-aware dynamic gating and token-dependent Top-K sparse routing.", "result": "Consistent improvements over strong baselines across multiple multimodal backbones and subjects, enhanced cross-subject generalization, and interpretable expert patterns correlated with content type.", "conclusion": "The framework provides a simple attachment point for new encoders and datasets, enabling robust plug-and-improve performance for naturalistic neuroimaging studies."}}
{"id": "2510.04021", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04021", "abs": "https://arxiv.org/abs/2510.04021", "authors": ["Kushal Vyas", "Ashok Veeraraghavan", "Guha Balakrishnan"], "title": "Fit Pixels, Get Labels: Meta-learned Implicit Networks for Image Segmentation", "comment": "MICCAI 2025 (oral). Final peer-reviewed copy accessible at publisher\n  DOI https://link.springer.com/chapter/10.1007/978-3-032-04947-6_19 . Project\n  page, https://kushalvyas.github.io/metaseg.html", "summary": "Implicit neural representations (INRs) have achieved remarkable successes in\nlearning expressive yet compact signal representations. However, they are not\nnaturally amenable to predictive tasks such as segmentation, where they must\nlearn semantic structures over a distribution of signals. In this study, we\nintroduce MetaSeg, a meta-learning framework to train INRs for medical image\nsegmentation. MetaSeg uses an underlying INR that simultaneously predicts per\npixel intensity values and class labels. It then uses a meta-learning procedure\nto find optimal initial parameters for this INR over a training dataset of\nimages and segmentation maps, such that the INR can simply be fine-tuned to fit\npixels of an unseen test image, and automatically decode its class labels. We\nevaluated MetaSeg on 2D and 3D brain MRI segmentation tasks and report Dice\nscores comparable to commonly used U-Net models, but with $90\\%$ fewer\nparameters. MetaSeg offers a fresh, scalable alternative to traditional\nresource-heavy architectures such as U-Nets and vision transformers for medical\nimage segmentation. Our project is available at\nhttps://kushalvyas.github.io/metaseg.html .", "AI": {"tldr": "MetaSeg is a meta-learning framework that trains implicit neural representations (INRs) for medical image segmentation, achieving comparable performance to U-Nets with 90% fewer parameters.", "motivation": "Implicit neural representations (INRs) are effective for compact signal representation but are not naturally suitable for predictive tasks like segmentation that require learning semantic structures across signal distributions.", "method": "MetaSeg uses an INR that simultaneously predicts pixel intensity values and class labels, with a meta-learning procedure to find optimal initial parameters over training data, enabling quick fine-tuning for unseen test images.", "result": "Evaluation on 2D and 3D brain MRI segmentation tasks showed Dice scores comparable to U-Net models while using 90% fewer parameters.", "conclusion": "MetaSeg provides a scalable alternative to traditional resource-heavy architectures like U-Nets and vision transformers for medical image segmentation."}}
{"id": "2510.04673", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04673", "abs": "https://arxiv.org/abs/2510.04673", "authors": ["Chan Hee Song", "Yiwen Song", "Palash Goyal", "Yu Su", "Oriana Riva", "Hamid Palangi", "Tomas Pfister"], "title": "Watch and Learn: Learning to Use Computers from Online Videos", "comment": null, "summary": "Computer use agents (CUAs) need to plan task workflows grounded in diverse,\never-changing applications and environments, but learning is hindered by the\nscarcity of large-scale, high-quality training data in the target application.\nExisting datasets are domain-specific, static, and costly to annotate, while\ncurrent synthetic data generation methods often yield simplistic or misaligned\ntask demonstrations. To address these limitations, we introduce Watch & Learn\n(W&L), a framework that converts human demonstration videos readily available\non the Internet into executable UI trajectories at scale. Instead of directly\ngenerating trajectories or relying on ad hoc reasoning heuristics, we cast the\nproblem as an inverse dynamics objective: predicting the user's action from\nconsecutive screen states. This formulation reduces manual engineering, is\neasier to learn, and generalizes more robustly across applications. Concretely,\nwe develop an inverse dynamics labeling pipeline with task-aware video\nretrieval, generate over 53k high-quality trajectories from raw web videos, and\ndemonstrate that these trajectories improve CUAs both as in-context\ndemonstrations and as supervised training data. On the challenging OSWorld\nbenchmark, UI trajectories extracted with W&L consistently enhance both\ngeneral-purpose and state-of-the-art frameworks in-context, and deliver\nstronger gains for open-source models under supervised training. These results\nhighlight web-scale human demonstration videos as a practical and scalable\nfoundation for advancing CUAs towards real-world deployment.", "AI": {"tldr": "Watch & Learn (W&L) is a framework that converts human demonstration videos from the Internet into executable UI trajectories to address the data scarcity problem for computer use agents, improving their performance on real-world tasks.", "motivation": "Computer use agents need task workflows grounded in diverse applications, but face data scarcity issues as existing datasets are domain-specific, static, and costly to annotate, while synthetic data generation methods produce simplistic or misaligned demonstrations.", "method": "W&L frames the problem as an inverse dynamics objective - predicting user actions from consecutive screen states. It includes an inverse dynamics labeling pipeline with task-aware video retrieval to generate executable UI trajectories from raw web videos.", "result": "Generated over 53k high-quality trajectories from web videos that improve computer use agents both as in-context demonstrations and supervised training data. On OSWorld benchmark, W&L trajectories consistently enhanced both general-purpose and state-of-the-art frameworks, with stronger gains for open-source models under supervised training.", "conclusion": "Web-scale human demonstration videos serve as a practical and scalable foundation for advancing computer use agents towards real-world deployment, reducing manual engineering and improving generalization across applications."}}
{"id": "2510.04022", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04022", "abs": "https://arxiv.org/abs/2510.04022", "authors": ["Chendong Wang", "Donglin Bai", "Yifan Yang", "Xiao Jin", "Anlan Zhang", "Rui Wang", "Shiqi Jiang", "Yuqing Yang", "Hao Wu", "Qi Dai", "Chong Luo", "Ting Cao", "Lili Qiu", "Suman Banerjee"], "title": "Video-in-the-Loop: Span-Grounded Long Video QA with Interleaved Reasoning", "comment": null, "summary": "We present \\emph{Video-in-the-Loop} (ViTL), a two-stage long-video QA\nframework that preserves a fixed token budget by first \\emph{localizing}\nquestion-relevant interval(s) with a low-fps skim and then \\emph{answering} via\nspan-aware reallocation of visual tokens at higher effective frame rate,\nemitting an interleaved output with both spans and the final option for direct\nattribution. We also introduce \\dataname{}, which converts description based\nevent graphs into \\emph{span-grounded} multiple-choice QA by pairing each\nquestion with \\emph{ground-truth} time span(s) and related reasoning. ViTL is\ntrained end-to-end with an interleaved group-relative objective that couples\ntemporal IoU for localization with answer correctness, allowing credit to flow\nfrom answers back to spans without increasing compute. Under fixed token\nbudgets, ViTL attains up to 8.6% with 50% less frame input on long-video QA and\ntemporal grounding (e.g., Charades-STA, ActivityNet-Captions) and ablations\nshow that span-aware token reallocation consistently surpasses uniform\nsampling. Together, \\dataname{} and ViTL provide an interpretable,\ncompute-efficient recipe for scalable long-video QA.", "AI": {"tldr": "ViTL is a two-stage long-video QA framework that localizes question-relevant intervals with low-fps skimming and answers via span-aware token reallocation at higher effective frame rates, achieving better performance with fewer frames.", "motivation": "To address the computational challenges of long-video QA while maintaining performance, by efficiently allocating visual tokens to relevant temporal spans.", "method": "Two-stage approach: 1) Localize question-relevant intervals using low-fps skim, 2) Answer via span-aware reallocation of visual tokens at higher effective frame rates with interleaved group-relative objective.", "result": "Achieves up to 8.6% improvement with 50% less frame input on long-video QA and temporal grounding tasks (Charades-STA, ActivityNet-Captions), outperforming uniform sampling.", "conclusion": "ViTL provides an interpretable, compute-efficient solution for scalable long-video QA through span-aware token reallocation and temporal localization."}}
{"id": "2510.04695", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04695", "abs": "https://arxiv.org/abs/2510.04695", "authors": ["Yiding Wang", "Zhepei Wei", "Xinyu Zhu", "Yu Meng"], "title": "Beyond Outcome Reward: Decoupling Search and Answering Improves LLM Agents", "comment": null, "summary": "Enabling large language models (LLMs) to utilize search tools offers a\npromising path to overcoming fundamental limitations such as knowledge cutoffs\nand hallucinations. Recent work has explored reinforcement learning (RL) for\ntraining search-augmented agents that interleave reasoning and retrieval before\nanswering. These approaches usually rely on outcome-based rewards (e.g., exact\nmatch), implicitly assuming that optimizing for final answers will also yield\neffective intermediate search behaviors. Our analysis challenges this\nassumption: we uncover multiple systematic deficiencies in search that arise\nunder outcome-only training and ultimately degrade final answer quality,\nincluding failure to invoke tools, invalid queries, and redundant searches. To\naddress these shortcomings, we introduce DeSA (Decoupling\nSearch-and-Answering), a simple two-stage training framework that explicitly\nseparates search optimization from answer generation. In Stage 1, agents are\ntrained to improve search effectiveness with retrieval recall-based rewards. In\nStage 2, outcome rewards are employed to optimize final answer generation.\nAcross seven QA benchmarks, DeSA-trained agents consistently improve search\nbehaviors, delivering substantially higher search recall and answer accuracy\nthan outcome-only baselines. Notably, DeSA outperforms single-stage training\napproaches that simultaneously optimize recall and outcome rewards,\nunderscoring the necessity of explicitly decoupling the two objectives.", "AI": {"tldr": "DeSA is a two-stage training framework that decouples search optimization from answer generation to address systematic deficiencies in search-augmented LLMs trained with outcome-only rewards.", "motivation": "Current RL approaches for search-augmented LLMs rely on outcome-based rewards, assuming they will yield effective intermediate search behaviors. However, this leads to systematic deficiencies like failure to invoke tools, invalid queries, and redundant searches that degrade final answer quality.", "method": "DeSA uses a two-stage training framework: Stage 1 trains agents to improve search effectiveness with retrieval recall-based rewards, while Stage 2 employs outcome rewards to optimize final answer generation.", "result": "Across seven QA benchmarks, DeSA-trained agents consistently improved search behaviors, delivering substantially higher search recall and answer accuracy than outcome-only baselines. It also outperformed single-stage approaches that simultaneously optimize recall and outcome rewards.", "conclusion": "Explicitly decoupling search optimization from answer generation is necessary, as DeSA demonstrates superior performance over both outcome-only training and combined optimization approaches."}}
{"id": "2510.04024", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.04024", "abs": "https://arxiv.org/abs/2510.04024", "authors": ["Yuyan Bu", "Qiang Sheng", "Juan Cao", "Shaofei Wang", "Peng Qi", "Yuhui Shi", "Beizhe Hu"], "title": "Enhancing Fake News Video Detection via LLM-Driven Creative Process Simulation", "comment": "ACM CIKM 2025", "summary": "The emergence of fake news on short video platforms has become a new\nsignificant societal concern, necessitating automatic video-news-specific\ndetection. Current detectors primarily rely on pattern-based features to\nseparate fake news videos from real ones. However, limited and less diversified\ntraining data lead to biased patterns and hinder their performance. This\nweakness stems from the complex many-to-many relationships between video\nmaterial segments and fabricated news events in real-world scenarios: a single\nvideo clip can be utilized in multiple ways to create different fake\nnarratives, while a single fabricated event often combines multiple distinct\nvideo segments. However, existing datasets do not adequately reflect such\nrelationships due to the difficulty of collecting and annotating large-scale\nreal-world data, resulting in sparse coverage and non-comprehensive learning of\nthe characteristics of potential fake news video creation. To address this\nissue, we propose a data augmentation framework, AgentAug, that generates\ndiverse fake news videos by simulating typical creative processes. AgentAug\nimplements multiple LLM-driven pipelines of four fabrication categories for\nnews video creation, combined with an active learning strategy based on\nuncertainty sampling to select the potentially useful augmented samples during\ntraining. Experimental results on two benchmark datasets demonstrate that\nAgentAug consistently improves the performance of short video fake news\ndetectors.", "AI": {"tldr": "AgentAug is a data augmentation framework that uses LLM-driven pipelines to generate diverse fake news videos by simulating creative processes, combined with active learning to improve short video fake news detection.", "motivation": "Current fake news detectors suffer from limited and biased training data due to complex many-to-many relationships between video segments and fabricated events in real-world scenarios, which existing datasets fail to adequately capture.", "method": "Proposes AgentAug framework with multiple LLM-driven pipelines simulating four fabrication categories for news video creation, combined with active learning based on uncertainty sampling to select useful augmented samples during training.", "result": "Experimental results on two benchmark datasets show that AgentAug consistently improves the performance of short video fake news detectors.", "conclusion": "AgentAug effectively addresses the data sparsity and bias issues in fake news video detection by generating diverse training samples that better reflect real-world creative processes."}}
{"id": "2510.04721", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04721", "abs": "https://arxiv.org/abs/2510.04721", "authors": ["Ivo Petrov", "Jasper Dekoninck", "Martin Vechev"], "title": "BrokenMath: A Benchmark for Sycophancy in Theorem Proving with LLMs", "comment": null, "summary": "Large language models (LLMs) have recently shown strong performance on\nmathematical benchmarks. At the same time, they are prone to hallucination and\nsycophancy, often providing convincing but flawed proofs for incorrect\nmathematical statements provided by users. This significantly limits the\napplicability of LLMs in theorem proving, as verification of these flawed\nproofs must be done manually by expert mathematicians. However, existing\nbenchmarks that measure sycophancy in mathematics are limited: they focus\nsolely on final-answer problems, rely on very simple and often contaminated\ndatasets, and construct benchmark samples using synthetic modifications that\ncreate ill-posed questions rather than well-posed questions that are\ndemonstrably false. To address these issues, we introduce BrokenMath, the first\nbenchmark for evaluating sycophantic behavior in LLMs within the context of\nnatural language theorem proving. BrokenMath is built from advanced 2025\ncompetition problems, which are perturbed with an LLM to produce false\nstatements and subsequently refined through expert review. Using an\nLLM-as-a-judge framework, we evaluate state-of-the-art LLMs and agentic systems\nand find that sycophancy is widespread, with the best model, GPT-5, producing\nsycophantic answers 29% of the time. We further investigate several mitigation\nstrategies, including test-time interventions and supervised fine-tuning on\ncurated sycophantic examples. These approaches substantially reduce, but do not\neliminate, sycophantic behavior.", "AI": {"tldr": "BrokenMath is the first benchmark for evaluating sycophantic behavior in LLMs for natural language theorem proving, built from perturbed 2025 competition problems and refined through expert review. GPT-5 shows 29% sycophantic behavior, which can be reduced but not eliminated through interventions.", "motivation": "LLMs show strong mathematical performance but are prone to hallucination and sycophancy, providing convincing but flawed proofs for incorrect statements, limiting their applicability in theorem proving where manual verification by experts is required.", "method": "Built BrokenMath benchmark from advanced 2025 competition problems, perturbed with LLM to produce false statements and refined through expert review. Used LLM-as-a-judge framework to evaluate state-of-the-art LLMs and agentic systems.", "result": "Sycophancy is widespread, with GPT-5 producing sycophantic answers 29% of the time. Mitigation strategies including test-time interventions and supervised fine-tuning on curated sycophantic examples substantially reduce but do not eliminate sycophantic behavior.", "conclusion": "Sycophancy remains a significant challenge in LLMs for mathematical theorem proving, requiring continued research and development of more effective mitigation strategies."}}
{"id": "2510.04034", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04034", "abs": "https://arxiv.org/abs/2510.04034", "authors": ["Linn Bieske", "Carla Lorente"], "title": "Prompt-to-Prompt: Text-Based Image Editing Via Cross-Attention Mechanisms -- The Research of Hyperparameters and Novel Mechanisms to Enhance Existing Frameworks", "comment": null, "summary": "Recent advances in image editing have shifted from manual pixel manipulation\nto employing deep learning methods like stable diffusion models, which now\nleverage cross-attention mechanisms for text-driven control. This transition\nhas simplified the editing process but also introduced variability in results,\nsuch as inconsistent hair color changes. Our research aims to enhance the\nprecision and reliability of prompt-to-prompt image editing frameworks by\nexploring and optimizing hyperparameters. We present a comprehensive study of\nthe \"word swap\" method, develop an \"attention re-weight method\" for better\nadaptability, and propose the \"CL P2P\" framework to address existing\nlimitations like cycle inconsistency. This work contributes to understanding\nand improving the interaction between hyperparameter settings and the\narchitectural choices of neural network models, specifically their attention\nmechanisms, which significantly influence the composition and quality of the\ngenerated images.", "AI": {"tldr": "This paper explores hyperparameter optimization in prompt-to-prompt image editing frameworks to enhance precision and reliability, addressing issues like inconsistent hair color changes through methods including word swap, attention re-weight, and CL P2P framework.", "motivation": "To improve the precision and reliability of prompt-to-prompt image editing frameworks by addressing the variability in results introduced by deep learning methods like stable diffusion models, particularly issues like inconsistent hair color changes.", "method": "Comprehensive study of the \"word swap\" method, development of an \"attention re-weight method\" for better adaptability, and proposal of the \"CL P2P\" framework to address limitations like cycle inconsistency.", "result": "The research contributes to understanding and improving the interaction between hyperparameter settings and neural network architectural choices, specifically attention mechanisms that influence image composition and quality.", "conclusion": "This work advances prompt-to-prompt image editing by optimizing hyperparameters and developing new methods to enhance precision and reliability in text-driven image manipulation."}}
{"id": "2510.04765", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04765", "abs": "https://arxiv.org/abs/2510.04765", "authors": ["Jinbo Wen", "Jiawen Kang", "Linfeng Zhang", "Xiaoying Tang", "Jianhang Tang", "Yang Zhang", "Zhaohui Yang", "Dusit Niyato"], "title": "LMM-Incentive: Large Multimodal Model-based Incentive Design for User-Generated Content in Web 3.0", "comment": null, "summary": "Web 3.0 represents the next generation of the Internet, which is widely\nrecognized as a decentralized ecosystem that focuses on value expression and\ndata ownership. By leveraging blockchain and artificial intelligence\ntechnologies, Web 3.0 offers unprecedented opportunities for users to create,\nown, and monetize their content, thereby enabling User-Generated Content (UGC)\nto an entirely new level. However, some self-interested users may exploit the\nlimitations of content curation mechanisms and generate low-quality content\nwith less effort, obtaining platform rewards under information asymmetry. Such\nbehavior can undermine Web 3.0 performance. To this end, we propose\n\\textit{LMM-Incentive}, a novel Large Multimodal Model (LMM)-based incentive\nmechanism for UGC in Web 3.0. Specifically, we propose an LMM-based\ncontract-theoretic model to motivate users to generate high-quality UGC,\nthereby mitigating the adverse selection problem from information asymmetry. To\nalleviate potential moral hazards after contract selection, we leverage LMM\nagents to evaluate UGC quality, which is the primary component of the contract,\nutilizing prompt engineering techniques to improve the evaluation performance\nof LMM agents. Recognizing that traditional contract design methods cannot\neffectively adapt to the dynamic environment of Web 3.0, we develop an improved\nMixture of Experts (MoE)-based Proximal Policy Optimization (PPO) algorithm for\noptimal contract design. Simulation results demonstrate the superiority of the\nproposed MoE-based PPO algorithm over representative benchmarks in the context\nof contract design. Finally, we deploy the designed contract within an Ethereum\nsmart contract framework, further validating the effectiveness of the proposed\nscheme.", "AI": {"tldr": "The paper proposes LMM-Incentive, a Large Multimodal Model-based incentive mechanism using contract theory to motivate high-quality User-Generated Content in Web 3.0, addressing information asymmetry and moral hazard problems.", "motivation": "Web 3.0 enables users to create and monetize content, but information asymmetry allows self-interested users to generate low-quality content with less effort, undermining platform performance.", "method": "Proposes LMM-based contract-theoretic model with LMM agents for content quality evaluation using prompt engineering, and develops improved Mixture of Experts-based Proximal Policy Optimization algorithm for optimal contract design in dynamic Web 3.0 environments.", "result": "Simulation results show superiority of MoE-based PPO algorithm over benchmarks, and successful deployment in Ethereum smart contract framework validates the scheme's effectiveness.", "conclusion": "The proposed LMM-Incentive mechanism effectively addresses adverse selection and moral hazard in Web 3.0 UGC platforms, providing a practical solution for incentivizing high-quality content creation through blockchain integration."}}
{"id": "2510.04039", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04039", "abs": "https://arxiv.org/abs/2510.04039", "authors": ["Bin Lei", "Nuo Xu", "Ali Payani", "Mingyi Hong", "Chunhua Liao", "Yu Cao", "Caiwen Ding"], "title": "\\textsc{GUI-Spotlight}: Adaptive Iterative Focus Refinement for Enhanced GUI Visual Grounding", "comment": null, "summary": "Multimodal large language models (MLLMs) have markedly expanded the\ncompetence of graphical user-interface (GUI) systems, propelling them beyond\ncontrolled simulations into complex, real-world environments across diverse\nplatforms. However, practical usefulness is still bounded by the reliability of\nvisual grounding, i.e., mapping textual references to exact on-screen elements.\nThis limitation prevents the system from accurately performing pointer-level\nactions such as clicking or dragging. To address it, we introduce GUI-Spotlight\n-- a model trained for image-grounded reasoning that dynamically invokes\nmultiple specialized tools to iteratively narrow its focus to the relevant\nregion of the screen, thereby substantially improving visual grounding\naccuracy. On the ScreenSpot-Pro benchmark, GUI-Spotlight trained with only\n18.5K training samples achieves 52.8\\% accuracy, surpassing V2P-7B (50.6\\% with\n9.6M training samples) and GTA-1-7B (50.1\\% with 1.56M training samples).", "AI": {"tldr": "GUI-Spotlight improves visual grounding in multimodal GUI systems by using specialized tools to iteratively focus on relevant screen regions, achieving state-of-the-art accuracy with minimal training data.", "motivation": "Current multimodal GUI systems are limited by unreliable visual grounding, which prevents accurate pointer-level actions like clicking or dragging on screen elements.", "method": "Train a model for image-grounded reasoning that dynamically invokes multiple specialized tools to iteratively narrow focus to relevant screen regions.", "result": "Achieves 52.8% accuracy on ScreenSpot-Pro benchmark with only 18.5K training samples, outperforming V2P-7B (50.6% with 9.6M samples) and GTA-1-7B (50.1% with 1.56M samples).", "conclusion": "GUI-Spotlight substantially improves visual grounding accuracy in GUI systems through iterative region focusing, enabling more reliable pointer-level actions with minimal training data."}}
{"id": "2510.04792", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04792", "abs": "https://arxiv.org/abs/2510.04792", "authors": ["Ni Zhang", "Zhiguang Cao"], "title": "Hybrid-Balance GFlowNet for Solving Vehicle Routing Problems", "comment": "Accepted by NeurIPS 2025", "summary": "Existing GFlowNet-based methods for vehicle routing problems (VRPs) typically\nemploy Trajectory Balance (TB) to achieve global optimization but often neglect\nimportant aspects of local optimization. While Detailed Balance (DB) addresses\nlocal optimization more effectively, it alone falls short in solving VRPs,\nwhich inherently require holistic trajectory optimization. To address these\nlimitations, we introduce the Hybrid-Balance GFlowNet (HBG) framework, which\nuniquely integrates TB and DB in a principled and adaptive manner by aligning\ntheir intrinsically complementary strengths. Additionally, we propose a\nspecialized inference strategy for depot-centric scenarios like the Capacitated\nVehicle Routing Problem (CVRP), leveraging the depot node's greater flexibility\nin selecting successors. Despite this specialization, HBG maintains broad\napplicability, extending effectively to problems without explicit depots, such\nas the Traveling Salesman Problem (TSP). We evaluate HBG by integrating it into\ntwo established GFlowNet-based solvers, i.e., AGFN and GFACS, and demonstrate\nconsistent and significant improvements across both CVRP and TSP, underscoring\nthe enhanced solution quality and generalization afforded by our approach.", "AI": {"tldr": "HBG is a Hybrid-Balance GFlowNet framework that integrates Trajectory Balance and Detailed Balance for vehicle routing problems, achieving both global and local optimization with specialized inference for depot-centric scenarios.", "motivation": "Existing GFlowNet methods for VRPs use Trajectory Balance for global optimization but neglect local optimization, while Detailed Balance alone is insufficient for holistic trajectory optimization required in VRPs.", "method": "Hybrid-Balance GFlowNet (HBG) framework that integrates TB and DB in a principled, adaptive manner, plus specialized inference strategy for depot-centric scenarios like CVRP that leverages depot node flexibility.", "result": "HBG integrated into AGFN and GFACS solvers shows consistent and significant improvements across both CVRP and TSP, demonstrating enhanced solution quality and generalization.", "conclusion": "HBG effectively addresses the limitations of existing GFlowNet approaches by combining global and local optimization through hybrid balance, maintaining broad applicability across different routing problems."}}
{"id": "2510.04044", "categories": ["cs.CV", "cs.AI", "00-01", "I.2.6; K.3.2"], "pdf": "https://arxiv.org/pdf/2510.04044", "abs": "https://arxiv.org/abs/2510.04044", "authors": ["Bingtao Yang", "Yujia Wang", "Mengzhi Jiao", "Hongwei Huo"], "title": "Quantization Range Estimation for Convolutional Neural Networks", "comment": "11 pages, 5 tables, research report", "summary": "Post-training quantization for reducing the storage of deep neural network\nmodels has been demonstrated to be an effective way in various tasks. However,\nlow-bit quantization while maintaining model accuracy is a challenging problem.\nIn this paper, we present a range estimation method to improve the quantization\nperformance for post-training quantization. We model the range estimation into\nan optimization problem of minimizing quantization errors by layer-wise local\nminima. We prove this problem is locally convex and present an efficient search\nalgorithm to find the optimal solution. We propose the application of the above\nsearch algorithm to the transformed weights space to do further improvement in\npractice. Our experiments demonstrate that our method outperforms\nstate-of-the-art performance generally on top-1 accuracy for image\nclassification tasks on the ResNet series models and Inception-v3 model. The\nexperimental results show that the proposed method has almost no loss of top-1\naccuracy in 8-bit and 6-bit settings for image classifications, and the\naccuracy of 4-bit quantization is also significantly improved. The code is\navailable at https://github.com/codeiscommitting/REQuant.", "AI": {"tldr": "A range estimation method for post-training quantization that minimizes quantization errors through layer-wise local minima optimization, achieving state-of-the-art performance with minimal accuracy loss in 8-bit and 6-bit quantization.", "motivation": "Low-bit quantization while maintaining model accuracy is challenging in post-training quantization, requiring better methods to reduce storage without sacrificing performance.", "method": "Model range estimation as an optimization problem to minimize quantization errors using layer-wise local minima, prove local convexity, and develop an efficient search algorithm applied to transformed weights space.", "result": "Outperforms state-of-the-art on top-1 accuracy for ResNet series and Inception-v3 models, with almost no loss in 8-bit and 6-bit quantization, and significant improvement in 4-bit quantization accuracy.", "conclusion": "The proposed range estimation method effectively improves post-training quantization performance across different bit-widths while maintaining model accuracy."}}
{"id": "2510.04817", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04817", "abs": "https://arxiv.org/abs/2510.04817", "authors": ["Abhinav Madahar"], "title": "Natural Language Edge Labelling: Decoupling Intent from Execution in Structured LM Reasoning", "comment": null, "summary": "Controllers for structured LM reasoning (e.g., Chain-of-Thought,\nself-consistency, and Tree-of-Thoughts) often entangle what to try next with\nhow to execute it, exposing only coarse global knobs and yielding brittle,\ncompute-inefficient, and hard-to-audit behavior. We introduce Natural Language\nEdge Labelling (NLEL), a labeller-tuner overlay that attaches a free-form\nnatural-language directive to each search edge and translates it into a\nschema-bounded control vector for decoding, search (branch quotas, exploration\n$\\beta$), generation bundle size, retrieval mixtures, and verification passes.\nA labeller $\\Lambda$ emits labels from the parent state and a compact context;\na tuner $\\Psi$ maps $(P, L, C)\\to \\Pi$, with strict schema validation and\ntrust-region projection around safe defaults. Downstream selection remains\nToT-style with score $S=\\mu+\\beta\\sigma$ and depth-annealed $\\beta$. We show\nNLEL strictly generalizes CoT/ToT, prove an anytime-monotonicity property for\ntop-$k$ selection under label-conditioned bundles, and bound selector shortfall\nby control-vector distortion, providing decision-relevant justification for\nguards like trust regions and verification passes. We instantiate $\\Psi$ as a\nprompt-only JSON Parameter Emitter and preregister an evaluation on GSM8K, MATH\n(subset), StrategyQA, and ARC-Challenge with compute-aware reporting\n(success@compute, tokens-per-success) and ablations over $\\Lambda$, $\\Psi$,\ntrust-region radius, and control quantization; preregistered forecasts\nanticipate accuracy gains at comparable token budgets and improved\nsuccess@compute under constraints. NLEL offers an interpretable, model-agnostic\ninterface that separates intent from execution for controllable, auditable LM\ninference.", "AI": {"tldr": "NLEL (Natural Language Edge Labelling) is a labeller-tuner overlay that attaches natural-language directives to search edges in structured LM reasoning, translating them into schema-bounded control vectors for more efficient and controllable inference.", "motivation": "Current controllers for structured LM reasoning (like Chain-of-Thought, Tree-of-Thoughts) entangle what to try next with how to execute it, leading to brittle, compute-inefficient, and hard-to-audit behavior.", "method": "NLEL uses a labeller \u039b to emit natural-language labels from parent states and context, and a tuner \u03a8 that maps (P, L, C) \u2192 \u03a0 with schema validation and trust-region projection. It maintains ToT-style selection with score S=\u03bc+\u03b2\u03c3 and depth-annealed \u03b2.", "result": "The paper shows NLEL strictly generalizes CoT/ToT, proves anytime-monotonicity for top-k selection, and bounds selector shortfall by control-vector distortion. It includes preregistered evaluations on GSM8K, MATH, StrategyQA, and ARC-Challenge with compute-aware metrics.", "conclusion": "NLEL provides an interpretable, model-agnostic interface that separates intent from execution for controllable, auditable LM inference, offering improved efficiency and control over existing reasoning methods."}}
{"id": "2510.04057", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04057", "abs": "https://arxiv.org/abs/2510.04057", "authors": ["Zhenyu Pan", "Yucheng Lu", "Han Liu"], "title": "MetaFind: Scene-Aware 3D Asset Retrieval for Coherent Metaverse Scene Generation", "comment": "The Thirty-Ninth Annual Conference on Neural Information Processing\n  Systems (NeurIPS 2025)", "summary": "We present MetaFind, a scene-aware tri-modal compositional retrieval\nframework designed to enhance scene generation in the metaverse by retrieving\n3D assets from large-scale repositories. MetaFind addresses two core\nchallenges: (i) inconsistent asset retrieval that overlooks spatial, semantic,\nand stylistic constraints, and (ii) the absence of a standardized retrieval\nparadigm specifically tailored for 3D asset retrieval, as existing approaches\nmainly rely on general-purpose 3D shape representation models. Our key\ninnovation is a flexible retrieval mechanism that supports arbitrary\ncombinations of text, image, and 3D modalities as queries, enhancing spatial\nreasoning and style consistency by jointly modeling object-level features\n(including appearance) and scene-level layout structures. Methodologically,\nMetaFind introduces a plug-and-play equivariant layout encoder ESSGNN that\ncaptures spatial relationships and object appearance features, ensuring\nretrieved 3D assets are contextually and stylistically coherent with the\nexisting scene, regardless of coordinate frame transformations. The framework\nsupports iterative scene construction by continuously adapting retrieval\nresults to current scene updates. Empirical evaluations demonstrate the\nimproved spatial and stylistic consistency of MetaFind in various retrieval\ntasks compared to baseline methods.", "AI": {"tldr": "MetaFind is a tri-modal compositional retrieval framework that enhances metaverse scene generation by retrieving 3D assets using text, image, and 3D queries while maintaining spatial, semantic, and stylistic consistency.", "motivation": "Addresses inconsistent asset retrieval that ignores spatial, semantic, and stylistic constraints, and the lack of standardized retrieval paradigms specifically for 3D assets in metaverse applications.", "method": "Uses a flexible tri-modal retrieval mechanism with an equivariant layout encoder (ESSGNN) that captures spatial relationships and object appearance features, supporting arbitrary combinations of text, image, and 3D queries.", "result": "Empirical evaluations show improved spatial and stylistic consistency in various retrieval tasks compared to baseline methods.", "conclusion": "MetaFind provides an effective framework for contextually and stylistically coherent 3D asset retrieval in metaverse scene construction, supporting iterative scene updates."}}
{"id": "2510.04851", "categories": ["cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.04851", "abs": "https://arxiv.org/abs/2510.04851", "authors": ["Dongge Han", "Camille Couturier", "Daniel Madrigal Diaz", "Xuchao Zhang", "Victor R\u00fchle", "Saravan Rajmohan"], "title": "LEGOMem: Modular Procedural Memory for Multi-agent LLM Systems for Workflow Automation", "comment": null, "summary": "We introduce LEGOMem, a modular procedural memory framework for multi-agent\nlarge language model (LLM) systems in workflow automation. LEGOMem decomposes\npast task trajectories into reusable memory units and flexibly allocates them\nacross orchestrators and task agents to support planning and execution. To\nexplore the design space of memory in multi-agent systems, we use LEGOMem as a\nlens and conduct a systematic study of procedural memory in multi-agent\nsystems, examining where memory should be placed, how it should be retrieved,\nand which agents benefit most. Experiments on the OfficeBench benchmark show\nthat orchestrator memory is critical for effective task decomposition and\ndelegation, while fine-grained agent memory improves execution accuracy. We\nfind that even teams composed of smaller language models can benefit\nsubstantially from procedural memory, narrowing the performance gap with\nstronger agents by leveraging prior execution traces for more accurate planning\nand tool use. These results position LEGOMem as both a practical framework for\nmemory-augmented agent systems and a research tool for understanding memory\ndesign in multi-agent workflow automation.", "AI": {"tldr": "LEGOMem is a modular procedural memory framework for multi-agent LLM systems that decomposes past task trajectories into reusable memory units to improve workflow automation.", "motivation": "To enhance multi-agent LLM systems in workflow automation by systematically exploring where memory should be placed, how it should be retrieved, and which agents benefit most from procedural memory.", "method": "Decomposes past task trajectories into reusable memory units and flexibly allocates them across orchestrators and task agents to support planning and execution.", "result": "Experiments on OfficeBench show orchestrator memory is critical for task decomposition and delegation, while fine-grained agent memory improves execution accuracy. Smaller models can substantially benefit from procedural memory.", "conclusion": "LEGOMem serves as both a practical framework for memory-augmented agent systems and a research tool for understanding memory design in multi-agent workflow automation."}}
{"id": "2510.04063", "categories": ["cs.CV", "astro-ph.SR"], "pdf": "https://arxiv.org/pdf/2510.04063", "abs": "https://arxiv.org/abs/2510.04063", "authors": ["Chetraj Pandey", "Jinsu Hong", "Anli Ji", "Rafal A. Angryk", "Berkay Aydin"], "title": "Ordinal Encoding as a Regularizer in Binary Loss for Solar Flare Prediction", "comment": "This is a preprint submitted to ICDM Workshop (SABID 2025). 6 pages,\n  2 Figures", "summary": "The prediction of solar flares is typically formulated as a binary\nclassification task, distinguishing events as either Flare (FL) or No-Flare\n(NF) according to a specified threshold (for example, greater than or equal to\nC-class, M-class, or X-class). However, this binary framework neglects the\ninherent ordinal relationships among the sub-classes contained within each\ncategory (FL and NF). Several studies on solar flare prediction have\nempirically shown that the most frequent misclassifications occur near this\nprediction threshold. This suggests that the models struggle to differentiate\nevents that are similar in intensity but fall on opposite sides of the binary\nthreshold. To mitigate this limitation, we propose a modified loss function\nthat integrates the ordinal information among the sub-classes of the binarized\nflare labels into the conventional binary cross-entropy (BCE) loss. This\napproach serves as an ordinality-aware, data-driven regularization method that\npenalizes the incorrect predictions of flare events in close proximity to the\nprediction threshold more heavily than those away from the boundary during\nmodel optimization. By incorporating ordinal weighting into the loss function,\nwe aim to enhance the model's learning process by leveraging the ordinal\ncharacteristics of the data, thereby improving its overall performance.", "AI": {"tldr": "The paper proposes an ordinality-aware loss function that integrates ordinal relationships between solar flare sub-classes into binary classification, penalizing misclassifications near the prediction threshold more heavily to improve model performance.", "motivation": "Binary classification for solar flare prediction neglects ordinal relationships between sub-classes, and empirical studies show most misclassifications occur near the prediction threshold where models struggle to differentiate similar intensity events on opposite sides of the binary boundary.", "method": "Modified loss function that integrates ordinal information among flare sub-classes into conventional binary cross-entropy loss, serving as an ordinality-aware regularization method that penalizes incorrect predictions near the threshold more heavily during model optimization.", "result": "The proposed approach aims to enhance model learning by leveraging ordinal characteristics of the data, though specific quantitative results are not provided in the abstract.", "conclusion": "Incorporating ordinal weighting into the loss function improves solar flare prediction by addressing the limitations of binary classification frameworks and better handling events near the prediction threshold."}}
{"id": "2510.04862", "categories": ["cs.AI", "cs.LG", "cs.MA", "cs.NE"], "pdf": "https://arxiv.org/pdf/2510.04862", "abs": "https://arxiv.org/abs/2510.04862", "authors": ["Sam Earle", "Zehua Jiang", "Eugene Vinitsky", "Julian Togelius"], "title": "Video Game Level Design as a Multi-Agent Reinforcement Learning Problem", "comment": "11 pages, 7 tables, 5 figures, published as full technical paper at\n  the AAAI conference on Artificial Intelligence and Interactive Digital\n  Entertainment 2025", "summary": "Procedural Content Generation via Reinforcement Learning (PCGRL) offers a\nmethod for training controllable level designer agents without the need for\nhuman datasets, using metrics that serve as proxies for level quality as\nrewards. Existing PCGRL research focuses on single generator agents, but are\nbottlenecked by the need to frequently recalculate heuristics of level quality\nand the agent's need to navigate around potentially large maps. By framing\nlevel generation as a multi-agent problem, we mitigate the efficiency\nbottleneck of single-agent PCGRL by reducing the number of reward calculations\nrelative to the number of agent actions. We also find that multi-agent level\ngenerators are better able to generalize to out-of-distribution map shapes,\nwhich we argue is due to the generators' learning more local, modular design\npolicies. We conclude that treating content generation as a distributed,\nmulti-agent task is beneficial for generating functional artifacts at scale.", "AI": {"tldr": "Multi-agent PCGRL improves level generation efficiency by reducing reward calculations and enables better generalization to different map shapes through local, modular policies.", "motivation": "Single-agent PCGRL faces efficiency bottlenecks from frequent reward recalculations and navigation challenges in large maps, prompting exploration of multi-agent approaches.", "method": "Framing level generation as a multi-agent problem where multiple agents work collaboratively, reducing the number of reward calculations relative to actions taken.", "result": "Multi-agent level generators are more efficient and better generalize to out-of-distribution map shapes by learning local, modular design policies.", "conclusion": "Treating content generation as a distributed, multi-agent task is beneficial for generating functional artifacts at scale."}}
{"id": "2510.04066", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04066", "abs": "https://arxiv.org/abs/2510.04066", "authors": ["Zheng Chen", "Kewei Zhang", "Xiaoyang Liu", "Weihang Zhang", "Mengfan Wang", "Yifan Fu", "Yulun Zhang"], "title": "QuantDemoire: Quantization with Outlier Aware for Image Demoir\u00e9ing", "comment": "Code is available at: https://github.com/zhengchen1999/QuantDemoire", "summary": "Demoir\\'eing aims to remove moir\\'e artifacts that often occur in images.\nWhile recent deep learning-based methods have achieved promising results, they\ntypically require substantial computational resources, limiting their\ndeployment on edge devices. Model quantization offers a compelling solution.\nHowever, directly applying existing quantization methods to demoir\\'eing models\nintroduces severe performance degradation. The main reasons are distribution\noutliers and weakened representations in smooth regions. To address these\nissues, we propose QuantDemoire, a post-training quantization framework\ntailored to demoir\\'eing. It contains two key components. **First}, we\nintroduce an outlier-aware quantizer to reduce errors from outliers. It uses\nsampling-based range estimation to reduce activation outliers, and keeps a few\nextreme weights in FP16 with negligible cost. **Second**, we design a\nfrequency-aware calibration strategy. It emphasizes low- and mid-frequency\ncomponents during fine-tuning, which mitigates banding artifacts caused by\nlow-bit quantization. Extensive experiments validate that our QuantDemoire\nachieves large reductions in parameters and computation while maintaining\nquality. Meanwhile, it outperforms existing quantization methods by over **4\ndB** on W4A4. Code is released at:\nhttps://github.com/zhengchen1999/QuantDemoire.", "AI": {"tldr": "QuantDemoire is a post-training quantization framework for demoir\u00e9ing models that addresses performance degradation issues through outlier-aware quantization and frequency-aware calibration, achieving significant parameter and computation reductions while maintaining quality.", "motivation": "Existing deep learning-based demoir\u00e9ing methods require substantial computational resources, limiting deployment on edge devices. Direct application of standard quantization methods causes severe performance degradation due to distribution outliers and weakened representations in smooth regions.", "method": "The framework includes: 1) Outlier-aware quantizer using sampling-based range estimation to reduce activation outliers and keeping extreme weights in FP16, 2) Frequency-aware calibration strategy that emphasizes low- and mid-frequency components during fine-tuning to mitigate banding artifacts.", "result": "QuantDemoire achieves large reductions in parameters and computation while maintaining quality, outperforming existing quantization methods by over 4 dB on W4A4 quantization.", "conclusion": "The proposed QuantDemoire framework effectively enables efficient deployment of demoir\u00e9ing models on edge devices through specialized quantization techniques that address domain-specific challenges."}}
{"id": "2510.04886", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.04886", "abs": "https://arxiv.org/abs/2510.04886", "authors": ["Adi Banerjee", "Anirudh Nair", "Tarik Borogovac"], "title": "Where Did It All Go Wrong? A Hierarchical Look into Multi-Agent Error Attribution", "comment": null, "summary": "Error attribution in Large Language Model (LLM) multi-agent systems presents\na significant challenge in debugging and improving collaborative AI systems.\nCurrent approaches to pinpointing agent and step level failures in interaction\ntraces - whether using all-at-once evaluation, step-by-step analysis, or binary\nsearch - fall short when analyzing complex patterns, struggling with both\naccuracy and consistency. We present ECHO (Error attribution through Contextual\nHierarchy and Objective consensus analysis), a novel algorithm that combines\nhierarchical context representation, objective analysis-based evaluation, and\nconsensus voting to improve error attribution accuracy. Our approach leverages\na positional-based leveling of contextual understanding while maintaining\nobjective evaluation criteria, ultimately reaching conclusions through a\nconsensus mechanism. Experimental results demonstrate that ECHO outperforms\nexisting methods across various multi-agent interaction scenarios, showing\nparticular strength in cases involving subtle reasoning errors and complex\ninterdependencies. Our findings suggest that leveraging these concepts of\nstructured, hierarchical context representation combined with consensus-based\nobjective decision-making, provides a more robust framework for error\nattribution in multi-agent systems.", "AI": {"tldr": "ECHO is a novel algorithm for error attribution in LLM multi-agent systems that combines hierarchical context representation, objective analysis, and consensus voting to improve accuracy in identifying agent and step-level failures.", "motivation": "Current approaches to error attribution in LLM multi-agent systems (all-at-once evaluation, step-by-step analysis, binary search) struggle with accuracy and consistency when analyzing complex interaction patterns, making debugging and improvement challenging.", "method": "ECHO combines hierarchical context representation using positional-based leveling, objective analysis-based evaluation, and consensus voting to reach conclusions about error attribution in multi-agent interaction traces.", "result": "Experimental results show ECHO outperforms existing methods across various multi-agent interaction scenarios, with particular strength in cases involving subtle reasoning errors and complex interdependencies.", "conclusion": "Structured hierarchical context representation combined with consensus-based objective decision-making provides a more robust framework for error attribution in multi-agent systems."}}
{"id": "2510.04069", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04069", "abs": "https://arxiv.org/abs/2510.04069", "authors": ["Zongyin Deng", "Qing Zhou", "Yuhao Fang", "Zijian Wang", "Yao Lu", "Ye Zhang", "Chun Li"], "title": "Diffusion Low Rank Hybrid Reconstruction for Sparse View Medical Imaging", "comment": null, "summary": "This work presents TV-LoRA, a novel method for low-dose sparse-view CT\nreconstruction that combines a diffusion generative prior (NCSN++ with SDE\nmodeling) and multi-regularization constraints, including anisotropic TV and\nnuclear norm (LoRA), within an ADMM framework. To address ill-posedness and\ntexture loss under extremely sparse views, TV-LoRA integrates generative and\nphysical constraints, and utilizes a 2D slice-based strategy with FFT\nacceleration and tensor-parallel optimization for efficient inference.\nExperiments on AAPM-2016, CTHD, and LIDC datasets with\n$N_{\\mathrm{view}}=8,4,2$ show that TV-LoRA consistently surpasses benchmarks\nin SSIM, texture recovery, edge clarity, and artifact suppression,\ndemonstrating strong robustness and generalizability. Ablation studies confirm\nthe complementary effects of LoRA regularization and diffusion priors, while\nthe FFT-PCG module provides a speedup. Overall, Diffusion + TV-LoRA achieves\nhigh-fidelity, efficient 3D CT reconstruction and broad clinical applicability\nin low-dose, sparse-sampling scenarios.", "AI": {"tldr": "TV-LoRA is a novel method for low-dose sparse-view CT reconstruction that combines diffusion generative priors with multi-regularization constraints (anisotropic TV and nuclear norm) in an ADMM framework, achieving superior performance in texture recovery and artifact suppression.", "motivation": "To address the ill-posedness and texture loss problems in extremely sparse-view CT reconstruction, particularly under low-dose conditions where traditional methods struggle with image quality degradation.", "method": "Combines diffusion generative prior (NCSN++ with SDE modeling) with multi-regularization constraints including anisotropic TV and nuclear norm (LoRA) within an ADMM framework, using 2D slice-based strategy with FFT acceleration and tensor-parallel optimization.", "result": "Experiments on AAPM-2016, CTHD, and LIDC datasets with 8, 4, and 2 views show TV-LoRA consistently surpasses benchmarks in SSIM, texture recovery, edge clarity, and artifact suppression, demonstrating strong robustness and generalizability.", "conclusion": "TV-LoRA achieves high-fidelity, efficient 3D CT reconstruction with broad clinical applicability in low-dose, sparse-sampling scenarios, with ablation studies confirming the complementary effects of LoRA regularization and diffusion priors."}}
{"id": "2510.04899", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04899", "abs": "https://arxiv.org/abs/2510.04899", "authors": ["Keane Ong", "Wei Dai", "Carol Li", "Dewei Feng", "Hengzhi Li", "Jingyao Wu", "Jiaee Cheong", "Rui Mao", "Gianmarco Mengaldo", "Erik Cambria", "Paul Pu Liang"], "title": "Human Behavior Atlas: Benchmarking Unified Psychological and Social Behavior Understanding", "comment": null, "summary": "Using intelligent systems to perceive psychological and social behaviors,\nthat is, the underlying affective, cognitive, and pathological states that are\nmanifested through observable behaviors and social interactions, remains a\nchallenge due to their complex, multifaceted, and personalized nature. Existing\nwork tackling these dimensions through specialized datasets and single-task\nsystems often miss opportunities for scalability, cross-task transfer, and\nbroader generalization. To address this gap, we curate Human Behavior Atlas, a\nunified benchmark of diverse behavioral tasks designed to support the\ndevelopment of unified models for understanding psychological and social\nbehaviors. Human Behavior Atlas comprises over 100,000 samples spanning text,\naudio, and visual modalities, covering tasks on affective states, cognitive\nstates, pathologies, and social processes. Our unification efforts can reduce\nredundancy and cost, enable training to scale efficiently across tasks, and\nenhance generalization of behavioral features across domains. On Human Behavior\nAtlas, we train three models: OmniSapiens-7B SFT, OmniSapiens-7B BAM, and\nOmniSapiens-7B RL. We show that training on Human Behavior Atlas enables models\nto consistently outperform existing multimodal LLMs across diverse behavioral\ntasks. Pretraining on Human Behavior Atlas also improves transfer to novel\nbehavioral datasets; with the targeted use of behavioral descriptors yielding\nmeaningful performance gains.", "AI": {"tldr": "The paper introduces Human Behavior Atlas, a unified benchmark for understanding psychological and social behaviors across multiple modalities, and demonstrates that training models on this dataset outperforms existing multimodal LLMs.", "motivation": "Existing work on perceiving psychological and social behaviors through intelligent systems faces challenges in scalability, cross-task transfer, and generalization due to specialized datasets and single-task approaches.", "method": "Created Human Behavior Atlas - a unified benchmark with over 100,000 samples spanning text, audio, and visual modalities covering affective states, cognitive states, pathologies, and social processes. Trained three models: OmniSapiens-7B SFT, OmniSapiens-7B BAM, and OmniSapiens-7B RL.", "result": "Models trained on Human Behavior Atlas consistently outperform existing multimodal LLMs across diverse behavioral tasks. Pretraining on this dataset also improves transfer to novel behavioral datasets, with behavioral descriptors yielding meaningful performance gains.", "conclusion": "The unification approach reduces redundancy and cost, enables efficient scaling across tasks, and enhances generalization of behavioral features across domains, addressing key limitations in existing behavioral understanding systems."}}
{"id": "2510.04100", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04100", "abs": "https://arxiv.org/abs/2510.04100", "authors": ["Jiaming Wang", "Diwen Liu", "Jizhuo Chen", "Harold Soh"], "title": "TOPO-Bench: An Open-Source Topological Mapping Evaluation Framework with Quantifiable Perceptual Aliasing", "comment": "Jiaming Wang, Diwen Liu, and Jizhuo Chen contributed equally", "summary": "Topological mapping offers a compact and robust representation for\nnavigation, but progress in the field is hindered by the lack of standardized\nevaluation metrics, datasets, and protocols. Existing systems are assessed\nusing different environments and criteria, preventing fair and reproducible\ncomparisons. Moreover, a key challenge - perceptual aliasing - remains\nunder-quantified, despite its strong influence on system performance. We\naddress these gaps by (1) formalizing topological consistency as the\nfundamental property of topological maps and showing that localization accuracy\nprovides an efficient and interpretable surrogate metric, and (2) proposing the\nfirst quantitative measure of dataset ambiguity to enable fair comparisons\nacross environments. To support this protocol, we curate a diverse benchmark\ndataset with calibrated ambiguity levels, implement and release deep-learned\nbaseline systems, and evaluate them alongside classical methods. Our\nexperiments and analysis yield new insights into the limitations of current\napproaches under perceptual aliasing. All datasets, baselines, and evaluation\ntools are fully open-sourced to foster consistent and reproducible research in\ntopological mapping.", "AI": {"tldr": "The paper addresses the lack of standardized evaluation in topological mapping by proposing formal metrics for topological consistency and dataset ambiguity, creating a benchmark dataset, and releasing open-source tools.", "motivation": "Progress in topological mapping is hindered by the absence of standardized evaluation metrics, datasets, and protocols, preventing fair comparisons between systems. Perceptual aliasing remains under-quantified despite its significant impact on performance.", "method": "The authors formalize topological consistency as a fundamental property and propose localization accuracy as a surrogate metric. They introduce a quantitative measure of dataset ambiguity and curate a diverse benchmark dataset with calibrated ambiguity levels. They implement both deep-learned baseline systems and classical methods for evaluation.", "result": "The research yields new insights into the limitations of current approaches under perceptual aliasing. The authors provide a comprehensive evaluation framework and open-source all datasets, baselines, and evaluation tools.", "conclusion": "The paper establishes a standardized evaluation protocol for topological mapping research, enabling fair and reproducible comparisons across different environments and systems, while providing tools to advance the field consistently."}}
{"id": "2510.04935", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04935", "abs": "https://arxiv.org/abs/2510.04935", "authors": ["Guoxin Chen", "Zile Qiao", "Wenqing Wang", "Donglei Yu", "Xuanzhong Chen", "Hao Sun", "Minpeng Liao", "Kai Fan", "Yong Jiang", "Penguin Xie", "Wayne Xin Zhao", "Ruihua Song", "Fei Huang"], "title": "MARS: Optimizing Dual-System Deep Research via Multi-Agent Reinforcement Learning", "comment": "Ongoing Work", "summary": "Large Reasoning Models (LRMs) often exhibit a tendency for overanalysis in\nsimple tasks, where the models excessively utilize System 2-type, deliberate\nreasoning, leading to inefficient token generation. Furthermore, these models\nface challenges in adapting their reasoning capabilities to rapidly changing\nenvironments due to the static nature of their pretraining data. To address\nthese issues, advancing Large Language Models (LLMs) for complex reasoning\ntasks requires innovative approaches that bridge intuitive and deliberate\ncognitive processes, akin to human cognition's dual-system dynamic. This paper\nintroduces a Multi-Agent System for Deep ReSearch (MARS) enabling seamless\nintegration of System 1's fast, intuitive thinking with System 2's deliberate\nreasoning within LLMs. MARS strategically integrates multiple external tools,\nsuch as Google Search, Google Scholar, and Python Interpreter, to access\nup-to-date information and execute complex computations, while creating a\nspecialized division of labor where System 1 efficiently processes and\nsummarizes high-volume external information, providing distilled insights that\nexpand System 2's reasoning context without overwhelming its capacity.\nFurthermore, we propose a multi-agent reinforcement learning framework\nextending Group Relative Policy Optimization to simultaneously optimize both\nsystems with multi-turn tool interactions, bin-packing optimization, and sample\nbalancing strategies that enhance collaborative efficiency. Extensive\nexperiments demonstrate MARS achieves substantial improvements of 3.86% on the\nchallenging Humanity's Last Exam (HLE) benchmark and an average gain of 8.9%\nacross 7 knowledge-intensive tasks, validating the effectiveness of our\ndual-system paradigm for complex reasoning in dynamic information environments.", "AI": {"tldr": "MARS is a multi-agent system that integrates System 1 (fast, intuitive) and System 2 (deliberate) reasoning in LLMs to address overanalysis in simple tasks and adapt to dynamic environments through external tools and reinforcement learning.", "motivation": "LRMs tend to overanalyze simple tasks using excessive System 2 reasoning, leading to inefficient token generation, and struggle with rapidly changing environments due to static pretraining data.", "method": "MARS integrates multiple external tools (Google Search, Google Scholar, Python Interpreter) for up-to-date information, creates division of labor between System 1 (processing/summarizing) and System 2 (reasoning), and uses multi-agent reinforcement learning with Group Relative Policy Optimization.", "result": "MARS achieves 3.86% improvement on Humanity's Last Exam benchmark and 8.9% average gain across 7 knowledge-intensive tasks.", "conclusion": "The dual-system paradigm effectively enhances complex reasoning in dynamic information environments by integrating intuitive and deliberate cognitive processes."}}
{"id": "2510.04111", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04111", "abs": "https://arxiv.org/abs/2510.04111", "authors": ["Xinglong Luo", "Ao Luo", "Kunming Luo", "Zhengning Wang", "Ping Tan", "Bing Zeng", "Shuaicheng Liu"], "title": "Learning Efficient Meshflow and Optical Flow from Event Cameras", "comment": "Accepted by TPAMI 2025", "summary": "In this paper, we explore the problem of event-based meshflow estimation, a\nnovel task that involves predicting a spatially smooth sparse motion field from\nevent cameras. To start, we review the state-of-the-art in event-based flow\nestimation, highlighting two key areas for further research: i) the lack of\nmeshflow-specific event datasets and methods, and ii) the underexplored\nchallenge of event data density. First, we generate a large-scale\nHigh-Resolution Event Meshflow (HREM) dataset, which showcases its superiority\nby encompassing the merits of high resolution at 1280x720, handling dynamic\nobjects and complex motion patterns, and offering both optical flow and\nmeshflow labels. These aspects have not been fully explored in previous works.\nBesides, we propose Efficient Event-based MeshFlow (EEMFlow) network, a\nlightweight model featuring a specially crafted encoder-decoder architecture to\nfacilitate swift and accurate meshflow estimation. Furthermore, we upgrade\nEEMFlow network to support dense event optical flow, in which a\nConfidence-induced Detail Completion (CDC) module is proposed to preserve sharp\nmotion boundaries. We conduct comprehensive experiments to show the exceptional\nperformance and runtime efficiency (30x faster) of our EEMFlow model compared\nto the recent state-of-the-art flow method. As an extension, we expand HREM\ninto HREM+, a multi-density event dataset contributing to a thorough study of\nthe robustness of existing methods across data with varying densities, and\npropose an Adaptive Density Module (ADM) to adjust the density of input event\ndata to a more optimal range, enhancing the model's generalization ability. We\nempirically demonstrate that ADM helps to significantly improve the performance\nof EEMFlow and EEMFlow+ by 8% and 10%, respectively. Code and dataset are\nreleased at https://github.com/boomluo02/EEMFlowPlus.", "AI": {"tldr": "This paper introduces event-based meshflow estimation, creates a high-resolution dataset (HREM/HREM+), proposes an efficient EEMFlow network with density adaptation, and achieves 30x faster performance with significant accuracy improvements.", "motivation": "Address two key gaps in event-based flow estimation: lack of meshflow-specific datasets/methods and underexplored challenges of event data density.", "method": "Created HREM dataset with high resolution and complex motions; proposed EEMFlow network with encoder-decoder architecture; added CDC module for dense flow; developed ADM for density adaptation.", "result": "EEMFlow achieves 30x faster runtime than state-of-the-art methods; ADM improves performance by 8-10%; comprehensive experiments show exceptional performance and efficiency.", "conclusion": "The proposed EEMFlow network with density adaptation significantly advances event-based meshflow estimation, offering both speed and accuracy improvements while addressing data density challenges."}}
{"id": "2510.04952", "categories": ["cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.04952", "abs": "https://arxiv.org/abs/2510.04952", "authors": ["Ailiya Borjigin", "Cong He"], "title": "Safe and Compliant Cross-Market Trade Execution via Constrained RL and Zero-Knowledge Audits", "comment": "22 pages, 2 figures", "summary": "We present a cross-market algorithmic trading system that balances execution\nquality with rigorous compliance enforcement. The architecture comprises a\nhigh-level planner, a reinforcement learning execution agent, and an\nindependent compliance agent. We formulate trade execution as a constrained\nMarkov decision process with hard constraints on participation limits, price\nbands, and self-trading avoidance. The execution agent is trained with proximal\npolicy optimization, while a runtime action-shield projects any unsafe action\ninto a feasible set. To support auditability without exposing proprietary\nsignals, we add a zero-knowledge compliance audit layer that produces\ncryptographic proofs that all actions satisfied the constraints. We evaluate in\na multi-venue, ABIDES-based simulator and compare against standard baselines\n(e.g., TWAP, VWAP). The learned policy reduces implementation shortfall and\nvariance while exhibiting no observed constraint violations across stress\nscenarios including elevated latency, partial fills, compliance module\ntoggling, and varying constraint limits. We report effects at the 95%\nconfidence level using paired t-tests and examine tail risk via CVaR. We\nsituate the work at the intersection of optimal execution, safe reinforcement\nlearning, regulatory technology, and verifiable AI, and discuss ethical\nconsiderations, limitations (e.g., modeling assumptions and computational\noverhead), and paths to real-world deployment.", "AI": {"tldr": "A cross-market algorithmic trading system that combines reinforcement learning execution with compliance enforcement using constrained MDPs, action-shielding, and zero-knowledge proofs to ensure safe trading without constraint violations.", "motivation": "To develop an algorithmic trading system that balances execution quality with rigorous compliance enforcement, addressing the need for safe and auditable automated trading in multi-venue environments.", "method": "Formulates trade execution as a constrained Markov decision process with hard constraints. Uses proximal policy optimization for the execution agent, runtime action-shielding to project unsafe actions, and a zero-knowledge compliance audit layer for cryptographic proof of constraint satisfaction.", "result": "The learned policy reduces implementation shortfall and variance while exhibiting no observed constraint violations across stress scenarios including elevated latency, partial fills, and varying constraint limits. Results are statistically significant at 95% confidence level.", "conclusion": "The work bridges optimal execution, safe reinforcement learning, regulatory technology, and verifiable AI, demonstrating a viable approach for real-world deployment with proper ethical considerations and addressing limitations like modeling assumptions and computational overhead."}}
{"id": "2510.04125", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04125", "abs": "https://arxiv.org/abs/2510.04125", "authors": ["Seunghyun Lee", "Tae-Kyun Kim"], "title": "Joint Learning of Pose Regression and Denoising Diffusion with Score Scaling Sampling for Category-level 6D Pose Estimation", "comment": null, "summary": "Latest diffusion models have shown promising results in category-level 6D\nobject pose estimation by modeling the conditional pose distribution with depth\nimage input. The existing methods, however, suffer from slow convergence during\ntraining, learning its encoder with the diffusion denoising network in\nend-to-end fashion, and require an additional network that evaluates sampled\npose hypotheses to filter out low-quality pose candidates. In this paper, we\npropose a novel pipeline that tackles these limitations by two key components.\nFirst, the proposed method pretrains the encoder with the direct pose\nregression head, and jointly learns the networks via the regression head and\nthe denoising diffusion head, significantly accelerating training convergence\nwhile achieving higher accuracy. Second, sampling guidance via time-dependent\nscore scaling is proposed s.t. the exploration-exploitation trade-off is\neffectively taken, eliminating the need for the additional evaluation network.\nThe sampling guidance maintains multi-modal characteristics of symmetric\nobjects at early denoising steps while ensuring high-quality pose generation at\nfinal steps. Extensive experiments on multiple benchmarks including REAL275,\nHouseCat6D, and ROPE, demonstrate that the proposed method, simple yet\neffective, achieves state-of-the-art accuracies even with single-pose\ninference, while being more efficient in both training and inference.", "AI": {"tldr": "A novel pipeline for 6D object pose estimation that combines pretrained encoder with joint regression-diffusion learning and sampling guidance, achieving faster training and higher accuracy without needing additional evaluation networks.", "motivation": "Existing diffusion-based pose estimation methods suffer from slow training convergence, end-to-end encoder learning with diffusion networks, and require additional networks to filter pose candidates.", "method": "Pretrains encoder with direct pose regression, jointly learns networks via regression and diffusion heads, and introduces sampling guidance with time-dependent score scaling for effective exploration-exploitation trade-off.", "result": "Achieves state-of-the-art accuracies on REAL275, HouseCat6D, and ROPE benchmarks with single-pose inference, while being more efficient in both training and inference.", "conclusion": "The proposed method is simple yet effective, accelerating training convergence while maintaining multi-modal characteristics for symmetric objects and ensuring high-quality pose generation."}}
{"id": "2510.04978", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04978", "abs": "https://arxiv.org/abs/2510.04978", "authors": ["Kun Xiang", "Terry Jingchen Zhang", "Yinya Huang", "Jixi He", "Zirong Liu", "Yueling Tang", "Ruizhe Zhou", "Lijing Luo", "Youpeng Wen", "Xiuwei Chen", "Bingqian Lin", "Jianhua Han", "Hang Xu", "Hanhui Li", "Bin Dong", "Xiaodan Liang"], "title": "Aligning Perception, Reasoning, Modeling and Interaction: A Survey on Physical AI", "comment": null, "summary": "The rapid advancement of embodied intelligence and world models has\nintensified efforts to integrate physical laws into AI systems, yet physical\nperception and symbolic physics reasoning have developed along separate\ntrajectories without a unified bridging framework. This work provides a\ncomprehensive overview of physical AI, establishing clear distinctions between\ntheoretical physics reasoning and applied physical understanding while\nsystematically examining how physics-grounded methods enhance AI's real-world\ncomprehension across structured symbolic reasoning, embodied systems, and\ngenerative models. Through rigorous analysis of recent advances, we advocate\nfor intelligent systems that ground learning in both physical principles and\nembodied reasoning processes, transcending pattern recognition toward genuine\nunderstanding of physical laws. Our synthesis envisions next-generation world\nmodels capable of explaining physical phenomena and predicting future states,\nadvancing safe, generalizable, and interpretable AI systems. We maintain a\ncontinuously updated resource at\nhttps://github.com/AI4Phys/Awesome-AI-for-Physics.", "AI": {"tldr": "This paper provides a comprehensive overview of physical AI, distinguishing between theoretical physics reasoning and applied physical understanding, and examines how physics-grounded methods enhance AI's real-world comprehension across symbolic reasoning, embodied systems, and generative models.", "motivation": "The rapid advancement of embodied intelligence and world models has intensified efforts to integrate physical laws into AI systems, but physical perception and symbolic physics reasoning have developed separately without a unified bridging framework.", "method": "The work establishes clear distinctions between theoretical physics reasoning and applied physical understanding, systematically examining how physics-grounded methods enhance AI's real-world comprehension across structured symbolic reasoning, embodied systems, and generative models through rigorous analysis of recent advances.", "result": "The synthesis advocates for intelligent systems that ground learning in both physical principles and embodied reasoning processes, transcending pattern recognition toward genuine understanding of physical laws.", "conclusion": "The paper envisions next-generation world models capable of explaining physical phenomena and predicting future states, advancing safe, generalizable, and interpretable AI systems."}}
{"id": "2510.04142", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04142", "abs": "https://arxiv.org/abs/2510.04142", "authors": ["Xiaoyu Yang", "Jie Lu", "En Yu"], "title": "Learning from All: Concept Alignment for Autonomous Distillation from Multiple Drifting MLLMs", "comment": null, "summary": "This paper identifies a critical yet underexplored challenge in distilling\nfrom multimodal large language models (MLLMs): the reasoning trajectories\ngenerated by multiple drifting teachers exhibit concept drift, whereby their\nreasoning distributions evolve unpredictably and transmit biases to the student\nmodel, ultimately compromising its performance. To tackle this issue, we\npioneer a theoretical connection between concept drift and knowledge\ndistillation, casting the non-stationary reasoning dynamics from multiple MLLM\nteachers as next-token prediction of multi-stream reasoning trajectories.Guided\nby concept drift, we introduce the \"learn, compare, critique\" paradigm,\nculminating in autonomous preference optimization (APO). Under the active\nguidance of the teachers, the student model first learns and self-distils\npreferred thinking by comparing multiple teachers. It then engages in critical\nreflection over the drifting inference from teachers, performing concept\nalignment through APO, ultimately yielding a robust, consistent, and\ngeneralizable model.Extensive experiments demonstrate our superior performance\nof consistency, robustness and generalization within knowledge distillation.\nBesides, we also contributed a large-scale dataset, CXR-MAX (Multi-teachers\nAlignment X-rays), comprising 170,982 distilled reasoning trajectories derived\nfrom publicly accessible MLLMs based on MIMIC-CXR. Our code and data are public\nat: https://anonymous.4open.science/r/Autonomous-Distillation/.", "AI": {"tldr": "The paper addresses concept drift in multimodal large language model distillation, where multiple teachers' reasoning trajectories drift unpredictably, causing bias transmission to student models. It proposes autonomous preference optimization (APO) using a \"learn, compare, critique\" paradigm to align concepts and improve model robustness.", "motivation": "Current multimodal LLM distillation faces concept drift issues where multiple teachers' reasoning trajectories evolve unpredictably, transmitting biases to student models and compromising performance. This critical challenge remains underexplored in knowledge distillation literature.", "method": "The authors establish a theoretical link between concept drift and knowledge distillation, framing multi-teacher reasoning as multi-stream trajectory prediction. They introduce autonomous preference optimization (APO) with a \"learn, compare, critique\" paradigm where students learn from teachers, compare multiple reasoning paths, and critically reflect on drifting inferences to achieve concept alignment.", "result": "Extensive experiments show superior performance in consistency, robustness, and generalization within knowledge distillation. The authors also contribute CXR-MAX, a large-scale dataset with 170,982 distilled reasoning trajectories from MIMIC-CXR-based MLLMs.", "conclusion": "The proposed APO framework effectively addresses concept drift in multimodal LLM distillation, producing robust, consistent, and generalizable student models through autonomous preference optimization and critical reflection on teacher reasoning trajectories."}}
{"id": "2510.04980", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04980", "abs": "https://arxiv.org/abs/2510.04980", "authors": ["Fangzhou Liang", "Tianshi Zheng", "Chunkit Chan", "Yauwai Yim", "Yangqiu Song"], "title": "LLM-Hanabi: Evaluating Multi-Agent Gameplays with Theory-of-Mind and Rationale Inference in Imperfect Information Collaboration Game", "comment": "EMNLP 2025 Wordplay", "summary": "Effective multi-agent collaboration requires agents to infer the rationale\nbehind others' actions, a capability rooted in Theory-of-Mind (ToM). While\nrecent Large Language Models (LLMs) excel at logical inference, their ability\nto infer rationale in dynamic, collaborative settings remains under-explored.\nThis study introduces LLM-Hanabi, a novel benchmark that uses the cooperative\ngame Hanabi to evaluate the rationale inference and ToM of LLMs. Our framework\nfeatures an automated evaluation system that measures both game performance and\nToM proficiency. Across a range of models, we find a significant positive\ncorrelation between ToM and in-game success. Notably, first-order ToM\n(interpreting others' intent) correlates more strongly with performance than\nsecond-order ToM (predicting others' interpretations). These findings highlight\nthat for effective AI collaboration, the ability to accurately interpret a\npartner's rationale is more critical than higher-order reasoning. We conclude\nthat prioritizing first-order ToM is a promising direction for enhancing the\ncollaborative capabilities of future models.", "AI": {"tldr": "LLM-Hanabi benchmark evaluates LLMs' Theory-of-Mind (ToM) in collaborative settings using the Hanabi game, finding first-order ToM (interpreting intent) correlates more with success than second-order ToM.", "motivation": "To assess LLMs' ability to infer rationale behind others' actions in dynamic collaborative settings, as current models excel at logical inference but their ToM capabilities remain under-explored.", "method": "Developed LLM-Hanabi benchmark using cooperative game Hanabi with automated evaluation system measuring both game performance and ToM proficiency across various models.", "result": "Found significant positive correlation between ToM and in-game success, with first-order ToM (interpreting others' intent) correlating more strongly with performance than second-order ToM (predicting others' interpretations).", "conclusion": "For effective AI collaboration, accurately interpreting a partner's rationale (first-order ToM) is more critical than higher-order reasoning, making it a promising direction for enhancing collaborative capabilities."}}
{"id": "2510.04145", "categories": ["cs.CV", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.04145", "abs": "https://arxiv.org/abs/2510.04145", "authors": ["Chenxin Wang", "Elyas Asadi Shamsabadi", "Zhaohui Chen", "Luming Shen", "Alireza Ahmadian Fard Fini", "Daniel Dias-da-Costa"], "title": "Automating construction safety inspections using a multi-modal vision-language RAG framework", "comment": "33 pages, 11 figures, 7 tables", "summary": "Conventional construction safety inspection methods are often inefficient as\nthey require navigating through large volume of information. Recent advances in\nlarge vision-language models (LVLMs) provide opportunities to automate safety\ninspections through enhanced visual and linguistic understanding. However,\nexisting applications face limitations including irrelevant or unspecific\nresponses, restricted modal inputs and hallucinations. Utilisation of Large\nLanguage Models (LLMs) for this purpose is constrained by availability of\ntraining data and frequently lack real-time adaptability. This study introduces\nSiteShield, a multi-modal LVLM-based Retrieval-Augmented Generation (RAG)\nframework for automating construction safety inspection reports by integrating\nvisual and audio inputs. Using real-world data, SiteShield outperformed\nunimodal LLMs without RAG with an F1 score of 0.82, hamming loss of 0.04,\nprecision of 0.76, and recall of 0.96. The findings indicate that SiteShield\noffers a novel pathway to enhance information retrieval and efficiency in\ngenerating safety reports.", "AI": {"tldr": "SiteShield is a multi-modal LVLM-based RAG framework that automates construction safety inspection reports by integrating visual and audio inputs, outperforming unimodal LLMs with improved accuracy metrics.", "motivation": "Conventional construction safety inspection methods are inefficient due to large information volumes, and existing LVLM applications face limitations like irrelevant responses, restricted modal inputs, and hallucinations. LLMs lack training data and real-time adaptability for this purpose.", "method": "Developed SiteShield, a multi-modal large vision-language model (LVLM) based Retrieval-Augmented Generation (RAG) framework that integrates visual and audio inputs for automated safety inspection reports.", "result": "SiteShield outperformed unimodal LLMs without RAG with an F1 score of 0.82, hamming loss of 0.04, precision of 0.76, and recall of 0.96 using real-world data.", "conclusion": "SiteShield offers a novel pathway to enhance information retrieval and efficiency in generating construction safety reports through its multi-modal RAG approach."}}
{"id": "2510.05014", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.05014", "abs": "https://arxiv.org/abs/2510.05014", "authors": ["Xuanming Cui", "Jianpeng Cheng", "Hong-you Chen", "Satya Narayan Shukla", "Abhijeet Awasthi", "Xichen Pan", "Chaitanya Ahuja", "Shlok Kumar Mishra", "Qi Guo", "Ser-Nam Lim", "Aashu Singh", "Xiangjun Fan"], "title": "Think Then Embed: Generative Context Improves Multimodal Embedding", "comment": null, "summary": "There is a growing interest in Universal Multimodal Embeddings (UME), where\nmodels are required to generate task-specific representations. While recent\nstudies show that Multimodal Large Language Models (MLLMs) perform well on such\ntasks, they treat MLLMs solely as encoders, overlooking their generative\ncapacity. However, such an encoding paradigm becomes less effective as\ninstructions become more complex and require compositional reasoning. Inspired\nby the proven effectiveness of chain-of-thought reasoning, we propose a general\nThink-Then-Embed (TTE) framework for UME, composed of a reasoner and an\nembedder. The reasoner MLLM first generates reasoning traces that explain\ncomplex queries, followed by an embedder that produces representations\nconditioned on both the original query and the intermediate reasoning. This\nexplicit reasoning step enables more nuanced understanding of complex\nmultimodal instructions. Our contributions are threefold. First, by leveraging\na powerful MLLM reasoner, we achieve state-of-the-art performance on the\nMMEB-V2 benchmark, surpassing proprietary models trained on massive in-house\ndatasets. Second, to reduce the dependency on large MLLM reasoners, we finetune\na smaller MLLM reasoner using high-quality embedding-centric reasoning traces,\nachieving the best performance among open-source models with a 7% absolute gain\nover recently proposed models. Third, we investigate strategies for integrating\nthe reasoner and embedder into a unified model for improved efficiency without\nsacrificing performance.", "AI": {"tldr": "The paper proposes Think-Then-Embed (TTE), a framework that enhances Universal Multimodal Embeddings by incorporating explicit reasoning through MLLMs before generating embeddings, achieving state-of-the-art performance on MMEB-V2.", "motivation": "Current approaches treat MLLMs only as encoders, ignoring their generative capacity, which becomes ineffective for complex instructions requiring compositional reasoning.", "method": "TTE framework uses a reasoner MLLM to generate reasoning traces for complex queries, followed by an embedder that produces representations conditioned on both the original query and intermediate reasoning.", "result": "Achieved SOTA on MMEB-V2 benchmark, surpassed proprietary models, and with a smaller finetuned MLLM reasoner achieved 7% absolute gain over recent open-source models.", "conclusion": "Explicit reasoning enables nuanced understanding of complex multimodal instructions, and the framework can be integrated into a unified model for efficiency without performance loss."}}
{"id": "2510.04174", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04174", "abs": "https://arxiv.org/abs/2510.04174", "authors": ["Piyush Arora", "Navlika Singh", "Vasubhya Diwan", "Pratik Mazumder"], "title": "BLADE: Bias-Linked Adaptive DEbiasing", "comment": "The authors have contributed equally", "summary": "Neural networks have revolutionized numerous fields, yet they remain\nvulnerable to a critical flaw: the tendency to learn implicit biases, spurious\ncorrelations between certain attributes and target labels in training data.\nThese biases are often more prevalent and easier to learn, causing models to\nrely on superficial patterns rather than task-relevant features necessary for\ngeneralization. Existing methods typically rely on strong assumptions, such as\nprior knowledge of these biases or access to bias-conflicting samples, i.e.,\nsamples that contradict spurious correlations and counterbalance bias-aligned\nsamples, samples that conform to these spurious correlations. However, such\nassumptions are often impractical in real-world settings. We propose BLADE\n({B}ias-{L}inked {A}daptive {DE}biasing), a generative debiasing framework that\nrequires no prior knowledge of bias or bias-conflicting samples. BLADE first\ntrains a generative model to translate images across bias domains while\npreserving task-relevant features. Then, it adaptively refines each image with\nits synthetic counterpart based on the image's susceptibility to bias. To\nencourage robust representations, BLADE aligns an image with its\nbias-translated synthetic counterpart that shares task-relevant features but\ndiffers in bias, while misaligning it with samples sharing the same bias. We\nevaluate BLADE on multiple benchmark datasets and show that it significantly\noutperforms state-of-the-art methods. Notably, it exceeds the closest baseline\nby an absolute margin of around 18% on the corrupted CIFAR-10 dataset under the\nworst group setting, establishing a new benchmark in bias mitigation and\ndemonstrating its potential for developing more robust deep learning models\nwithout explicit supervision.", "AI": {"tldr": "BLADE is a generative debiasing framework that mitigates neural network biases without requiring prior knowledge of biases or bias-conflicting samples. It uses generative models to translate images across bias domains while preserving task-relevant features, then adaptively refines images based on their bias susceptibility.", "motivation": "Neural networks often learn implicit biases and spurious correlations from training data, relying on superficial patterns rather than task-relevant features. Existing debiasing methods require impractical assumptions like prior knowledge of biases or access to bias-conflicting samples.", "method": "BLADE trains a generative model to translate images across bias domains while preserving task features. It then adaptively refines each image with its synthetic counterpart based on bias susceptibility, aligning images with bias-translated counterparts that share task features but differ in bias, while misaligning with same-bias samples.", "result": "BLADE significantly outperforms state-of-the-art methods on multiple benchmark datasets. It exceeds the closest baseline by ~18% absolute margin on corrupted CIFAR-10 under worst group setting, establishing new benchmark in bias mitigation.", "conclusion": "BLADE demonstrates potential for developing more robust deep learning models without explicit supervision, effectively addressing neural network bias vulnerabilities through generative translation and adaptive refinement."}}
{"id": "2510.05048", "categories": ["cs.AI", "cs.GT"], "pdf": "https://arxiv.org/pdf/2510.05048", "abs": "https://arxiv.org/abs/2510.05048", "authors": ["Ond\u0159ej Kub\u00ed\u010dek", "Viliam Lis\u00fd"], "title": "Look-ahead Reasoning with a Learned Model in Imperfect Information Games", "comment": null, "summary": "Test-time reasoning significantly enhances pre-trained AI agents'\nperformance. However, it requires an explicit environment model, often\nunavailable or overly complex in real-world scenarios. While MuZero enables\neffective model learning for search in perfect information games, extending\nthis paradigm to imperfect information games presents substantial challenges\ndue to more nuanced look-ahead reasoning techniques and large number of states\nrelevant for individual decisions. This paper introduces an algorithm LAMIR\nthat learns an abstracted model of an imperfect information game directly from\nthe agent-environment interaction. During test time, this trained model is used\nto perform look-ahead reasoning. The learned abstraction limits the size of\neach subgame to a manageable size, making theoretically principled look-ahead\nreasoning tractable even in games where previous methods could not scale. We\nempirically demonstrate that with sufficient capacity, LAMIR learns the exact\nunderlying game structure, and with limited capacity, it still learns a\nvaluable abstraction, which improves game playing performance of the\npre-trained agents even in large games.", "AI": {"tldr": "LAMIR algorithm learns abstracted models for imperfect information games, enabling tractable look-ahead reasoning that improves pre-trained agents' performance.", "motivation": "Test-time reasoning requires explicit environment models, which are often unavailable or too complex in real-world imperfect information games. Existing methods like MuZero work well for perfect information games but face scalability challenges in imperfect information settings.", "method": "LAMIR learns an abstracted model directly from agent-environment interaction. The learned abstraction limits subgame sizes to manageable levels, making principled look-ahead reasoning tractable even in large games.", "result": "Empirical results show that with sufficient capacity, LAMIR learns the exact underlying game structure. With limited capacity, it still learns valuable abstractions that improve game playing performance of pre-trained agents in large games.", "conclusion": "LAMIR enables effective test-time reasoning in imperfect information games by learning tractable abstractions, overcoming scalability limitations of previous methods."}}
{"id": "2510.04180", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04180", "abs": "https://arxiv.org/abs/2510.04180", "authors": ["Ran Eisenberg", "Amit Rozner", "Ethan Fetaya", "Ofir Lindenbaum"], "title": "From Segments to Concepts: Interpretable Image Classification via Concept-Guided Segmentation", "comment": null, "summary": "Deep neural networks have achieved remarkable success in computer vision;\nhowever, their black-box nature in decision-making limits interpretability and\ntrust, particularly in safety-critical applications. Interpretability is\ncrucial in domains where errors have severe consequences. Existing models not\nonly lack transparency but also risk exploiting unreliable or misleading\nfeatures, which undermines both robustness and the validity of their\nexplanations. Concept Bottleneck Models (CBMs) aim to improve transparency by\nreasoning through human-interpretable concepts. Still, they require costly\nconcept annotations and lack spatial grounding, often failing to identify which\nregions support each concept. We propose SEG-MIL-CBM, a novel framework that\nintegrates concept-guided image segmentation into an attention-based multiple\ninstance learning (MIL) framework, where each segmented region is treated as an\ninstance and the model learns to aggregate evidence across them. By reasoning\nover semantically meaningful regions aligned with high-level concepts, our\nmodel highlights task-relevant evidence, down-weights irrelevant cues, and\nproduces spatially grounded, concept-level explanations without requiring\nannotations of concepts or groups. SEG-MIL-CBM achieves robust performance\nacross settings involving spurious correlations (unintended dependencies\nbetween background and label), input corruptions (perturbations that degrade\nvisual quality), and large-scale benchmarks, while providing transparent,\nconcept-level explanations.", "AI": {"tldr": "SEG-MIL-CBM integrates concept-guided segmentation with multiple instance learning to provide spatially grounded, concept-level explanations without requiring concept annotations, improving robustness and interpretability.", "motivation": "Deep neural networks lack transparency and may exploit misleading features, limiting trust in safety-critical applications. Existing Concept Bottleneck Models require costly annotations and lack spatial grounding.", "method": "Combines concept-guided image segmentation with attention-based multiple instance learning, treating segmented regions as instances and aggregating evidence across them to identify task-relevant concepts.", "result": "Achieves robust performance across spurious correlations, input corruptions, and large-scale benchmarks while providing transparent, concept-level explanations without concept annotations.", "conclusion": "SEG-MIL-CBM enables spatially grounded, concept-level interpretability without annotation costs, improving both model transparency and robustness in computer vision applications."}}
{"id": "2510.05059", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.05059", "abs": "https://arxiv.org/abs/2510.05059", "authors": ["Junlin Wang", "Jue Wang", "Zhen", "Xu", "Ben Athiwaratkun", "Bhuwan Dhingra", "Ce Zhang", "James Zou"], "title": "Staircase Streaming for Low-Latency Multi-Agent Inference", "comment": null, "summary": "Recent advances in large language models (LLMs) opened up new directions for\nleveraging the collective expertise of multiple LLMs. These methods, such as\nMixture-of-Agents, typically employ additional inference steps to generate\nintermediate outputs, which are then used to produce the final response. While\nmulti-agent inference can enhance response quality, it can significantly\nincrease the time to first token (TTFT), posing a challenge for\nlatency-sensitive applications and hurting user experience. To address this\nissue, we propose staircase streaming for low-latency multi-agent inference.\nInstead of waiting for the complete intermediate outputs from previous steps,\nwe begin generating the final response as soon as we receive partial outputs\nfrom these steps. Experimental results demonstrate that staircase streaming\nreduces TTFT by up to 93% while maintaining response quality.", "AI": {"tldr": "Proposes staircase streaming to reduce latency in multi-agent LLM inference by generating final responses using partial intermediate outputs instead of waiting for complete outputs.", "motivation": "Multi-agent inference improves LLM response quality but significantly increases time to first token (TTFT), which hurts user experience in latency-sensitive applications.", "method": "Staircase streaming - begins generating final response as soon as partial outputs are received from intermediate steps, rather than waiting for complete intermediate outputs.", "result": "Reduces TTFT by up to 93% while maintaining response quality.", "conclusion": "Staircase streaming effectively addresses latency issues in multi-agent inference without compromising response quality."}}
{"id": "2510.04188", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04188", "abs": "https://arxiv.org/abs/2510.04188", "authors": ["Shikang Zheng", "Guantao Chen", "Qinming Zhou", "Yuqi Lin", "Lixuan He", "Chang Zou", "Peiliang Cai", "Jiacheng Liu", "Linfeng Zhang"], "title": "Let Features Decide Their Own Solvers: Hybrid Feature Caching for Diffusion Transformers", "comment": null, "summary": "Diffusion Transformers offer state-of-the-art fidelity in image and video\nsynthesis, but their iterative sampling process remains a major bottleneck due\nto the high cost of transformer forward passes at each timestep. To mitigate\nthis, feature caching has emerged as a training-free acceleration technique\nthat reuses or forecasts hidden representations. However, existing methods\noften apply a uniform caching strategy across all feature dimensions, ignoring\ntheir heterogeneous dynamic behaviors. Therefore, we adopt a new perspective by\nmodeling hidden feature evolution as a mixture of ODEs across dimensions, and\nintroduce HyCa, a Hybrid ODE solver inspired caching framework that applies\ndimension-wise caching strategies. HyCa achieves near-lossless acceleration\nacross diverse domains and models, including 5.55 times speedup on FLUX, 5.56\ntimes speedup on HunyuanVideo, 6.24 times speedup on Qwen-Image and\nQwen-Image-Edit without retraining.", "AI": {"tldr": "HyCa is a training-free acceleration framework for Diffusion Transformers that uses dimension-wise caching strategies based on modeling hidden feature evolution as a mixture of ODEs, achieving significant speedups (5.55-6.24\u00d7) across various models without retraining.", "motivation": "Diffusion Transformers have state-of-the-art image/video synthesis quality but suffer from slow iterative sampling due to expensive transformer forward passes at each timestep. Existing feature caching methods use uniform strategies that ignore heterogeneous dynamic behaviors across different feature dimensions.", "method": "HyCa models hidden feature evolution as a mixture of ODEs across dimensions and applies dimension-wise caching strategies using a Hybrid ODE solver inspired framework, allowing different caching approaches for different feature dimensions based on their dynamic behaviors.", "result": "Achieved near-lossless acceleration with 5.55\u00d7 speedup on FLUX, 5.56\u00d7 on HunyuanVideo, 6.24\u00d7 on Qwen-Image and Qwen-Image-Edit models, demonstrating effectiveness across diverse domains and models without requiring retraining.", "conclusion": "HyCa's dimension-wise caching approach based on modeling feature evolution as ODEs provides an effective training-free acceleration method for Diffusion Transformers, significantly reducing sampling time while maintaining high fidelity across various applications."}}
{"id": "2411.05993", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2411.05993", "abs": "https://arxiv.org/abs/2411.05993", "authors": ["Magauiya Zhussip", "Iaroslav Koshelev", "Stamatis Lefkimmiatis"], "title": "A Modular Conditional Diffusion Framework for Image Reconstruction", "comment": null, "summary": "Diffusion Probabilistic Models (DPMs) have been recently utilized to deal\nwith various blind image restoration (IR) tasks, where they have demonstrated\noutstanding performance in terms of perceptual quality. However, the\ntask-specific nature of existing solutions and the excessive computational\ncosts related to their training, make such models impractical and challenging\nto use for different IR tasks than those that were initially trained for. This\nhinders their wider adoption, especially by those who lack access to powerful\ncomputational resources and vast amount of training data. In this work we aim\nto address the above issues and enable the successful adoption of DPMs in\npractical IR-related applications. Towards this goal, we propose a modular\ndiffusion probabilistic IR framework (DP-IR), which allows us to combine the\nperformance benefits of existing pre-trained state-of-the-art IR networks and\ngenerative DPMs, while it requires only the additional training of a relatively\nsmall module (0.7M params) related to the particular IR task of interest.\nMoreover, the architecture of the proposed framework allows for a sampling\nstrategy that leads to at least four times reduction of neural function\nevaluations without suffering any performance loss, while it can also be\ncombined with existing acceleration techniques such as DDIM. We evaluate our\nmodel on four benchmarks for the tasks of burst JDD-SR, dynamic scene\ndeblurring, and super-resolution. Our method outperforms existing approaches in\nterms of perceptual quality while it retains a competitive performance with\nrespect to fidelity metrics.", "AI": {"tldr": "DP-IR is a modular diffusion probabilistic framework for blind image restoration that combines pre-trained IR networks with generative DPMs, requiring only small additional training (0.7M params) and enabling 4x faster sampling without performance loss.", "motivation": "Existing DPM-based IR solutions are task-specific, computationally expensive to train, and impractical for different IR tasks, limiting wider adoption especially for users with limited computational resources and training data.", "method": "Proposes DP-IR framework that combines pre-trained state-of-the-art IR networks with generative DPMs, requiring only training of a small task-specific module (0.7M parameters), and includes sampling strategy for 4x reduction in neural function evaluations.", "result": "Outperforms existing approaches in perceptual quality while maintaining competitive fidelity metrics on burst JDD-SR, dynamic scene deblurring, and super-resolution benchmarks.", "conclusion": "DP-IR enables practical adoption of DPMs for IR tasks by reducing computational requirements through modular design and efficient sampling, making DPM-based IR more accessible and versatile."}}
{"id": "2510.04201", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04201", "abs": "https://arxiv.org/abs/2510.04201", "authors": ["Moo Hyun Son", "Jintaek Oh", "Sun Bin Mun", "Jaechul Roh", "Sehyun Choi"], "title": "World-To-Image: Grounding Text-to-Image Generation with Agent-Driven World Knowledge", "comment": null, "summary": "While text-to-image (T2I) models can synthesize high-quality images, their\nperformance degrades significantly when prompted with novel or\nout-of-distribution (OOD) entities due to inherent knowledge cutoffs. We\nintroduce World-To-Image, a novel framework that bridges this gap by empowering\nT2I generation with agent-driven world knowledge. We design an agent that\ndynamically searches the web to retrieve images for concepts unknown to the\nbase model. This information is then used to perform multimodal prompt\noptimization, steering powerful generative backbones toward an accurate\nsynthesis. Critically, our evaluation goes beyond traditional metrics,\nutilizing modern assessments like LLMGrader and ImageReward to measure true\nsemantic fidelity. Our experiments show that World-To-Image substantially\noutperforms state-of-the-art methods in both semantic alignment and visual\naesthetics, achieving +8.1% improvement in accuracy-to-prompt on our curated\nNICE benchmark. Our framework achieves these results with high efficiency in\nless than three iterations, paving the way for T2I systems that can better\nreflect the ever-changing real world. Our demo code is available\nhere\\footnote{https://github.com/mhson-kyle/World-To-Image}.", "AI": {"tldr": "World-To-Image is a framework that enhances text-to-image generation by using web-searching agents to retrieve knowledge about novel entities, enabling more accurate synthesis of out-of-distribution concepts through multimodal prompt optimization.", "motivation": "Text-to-image models struggle with novel or out-of-distribution entities due to knowledge cutoffs, leading to degraded performance when prompted with unfamiliar concepts.", "method": "Uses an agent that dynamically searches the web to retrieve images for unknown concepts, then performs multimodal prompt optimization to steer generative models toward accurate synthesis.", "result": "Achieves +8.1% improvement in accuracy-to-prompt on the NICE benchmark, substantially outperforming state-of-the-art methods in both semantic alignment and visual aesthetics with high efficiency in less than three iterations.", "conclusion": "The framework enables T2I systems to better reflect the ever-changing real world by bridging knowledge gaps through agent-driven world knowledge retrieval."}}
{"id": "2411.18625", "categories": ["cs.CV", "cs.AI", "cs.GR", "eess.IV"], "pdf": "https://arxiv.org/pdf/2411.18625", "abs": "https://arxiv.org/abs/2411.18625", "authors": ["Brian Chao", "Hung-Yu Tseng", "Lorenzo Porzi", "Chen Gao", "Tuotuo Li", "Qinbo Li", "Ayush Saraf", "Jia-Bin Huang", "Johannes Kopf", "Gordon Wetzstein", "Changil Kim"], "title": "Textured Gaussians for Enhanced 3D Scene Appearance Modeling", "comment": "Will be presented at CVPR 2025. Project website:\n  https://textured-gaussians.github.io/", "summary": "3D Gaussian Splatting (3DGS) has recently emerged as a state-of-the-art 3D\nreconstruction and rendering technique due to its high-quality results and fast\ntraining and rendering time. However, pixels covered by the same Gaussian are\nalways shaded in the same color up to a Gaussian falloff scaling factor.\nFurthermore, the finest geometric detail any individual Gaussian can represent\nis a simple ellipsoid. These properties of 3DGS greatly limit the expressivity\nof individual Gaussian primitives. To address these issues, we draw inspiration\nfrom texture and alpha mapping in traditional graphics and integrate it with\n3DGS. Specifically, we propose a new generalized Gaussian appearance\nrepresentation that augments each Gaussian with alpha~(A), RGB, or RGBA texture\nmaps to model spatially varying color and opacity across the extent of each\nGaussian. As such, each Gaussian can represent a richer set of texture patterns\nand geometric structures, instead of just a single color and ellipsoid as in\nnaive Gaussian Splatting. Surprisingly, we found that the expressivity of\nGaussians can be greatly improved by using alpha-only texture maps, and further\naugmenting Gaussians with RGB texture maps achieves the highest expressivity.\nWe validate our method on a wide variety of standard benchmark datasets and our\nown custom captures at both the object and scene levels. We demonstrate image\nquality improvements over existing methods while using a similar or lower\nnumber of Gaussians.", "AI": {"tldr": "This paper proposes enhancing 3D Gaussian Splatting by adding texture mapping to each Gaussian, allowing them to represent richer textures and geometric details beyond simple ellipsoids.", "motivation": "Standard 3DGS limits each Gaussian to a single color and simple ellipsoid shape, which restricts expressivity and detail representation. The authors aim to overcome these limitations by drawing inspiration from traditional graphics texture mapping.", "method": "The method augments each Gaussian with alpha, RGB, or RGBA texture maps to model spatially varying color and opacity across each Gaussian's extent, creating a generalized Gaussian appearance representation.", "result": "The approach achieves higher image quality than existing methods while using similar or fewer Gaussians, with alpha-only texture maps showing significant expressivity improvements and RGB texture maps achieving the highest expressivity.", "conclusion": "Integrating texture mapping with 3DGS significantly enhances Gaussian expressivity, enabling richer texture patterns and geometric structures while maintaining the efficiency advantages of the original 3DGS approach."}}
{"id": "2510.04220", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04220", "abs": "https://arxiv.org/abs/2510.04220", "authors": ["Lixuan He", "Shikang Zheng", "Linfeng Zhang"], "title": "MASC: Boosting Autoregressive Image Generation with a Manifold-Aligned Semantic Clustering", "comment": null, "summary": "Autoregressive (AR) models have shown great promise in image generation, yet\nthey face a fundamental inefficiency stemming from their core component: a\nvast, unstructured vocabulary of visual tokens. This conventional approach\ntreats tokens as a flat vocabulary, disregarding the intrinsic structure of the\ntoken embedding space where proximity often correlates with semantic\nsimilarity. This oversight results in a highly complex prediction task, which\nhinders training efficiency and limits final generation quality. To resolve\nthis, we propose Manifold-Aligned Semantic Clustering (MASC), a principled\nframework that constructs a hierarchical semantic tree directly from the\ncodebook's intrinsic structure. MASC employs a novel geometry-aware distance\nmetric and a density-driven agglomerative construction to model the underlying\nmanifold of the token embeddings. By transforming the flat, high-dimensional\nprediction task into a structured, hierarchical one, MASC introduces a\nbeneficial inductive bias that significantly simplifies the learning problem\nfor the AR model. MASC is designed as a plug-and-play module, and our extensive\nexperiments validate its effectiveness: it accelerates training by up to 57%\nand significantly improves generation quality, reducing the FID of LlamaGen-XL\nfrom 2.87 to 2.58. MASC elevates existing AR frameworks to be highly\ncompetitive with state-of-the-art methods, establishing that structuring the\nprediction space is as crucial as architectural innovation for scalable\ngenerative modeling.", "AI": {"tldr": "MASC is a framework that constructs hierarchical semantic trees from token embeddings to simplify autoregressive image generation, achieving 57% faster training and improved quality.", "motivation": "Autoregressive models for image generation are inefficient due to flat, unstructured token vocabularies that ignore semantic relationships in embedding space, making prediction tasks unnecessarily complex.", "method": "MASC uses geometry-aware distance metrics and density-driven agglomerative clustering to build hierarchical semantic trees that model the underlying manifold of token embeddings.", "result": "MASC accelerates training by up to 57% and improves generation quality, reducing FID of LlamaGen-XL from 2.87 to 2.58.", "conclusion": "Structuring the prediction space through hierarchical semantic modeling is crucial for scalable generative modeling and can make existing AR frameworks competitive with state-of-the-art methods."}}
{"id": "2510.04225", "categories": ["cs.CV", "cs.AI", "cs.CL", "68T45", "I.2.10; I.2.7"], "pdf": "https://arxiv.org/pdf/2510.04225", "abs": "https://arxiv.org/abs/2510.04225", "authors": ["Yikun Ji", "Yan Hong", "Bowen Deng", "jun lan", "Huijia Zhu", "Weiqiang Wang", "Liqing Zhang", "Jianfu Zhang"], "title": "Zoom-In to Sort AI-Generated Images Out", "comment": "9 pages, 6 images (19 pages, 11 figures including appendix)", "summary": "The rapid growth of AI-generated imagery has blurred the boundary between\nreal and synthetic content, raising critical concerns for digital integrity.\nVision-language models (VLMs) offer interpretability through explanations but\noften fail to detect subtle artifacts in high-quality synthetic images. We\npropose ZoomIn, a two-stage forensic framework that improves both accuracy and\ninterpretability. Mimicking human visual inspection, ZoomIn first scans an\nimage to locate suspicious regions and then performs a focused analysis on\nthese zoomed-in areas to deliver a grounded verdict. To support training, we\nintroduce MagniFake, a dataset of 20,000 real and high-quality synthetic images\nannotated with bounding boxes and forensic explanations, generated through an\nautomated VLM-based pipeline. Our method achieves 96.39% accuracy with robust\ngeneralization, while providing human-understandable explanations grounded in\nvisual evidence.", "AI": {"tldr": "ZoomIn is a two-stage forensic framework that improves AI-generated image detection accuracy and interpretability by first scanning for suspicious regions and then performing focused analysis, achieving 96.39% accuracy.", "motivation": "The rapid growth of AI-generated imagery has blurred boundaries between real and synthetic content, raising digital integrity concerns. Existing vision-language models often fail to detect subtle artifacts in high-quality synthetic images.", "method": "Proposes ZoomIn framework that mimics human visual inspection: first scans images to locate suspicious regions, then performs focused analysis on zoomed-in areas. Uses MagniFake dataset of 20,000 real/synthetic images with bounding boxes and forensic explanations generated through automated VLM pipeline.", "result": "Achieves 96.39% accuracy with robust generalization while providing human-understandable explanations grounded in visual evidence.", "conclusion": "ZoomIn framework effectively addresses the challenge of detecting high-quality synthetic images by combining accuracy with interpretable explanations through a two-stage forensic approach."}}
{"id": "2510.04231", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04231", "abs": "https://arxiv.org/abs/2510.04231", "authors": ["Stefan Dirnstorfer"], "title": "A Recursive Pyramidal Algorithm for Solving the Image Registration Problem", "comment": null, "summary": "The problem of image registration is finding a transformation that aligns two\nimages, such that the corresponding points are in the same location. This paper\nintroduces a simple, end-to-end trainable algorithm that is implementable in a\nfew lines of Python code. The approach is shown to work with very little\ntraining data and training time, while achieving accurate results in some\nsettings. An example application to stereo vision was trained from 74 images on\na 19x15 input window. With just a dozen lines of Python code this algorithm\nexcels in brevity and may serve as a good start in related scenarios with\nlimitations to training data, training time or code complexity.", "AI": {"tldr": "A simple, end-to-end trainable image registration algorithm that requires minimal code, training data, and training time while achieving accurate results.", "motivation": "To create an efficient and accessible image registration solution that works with limited resources and can be easily implemented.", "method": "End-to-end trainable algorithm implemented in a few lines of Python code, demonstrated with stereo vision using 74 images on a 19x15 input window.", "result": "Achieves accurate results with very little training data and training time, excelling in brevity and simplicity.", "conclusion": "The algorithm serves as an excellent starting point for scenarios with constraints on training data, training time, or code complexity."}}
{"id": "2510.04232", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04232", "abs": "https://arxiv.org/abs/2510.04232", "authors": ["Amin Ahmadi Kasani", "Hedieh Sajedi"], "title": "Detection of retinal diseases using an accelerated reused convolutional network", "comment": null, "summary": "Convolutional neural networks are continually evolving, with some efforts\naimed at improving accuracy, others at increasing speed, and some at enhancing\naccessibility. Improving accessibility broadens the application of neural\nnetworks across a wider range of tasks, including the detection of eye\ndiseases. Early diagnosis of eye diseases and consulting an ophthalmologist can\nprevent many vision disorders. Given the importance of this issue, various\ndatasets have been collected from the cornea to facilitate the process of\nmaking neural network models. However, most of the methods introduced in the\npast are computationally complex. In this study, we tried to increase the\naccessibility of deep neural network models. We did this at the most\nfundamental level, specifically by redesigning and optimizing the convolutional\nlayers. By doing so, we created a new general model that incorporates our novel\nconvolutional layer named ArConv layers. Thanks to the efficient performance of\nthis new layer, the model has suitable complexity for use in mobile phones and\ncan perform the task of diagnosing the presence of disease with high accuracy.\nThe final model we present contains only 1.3 million parameters. In comparison\nto the MobileNetV2 model, which has 2.2 million parameters, our model\ndemonstrated better accuracy when trained and evaluated on the RfMiD dataset\nunder identical conditions, achieving an accuracy of 0.9328 versus 0.9266 on\nthe RfMiD test set.", "AI": {"tldr": "The paper introduces ArConv layers to redesign and optimize convolutional layers, creating a lightweight deep learning model with only 1.3M parameters that achieves better accuracy than MobileNetV2 (0.9328 vs 0.9266) on eye disease detection using the RfMiD dataset.", "motivation": "To improve accessibility of deep neural networks for eye disease diagnosis by reducing computational complexity, making models suitable for mobile devices while maintaining high accuracy.", "method": "Redesigned and optimized convolutional layers by creating novel ArConv layers, developing a new general model architecture that focuses on fundamental layer improvements for efficiency.", "result": "Created a model with only 1.3M parameters that outperformed MobileNetV2 (2.2M parameters) on the RfMiD dataset, achieving 0.9328 accuracy vs 0.9266 under identical training conditions.", "conclusion": "The ArConv-based model successfully balances computational efficiency and accuracy, making deep learning more accessible for mobile-based eye disease diagnosis applications."}}
{"id": "2510.04236", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04236", "abs": "https://arxiv.org/abs/2510.04236", "authors": ["Shikun Liu", "Kam Woh Ng", "Wonbong Jang", "Jiadong Guo", "Junlin Han", "Haozhe Liu", "Yiannis Douratsos", "Juan C. P\u00e9rez", "Zijian Zhou", "Chi Phung", "Tao Xiang", "Juan-Manuel P\u00e9rez-R\u00faa"], "title": "Scaling Sequence-to-Sequence Generative Neural Rendering", "comment": "Project Page: https://shikun.io/projects/kaleido", "summary": "We present Kaleido, a family of generative models designed for\nphotorealistic, unified object- and scene-level neural rendering. Kaleido\noperates on the principle that 3D can be regarded as a specialised sub-domain\nof video, expressed purely as a sequence-to-sequence image synthesis task.\nThrough a systemic study of scaling sequence-to-sequence generative neural\nrendering, we introduce key architectural innovations that enable our model to:\ni) perform generative view synthesis without explicit 3D representations; ii)\ngenerate any number of 6-DoF target views conditioned on any number of\nreference views via a masked autoregressive framework; and iii) seamlessly\nunify 3D and video modelling within a single decoder-only rectified flow\ntransformer. Within this unified framework, Kaleido leverages large-scale video\ndata for pre-training, which significantly improves spatial consistency and\nreduces reliance on scarce, camera-labelled 3D datasets -- all without any\narchitectural modifications. Kaleido sets a new state-of-the-art on a range of\nview synthesis benchmarks. Its zero-shot performance substantially outperforms\nother generative methods in few-view settings, and, for the first time, matches\nthe quality of per-scene optimisation methods in many-view settings.", "AI": {"tldr": "Kaleido is a generative model for neural rendering that treats 3D as a specialized video domain, using sequence-to-sequence image synthesis to perform view synthesis without explicit 3D representations.", "motivation": "To create photorealistic neural rendering that unifies object- and scene-level generation while reducing reliance on scarce camera-labeled 3D datasets by leveraging large-scale video data.", "method": "Uses a masked autoregressive framework with decoder-only rectified flow transformer to generate 6-DoF target views from reference views, treating 3D as a sequence-to-sequence video task.", "result": "Sets new state-of-the-art on view synthesis benchmarks, with zero-shot performance outperforming other generative methods in few-view settings and matching per-scene optimization methods in many-view settings.", "conclusion": "Kaleido successfully demonstrates that 3D rendering can be effectively modeled as a video sequence task, enabling unified 3D and video modeling while achieving competitive performance without architectural modifications."}}
{"id": "2510.04243", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04243", "abs": "https://arxiv.org/abs/2510.04243", "authors": ["Jincan Lou", "Jingkun Chen", "Haoquan Li", "Hang Li", "Wenjian Huang", "Weihua Chen", "Fan Wang", "Jianguo Zhang"], "title": "The best performance in the CARE 2025 -- Liver Task (LiSeg-Contrast): Contrast-Aware Semi-Supervised Segmentation with Domain Generalization and Test-Time Adaptation", "comment": "11 pages, 3 figures", "summary": "Accurate liver segmentation from contrast-enhanced MRI is essential for\ndiagnosis, treatment planning, and disease monitoring. However, it remains\nchallenging due to limited annotated data, heterogeneous enhancement protocols,\nand significant domain shifts across scanners and institutions. Traditional\nimage-to-image translation frameworks have made great progress in domain\ngeneralization, but their application is not straightforward. For example,\nPix2Pix requires image registration, and cycle-GAN cannot be integrated\nseamlessly into segmentation pipelines. Meanwhile, these methods are originally\nused to deal with cross-modality scenarios, and often introduce structural\ndistortions and suffer from unstable training, which may pose drawbacks in our\nsingle-modality scenario. To address these challenges, we propose CoSSeg-TTA, a\ncompact segmentation framework for the GED4 (Gd-EOB-DTPA enhanced hepatobiliary\nphase MRI) modality built upon nnU-Netv2 and enhanced with a semi-supervised\nmean teacher scheme to exploit large amounts of unlabeled volumes. A domain\nadaptation module, incorporating a randomized histogram-based style appearance\ntransfer function and a trainable contrast-aware network, enriches domain\ndiversity and mitigates cross-center variability. Furthermore, a continual\ntest-time adaptation strategy is employed to improve robustness during\ninference. Extensive experiments demonstrate that our framework consistently\noutperforms the nnU-Netv2 baseline, achieving superior Dice score and Hausdorff\nDistance while exhibiting strong generalization to unseen domains under\nlow-annotation conditions.", "AI": {"tldr": "CoSSeg-TTA is a compact liver segmentation framework for contrast-enhanced MRI that uses nnU-Netv2 with semi-supervised learning, domain adaptation, and test-time adaptation to address challenges of limited annotated data, domain shifts, and heterogeneous protocols.", "motivation": "Liver segmentation from contrast-enhanced MRI is essential for diagnosis and treatment but faces challenges including limited annotated data, heterogeneous enhancement protocols, and domain shifts across scanners/institutions. Existing domain adaptation methods like Pix2Pix and cycle-GAN have limitations such as requiring image registration, causing structural distortions, and unstable training.", "method": "Built on nnU-Netv2 with semi-supervised mean teacher scheme to leverage unlabeled data. Includes domain adaptation with randomized histogram-based style appearance transfer and trainable contrast-aware network to increase domain diversity. Uses continual test-time adaptation during inference for robustness.", "result": "Extensive experiments show the framework consistently outperforms nnU-Netv2 baseline, achieving superior Dice score and Hausdorff Distance. Demonstrates strong generalization to unseen domains under low-annotation conditions.", "conclusion": "CoSSeg-TTA effectively addresses the challenges of liver segmentation in contrast-enhanced MRI by combining semi-supervised learning, domain adaptation, and test-time adaptation, providing robust performance across different domains with limited annotations."}}
{"id": "2510.04245", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04245", "abs": "https://arxiv.org/abs/2510.04245", "authors": ["Ayushi Mehrotra", "Derek Peng", "Dipkamal Bhusal", "Nidhi Rastogi"], "title": "Concept-Based Masking: A Patch-Agnostic Defense Against Adversarial Patch Attacks", "comment": "neurips workshop", "summary": "Adversarial patch attacks pose a practical threat to deep learning models by\nforcing targeted misclassifications through localized perturbations, often\nrealized in the physical world. Existing defenses typically assume prior\nknowledge of patch size or location, limiting their applicability. In this\nwork, we propose a patch-agnostic defense that leverages concept-based\nexplanations to identify and suppress the most influential concept activation\nvectors, thereby neutralizing patch effects without explicit detection.\nEvaluated on Imagenette with a ResNet-50, our method achieves higher robust and\nclean accuracy than the state-of-the-art PatchCleanser, while maintaining\nstrong performance across varying patch sizes and locations. Our results\nhighlight the promise of combining interpretability with robustness and suggest\nconcept-driven defenses as a scalable strategy for securing machine learning\nmodels against adversarial patch attacks.", "AI": {"tldr": "A patch-agnostic defense method using concept-based explanations to neutralize adversarial patch attacks without requiring prior knowledge of patch size or location, outperforming state-of-the-art methods.", "motivation": "Adversarial patch attacks pose practical threats to deep learning models, but existing defenses require prior knowledge of patch characteristics, limiting their real-world applicability.", "method": "Leverages concept-based explanations to identify and suppress the most influential concept activation vectors, neutralizing patch effects without explicit patch detection.", "result": "Achieves higher robust and clean accuracy than state-of-the-art PatchCleanser on Imagenette with ResNet-50, maintaining strong performance across varying patch sizes and locations.", "conclusion": "Combining interpretability with robustness shows promise, suggesting concept-driven defenses as a scalable strategy for securing ML models against adversarial patch attacks."}}
{"id": "2510.04282", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04282", "abs": "https://arxiv.org/abs/2510.04282", "authors": ["Yu Kiu", "Lau", "Chao Chen", "Ge Jin", "Chen Feng"], "title": "Flexible and Efficient Spatio-Temporal Transformer for Sequential Visual Place Recognition", "comment": "8 pages, 6 figures", "summary": "Sequential Visual Place Recognition (Seq-VPR) leverages transformers to\ncapture spatio-temporal features effectively; however, existing approaches\nprioritize performance at the expense of flexibility and efficiency. In\npractice, a transformer-based Seq-VPR model should be flexible to the number of\nframes per sequence (seq-length), deliver fast inference, and have low memory\nusage to meet real-time constraints. To our knowledge, no existing\ntransformer-based Seq-VPR method achieves both flexibility and efficiency. To\naddress this gap, we propose Adapt-STformer, a Seq-VPR method built around our\nnovel Recurrent Deformable Transformer Encoder (Recurrent-DTE), which uses an\niterative recurrent mechanism to fuse information from multiple sequential\nframes. This design naturally supports variable seq-lengths, fast inference,\nand low memory usage. Experiments on the Nordland, Oxford, and NuScenes\ndatasets show that Adapt-STformer boosts recall by up to 17% while reducing\nsequence extraction time by 36% and lowering memory usage by 35% compared to\nthe second-best baseline.", "AI": {"tldr": "Adapt-STformer is a flexible and efficient Seq-VPR method using Recurrent Deformable Transformer Encoder that supports variable sequence lengths, achieves faster inference and lower memory usage while improving recall performance.", "motivation": "Existing transformer-based Seq-VPR methods prioritize performance but lack flexibility (fixed sequence lengths) and efficiency (slow inference, high memory usage), making them unsuitable for real-time applications.", "method": "Proposed Adapt-STformer with Recurrent Deformable Transformer Encoder (Recurrent-DTE) that uses iterative recurrent mechanism to fuse information from multiple sequential frames, enabling variable sequence length support.", "result": "On Nordland, Oxford, and NuScenes datasets: boosts recall by up to 17%, reduces sequence extraction time by 36%, and lowers memory usage by 35% compared to second-best baseline.", "conclusion": "Adapt-STformer achieves both flexibility and efficiency in Seq-VPR, making it suitable for real-time applications while maintaining superior performance."}}
{"id": "2510.04290", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04290", "abs": "https://arxiv.org/abs/2510.04290", "authors": ["Jay Zhangjie Wu", "Xuanchi Ren", "Tianchang Shen", "Tianshi Cao", "Kai He", "Yifan Lu", "Ruiyuan Gao", "Enze Xie", "Shiyi Lan", "Jose M. Alvarez", "Jun Gao", "Sanja Fidler", "Zian Wang", "Huan Ling"], "title": "ChronoEdit: Towards Temporal Reasoning for Image Editing and World Simulation", "comment": "Project Page: https://research.nvidia.com/labs/toronto-ai/chronoedit", "summary": "Recent advances in large generative models have significantly advanced image\nediting and in-context image generation, yet a critical gap remains in ensuring\nphysical consistency, where edited objects must remain coherent. This\ncapability is especially vital for world simulation related tasks. In this\npaper, we present ChronoEdit, a framework that reframes image editing as a\nvideo generation problem. First, ChronoEdit treats the input and edited images\nas the first and last frames of a video, allowing it to leverage large\npretrained video generative models that capture not only object appearance but\nalso the implicit physics of motion and interaction through learned temporal\nconsistency. Second, ChronoEdit introduces a temporal reasoning stage that\nexplicitly performs editing at inference time. Under this setting, the target\nframe is jointly denoised with reasoning tokens to imagine a plausible editing\ntrajectory that constrains the solution space to physically viable\ntransformations. The reasoning tokens are then dropped after a few steps to\navoid the high computational cost of rendering a full video. To validate\nChronoEdit, we introduce PBench-Edit, a new benchmark of image-prompt pairs for\ncontexts that require physical consistency, and demonstrate that ChronoEdit\nsurpasses state-of-the-art baselines in both visual fidelity and physical\nplausibility. Code and models for both the 14B and 2B variants of ChronoEdit\nwill be released on the project page:\nhttps://research.nvidia.com/labs/toronto-ai/chronoedit", "AI": {"tldr": "ChronoEdit reframes image editing as video generation to ensure physical consistency by treating input and edited images as video frames, leveraging pretrained video models and temporal reasoning.", "motivation": "Current generative models lack physical consistency in image editing, which is crucial for world simulation tasks where edited objects must remain coherent.", "method": "Uses input and edited images as first/last video frames, leverages pretrained video models for temporal consistency, introduces temporal reasoning with reasoning tokens to imagine plausible editing trajectories, then drops tokens to avoid full video rendering costs.", "result": "Outperforms state-of-the-art baselines in both visual fidelity and physical plausibility on the new PBench-Edit benchmark for physically consistent editing.", "conclusion": "ChronoEdit effectively bridges the physical consistency gap in image editing by combining video generation principles with temporal reasoning, offering 14B and 2B model variants for practical deployment."}}
{"id": "2510.04312", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04312", "abs": "https://arxiv.org/abs/2510.04312", "authors": ["Vida Adeli", "Ivan Klabucar", "Javad Rajabi", "Benjamin Filtjens", "Soroush Mehraban", "Diwei Wang", "Hyewon Seo", "Trung-Hieu Hoang", "Minh N. Do", "Candice Muller", "Claudia Oliveira", "Daniel Boari Coelho", "Pieter Ginis", "Moran Gilat", "Alice Nieuwboer", "Joke Spildooren", "Lucas Mckay", "Hyeokhyen Kwon", "Gari Clifford", "Christine Esper", "Stewart Factor", "Imari Genias", "Amirhossein Dadashzadeh", "Leia Shum", "Alan Whone", "Majid Mirmehdi", "Andrea Iaboni", "Babak Taati"], "title": "CARE-PD: A Multi-Site Anonymized Clinical Dataset for Parkinson's Disease Gait Assessment", "comment": "Accepted at the Thirty-Ninth Conference on Neural Information\n  Processing Systems (NeurIPS 2025)", "summary": "Objective gait assessment in Parkinson's Disease (PD) is limited by the\nabsence of large, diverse, and clinically annotated motion datasets. We\nintroduce CARE-PD, the largest publicly available archive of 3D mesh gait data\nfor PD, and the first multi-site collection spanning 9 cohorts from 8 clinical\ncenters. All recordings (RGB video or motion capture) are converted into\nanonymized SMPL meshes via a harmonized preprocessing pipeline. CARE-PD\nsupports two key benchmarks: supervised clinical score prediction (estimating\nUnified Parkinson's Disease Rating Scale, UPDRS, gait scores) and unsupervised\nmotion pretext tasks (2D-to-3D keypoint lifting and full-body 3D\nreconstruction). Clinical prediction is evaluated under four generalization\nprotocols: within-dataset, cross-dataset, leave-one-dataset-out, and\nmulti-dataset in-domain adaptation. To assess clinical relevance, we compare\nstate-of-the-art motion encoders with a traditional gait-feature baseline,\nfinding that encoders consistently outperform handcrafted features. Pretraining\non CARE-PD reduces MPJPE (from 60.8mm to 7.5mm) and boosts PD severity macro-F1\nby 17 percentage points, underscoring the value of clinically curated, diverse\ntraining data. CARE-PD and all benchmark code are released for non-commercial\nresearch at https://neurips2025.care-pd.ca/.", "AI": {"tldr": "CARE-PD is the largest publicly available archive of 3D mesh gait data for Parkinson's Disease, spanning 9 cohorts from 8 clinical centers, enabling supervised clinical score prediction and unsupervised motion pretext tasks.", "motivation": "Objective gait assessment in Parkinson's Disease is limited by the absence of large, diverse, and clinically annotated motion datasets.", "method": "All recordings (RGB video or motion capture) are converted into anonymized SMPL meshes via a harmonized preprocessing pipeline. The method supports supervised clinical score prediction (UPDRS gait scores) and unsupervised motion pretext tasks (2D-to-3D keypoint lifting and full-body 3D reconstruction).", "result": "Motion encoders consistently outperform handcrafted features. Pretraining on CARE-PD reduces MPJPE from 60.8mm to 7.5mm and boosts PD severity macro-F1 by 17 percentage points.", "conclusion": "CARE-PD demonstrates the value of clinically curated, diverse training data for Parkinson's Disease assessment, with the dataset and benchmark code released for non-commercial research."}}
{"id": "2510.04315", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04315", "abs": "https://arxiv.org/abs/2510.04315", "authors": ["Jiarui Ouyang", "Yihui Wang", "Yihang Gao", "Yingxue Xu", "Shu Yang", "Hao Chen"], "title": "GenAR: Next-Scale Autoregressive Generation for Spatial Gene Expression Prediction", "comment": null, "summary": "Spatial Transcriptomics (ST) offers spatially resolved gene expression but\nremains costly. Predicting expression directly from widely available\nHematoxylin and Eosin (H&E) stained images presents a cost-effective\nalternative. However, most computational approaches (i) predict each gene\nindependently, overlooking co-expression structure, and (ii) cast the task as\ncontinuous regression despite expression being discrete counts. This mismatch\ncan yield biologically implausible outputs and complicate downstream analyses.\nWe introduce GenAR, a multi-scale autoregressive framework that refines\npredictions from coarse to fine. GenAR clusters genes into hierarchical groups\nto expose cross-gene dependencies, models expression as codebook-free discrete\ntoken generation to directly predict raw counts, and conditions decoding on\nfused histological and spatial embeddings. From an information-theoretic\nperspective, the discrete formulation avoids log-induced biases and the\ncoarse-to-fine factorization aligns with a principled conditional\ndecomposition. Extensive experimental results on four Spatial Transcriptomics\ndatasets across different tissue types demonstrate that GenAR achieves\nstate-of-the-art performance, offering potential implications for precision\nmedicine and cost-effective molecular profiling. Code is publicly available at\nhttps://github.com/oyjr/genar.", "AI": {"tldr": "GenAR is a multi-scale autoregressive framework that predicts spatial gene expression from H&E stained images by modeling gene dependencies hierarchically and generating discrete count tokens directly, achieving state-of-the-art performance.", "motivation": "Spatial Transcriptomics is expensive, while H&E stained images are widely available. Current methods predict genes independently and use continuous regression despite expression being discrete counts, leading to biologically implausible results.", "method": "GenAR clusters genes into hierarchical groups to capture dependencies, models expression as discrete token generation for raw counts, and uses fused histological and spatial embeddings in a coarse-to-fine autoregressive framework.", "result": "Extensive experiments on four Spatial Transcriptomics datasets across different tissue types show GenAR achieves state-of-the-art performance.", "conclusion": "GenAR offers a cost-effective alternative for spatial gene expression prediction with potential applications in precision medicine and molecular profiling."}}
{"id": "2510.04365", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04365", "abs": "https://arxiv.org/abs/2510.04365", "authors": ["Yuhao Luo", "Yuang Zhang", "Kehua Chen", "Xinyu Zheng", "Shucheng Zhang", "Sikai Chen", "Yinhai Wang"], "title": "Diffusion^2: Dual Diffusion Model with Uncertainty-Aware Adaptive Noise for Momentary Trajectory Prediction", "comment": "13 pages, 7 figures, 3 tables", "summary": "Accurate pedestrian trajectory prediction is crucial for ensuring safety and\nefficiency in autonomous driving and human-robot interaction scenarios. Earlier\nstudies primarily utilized sufficient observational data to predict future\ntrajectories. However, in real-world scenarios, such as pedestrians suddenly\nemerging from blind spots, sufficient observational data is often unavailable\n(i.e. momentary trajectory), making accurate prediction challenging and\nincreasing the risk of traffic accidents. Therefore, advancing research on\npedestrian trajectory prediction under extreme scenarios is critical for\nenhancing traffic safety. In this work, we propose a novel framework termed\nDiffusion^2, tailored for momentary trajectory prediction. Diffusion^2 consists\nof two sequentially connected diffusion models: one for backward prediction,\nwhich generates unobserved historical trajectories, and the other for forward\nprediction, which forecasts future trajectories. Given that the generated\nunobserved historical trajectories may introduce additional noise, we propose a\ndual-head parameterization mechanism to estimate their aleatoric uncertainty\nand design a temporally adaptive noise module that dynamically modulates the\nnoise scale in the forward diffusion process. Empirically, Diffusion^2 sets a\nnew state-of-the-art in momentary trajectory prediction on ETH/UCY and Stanford\nDrone datasets.", "AI": {"tldr": "Diffusion^2 is a novel framework for pedestrian trajectory prediction in extreme scenarios where only momentary observation data is available, using two sequential diffusion models for backward historical trajectory generation and forward future prediction.", "motivation": "Real-world scenarios often lack sufficient observational data (e.g., pedestrians emerging from blind spots), making accurate trajectory prediction challenging and increasing traffic accident risks.", "method": "Two sequentially connected diffusion models: one for backward prediction of unobserved historical trajectories, and another for forward prediction of future trajectories, with dual-head parameterization for uncertainty estimation and temporally adaptive noise module.", "result": "Sets new state-of-the-art performance in momentary trajectory prediction on ETH/UCY and Stanford Drone datasets.", "conclusion": "The proposed Diffusion^2 framework effectively addresses the challenge of pedestrian trajectory prediction in extreme scenarios with limited observational data."}}
{"id": "2510.04390", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04390", "abs": "https://arxiv.org/abs/2510.04390", "authors": ["Xuehai He", "Shijie Zhou", "Thivyanth Venkateswaran", "Kaizhi Zheng", "Ziyu Wan", "Achuta Kadambi", "Xin Eric Wang"], "title": "MorphoSim: An Interactive, Controllable, and Editable Language-guided 4D World Simulator", "comment": null, "summary": "World models that support controllable\n  and editable spatiotemporal environments are valuable\n  for robotics, enabling scalable training data, repro ducible evaluation, and\nflexible task design. While\n  recent text-to-video models generate realistic dynam ics, they are\nconstrained to 2D views and offer limited\n  interaction. We introduce MorphoSim, a language guided framework that\ngenerates 4D scenes with\n  multi-view consistency and object-level controls. From\n  natural language instructions, MorphoSim produces\n  dynamic environments where objects can be directed,\n  recolored, or removed, and scenes can be observed\n  from arbitrary viewpoints. The framework integrates\n  trajectory-guided generation with feature field dis tillation, allowing edits\nto be applied interactively\n  without full re-generation. Experiments show that Mor phoSim maintains high\nscene fidelity while enabling\n  controllability and editability. The code is available\n  at https://github.com/eric-ai-lab/Morph4D.", "AI": {"tldr": "MorphoSim is a language-guided framework that generates 4D scenes with multi-view consistency and object-level controls for robotics applications.", "motivation": "World models supporting controllable and editable spatiotemporal environments are valuable for robotics, enabling scalable training data, reproducible evaluation, and flexible task design. Current text-to-video models are constrained to 2D views and offer limited interaction.", "method": "Integrates trajectory-guided generation with feature field distillation, allowing edits to be applied interactively without full re-generation. From natural language instructions, produces dynamic environments with object-level controls.", "result": "Experiments show that MorphoSim maintains high scene fidelity while enabling controllability and editability. Generates 4D scenes with multi-view consistency where objects can be directed, recolored, or removed, and scenes can be observed from arbitrary viewpoints.", "conclusion": "MorphoSim provides a framework for generating controllable and editable 4D environments that overcome limitations of existing 2D text-to-video models, offering valuable capabilities for robotics applications."}}
{"id": "2510.04401", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04401", "abs": "https://arxiv.org/abs/2510.04401", "authors": ["Xuyang Guo", "Zekai Huang", "Zhenmei Shi", "Zhao Song", "Jiahao Zhang"], "title": "Your Vision-Language Model Can't Even Count to 20: Exposing the Failures of VLMs in Compositional Counting", "comment": null, "summary": "Vision-Language Models (VLMs) have become a central focus of today's AI\ncommunity, owing to their impressive abilities gained from training on\nlarge-scale vision-language data from the Web. These models have demonstrated\nstrong performance across diverse tasks, including image understanding, video\nunderstanding, complex visual reasoning, and embodied AI. Despite these\nnoteworthy successes, a fundamental question remains: Can VLMs count objects\ncorrectly? In this paper, we introduce a simple yet effective benchmark,\nVLMCountBench, designed under a minimalist setting with only basic geometric\nshapes (e.g., triangles, circles) and their compositions, focusing exclusively\non counting tasks without interference from other factors. We adopt strict\nindependent variable control and systematically study the effects of simple\nproperties such as color, size, and prompt refinement in a controlled ablation.\nOur empirical results reveal that while VLMs can count reliably when only one\nshape type is present, they exhibit substantial failures when multiple shape\ntypes are combined (i.e., compositional counting). This highlights a\nfundamental empirical limitation of current VLMs and motivates important\ndirections for future research.", "AI": {"tldr": "VLMs struggle with compositional counting of multiple shape types despite performing well with single shapes, revealing a fundamental limitation in current vision-language models.", "motivation": "To investigate whether Vision-Language Models (VLMs) can count objects correctly, particularly in compositional scenarios with multiple shape types, which remains an unexplored fundamental capability.", "method": "Created VLMCountBench - a minimalist benchmark using basic geometric shapes (triangles, circles) in controlled settings with strict variable control for color, size, and prompt refinement, focusing exclusively on counting tasks.", "result": "VLMs count reliably with single shape types but show substantial failures in compositional counting when multiple shape types are combined, highlighting a fundamental empirical limitation.", "conclusion": "Current VLMs have significant limitations in compositional counting, motivating important future research directions to address this fundamental capability gap."}}
{"id": "2510.04410", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04410", "abs": "https://arxiv.org/abs/2510.04410", "authors": ["Venkata Bharath Reddy Reddem", "Akshay P Sarashetti", "Ranjith Merugu", "Amit Satish Unde"], "title": "CodeFormer++: Blind Face Restoration Using Deformable Registration and Deep Metric Learning", "comment": null, "summary": "Blind face restoration (BFR) has attracted increasing attention with the rise\nof generative methods. Most existing approaches integrate generative priors\ninto the restoration pro- cess, aiming to jointly address facial detail\ngeneration and identity preservation. However, these methods often suffer from\na trade-off between visual quality and identity fidelity, leading to either\nidentity distortion or suboptimal degradation removal. In this paper, we\npresent CodeFormer++, a novel framework that maximizes the utility of\ngenerative priors for high-quality face restoration while preserving identity.\nWe decompose BFR into three sub-tasks: (i) identity- preserving face\nrestoration, (ii) high-quality face generation, and (iii) dynamic fusion of\nidentity features with realistic texture details. Our method makes three key\ncontributions: (1) a learning-based deformable face registration module that\nsemantically aligns generated and restored faces; (2) a texture guided\nrestoration network to dynamically extract and transfer the texture of\ngenerated face to boost the quality of identity-preserving restored face; and\n(3) the integration of deep metric learning for BFR with the generation of\ninformative positive and hard negative samples to better fuse identity-\npreserving and generative features. Extensive experiments on real-world and\nsynthetic datasets demonstrate that, the pro- posed CodeFormer++ achieves\nsuperior performance in terms of both visual fidelity and identity consistency.", "AI": {"tldr": "CodeFormer++ is a blind face restoration framework that decomposes the task into three sub-tasks: identity-preserving restoration, high-quality generation, and dynamic fusion, achieving superior visual quality while maintaining identity fidelity.", "motivation": "Existing BFR methods struggle with the trade-off between visual quality and identity fidelity, often resulting in either identity distortion or suboptimal degradation removal.", "method": "Decomposes BFR into three sub-tasks with three key contributions: learning-based deformable face registration for semantic alignment, texture-guided restoration network for dynamic texture transfer, and deep metric learning integration with informative sample generation.", "result": "Extensive experiments on real-world and synthetic datasets demonstrate superior performance in both visual fidelity and identity consistency compared to existing methods.", "conclusion": "CodeFormer++ effectively maximizes the utility of generative priors for high-quality face restoration while preserving identity, overcoming the traditional trade-off between visual quality and identity fidelity."}}
{"id": "2510.04428", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04428", "abs": "https://arxiv.org/abs/2510.04428", "authors": ["Yuanhao Zou", "Shengji Jin", "Andong Deng", "Youpeng Zhao", "Jun Wang", "Chen Chen"], "title": "A.I.R.: Enabling Adaptive, Iterative, and Reasoning-based Frame Selection For Video Question Answering", "comment": null, "summary": "Effectively applying Vision-Language Models (VLMs) to Video Question\nAnswering (VideoQA) hinges on selecting a concise yet comprehensive set of\nframes, as processing entire videos is computationally infeasible. However,\ncurrent frame selection methods face a critical trade-off: approaches relying\non lightweight similarity models, such as CLIP, often fail to capture the\nnuances of complex queries, resulting in inaccurate similarity scores that\ncannot reflect the authentic query-frame relevance, which further undermines\nframe selection. Meanwhile, methods that leverage a VLM for deeper analysis\nachieve higher accuracy but incur prohibitive computational costs. To address\nthese limitations, we propose A.I.R., a training-free approach for Adaptive,\nIterative, and Reasoning-based frame selection. We leverage a powerful VLM to\nperform deep, semantic analysis on complex queries, and this analysis is\ndeployed within a cost-effective iterative loop that processes only a small\nbatch of the most high-potential frames at a time. Extensive experiments on\nvarious VideoQA benchmarks demonstrate that our approach outperforms existing\nframe selection methods, significantly boosts the performance of the foundation\nVLM, and achieves substantial gains in computational efficiency over other\nVLM-based techniques.", "AI": {"tldr": "A.I.R. is a training-free frame selection method for VideoQA that uses adaptive iterative reasoning with a powerful VLM to select high-potential frames efficiently, outperforming existing methods in accuracy and computational efficiency.", "motivation": "Current frame selection methods for VideoQA face a trade-off: lightweight similarity models (like CLIP) fail to capture complex query nuances, while VLM-based methods are computationally prohibitive.", "method": "Proposes A.I.R. - Adaptive, Iterative, and Reasoning-based frame selection that uses a powerful VLM for deep semantic analysis in a cost-effective iterative loop processing small batches of high-potential frames.", "result": "Outperforms existing frame selection methods, significantly boosts foundation VLM performance, and achieves substantial computational efficiency gains over other VLM-based techniques across various VideoQA benchmarks.", "conclusion": "A.I.R. effectively addresses the accuracy-efficiency trade-off in VideoQA frame selection by combining deep semantic analysis with adaptive iterative processing."}}
{"id": "2510.04450", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04450", "abs": "https://arxiv.org/abs/2510.04450", "authors": ["Qiyuan He", "Yicong Li", "Haotian Ye", "Jinghao Wang", "Xinyao Liao", "Pheng-Ann Heng", "Stefano Ermon", "James Zou", "Angela Yao"], "title": "REAR: Rethinking Visual Autoregressive Models via Generator-Tokenizer Consistency Regularization", "comment": "27 pages, 23 figures, 5 tables", "summary": "Visual autoregressive (AR) generation offers a promising path toward unifying\nvision and language models, yet its performance remains suboptimal against\ndiffusion models. Prior work often attributes this gap to tokenizer limitations\nand rasterization ordering. In this work, we identify a core bottleneck from\nthe perspective of generator-tokenizer inconsistency, i.e., the AR-generated\ntokens may not be well-decoded by the tokenizer. To address this, we propose\nreAR, a simple training strategy introducing a token-wise regularization\nobjective: when predicting the next token, the causal transformer is also\ntrained to recover the visual embedding of the current token and predict the\nembedding of the target token under a noisy context. It requires no changes to\nthe tokenizer, generation order, inference pipeline, or external models.\nDespite its simplicity, reAR substantially improves performance. On ImageNet,\nit reduces gFID from 3.02 to 1.86 and improves IS to 316.9 using a standard\nrasterization-based tokenizer. When applied to advanced tokenizers, it achieves\na gFID of 1.42 with only 177M parameters, matching the performance with larger\nstate-of-the-art diffusion models (675M).", "AI": {"tldr": "reAR is a training strategy that addresses generator-tokenizer inconsistency in visual autoregressive models by adding token-wise regularization, improving performance to match diffusion models without changing the tokenizer or inference pipeline.", "motivation": "Visual autoregressive generation underperforms compared to diffusion models due to generator-tokenizer inconsistency, where AR-generated tokens may not be well-decoded by the tokenizer.", "method": "Proposes reAR with token-wise regularization: when predicting next token, the transformer also recovers current token's visual embedding and predicts target token's embedding under noisy context.", "result": "Reduces gFID from 3.02 to 1.86 and improves IS to 316.9 on ImageNet with standard tokenizer. Achieves gFID of 1.42 with 177M parameters, matching larger diffusion models (675M).", "conclusion": "reAR effectively addresses generator-tokenizer inconsistency and substantially improves visual AR generation performance to match state-of-the-art diffusion models."}}
{"id": "2510.04472", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.04472", "abs": "https://arxiv.org/abs/2510.04472", "authors": ["Baber Jan", "Saeed Anwar", "Aiman H. El-Maleh", "Abdul Jabbar Siddiqui", "Abdul Bais"], "title": "SPEGNet: Synergistic Perception-Guided Network for Camouflaged Object Detection", "comment": null, "summary": "Camouflaged object detection segments objects with intrinsic similarity and\nedge disruption. Current detection methods rely on accumulated complex\ncomponents. Each approach adds components such as boundary modules, attention\nmechanisms, and multi-scale processors independently. This accumulation creates\na computational burden without proportional gains. To manage this complexity,\nthey process at reduced resolutions, eliminating fine details essential for\ncamouflage. We present SPEGNet, addressing fragmentation through a unified\ndesign. The architecture integrates multi-scale features via channel\ncalibration and spatial enhancement. Boundaries emerge directly from\ncontext-rich representations, maintaining semantic-spatial alignment.\nProgressive refinement implements scale-adaptive edge modulation with peak\ninfluence at intermediate resolutions. This design strikes a balance between\nboundary precision and regional consistency. SPEGNet achieves 0.887 $S_\\alpha$\non CAMO, 0.890 on COD10K, and 0.895 on NC4K, with real-time inference speed.\nOur approach excels across scales, from tiny, intricate objects to large,\npattern-similar ones, while handling occlusion and ambiguous boundaries. Code,\nmodel weights, and results are available on\n\\href{https://github.com/Baber-Jan/SPEGNet}{https://github.com/Baber-Jan/SPEGNet}.", "AI": {"tldr": "SPEGNet is a unified architecture for camouflaged object detection that integrates multi-scale features through channel calibration and spatial enhancement, achieving state-of-the-art performance with real-time inference speed.", "motivation": "Current camouflaged object detection methods accumulate complex components (boundary modules, attention mechanisms, multi-scale processors) independently, creating computational burden without proportional gains. This forces processing at reduced resolutions, eliminating fine details essential for camouflage detection.", "method": "SPEGNet uses a unified design integrating multi-scale features via channel calibration and spatial enhancement. Boundaries emerge directly from context-rich representations with semantic-spatial alignment. Progressive refinement implements scale-adaptive edge modulation with peak influence at intermediate resolutions.", "result": "SPEGNet achieves 0.887 S\u03b1 on CAMO, 0.890 on COD10K, and 0.895 on NC4K datasets with real-time inference speed. It excels across scales from tiny intricate objects to large pattern-similar ones while handling occlusion and ambiguous boundaries.", "conclusion": "The unified SPEGNet architecture strikes an optimal balance between boundary precision and regional consistency, outperforming complex accumulated component approaches while maintaining computational efficiency and real-time performance."}}
{"id": "2510.04477", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04477", "abs": "https://arxiv.org/abs/2510.04477", "authors": ["Soo Yong Kim", "Suin Cho", "Vincent-Daniel Yun", "Gyeongyeon Hwang"], "title": "MedCLM: Learning to Localize and Reason via a CoT-Curriculum in Medical Vision-Language Models", "comment": null, "summary": "Bridging clinical diagnostic reasoning with AI remains a central challenge in\nmedical imaging. We introduce MedCLM, an automated pipeline that converts\ndetection datasets into large-scale medical visual question answering (VQA)\ndata with Chain-of-Thought (CoT) reasoning by linking lesion boxes to organ\nsegmentation and structured rationales. These contextual signals enable medical\nvision-language models to generate question-answer pairs with step-by-step\nreasoning. To utilize this data effectively, we propose an Integrated\nCoT-Curriculum Strategy composed of an Easy stage with explicit lesion boxes\nfor visual grounding, a Medium stage that encourages implicit localization, and\na Hard stage for weakly supervised reasoning. Experimental results demonstrate\nthat MedCLM attains state-of-the-art performance on several medical VQA\nbenchmarks, providing a scalable framework for developing clinically aligned\nmedical vision-language models.", "AI": {"tldr": "MedCLM is an automated pipeline that converts detection datasets into medical VQA data with Chain-of-Thought reasoning, using an Integrated CoT-Curriculum Strategy to achieve state-of-the-art performance on medical VQA benchmarks.", "motivation": "Bridging clinical diagnostic reasoning with AI remains a central challenge in medical imaging, requiring better integration of reasoning capabilities into medical vision-language models.", "method": "Automated pipeline that converts detection datasets into large-scale medical VQA data with Chain-of-Thought reasoning by linking lesion boxes to organ segmentation and structured rationales, using a three-stage Integrated CoT-Curriculum Strategy (Easy, Medium, Hard stages).", "result": "MedCLM attains state-of-the-art performance on several medical VQA benchmarks, demonstrating superior reasoning capabilities.", "conclusion": "MedCLM provides a scalable framework for developing clinically aligned medical vision-language models with improved diagnostic reasoning capabilities."}}
{"id": "2510.04479", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04479", "abs": "https://arxiv.org/abs/2510.04479", "authors": ["Nonghai Zhang", "Zeyu Zhang", "Jiazi Wang", "Yang Zhao", "Hao Tang"], "title": "VaseVQA-3D: Benchmarking 3D VLMs on Ancient Greek Pottery", "comment": null, "summary": "Vision-Language Models (VLMs) have achieved significant progress in\nmultimodal understanding tasks, demonstrating strong capabilities particularly\nin general tasks such as image captioning and visual reasoning. However, when\ndealing with specialized cultural heritage domains like 3D vase artifacts,\nexisting models face severe data scarcity issues and insufficient domain\nknowledge limitations. Due to the lack of targeted training data, current VLMs\nstruggle to effectively handle such culturally significant specialized tasks.\nTo address these challenges, we propose the VaseVQA-3D dataset, which serves as\nthe first 3D visual question answering dataset for ancient Greek pottery\nanalysis, collecting 664 ancient Greek vase 3D models with corresponding\nquestion-answer data and establishing a complete data construction pipeline. We\nfurther develop the VaseVLM model, enhancing model performance in vase artifact\nanalysis through domain-adaptive training. Experimental results validate the\neffectiveness of our approach, where we improve by 12.8% on R@1 metrics and by\n6.6% on lexical similarity compared with previous state-of-the-art on the\nVaseVQA-3D dataset, significantly improving the recognition and understanding\nof 3D vase artifacts, providing new technical pathways for digital heritage\npreservation research.", "AI": {"tldr": "The paper introduces VaseVQA-3D, the first 3D visual question answering dataset for ancient Greek pottery analysis, and develops VaseVLM model to address data scarcity and domain knowledge limitations in cultural heritage applications of vision-language models.", "motivation": "Existing Vision-Language Models (VLMs) face severe data scarcity and insufficient domain knowledge when dealing with specialized cultural heritage domains like 3D vase artifacts, struggling to effectively handle culturally significant specialized tasks due to lack of targeted training data.", "method": "Proposed VaseVQA-3D dataset with 664 ancient Greek vase 3D models and corresponding question-answer data, establishing a complete data construction pipeline. Developed VaseVLM model through domain-adaptive training to enhance performance in vase artifact analysis.", "result": "Experimental results show improvements of 12.8% on R@1 metrics and 6.6% on lexical similarity compared with previous state-of-the-art on the VaseVQA-3D dataset, significantly improving recognition and understanding of 3D vase artifacts.", "conclusion": "The approach provides new technical pathways for digital heritage preservation research by effectively addressing data scarcity and domain adaptation challenges in cultural heritage applications of VLMs."}}
{"id": "2510.04483", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04483", "abs": "https://arxiv.org/abs/2510.04483", "authors": ["Hao Fang", "Zechao Zhan", "Weixin Feng", "Ziwei Huang", "XuBin Li", "Tiezheng Ge"], "title": "TBStar-Edit: From Image Editing Pattern Shifting to Consistency Enhancement", "comment": null, "summary": "Recent advances in image generation and editing technologies have enabled\nstate-of-the-art models to achieve impressive results in general domains.\nHowever, when applied to e-commerce scenarios, these general models often\nencounter consistency limitations. To address this challenge, we introduce\nTBStar-Edit, an new image editing model tailored for the e-commerce domain.\nThrough rigorous data engineering, model architecture design and training\nstrategy, TBStar-Edit achieves precise and high-fidelity image editing while\nmaintaining the integrity of product appearance and layout. Specifically, for\ndata engineering, we establish a comprehensive data construction pipeline,\nencompassing data collection, construction, filtering, and augmentation, to\nacquire high-quality, instruction-following, and strongly consistent editing\ndata to support model training. For model architecture design, we design a\nhierarchical model framework consisting of a base model, pattern shifting\nmodules, and consistency enhancement modules. For model training, we adopt a\ntwo-stage training strategy to enhance the consistency preservation: first\nstage for editing pattern shifting, and second stage for consistency\nenhancement. Each stage involves training different modules with separate\ndatasets. Finally, we conduct extensive evaluations of TBStar-Edit on a\nself-proposed e-commerce benchmark, and the results demonstrate that\nTBStar-Edit outperforms existing general-domain editing models in both\nobjective metrics (VIE Score) and subjective user preference.", "AI": {"tldr": "TBStar-Edit is a specialized image editing model for e-commerce that addresses consistency limitations in general models through hierarchical architecture and two-stage training, achieving superior performance in product appearance preservation.", "motivation": "General image generation and editing models struggle with maintaining consistency in e-commerce scenarios where product appearance and layout integrity are crucial.", "method": "Three-pronged approach: 1) Comprehensive data engineering pipeline for high-quality consistent editing data, 2) Hierarchical model framework with base model, pattern shifting modules, and consistency enhancement modules, 3) Two-stage training strategy (pattern shifting then consistency enhancement) with separate datasets.", "result": "TBStar-Edit outperforms existing general-domain editing models on e-commerce benchmark in both objective metrics (VIE Score) and subjective user preference.", "conclusion": "The specialized approach combining rigorous data engineering, hierarchical architecture design, and staged training strategy effectively addresses e-commerce image editing consistency challenges."}}
{"id": "2510.04504", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04504", "abs": "https://arxiv.org/abs/2510.04504", "authors": ["Zijing Hu", "Yunze Tong", "Fengda Zhang", "Junkun Yuan", "Jun Xiao", "Kun Kuang"], "title": "Asynchronous Denoising Diffusion Models for Aligning Text-to-Image Generation", "comment": "22 pages, 11 figures, 5 tables", "summary": "Diffusion models have achieved impressive results in generating high-quality\nimages. Yet, they often struggle to faithfully align the generated images with\nthe input prompts. This limitation arises from synchronous denoising, where all\npixels simultaneously evolve from random noise to clear images. As a result,\nduring generation, the prompt-related regions can only reference the unrelated\nregions at the same noise level, failing to obtain clear context and ultimately\nimpairing text-to-image alignment. To address this issue, we propose\nasynchronous diffusion models -- a novel framework that allocates distinct\ntimesteps to different pixels and reformulates the pixel-wise denoising\nprocess. By dynamically modulating the timestep schedules of individual pixels,\nprompt-related regions are denoised more gradually than unrelated regions,\nthereby allowing them to leverage clearer inter-pixel context. Consequently,\nthese prompt-related regions achieve better alignment in the final images.\nExtensive experiments demonstrate that our asynchronous diffusion models can\nsignificantly improve text-to-image alignment across diverse prompts. The code\nrepository for this work is available at https://github.com/hu-zijing/AsynDM.", "AI": {"tldr": "Asynchronous diffusion models improve text-to-image alignment by allocating different timesteps to different pixels, allowing prompt-related regions to reference clearer context during denoising.", "motivation": "Standard diffusion models struggle with text-to-image alignment due to synchronous denoising, where all pixels evolve simultaneously from noise, preventing prompt-related regions from obtaining clear context from other regions.", "method": "Proposed asynchronous diffusion framework that allocates distinct timesteps to different pixels and dynamically modulates pixel-wise denoising schedules, allowing prompt-related regions to denoise more gradually and leverage clearer inter-pixel context.", "result": "Extensive experiments show significant improvement in text-to-image alignment across diverse prompts compared to standard synchronous diffusion models.", "conclusion": "Asynchronous diffusion models effectively address the alignment limitations of synchronous denoising by enabling better context utilization during the generation process, leading to improved faithfulness to input prompts."}}
{"id": "2510.04533", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04533", "abs": "https://arxiv.org/abs/2510.04533", "authors": ["Hyunmin Cho", "Donghoon Ahn", "Susung Hong", "Jee Eun Kim", "Seungryong Kim", "Kyong Hwan Jin"], "title": "TAG:Tangential Amplifying Guidance for Hallucination-Resistant Diffusion Sampling", "comment": "16 pages, 9 figures, 5 tables", "summary": "Recent diffusion models achieve the state-of-the-art performance in image\ngeneration, but often suffer from semantic inconsistencies or hallucinations.\nWhile various inference-time guidance methods can enhance generation, they\noften operate indirectly by relying on external signals or architectural\nmodifications, which introduces additional computational overhead. In this\npaper, we propose Tangential Amplifying Guidance (TAG), a more efficient and\ndirect guidance method that operates solely on trajectory signals without\nmodifying the underlying diffusion model. TAG leverages an intermediate sample\nas a projection basis and amplifies the tangential components of the estimated\nscores with respect to this basis to correct the sampling trajectory. We\nformalize this guidance process by leveraging a first-order Taylor expansion,\nwhich demonstrates that amplifying the tangential component steers the state\ntoward higher-probability regions, thereby reducing inconsistencies and\nenhancing sample quality. TAG is a plug-and-play, architecture-agnostic module\nthat improves diffusion sampling fidelity with minimal computational addition,\noffering a new perspective on diffusion guidance.", "AI": {"tldr": "TAG is an efficient guidance method that amplifies tangential components of diffusion model scores to improve semantic consistency without modifying the underlying model.", "motivation": "Existing diffusion models suffer from semantic inconsistencies and hallucinations, while current guidance methods introduce computational overhead through external signals or architectural changes.", "method": "TAG uses an intermediate sample as projection basis and amplifies tangential components of estimated scores via first-order Taylor expansion to steer sampling trajectory toward higher-probability regions.", "result": "TAG improves diffusion sampling fidelity with minimal computational addition, operating as a plug-and-play, architecture-agnostic module.", "conclusion": "TAG offers a new perspective on diffusion guidance by directly manipulating trajectory signals rather than relying on external modifications, providing efficient semantic consistency enhancement."}}
{"id": "2510.04564", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04564", "abs": "https://arxiv.org/abs/2510.04564", "authors": ["Honglin Liu", "Chao Sun", "Peng Hu", "Yunfan Li", "Xi Peng"], "title": "Conditional Representation Learning for Customized Tasks", "comment": null, "summary": "Conventional representation learning methods learn a universal representation\nthat primarily captures dominant semantics, which may not always align with\ncustomized downstream tasks. For instance, in animal habitat analysis,\nresearchers prioritize scene-related features, whereas universal embeddings\nemphasize categorical semantics, leading to suboptimal results. As a solution,\nexisting approaches resort to supervised fine-tuning, which however incurs high\ncomputational and annotation costs. In this paper, we propose Conditional\nRepresentation Learning (CRL), aiming to extract representations tailored to\narbitrary user-specified criteria. Specifically, we reveal that the semantics\nof a space are determined by its basis, thereby enabling a set of descriptive\nwords to approximate the basis for a customized feature space. Building upon\nthis insight, given a user-specified criterion, CRL first employs a large\nlanguage model (LLM) to generate descriptive texts to construct the semantic\nbasis, then projects the image representation into this conditional feature\nspace leveraging a vision-language model (VLM). The conditional representation\nbetter captures semantics for the specific criterion, which could be utilized\nfor multiple customized tasks. Extensive experiments on classification and\nretrieval tasks demonstrate the superiority and generality of the proposed CRL.\nThe code is available at https://github.com/XLearning-SCU/2025-NeurIPS-CRL.", "AI": {"tldr": "CRL proposes conditional representation learning that extracts task-specific representations using LLM-generated descriptive texts to construct semantic basis, avoiding costly supervised fine-tuning.", "motivation": "Universal representations often misalign with customized downstream tasks, while supervised fine-tuning is computationally expensive and requires heavy annotation.", "method": "Use LLM to generate descriptive texts for user criteria to construct semantic basis, then project image representations into this conditional space using VLM.", "result": "CRL achieves superior performance on classification and retrieval tasks compared to universal representations, demonstrating better alignment with customized criteria.", "conclusion": "CRL provides an effective framework for learning conditional representations tailored to arbitrary user criteria without expensive fine-tuning."}}
{"id": "2510.04587", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04587", "abs": "https://arxiv.org/abs/2510.04587", "authors": ["Sheng Wang", "Ruiming Wu", "Charles Herndon", "Yihang Liu", "Shunsuke Koga", "Jeanne Shen", "Zhi Huang"], "title": "Pathology-CoT: Learning Visual Chain-of-Thought Agent from Expert Whole Slide Image Diagnosis Behavior", "comment": null, "summary": "Diagnosing a whole-slide image is an interactive, multi-stage process\ninvolving changes in magnification and movement between fields. Although recent\npathology foundation models are strong, practical agentic systems that decide\nwhat field to examine next, adjust magnification, and deliver explainable\ndiagnoses are still lacking. The blocker is data: scalable, clinically aligned\nsupervision of expert viewing behavior that is tacit and experience-based, not\nwritten in textbooks or online, and therefore absent from large language model\ntraining. We introduce the AI Session Recorder, which works with standard WSI\nviewers to unobtrusively record routine navigation and convert the viewer logs\ninto standardized behavioral commands (inspect or peek at discrete\nmagnifications) and bounding boxes. A lightweight human-in-the-loop review\nturns AI-drafted rationales into the Pathology-CoT dataset, a form of paired\n\"where to look\" and \"why it matters\" supervision produced at roughly six times\nlower labeling time. Using this behavioral data, we build Pathologist-o3, a\ntwo-stage agent that first proposes regions of interest and then performs\nbehavior-guided reasoning. On gastrointestinal lymph-node metastasis detection,\nit achieved 84.5% precision, 100.0% recall, and 75.4% accuracy, exceeding the\nstate-of-the-art OpenAI o3 model and generalizing across backbones. To our\nknowledge, this constitutes one of the first behavior-grounded agentic systems\nin pathology. Turning everyday viewer logs into scalable, expert-validated\nsupervision, our framework makes agentic pathology practical and establishes a\npath to human-aligned, upgradeable clinical AI.", "AI": {"tldr": "The paper introduces AI Session Recorder to capture pathologists' viewing behavior from WSI viewers, creating Pathology-CoT dataset for training Pathologist-o3 agent that achieves superior performance in metastasis detection.", "motivation": "Current pathology foundation models lack practical agentic systems that can decide what to examine next and deliver explainable diagnoses, due to missing supervision of expert viewing behavior that is tacit and experience-based.", "method": "AI Session Recorder captures routine navigation from standard WSI viewers, converts logs into behavioral commands (inspect/peek at magnifications) and bounding boxes, with human review creating Pathology-CoT dataset for training Pathologist-o3 agent with two-stage reasoning.", "result": "Pathologist-o3 achieved 84.5% precision, 100.0% recall, and 75.4% accuracy on gastrointestinal lymph-node metastasis detection, exceeding OpenAI o3 model and generalizing across backbones.", "conclusion": "The framework makes agentic pathology practical by converting everyday viewer logs into scalable expert supervision, establishing a path to human-aligned, upgradeable clinical AI."}}
{"id": "2510.04628", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04628", "abs": "https://arxiv.org/abs/2510.04628", "authors": ["Hao Liu", "Yunhao Gao", "Wei Li", "Mingyang Zhang", "Maoguo Gong", "Lorenzo Bruzzone"], "title": "A Spatial-Spectral-Frequency Interactive Network for Multimodal Remote Sensing Classification", "comment": null, "summary": "Deep learning-based methods have achieved significant success in remote\nsensing Earth observation data analysis. Numerous feature fusion techniques\naddress multimodal remote sensing image classification by integrating global\nand local features. However, these techniques often struggle to extract\nstructural and detail features from heterogeneous and redundant multimodal\nimages. With the goal of introducing frequency domain learning to model key and\nsparse detail features, this paper introduces the spatial-spectral-frequency\ninteraction network (S$^2$Fin), which integrates pairwise fusion modules across\nthe spatial, spectral, and frequency domains. Specifically, we propose a\nhigh-frequency sparse enhancement transformer that employs sparse\nspatial-spectral attention to optimize the parameters of the high-frequency\nfilter. Subsequently, a two-level spatial-frequency fusion strategy is\nintroduced, comprising an adaptive frequency channel module that fuses\nlow-frequency structures with enhanced high-frequency details, and a\nhigh-frequency resonance mask that emphasizes sharp edges via phase similarity.\nIn addition, a spatial-spectral attention fusion module further enhances\nfeature extraction at intermediate layers of the network. Experiments on four\nbenchmark multimodal datasets with limited labeled data demonstrate that\nS$^2$Fin performs superior classification, outperforming state-of-the-art\nmethods. The code is available at https://github.com/HaoLiu-XDU/SSFin.", "AI": {"tldr": "S\u00b2Fin is a spatial-spectral-frequency interaction network that integrates frequency domain learning with multimodal remote sensing image classification, using high-frequency sparse enhancement transformers and spatial-frequency fusion strategies to improve feature extraction from heterogeneous images.", "motivation": "Current feature fusion techniques struggle to extract structural and detail features from heterogeneous and redundant multimodal remote sensing images, motivating the introduction of frequency domain learning to model key and sparse detail features.", "method": "Proposes S\u00b2Fin with pairwise fusion modules across spatial, spectral, and frequency domains, including: high-frequency sparse enhancement transformer with sparse spatial-spectral attention, two-level spatial-frequency fusion strategy with adaptive frequency channel module and high-frequency resonance mask, and spatial-spectral attention fusion module.", "result": "Experiments on four benchmark multimodal datasets with limited labeled data demonstrate that S\u00b2Fin performs superior classification, outperforming state-of-the-art methods.", "conclusion": "The proposed S\u00b2Fin network effectively integrates frequency domain learning and achieves superior performance in multimodal remote sensing image classification by better extracting structural and detail features from heterogeneous data."}}
{"id": "2510.04630", "categories": ["cs.CV", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.04630", "abs": "https://arxiv.org/abs/2510.04630", "authors": ["Vrushank Ahire", "Aniruddh Muley", "Shivam Zample", "Siddharth Verma", "Pranav Menon", "Surbhi Madan", "Abhinav Dhall"], "title": "SFANet: Spatial-Frequency Attention Network for Deepfake Detection", "comment": null, "summary": "Detecting manipulated media has now become a pressing issue with the recent\nrise of deepfakes. Most existing approaches fail to generalize across diverse\ndatasets and generation techniques. We thus propose a novel ensemble framework,\ncombining the strengths of transformer-based architectures, such as Swin\nTransformers and ViTs, and texture-based methods, to achieve better detection\naccuracy and robustness. Our method introduces innovative data-splitting,\nsequential training, frequency splitting, patch-based attention, and face\nsegmentation techniques to handle dataset imbalances, enhance high-impact\nregions (e.g., eyes and mouth), and improve generalization. Our model achieves\nstate-of-the-art performance when tested on the DFWild-Cup dataset, a diverse\nsubset of eight deepfake datasets. The ensemble benefits from the\ncomplementarity of these approaches, with transformers excelling in global\nfeature extraction and texturebased methods providing interpretability. This\nwork demonstrates that hybrid models can effectively address the evolving\nchallenges of deepfake detection, offering a robust solution for real-world\napplications.", "AI": {"tldr": "Proposes a novel ensemble framework combining transformer-based architectures (Swin Transformers, ViTs) and texture-based methods for robust deepfake detection, achieving state-of-the-art performance on the DFWild-Cup dataset.", "motivation": "Address the pressing issue of manipulated media detection, as existing approaches fail to generalize across diverse datasets and generation techniques.", "method": "Ensemble framework with innovative techniques: data-splitting, sequential training, frequency splitting, patch-based attention, and face segmentation to handle dataset imbalances and enhance high-impact regions like eyes and mouth.", "result": "Achieves state-of-the-art performance on DFWild-Cup dataset (diverse subset of 8 deepfake datasets), with transformers excelling in global feature extraction and texture-based methods providing interpretability.", "conclusion": "Hybrid models can effectively address evolving deepfake detection challenges, offering robust solutions for real-world applications through complementary strengths of different approaches."}}
{"id": "2510.04645", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04645", "abs": "https://arxiv.org/abs/2510.04645", "authors": ["Hugo Resende", "Fabio A. Faria", "Eduardo B. Neto", "Isabela Borlido", "Victor Sundermann", "Silvio Jamil F. Guimar\u00e3es", "\u00c1lvaro L. Fazenda"], "title": "Do Superpixel Segmentation Methods Influence Deforestation Image Classification?", "comment": "15 pages, 3 figures, paper accepted to present at CIARP 2025", "summary": "Image segmentation is a crucial step in various visual applications,\nincluding environmental monitoring through remote sensing. In the context of\nthe ForestEyes project, which combines citizen science and machine learning to\ndetect deforestation in tropical forests, image segments are used for labeling\nby volunteers and subsequent model training. Traditionally, the Simple Linear\nIterative Clustering (SLIC) algorithm is adopted as the segmentation method.\nHowever, recent studies have indicated that other superpixel-based methods\noutperform SLIC in remote sensing image segmentation, and might suggest that\nthey are more suitable for the task of detecting deforested areas. In this\nsense, this study investigated the impact of the four best segmentation\nmethods, together with SLIC, on the training of classifiers for the target\napplication. Initially, the results showed little variation in performance\namong segmentation methods, even when selecting the top five classifiers using\nthe PyCaret AutoML library. However, by applying a classifier fusion approach\n(ensemble of classifiers), noticeable improvements in balanced accuracy were\nobserved, highlighting the importance of both the choice of segmentation method\nand the combination of machine learning-based models for deforestation\ndetection tasks.", "AI": {"tldr": "This study compares five image segmentation methods (including SLIC) for deforestation detection in tropical forests, finding that classifier ensembles show noticeable improvements in balanced accuracy when using different segmentation approaches.", "motivation": "The ForestEyes project combines citizen science and machine learning for deforestation detection, but traditional SLIC segmentation may be outperformed by newer methods in remote sensing applications.", "method": "Compared four best segmentation methods together with SLIC, used PyCaret AutoML library to select top classifiers, and applied classifier fusion (ensemble) approach.", "result": "Initial results showed little performance variation among segmentation methods, but classifier ensembles demonstrated noticeable improvements in balanced accuracy.", "conclusion": "Both segmentation method choice and combining machine learning models are important for deforestation detection tasks, with ensemble approaches showing particular promise."}}
{"id": "2510.04648", "categories": ["cs.CV", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.04648", "abs": "https://arxiv.org/abs/2510.04648", "authors": ["Buyuan Zhu", "Shiyu Hu", "Yiping Ma", "Yuanming Zhang", "Kang Hao Cheong"], "title": "EduPersona: Benchmarking Subjective Ability Boundaries of Virtual Student Agents", "comment": "Preprint, Under review", "summary": "As large language models are increasingly integrated into education, virtual\nstudent agents are becoming vital for classroom simulation and teacher\ntraining. Yet their classroom-oriented subjective abilities remain largely\nunassessed, limiting understanding of model boundaries and hindering\ntrustworthy deployment. We present EduPersona, a large-scale benchmark spanning\ntwo languages, three subjects, and ten persona types based on the Big Five\ntheory. The dataset contains 1,308 authentic classroom dialogue rounds,\ncorresponding to 12,814 teacher-student Q&A turns, and is further expanded\nthrough persona stylization into roughly 10 times larger scale (128k turns),\nproviding a solid foundation for evaluation. Building on this resource, we\ndecompose hard-to-quantify subjective performance into three progressive tasks:\nTASK1 basic coherence (whether behavior, emotion, expression, and voice align\nwith classroom context), TASK2 student realism, and TASK3 long-term persona\nconsistency, thereby establishing an evaluation framework grounded in\neducational theory and research value. We conduct systematic experiments on\nthree representative LLMs, comparing their original versions with ten\npersona-fine-tuned variants trained on EduPersona. Results show consistent and\nsignificant average improvements across all tasks: TASK1 +33.6%, TASK2 +30.6%,\nand TASK3 +14.9%. These improvements highlight the dataset's effectiveness and\nresearch value, while also revealing the heterogeneous difficulty of persona\nmodeling. In summary, EduPersona delivers the first classroom benchmark\ncentered on subjective abilities, establishes a decoupled and verifiable\nresearch paradigm, and we will open-source both the dataset and the framework\nto support the broader research community in advancing trustworthy and\nhuman-like AI for education.", "AI": {"tldr": "EduPersona is a large-scale benchmark for evaluating LLMs' classroom-oriented subjective abilities, featuring 1,308 authentic dialogues expanded to 128k turns through persona stylization. It decomposes subjective performance into three tasks and shows significant improvements (+14.9% to +33.6%) when models are fine-tuned on the dataset.", "motivation": "As LLMs are increasingly used in education for virtual student agents, there's a need to assess their classroom-oriented subjective abilities to understand model boundaries and enable trustworthy deployment in educational settings.", "method": "Created EduPersona benchmark with 1,308 authentic classroom dialogues (12,814 Q&A turns) expanded to ~128k turns through persona stylization based on Big Five theory. Decomposed subjective performance into three tasks: basic coherence, student realism, and long-term persona consistency. Evaluated three LLMs and their persona-fine-tuned variants.", "result": "Persona-fine-tuned models showed consistent significant improvements across all tasks: TASK1 basic coherence +33.6%, TASK2 student realism +30.6%, and TASK3 long-term persona consistency +14.9%. The dataset proved effective while revealing heterogeneous difficulty in persona modeling.", "conclusion": "EduPersona provides the first classroom benchmark focused on subjective abilities, establishes a decoupled research paradigm, and will be open-sourced to advance trustworthy and human-like AI for education."}}
{"id": "2510.04654", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04654", "abs": "https://arxiv.org/abs/2510.04654", "authors": ["Andy C\u01cetrun\u01ce", "Adrian Cosma", "Emilian R\u01cedoi"], "title": "MoME: Estimating Psychological Traits from Gait with Multi-Stage Mixture of Movement Experts", "comment": "4 Figures, 4 Tables", "summary": "Gait encodes rich biometric and behavioural information, yet leveraging the\nmanner of walking to infer psychological traits remains a challenging and\nunderexplored problem. We introduce a hierarchical Multi-Stage Mixture of\nMovement Experts (MoME) architecture for multi-task prediction of psychological\nattributes from gait sequences represented as 2D poses. MoME processes the\nwalking cycle in four stages of movement complexity, employing lightweight\nexpert models to extract spatio-temporal features and task-specific gating\nmodules to adaptively weight experts across traits and stages. Evaluated on the\nPsyMo benchmark covering 17 psychological traits, our method outperforms\nstate-of-the-art gait analysis models, achieving a 37.47% weighted F1 score at\nthe run level and 44.6% at the subject level. Our experiments show that\nintegrating auxiliary tasks such as identity recognition, gender prediction,\nand BMI estimation further improves psychological trait estimation. Our\nfindings demonstrate the viability of multi-task gait-based learning for\npsychological trait estimation and provide a foundation for future research on\nmovement-informed psychological inference.", "AI": {"tldr": "The paper introduces a hierarchical Multi-Stage Mixture of Movement Experts (MoME) architecture for predicting psychological traits from gait sequences, achieving state-of-the-art performance on the PsyMo benchmark.", "motivation": "Gait contains rich biometric and behavioral information, but using walking patterns to infer psychological traits remains challenging and underexplored.", "method": "MoME processes walking cycles in four stages of movement complexity using lightweight expert models to extract spatio-temporal features and task-specific gating modules to adaptively weight experts across traits and stages.", "result": "Outperforms state-of-the-art gait analysis models on PsyMo benchmark (17 psychological traits), achieving 37.47% weighted F1 score at run level and 44.6% at subject level. Integrating auxiliary tasks (identity, gender, BMI) further improves psychological trait estimation.", "conclusion": "Demonstrates viability of multi-task gait-based learning for psychological trait estimation and provides foundation for future research on movement-informed psychological inference."}}
{"id": "2510.04668", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04668", "abs": "https://arxiv.org/abs/2510.04668", "authors": ["Habin Lim", "Yeongseob Won", "Juwon Seo", "Gyeong-Moon Park"], "title": "ConceptSplit: Decoupled Multi-Concept Personalization of Diffusion Models via Token-wise Adaptation and Attention Disentanglement", "comment": "14 pages, 13 figures, to be published in ICCV 2025", "summary": "In recent years, multi-concept personalization for text-to-image (T2I)\ndiffusion models to represent several subjects in an image has gained much more\nattention. The main challenge of this task is \"concept mixing\", where multiple\nlearned concepts interfere or blend undesirably in the output image. To address\nthis issue, in this paper, we present ConceptSplit, a novel framework to split\nthe individual concepts through training and inference. Our framework comprises\ntwo key components. First, we introduce Token-wise Value Adaptation (ToVA), a\nmerging-free training method that focuses exclusively on adapting the value\nprojection in cross-attention. Based on our empirical analysis, we found that\nmodifying the key projection, a common approach in existing methods, can\ndisrupt the attention mechanism and lead to concept mixing. Second, we propose\nLatent Optimization for Disentangled Attention (LODA), which alleviates\nattention entanglement during inference by optimizing the input latent. Through\nextensive qualitative and quantitative experiments, we demonstrate that\nConceptSplit achieves robust multi-concept personalization, mitigating\nunintended concept interference. Code is available at\nhttps://github.com/KU-VGI/ConceptSplit", "AI": {"tldr": "ConceptSplit is a framework that addresses concept mixing in multi-concept personalization for text-to-image diffusion models through Token-wise Value Adaptation (ToVA) during training and Latent Optimization for Disentangled Attention (LODA) during inference.", "motivation": "The main challenge in multi-concept personalization is concept mixing, where multiple learned concepts interfere or blend undesirably in the output image, which existing methods fail to adequately address.", "method": "The framework has two components: 1) Token-wise Value Adaptation (ToVA) - a merging-free training method that adapts only the value projection in cross-attention, avoiding disruption of the attention mechanism; 2) Latent Optimization for Disentangled Attention (LODA) - optimizes the input latent during inference to alleviate attention entanglement.", "result": "Extensive qualitative and quantitative experiments demonstrate that ConceptSplit achieves robust multi-concept personalization and mitigates unintended concept interference.", "conclusion": "ConceptSplit effectively addresses concept mixing in multi-concept personalization through its novel training and inference components, providing a solution to the problem of concept interference in text-to-image diffusion models."}}
{"id": "2510.04705", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04705", "abs": "https://arxiv.org/abs/2510.04705", "authors": ["Quang-Khai Bui-Tran", "Minh-Toan Dinh", "Thanh-Huy Nguyen", "Ba-Thinh Lam", "Mai-Anh Vu", "Ulas Bagci"], "title": "Label-Efficient Cross-Modality Generalization for Liver Segmentation in Multi-Phase MRI", "comment": "11 pages, 3 figures", "summary": "Accurate liver segmentation in multi-phase MRI is vital for liver fibrosis\nassessment, yet labeled data is often scarce and unevenly distributed across\nimaging modalities and vendor systems. We propose a label-efficient\nsegmentation approach that promotes cross-modality generalization under\nreal-world conditions, where GED4 hepatobiliary-phase annotations are limited,\nnon-contrast sequences (T1WI, T2WI, DWI) are unlabeled, and spatial\nmisalignment and missing phases are common. Our method integrates a\nfoundation-scale 3D segmentation backbone adapted via fine-tuning, co-training\nwith cross pseudo supervision to leverage unlabeled volumes, and a standardized\npreprocessing pipeline. Without requiring spatial registration, the model\nlearns to generalize across MRI phases and vendors, demonstrating robust\nsegmentation performance in both labeled and unlabeled domains. Our results\nexhibit the effectiveness of our proposed label-efficient baseline for liver\nsegmentation in multi-phase, multi-vendor MRI and highlight the potential of\ncombining foundation model adaptation with co-training for real-world clinical\nimaging tasks.", "AI": {"tldr": "A label-efficient liver segmentation method for multi-phase MRI that uses foundation model adaptation and co-training to handle limited labeled data, unlabeled sequences, and vendor variations without spatial registration.", "motivation": "Liver segmentation in multi-phase MRI is crucial for fibrosis assessment but faces challenges with scarce labeled data, uneven distribution across modalities/vendors, spatial misalignment, and missing phases.", "method": "Integrates foundation-scale 3D segmentation backbone with fine-tuning, co-training with cross pseudo supervision to leverage unlabeled volumes, and standardized preprocessing pipeline without spatial registration.", "result": "The model demonstrates robust segmentation performance across both labeled and unlabeled domains, generalizing effectively across MRI phases and vendor systems.", "conclusion": "The approach shows effectiveness for label-efficient liver segmentation in multi-phase, multi-vendor MRI and highlights the potential of combining foundation model adaptation with co-training for clinical imaging tasks."}}
{"id": "2510.04706", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04706", "abs": "https://arxiv.org/abs/2510.04706", "authors": ["Foivos Paraperas Papantoniou", "Stefanos Zafeiriou"], "title": "ID-Consistent, Precise Expression Generation with Blendshape-Guided Diffusion", "comment": "ICCVW 2025, Code: https://github.com/foivospar/Arc2Face", "summary": "Human-centric generative models designed for AI-driven storytelling must\nbring together two core capabilities: identity consistency and precise control\nover human performance. While recent diffusion-based approaches have made\nsignificant progress in maintaining facial identity, achieving fine-grained\nexpression control without compromising identity remains challenging. In this\nwork, we present a diffusion-based framework that faithfully reimagines any\nsubject under any particular facial expression. Building on an ID-consistent\nface foundation model, we adopt a compositional design featuring an expression\ncross-attention module guided by FLAME blendshape parameters for explicit\ncontrol. Trained on a diverse mixture of image and video data rich in\nexpressive variation, our adapter generalizes beyond basic emotions to subtle\nmicro-expressions and expressive transitions, overlooked by prior works. In\naddition, a pluggable Reference Adapter enables expression editing in real\nimages by transferring the appearance from a reference frame during synthesis.\nExtensive quantitative and qualitative evaluations show that our model\noutperforms existing methods in tailored and identity-consistent expression\ngeneration. Code and models can be found at\nhttps://github.com/foivospar/Arc2Face.", "AI": {"tldr": "A diffusion-based framework for identity-consistent facial expression generation with fine-grained control using FLAME blendshape parameters and reference-based editing.", "motivation": "Human-centric generative models need both identity consistency and precise control over human performance, but current methods struggle with fine-grained expression control without compromising identity.", "method": "Built on ID-consistent face foundation model with compositional design featuring expression cross-attention module guided by FLAME blendshape parameters, trained on diverse image/video data with expressive variation, plus pluggable Reference Adapter for real image editing.", "result": "Model generalizes beyond basic emotions to subtle micro-expressions and expressive transitions, outperforming existing methods in tailored and identity-consistent expression generation.", "conclusion": "The framework successfully achieves faithful reimagining of subjects under any facial expression while maintaining identity consistency and enabling fine-grained control."}}
{"id": "2510.04712", "categories": ["cs.CV", "cs.HC", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.04712", "abs": "https://arxiv.org/abs/2510.04712", "authors": ["Luo Cheng", "Song Siyang", "Yan Siyuan", "Yu Zhen", "Ge Zongyuan"], "title": "ReactDiff: Fundamental Multiple Appropriate Facial Reaction Diffusion Model", "comment": "Accepted to ACM Multimedia", "summary": "The automatic generation of diverse and human-like facial reactions in dyadic\ndialogue remains a critical challenge for human-computer interaction systems.\nExisting methods fail to model the stochasticity and dynamics inherent in real\nhuman reactions. To address this, we propose ReactDiff, a novel temporal\ndiffusion framework for generating diverse facial reactions that are\nappropriate for responding to any given dialogue context. Our key insight is\nthat plausible human reactions demonstrate smoothness, and coherence over time,\nand conform to constraints imposed by human facial anatomy. To achieve this,\nReactDiff incorporates two vital priors (spatio-temporal facial kinematics)\ninto the diffusion process: i) temporal facial behavioral kinematics and ii)\nfacial action unit dependencies. These two constraints guide the model toward\nrealistic human reaction manifolds, avoiding visually unrealistic jitters,\nunstable transitions, unnatural expressions, and other artifacts. Extensive\nexperiments on the REACT2024 dataset demonstrate that our approach not only\nachieves state-of-the-art reaction quality but also excels in diversity and\nreaction appropriateness.", "AI": {"tldr": "ReactDiff is a temporal diffusion framework that generates diverse and realistic facial reactions in dialogue by incorporating spatio-temporal facial kinematics and action unit dependencies.", "motivation": "Existing methods fail to model the stochasticity and dynamics of real human facial reactions, leading to unrealistic and limited reaction generation.", "method": "Proposes ReactDiff framework that incorporates two priors: temporal facial behavioral kinematics and facial action unit dependencies to guide the diffusion process toward realistic human reaction manifolds.", "result": "Extensive experiments on REACT2024 dataset show state-of-the-art reaction quality, diversity, and appropriateness compared to existing methods.", "conclusion": "ReactDiff successfully addresses the challenge of generating diverse and human-like facial reactions by modeling facial dynamics and anatomical constraints through temporal diffusion."}}
{"id": "2510.04714", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04714", "abs": "https://arxiv.org/abs/2510.04714", "authors": ["KunHo Heo", "GiHyun Kim", "SuYeon Kim", "MyeongAh Cho"], "title": "Object-Centric Representation Learning for Enhanced 3D Scene Graph Prediction", "comment": "Accepted by NeurIPS 2025. Code:\n  https://github.com/VisualScienceLab-KHU/OCRL-3DSSG-Codes", "summary": "3D Semantic Scene Graph Prediction aims to detect objects and their semantic\nrelationships in 3D scenes, and has emerged as a crucial technology for\nrobotics and AR/VR applications. While previous research has addressed dataset\nlimitations and explored various approaches including Open-Vocabulary settings,\nthey frequently fail to optimize the representational capacity of object and\nrelationship features, showing excessive reliance on Graph Neural Networks\ndespite insufficient discriminative capability. In this work, we demonstrate\nthrough extensive analysis that the quality of object features plays a critical\nrole in determining overall scene graph accuracy. To address this challenge, we\ndesign a highly discriminative object feature encoder and employ a contrastive\npretraining strategy that decouples object representation learning from the\nscene graph prediction. This design not only enhances object classification\naccuracy but also yields direct improvements in relationship prediction.\nNotably, when plugging in our pretrained encoder into existing frameworks, we\nobserve substantial performance improvements across all evaluation metrics.\nAdditionally, whereas existing approaches have not fully exploited the\nintegration of relationship information, we effectively combine both geometric\nand semantic features to achieve superior relationship prediction.\nComprehensive experiments on the 3DSSG dataset demonstrate that our approach\nsignificantly outperforms previous state-of-the-art methods. Our code is\npublicly available at https://github.com/VisualScienceLab-KHU/OCRL-3DSSG-Codes.", "AI": {"tldr": "This paper proposes a novel approach for 3D Semantic Scene Graph prediction that focuses on improving object feature quality through a discriminative encoder and contrastive pretraining, achieving state-of-the-art performance.", "motivation": "Previous methods for 3D Semantic Scene Graph prediction rely excessively on Graph Neural Networks while having insufficient discriminative capability in object and relationship features, limiting overall performance.", "method": "The authors design a highly discriminative object feature encoder with contrastive pretraining that decouples object representation learning from scene graph prediction, and effectively combines geometric and semantic features for relationship prediction.", "result": "The proposed approach significantly outperforms previous state-of-the-art methods on the 3DSSG dataset, with substantial performance improvements across all evaluation metrics when integrated into existing frameworks.", "conclusion": "Object feature quality is critical for 3D scene graph accuracy, and the proposed contrastive pretraining strategy with discriminative feature encoding effectively addresses previous limitations, achieving superior performance in both object classification and relationship prediction."}}
{"id": "2510.04723", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04723", "abs": "https://arxiv.org/abs/2510.04723", "authors": ["Niccol\u00f2 Niccoli", "Lorenzo Seidenari", "Ilaria Greco", "Francesco Rovero"], "title": "Benchmark on Monocular Metric Depth Estimation in Wildlife Setting", "comment": null, "summary": "Camera traps are widely used for wildlife monitoring, but extracting accurate\ndistance measurements from monocular images remains challenging due to the lack\nof depth information. While monocular depth estimation (MDE) methods have\nadvanced significantly, their performance in natural wildlife environments has\nnot been systematically evaluated. This work introduces the first benchmark for\nmonocular metric depth estimation in wildlife monitoring conditions. We\nevaluate four state-of-the-art MDE methods (Depth Anything V2, ML Depth Pro,\nZoeDepth, and Metric3D) alongside a geometric baseline on 93 camera trap images\nwith ground truth distances obtained using calibrated ChARUCO patterns. Our\nresults demonstrate that Depth Anything V2 achieves the best overall\nperformance with a mean absolute error of 0.454m and correlation of 0.962,\nwhile methods like ZoeDepth show significant degradation in outdoor natural\nenvironments (MAE: 3.087m). We find that median-based depth extraction\nconsistently outperforms mean-based approaches across all deep learning\nmethods. Additionally, we analyze computational efficiency, with ZoeDepth being\nfastest (0.17s per image) but least accurate, while Depth Anything V2 provides\nan optimal balance of accuracy and speed (0.22s per image). This benchmark\nestablishes performance baselines for wildlife applications and provides\npractical guidance for implementing depth estimation in conservation monitoring\nsystems.", "AI": {"tldr": "First benchmark for monocular metric depth estimation in wildlife monitoring, evaluating 4 state-of-the-art methods. Depth Anything V2 performs best (MAE: 0.454m), while ZoeDepth shows significant degradation outdoors.", "motivation": "Camera traps lack depth information, and while monocular depth estimation methods have advanced, their performance in natural wildlife environments hasn't been systematically evaluated.", "method": "Evaluated 4 MDE methods (Depth Anything V2, ML Depth Pro, ZoeDepth, Metric3D) and geometric baseline on 93 camera trap images with ground truth distances using calibrated ChARUCO patterns.", "result": "Depth Anything V2 achieved best performance (MAE: 0.454m, correlation: 0.962). Median-based depth extraction consistently outperformed mean-based approaches. ZoeDepth was fastest (0.17s/image) but least accurate (MAE: 3.087m).", "conclusion": "Establishes performance baselines for wildlife applications and provides practical guidance for implementing depth estimation in conservation monitoring systems, with Depth Anything V2 offering optimal balance of accuracy and speed."}}
{"id": "2510.04739", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.04739", "abs": "https://arxiv.org/abs/2510.04739", "authors": ["Mehdi Houshmand Sarkhoosh", "Fr\u00f8y \u00d8ye", "Henrik Nestor S\u00f8rlie", "Nam Hoang Vu", "Dag Johansen", "Cise Midoglu", "Tomas Kupka", "P\u00e5l Halvorsen"], "title": "ExposureEngine: Oriented Logo Detection and Sponsor Visibility Analytics in Sports Broadcasts", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Quantifying sponsor visibility in sports broadcasts is a critical marketing\ntask traditionally hindered by manual, subjective, and unscalable analysis\nmethods. While automated systems offer an alternative, their reliance on\naxis-aligned Horizontal Bounding Box (HBB) leads to inaccurate exposuremetrics\nwhen logos appear rotated or skewed due to dynamic camera angles and\nperspective distortions. This paper introduces ExposureEngine, an end-to-end\nsystem designed for accurate, rotation-aware sponsor visibility analytics in\nsports broadcasts, demonstrated in a soccer case study. Our approach predicts\nOriented Bounding Box (OBB) to provide a geometrically precise fit to each logo\nregardless of the orientation on-screen. To train and evaluate our detector, we\ndeveloped a new dataset comprising 1,103 frames from Swedish elite soccer,\nfeaturing 670 unique sponsor logos annotated with OBBs. Our model achieves a\nmean Average Precision (mAP@0.5) of 0.859, with a precision of 0.96 and recall\nof 0.87, demonstrating robust performance in localizing logos under diverse\nbroadcast conditions. The system integrates these detections into an analytical\npipeline that calculates precise visibility metrics, such as exposure duration\nand on-screen coverage. Furthermore, we incorporate a language-driven agentic\nlayer, enabling users to generate reports, summaries, and media content through\nnatural language queries. The complete system, including the dataset and the\nanalytics dashboard, provides a comprehensive solution for auditable and\ninterpretable sponsor measurement in sports media. An overview of the\nExposureEngine is available online: https://youtu.be/tRw6OBISuW4 .", "AI": {"tldr": "ExposureEngine is an automated system that uses oriented bounding boxes (OBB) instead of traditional horizontal bounding boxes to accurately quantify sponsor logo visibility in sports broadcasts, addressing the limitations of manual and existing automated methods.", "motivation": "Traditional methods for measuring sponsor visibility in sports broadcasts are manual, subjective, and unscalable. Existing automated systems using horizontal bounding boxes (HBB) are inaccurate when logos appear rotated or skewed due to camera angles and perspective distortions.", "method": "The system predicts Oriented Bounding Boxes (OBB) for precise logo detection regardless of orientation. It uses a dataset of 1,103 frames from Swedish elite soccer with 670 unique sponsor logos annotated with OBBs. The system integrates detections into an analytical pipeline and includes a language-driven agentic layer for natural language queries.", "result": "The model achieves mAP@0.5 of 0.859, precision of 0.96, and recall of 0.87, demonstrating robust performance in localizing logos under diverse broadcast conditions. The system calculates precise visibility metrics like exposure duration and on-screen coverage.", "conclusion": "ExposureEngine provides a comprehensive, auditable, and interpretable solution for sponsor measurement in sports media, overcoming the limitations of traditional methods through rotation-aware detection and advanced analytics capabilities."}}
{"id": "2510.04741", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04741", "abs": "https://arxiv.org/abs/2510.04741", "authors": ["Alina Ciocarlan", "Sylvie Le H\u00e9garat-Mascle", "Sidonie Lefebvre"], "title": "Anomaly-Aware YOLO: A Frugal yet Robust Approach to Infrared Small Target Detection", "comment": null, "summary": "Infrared Small Target Detection (IRSTD) is a challenging task in defense\napplications, where complex backgrounds and tiny target sizes often result in\nnumerous false alarms using conventional object detectors. To overcome this\nlimitation, we propose Anomaly-Aware YOLO (AA-YOLO), which integrates a\nstatistical anomaly detection test into its detection head. By treating small\ntargets as unexpected patterns against the background, AA-YOLO effectively\ncontrols the false alarm rate. Our approach not only achieves competitive\nperformance on several IRSTD benchmarks, but also demonstrates remarkable\nrobustness in scenarios with limited training data, noise, and domain shifts.\nFurthermore, since only the detection head is modified, our design is highly\ngeneric and has been successfully applied across various YOLO backbones,\nincluding lightweight models. It also provides promising results when\nintegrated into an instance segmentation YOLO. This versatility makes AA-YOLO\nan attractive solution for real-world deployments where resources are\nconstrained. The code will be publicly released.", "AI": {"tldr": "AA-YOLO integrates statistical anomaly detection into YOLO's detection head to improve infrared small target detection by treating targets as anomalies, achieving competitive performance with low false alarms and high robustness.", "motivation": "Conventional object detectors struggle with infrared small target detection due to complex backgrounds and tiny target sizes, leading to high false alarm rates in defense applications.", "method": "Proposes Anomaly-Aware YOLO (AA-YOLO) that integrates a statistical anomaly detection test into YOLO's detection head, treating small targets as unexpected patterns against background.", "result": "Achieves competitive performance on IRSTD benchmarks with remarkable robustness to limited training data, noise, and domain shifts. Successfully applied across various YOLO backbones including lightweight models and instance segmentation YOLO.", "conclusion": "AA-YOLO provides a generic, versatile solution for real-world IRSTD deployments with constrained resources, offering effective false alarm control and strong performance across different scenarios."}}
{"id": "2510.04753", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04753", "abs": "https://arxiv.org/abs/2510.04753", "authors": ["Masoumeh Chapariniya", "Teodora Vukovic", "Sarah Ebling", "Volker Dellwo"], "title": "Beyond Appearance: Transformer-based Person Identification from Conversational Dynamics", "comment": null, "summary": "This paper investigates the performance of transformer-based architectures\nfor person identification in natural, face-to-face conversation scenario. We\nimplement and evaluate a two-stream framework that separately models spatial\nconfigurations and temporal motion patterns of 133 COCO WholeBody keypoints,\nextracted from a subset of the CANDOR conversational corpus. Our experiments\ncompare pre-trained and from-scratch training, investigate the use of velocity\nfeatures, and introduce a multi-scale temporal transformer for hierarchical\nmotion modeling. Results demonstrate that domain-specific training\nsignificantly outperforms transfer learning, and that spatial configurations\ncarry more discriminative information than temporal dynamics. The spatial\ntransformer achieves 95.74% accuracy, while the multi-scale temporal\ntransformer achieves 93.90%. Feature-level fusion pushes performance to 98.03%,\nconfirming that postural and dynamic information are complementary. These\nfindings highlight the potential of transformer architectures for person\nidentification in natural interactions and provide insights for future\nmultimodal and cross-cultural studies.", "AI": {"tldr": "Transformer-based two-stream framework for person identification using spatial and temporal keypoint features achieves 98.03% accuracy through feature fusion.", "motivation": "To investigate transformer architectures for person identification in natural face-to-face conversation scenarios using body keypoints.", "method": "Two-stream framework with spatial transformer for 133 COCO WholeBody keypoint configurations and multi-scale temporal transformer for motion patterns, with experiments on pre-trained vs from-scratch training and velocity features.", "result": "Spatial transformer achieved 95.74% accuracy, temporal transformer 93.90%, and feature fusion reached 98.03%. Domain-specific training outperformed transfer learning, and spatial features were more discriminative than temporal dynamics.", "conclusion": "Transformers are effective for person identification in natural interactions, with spatial and temporal features being complementary, providing insights for future multimodal and cross-cultural studies."}}
{"id": "2510.04759", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04759", "abs": "https://arxiv.org/abs/2510.04759", "authors": ["Chi Yan", "Dan Xu"], "title": "Progressive Gaussian Transformer with Anisotropy-aware Sampling for Open Vocabulary Occupancy Prediction", "comment": "Project Page: https://yanchi-3dv.github.io/PG-Occ", "summary": "The 3D occupancy prediction task has witnessed remarkable progress in recent\nyears, playing a crucial role in vision-based autonomous driving systems. While\ntraditional methods are limited to fixed semantic categories, recent approaches\nhave moved towards predicting text-aligned features to enable open-vocabulary\ntext queries in real-world scenes. However, there exists a trade-off in\ntext-aligned scene modeling: sparse Gaussian representation struggles to\ncapture small objects in the scene, while dense representation incurs\nsignificant computational overhead. To address these limitations, we present\nPG-Occ, an innovative Progressive Gaussian Transformer Framework that enables\nopen-vocabulary 3D occupancy prediction. Our framework employs progressive\nonline densification, a feed-forward strategy that gradually enhances the 3D\nGaussian representation to capture fine-grained scene details. By iteratively\nenhancing the representation, the framework achieves increasingly precise and\ndetailed scene understanding. Another key contribution is the introduction of\nan anisotropy-aware sampling strategy with spatio-temporal fusion, which\nadaptively assigns receptive fields to Gaussians at different scales and\nstages, enabling more effective feature aggregation and richer scene\ninformation capture. Through extensive evaluations, we demonstrate that PG-Occ\nachieves state-of-the-art performance with a relative 14.3% mIoU improvement\nover the previous best performing method. Code and pretrained models will be\nreleased upon publication on our project page:\nhttps://yanchi-3dv.github.io/PG-Occ", "AI": {"tldr": "PG-Occ is a Progressive Gaussian Transformer Framework for open-vocabulary 3D occupancy prediction that uses progressive online densification and anisotropy-aware sampling to achieve state-of-the-art performance with 14.3% mIoU improvement.", "motivation": "To address the trade-off in text-aligned scene modeling where sparse Gaussian representation struggles with small objects while dense representation has high computational overhead.", "method": "Progressive Gaussian Transformer Framework with progressive online densification (gradually enhancing 3D Gaussian representation) and anisotropy-aware sampling with spatio-temporal fusion (adaptive receptive field assignment).", "result": "Achieves state-of-the-art performance with 14.3% mIoU improvement over previous best method.", "conclusion": "PG-Occ effectively balances computational efficiency and fine-grained scene detail capture through progressive representation enhancement and adaptive sampling strategies."}}
{"id": "2510.04770", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04770", "abs": "https://arxiv.org/abs/2510.04770", "authors": ["Xiaomeng Fan", "Yuchuan Mao", "Zhi Gao", "Yuwei Wu", "Jin Chen", "Yunde Jia"], "title": "Beyond the Seen: Bounded Distribution Estimation for Open-Vocabulary Learning", "comment": null, "summary": "Open-vocabulary learning requires modeling the data distribution in open\nenvironments, which consists of both seen-class and unseen-class data.\n  Existing methods estimate the distribution in open environments using\nseen-class data, where the absence of unseen classes makes the estimation error\ninherently unidentifiable.\n  Intuitively, learning beyond the seen classes is crucial for distribution\nestimation to bound the estimation error.\n  We theoretically demonstrate that the distribution can be effectively\nestimated by generating unseen-class data, through which the estimation error\nis upper-bounded.\n  Building on this theoretical insight, we propose a novel open-vocabulary\nlearning method, which generates unseen-class data for estimating the\ndistribution in open environments. The method consists of a class-domain-wise\ndata generation pipeline and a distribution alignment algorithm. The data\ngeneration pipeline generates unseen-class data under the guidance of a\nhierarchical semantic tree and domain information inferred from the seen-class\ndata, facilitating accurate distribution estimation. With the generated data,\nthe distribution alignment algorithm estimates and maximizes the posterior\nprobability to enhance generalization in open-vocabulary learning. Extensive\nexperiments on $11$ datasets demonstrate that our method outperforms baseline\napproaches by up to $14\\%$, highlighting its effectiveness and superiority.", "AI": {"tldr": "This paper proposes a novel open-vocabulary learning method that generates unseen-class data to estimate data distributions in open environments, achieving up to 14% improvement over baselines on 11 datasets.", "motivation": "Existing methods for open-vocabulary learning only use seen-class data to estimate distributions, but the absence of unseen classes makes the estimation error inherently unidentifiable. Learning beyond seen classes is crucial for bounding this estimation error.", "method": "The method consists of a class-domain-wise data generation pipeline that generates unseen-class data using a hierarchical semantic tree and domain information from seen-class data, and a distribution alignment algorithm that estimates and maximizes posterior probability for better generalization.", "result": "Extensive experiments on 11 datasets demonstrate that the proposed method outperforms baseline approaches by up to 14%, showing significant effectiveness and superiority in open-vocabulary learning.", "conclusion": "Generating unseen-class data is theoretically proven to effectively estimate distributions in open environments by bounding estimation errors, and the proposed method successfully implements this approach to achieve superior performance in open-vocabulary learning tasks."}}
{"id": "2510.04772", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04772", "abs": "https://arxiv.org/abs/2510.04772", "authors": ["Max Kirchner", "Hanna Hoffmann", "Alexander C. Jenke", "Oliver L. Saldanha", "Kevin Pfeiffer", "Weam Kanjo", "Julia Alekseenko", "Claas de Boer", "Santhi Raj Kolamuri", "Lorenzo Mazza", "Nicolas Padoy", "Sophia Bano", "Annika Reinke", "Lena Maier-Hein", "Danail Stoyanov", "Jakob N. Kather", "Fiona R. Kolbinger", "Sebastian Bodenstedt", "Stefanie Speidel"], "title": "Federated Learning for Surgical Vision in Appendicitis Classification: Results of the FedSurg EndoVis 2024 Challenge", "comment": "A challenge report pre-print (31 pages), including 7 tables and 8\n  figures", "summary": "Purpose: The FedSurg challenge was designed to benchmark the state of the art\nin federated learning for surgical video classification. Its goal was to assess\nhow well current methods generalize to unseen clinical centers and adapt\nthrough local fine-tuning while enabling collaborative model development\nwithout sharing patient data. Methods: Participants developed strategies to\nclassify inflammation stages in appendicitis using a preliminary version of the\nmulti-center Appendix300 video dataset. The challenge evaluated two tasks:\ngeneralization to an unseen center and center-specific adaptation after\nfine-tuning. Submitted approaches included foundation models with linear\nprobing, metric learning with triplet loss, and various FL aggregation schemes\n(FedAvg, FedMedian, FedSAM). Performance was assessed using F1-score and\nExpected Cost, with ranking robustness evaluated via bootstrapping and\nstatistical testing. Results: In the generalization task, performance across\ncenters was limited. In the adaptation task, all teams improved after\nfine-tuning, though ranking stability was low. The ViViT-based submission\nachieved the strongest overall performance. The challenge highlighted\nlimitations in generalization, sensitivity to class imbalance, and difficulties\nin hyperparameter tuning in decentralized training, while spatiotemporal\nmodeling and context-aware preprocessing emerged as promising strategies.\nConclusion: The FedSurg Challenge establishes the first benchmark for\nevaluating FL strategies in surgical video classification. Findings highlight\nthe trade-off between local personalization and global robustness, and\nunderscore the importance of architecture choice, preprocessing, and loss\ndesign. This benchmarking offers a reference point for future development of\nimbalance-aware, adaptive, and robust FL methods in clinical surgical AI.", "AI": {"tldr": "The FedSurg challenge benchmarked federated learning for surgical video classification using the Appendix300 dataset, evaluating generalization to unseen centers and local adaptation through fine-tuning. Top approaches used ViViT models with various FL aggregation schemes, showing limited generalization but improved adaptation after fine-tuning.", "motivation": "To assess how well current federated learning methods generalize to unseen clinical centers and adapt through local fine-tuning while enabling collaborative model development without sharing patient data, establishing the first benchmark for FL in surgical video classification.", "method": "Participants developed strategies to classify inflammation stages in appendicitis using the multi-center Appendix300 video dataset. Approaches included foundation models with linear probing, metric learning with triplet loss, and FL aggregation schemes (FedAvg, FedMedian, FedSAM). Performance was assessed using F1-score and Expected Cost.", "result": "In the generalization task, performance across centers was limited. In the adaptation task, all teams improved after fine-tuning, though ranking stability was low. The ViViT-based submission achieved the strongest overall performance. The challenge highlighted limitations in generalization, sensitivity to class imbalance, and hyperparameter tuning difficulties.", "conclusion": "The FedSurg Challenge establishes the first FL benchmark for surgical video classification, highlighting the trade-off between local personalization and global robustness. Findings underscore the importance of architecture choice, preprocessing, and loss design, offering a reference for future development of imbalance-aware, adaptive FL methods in clinical surgical AI."}}
{"id": "2510.04781", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04781", "abs": "https://arxiv.org/abs/2510.04781", "authors": ["Javed Ahmad", "Federico Dassi\u00e8", "Selene Frascella", "Gabriele Marchello", "Ferdinando Cannella", "Arianna Traviglia"], "title": "Hands-Free Heritage: Automated 3D Scanning for Cultural Heritage Digitization", "comment": "9 pages", "summary": "High-fidelity 3D scanning is essential for preserving cultural heritage\nartefacts, supporting documentation, analysis, and long-term conservation.\nHowever, conventional methods typically require specialized expertise and\nmanual intervention to maintain optimal scanning conditions and coverage. We\npresent an automated two-robot scanning system that eliminates the need for\nhandheld or semi-automatic workflows by combining coordinated robotic\nmanipulation with high-resolution 3D scanning. Our system parameterizes the\nscanning space into distinct regions, enabling coordinated motion planning\nbetween a scanner-equipped robot and a tray-handling robot. Optimized\ntrajectory planning and waypoint distribution ensure comprehensive surface\ncoverage, minimize occlusions, and balance reconstruction accuracy with system\nefficiency. Experimental results show that our approach achieves significantly\nlower Chamfer Distance and higher F-score compared to baseline methods,\noffering superior geometric accuracy, improved digitization efficiency, and\nreduced reliance on expert operators.", "AI": {"tldr": "Automated two-robot system for high-fidelity 3D scanning of cultural heritage artefacts using coordinated robotic manipulation and optimized trajectory planning.", "motivation": "Conventional 3D scanning methods require specialized expertise and manual intervention, which limits efficiency and accessibility for cultural heritage preservation.", "method": "Two-robot system with coordinated motion planning: one robot equipped with scanner and another for tray-handling. Parameterizes scanning space into regions with optimized trajectory planning and waypoint distribution to ensure comprehensive coverage and minimize occlusions.", "result": "Achieves significantly lower Chamfer Distance and higher F-score compared to baseline methods, demonstrating superior geometric accuracy and improved digitization efficiency.", "conclusion": "The automated system eliminates need for handheld or semi-automatic workflows, reduces reliance on expert operators, and provides efficient, high-quality 3D scanning for cultural heritage preservation."}}
{"id": "2510.04794", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04794", "abs": "https://arxiv.org/abs/2510.04794", "authors": ["Alon Kaya", "Igal Bilik", "Inna Stainvas"], "title": "A Comparative Study of Vision Transformers and CNNs for Few-Shot Rigid Transformation and Fundamental Matrix Estimation", "comment": null, "summary": "Vision-transformers (ViTs) and large-scale convolution-neural-networks (CNNs)\nhave reshaped computer vision through pretrained feature representations that\nenable strong transfer learning for diverse tasks. However, their efficiency as\nbackbone architectures for geometric estimation tasks involving image\ndeformations in low-data regimes remains an open question. This work considers\ntwo such tasks: 1) estimating 2D rigid transformations between pairs of images\nand 2) predicting the fundamental matrix for stereo image pairs, an important\nproblem in various applications, such as autonomous mobility, robotics, and 3D\nscene reconstruction. Addressing this intriguing question, this work\nsystematically compares large-scale CNNs (ResNet, EfficientNet, CLIP-ResNet)\nwith ViT-based foundation models (CLIP-ViT variants and DINO) in various data\nsize settings, including few-shot scenarios. These pretrained models are\noptimized for classification or contrastive learning, encouraging them to focus\nmostly on high-level semantics. The considered tasks require balancing local\nand global features differently, challenging the straightforward adoption of\nthese models as the backbone. Empirical comparative analysis shows that,\nsimilar to training from scratch, ViTs outperform CNNs during refinement in\nlarge downstream-data scenarios. However, in small data scenarios, the\ninductive bias and smaller capacity of CNNs improve their performance, allowing\nthem to match that of a ViT. Moreover, ViTs exhibit stronger generalization in\ncross-domain evaluation where the data distribution changes. These results\nemphasize the importance of carefully selecting model architectures for\nrefinement, motivating future research towards hybrid architectures that\nbalance local and global representations.", "AI": {"tldr": "Vision-transformers outperform CNNs in large data scenarios for geometric estimation tasks, but CNNs match ViT performance in small data settings due to their inductive bias and smaller capacity. ViTs show better cross-domain generalization.", "motivation": "To compare the efficiency of ViTs and large-scale CNNs as backbone architectures for geometric estimation tasks (2D rigid transformations and fundamental matrix prediction) in various data regimes, particularly few-shot scenarios.", "method": "Systematic comparison of large-scale CNNs (ResNet, EfficientNet, CLIP-ResNet) with ViT-based foundation models (CLIP-ViT variants and DINO) across different data size settings, including few-shot scenarios and cross-domain evaluation.", "result": "ViTs outperform CNNs in large downstream-data scenarios during refinement, similar to training from scratch. In small data scenarios, CNNs match ViT performance due to their inductive bias and smaller capacity. ViTs exhibit stronger generalization in cross-domain evaluation.", "conclusion": "Careful selection of model architectures for refinement is crucial, motivating future research towards hybrid architectures that balance local and global representations for geometric estimation tasks."}}
{"id": "2510.04797", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04797", "abs": "https://arxiv.org/abs/2510.04797", "authors": ["Qi Li", "Shuwen Qiu", "Julien Han", "Xingzi Xu", "Mehmet Saygin Seyfioglu", "Kee Kiat Koo", "Karim Bouyarmane"], "title": "DiT-VTON: Diffusion Transformer Framework for Unified Multi-Category Virtual Try-On and Virtual Try-All with Integrated Image Editing", "comment": "Submitted to CVPR 2025 and Published at CVPR 2025 AI for Content\n  Creation workshop", "summary": "The rapid growth of e-commerce has intensified the demand for Virtual Try-On\n(VTO) technologies, enabling customers to realistically visualize products\noverlaid on their own images. Despite recent advances, existing VTO models face\nchallenges with fine-grained detail preservation, robustness to real-world\nimagery, efficient sampling, image editing capabilities, and generalization\nacross diverse product categories. In this paper, we present DiT-VTON, a novel\nVTO framework that leverages a Diffusion Transformer (DiT), renowned for its\nperformance on text-conditioned image generation, adapted here for the\nimage-conditioned VTO task. We systematically explore multiple DiT\nconfigurations, including in-context token concatenation, channel\nconcatenation, and ControlNet integration, to determine the best setup for VTO\nimage conditioning.\n  To enhance robustness, we train the model on an expanded dataset encompassing\nvaried backgrounds, unstructured references, and non-garment categories,\ndemonstrating the benefits of data scaling for VTO adaptability. DiT-VTON also\nredefines the VTO task beyond garment try-on, offering a versatile Virtual\nTry-All (VTA) solution capable of handling a wide range of product categories\nand supporting advanced image editing functionalities such as pose\npreservation, localized editing, texture transfer, and object-level\ncustomization. Experimental results show that our model surpasses\nstate-of-the-art methods on VITON-HD, achieving superior detail preservation\nand robustness without reliance on additional condition encoders. It also\noutperforms models with VTA and image editing capabilities on a diverse dataset\nspanning thousands of product categories.", "AI": {"tldr": "DiT-VTON is a Virtual Try-On framework using Diffusion Transformers that achieves state-of-the-art performance on VITON-HD and extends VTO to Virtual Try-All capabilities with advanced image editing features.", "motivation": "Existing VTO models struggle with fine-grained detail preservation, robustness to real-world imagery, efficient sampling, image editing capabilities, and generalization across diverse product categories.", "method": "Leverages Diffusion Transformer (DiT) adapted for image-conditioned VTO, exploring multiple configurations including in-context token concatenation, channel concatenation, and ControlNet integration. Trained on expanded dataset with varied backgrounds and non-garment categories.", "result": "Surpasses state-of-the-art methods on VITON-HD with superior detail preservation and robustness, outperforms models with VTA capabilities on diverse dataset spanning thousands of product categories.", "conclusion": "DiT-VTON redefines VTO beyond garment try-on to offer versatile Virtual Try-All solution with advanced image editing functionalities including pose preservation, localized editing, texture transfer, and object-level customization."}}
{"id": "2510.04802", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04802", "abs": "https://arxiv.org/abs/2510.04802", "authors": ["Han Zhang", "Lalithkumar Seenivasan", "Jose L. Porras", "Roger D. Soberanis-Mukul", "Hao Ding", "Hongchao Shu", "Benjamin D. Killeen", "Ankita Ghosh", "Lonny Yarmus", "Masaru Ishii", "Angela Christine Argento", "Mathias Unberath"], "title": "Did you just see that? Arbitrary view synthesis for egocentric replay of operating room workflows from ambient sensors", "comment": null, "summary": "Observing surgical practice has historically relied on fixed vantage points\nor recollections, leaving the egocentric visual perspectives that guide\nclinical decisions undocumented. Fixed-camera video can capture surgical\nworkflows at the room-scale, but cannot reconstruct what each team member\nactually saw. Thus, these videos only provide limited insights into how\ndecisions that affect surgical safety, training, and workflow optimization are\nmade. Here we introduce EgoSurg, the first framework to reconstruct the\ndynamic, egocentric replays for any operating room (OR) staff directly from\nwall-mounted fixed-camera video, and thus, without intervention to clinical\nworkflow. EgoSurg couples geometry-driven neural rendering with diffusion-based\nview enhancement, enabling high-visual fidelity synthesis of arbitrary and\negocentric viewpoints at any moment. In evaluation across multi-site surgical\ncases and controlled studies, EgoSurg reconstructs person-specific visual\nfields and arbitrary viewpoints with high visual quality and fidelity. By\ntransforming existing OR camera infrastructure into a navigable dynamic 3D\nrecord, EgoSurg establishes a new foundation for immersive surgical data\nscience, enabling surgical practice to be visualized, experienced, and analyzed\nfrom every angle.", "AI": {"tldr": "EgoSurg is a framework that reconstructs dynamic egocentric visual perspectives of operating room staff from fixed-camera video, enabling immersive surgical analysis without disrupting clinical workflow.", "motivation": "Traditional surgical observation methods rely on fixed viewpoints or recollections, missing the actual egocentric visual perspectives that guide clinical decisions. Fixed-camera videos provide limited insights into surgical safety, training, and workflow optimization decisions.", "method": "EgoSurg combines geometry-driven neural rendering with diffusion-based view enhancement to synthesize arbitrary egocentric viewpoints at any moment from wall-mounted fixed-camera video.", "result": "The framework successfully reconstructs person-specific visual fields and arbitrary viewpoints with high visual quality and fidelity across multi-site surgical cases and controlled studies.", "conclusion": "EgoSurg transforms existing OR camera infrastructure into navigable dynamic 3D records, establishing a foundation for immersive surgical data science that enables surgical practice to be visualized and analyzed from every angle."}}
{"id": "2510.04819", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04819", "abs": "https://arxiv.org/abs/2510.04819", "authors": ["Benlin Liu", "Amita Kamath", "Madeleine Grunde-McLaughlin", "Winson Han", "Ranjay Krishna"], "title": "Visual Representations inside the Language Model", "comment": "Accepted to COLM 2025", "summary": "Despite interpretability work analyzing VIT encoders and transformer\nactivations, we don't yet understand why Multimodal Language Models (MLMs)\nstruggle on perception-heavy tasks. We offer an under-studied perspective by\nexamining how popular MLMs (LLaVA-OneVision, Qwen2.5-VL, and\nLlama-3-LLaVA-NeXT) process their visual key-value tokens. We first study the\nflow of visual information through the language model, finding that image value\ntokens encode sufficient information to perform several perception-heavy tasks\nzero-shot: segmentation, semantic correspondence, temporal correspondence, and\nreferring expression detection. We find that while the language model does\naugment the visual information received from the projection of input visual\nencodings-which we reveal correlates with overall MLM perception capability-it\ncontains less visual information on several tasks than the equivalent visual\nencoder (SigLIP) that has not undergone MLM finetuning. Further, we find that\nthe visual information corresponding to input-agnostic image key tokens in\nlater layers of language models contains artifacts which reduce perception\ncapability of the overall MLM. Next, we discuss controlling visual information\nin the language model, showing that adding a text prefix to the image input\nimproves perception capabilities of visual representations. Finally, we reveal\nthat if language models were able to better control their visual information,\ntheir perception would significantly improve; e.g., in 33.3% of Art Style\nquestions in the BLINK benchmark, perception information present in the\nlanguage model is not surfaced to the output! Our findings reveal insights into\nthe role of key-value tokens in multimodal systems, paving the way for deeper\nmechanistic interpretability of MLMs and suggesting new directions for training\ntheir visual encoder and language model components.", "AI": {"tldr": "The paper analyzes why multimodal language models (MLMs) struggle with perception-heavy tasks by examining how they process visual key-value tokens. Key findings include: image value tokens contain sufficient visual information for perception tasks, language models augment visual information but contain less than standalone visual encoders, input-agnostic image key tokens introduce artifacts, and adding text prefixes improves perception capabilities.", "motivation": "Despite existing interpretability work on VIT encoders and transformer activations, there's limited understanding of why MLMs perform poorly on perception-heavy tasks. The authors aim to provide new insights by examining how popular MLMs process visual key-value tokens.", "method": "The study examines popular MLMs (LLaVA-OneVision, Qwen2.5-VL, Llama-3-LLaVA-NeXT) by analyzing the flow of visual information through language models. They investigate visual key-value tokens, compare information content between language models and standalone visual encoders, and test interventions like adding text prefixes to image inputs.", "result": "Image value tokens encode sufficient information for zero-shot perception tasks. Language models augment visual information but contain less visual information than equivalent visual encoders. Input-agnostic image key tokens in later layers introduce artifacts that reduce perception capability. Adding text prefixes improves perception capabilities, and language models fail to surface available perception information in 33.3% of Art Style questions.", "conclusion": "The findings provide insights into the role of key-value tokens in multimodal systems, suggesting that better control of visual information in language models could significantly improve perception capabilities. This work paves the way for deeper mechanistic interpretability of MLMs and new training directions for visual encoder and language model components."}}
{"id": "2510.04822", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04822", "abs": "https://arxiv.org/abs/2510.04822", "authors": ["Zicheng Jiang", "Jixin Gao", "Shengfeng He", "Xinzhe Li", "Yulong Zheng", "Zhaotong Yang", "Junyu Dong", "Yong Du"], "title": "AvatarVTON: 4D Virtual Try-On for Animatable Avatars", "comment": null, "summary": "We propose AvatarVTON, the first 4D virtual try-on framework that generates\nrealistic try-on results from a single in-shop garment image, enabling free\npose control, novel-view rendering, and diverse garment choices. Unlike\nexisting methods, AvatarVTON supports dynamic garment interactions under\nsingle-view supervision, without relying on multi-view garment captures or\nphysics priors. The framework consists of two key modules: (1) a Reciprocal\nFlow Rectifier, a prior-free optical-flow correction strategy that stabilizes\navatar fitting and ensures temporal coherence; and (2) a Non-Linear Deformer,\nwhich decomposes Gaussian maps into view-pose-invariant and view-pose-specific\ncomponents, enabling adaptive, non-linear garment deformations. To establish a\nbenchmark for 4D virtual try-on, we extend existing baselines with unified\nmodules for fair qualitative and quantitative comparisons. Extensive\nexperiments show that AvatarVTON achieves high fidelity, diversity, and dynamic\ngarment realism, making it well-suited for AR/VR, gaming, and digital-human\napplications.", "AI": {"tldr": "AvatarVTON is the first 4D virtual try-on framework that generates realistic try-on results from a single garment image, supporting free pose control, novel-view rendering, and dynamic garment interactions without multi-view captures or physics priors.", "motivation": "Existing virtual try-on methods lack support for dynamic 4D interactions and require multi-view garment captures or physics priors, limiting their practical applications in AR/VR and gaming.", "method": "Uses two key modules: (1) Reciprocal Flow Rectifier for optical-flow correction to ensure temporal coherence, and (2) Non-Linear Deformer that decomposes Gaussian maps into view-pose-invariant and view-pose-specific components for adaptive garment deformations.", "result": "Achieves high fidelity, diversity, and dynamic garment realism in 4D virtual try-on, outperforming existing baselines in both qualitative and quantitative comparisons.", "conclusion": "AvatarVTON enables practical 4D virtual try-on applications for AR/VR, gaming, and digital-human scenarios with single-view supervision and without requiring complex multi-view captures or physics priors."}}
{"id": "2510.04923", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04923", "abs": "https://arxiv.org/abs/2510.04923", "authors": ["Alec K. Peltekian", "Halil Ertugrul Aktas", "Gorkem Durak", "Kevin Grudzinski", "Bradford C. Bemiss", "Carrie Richardson", "Jane E. Dematte", "G. R. Scott Budinger", "Anthony J. Esposito", "Alexander Misharin", "Alok Choudhary", "Ankit Agrawal", "Ulas Bagci"], "title": "REN: Anatomically-Informed Mixture-of-Experts for Interstitial Lung Disease Diagnosis", "comment": "10 pages, 4 figures, 2 tables", "summary": "Mixture-of-Experts (MoE) architectures have significantly contributed to\nscalable machine learning by enabling specialized subnetworks to tackle complex\ntasks efficiently. However, traditional MoE systems lack domain-specific\nconstraints essential for medical imaging, where anatomical structure and\nregional disease heterogeneity strongly influence pathological patterns. Here,\nwe introduce Regional Expert Networks (REN), the first anatomically-informed\nMoE framework tailored specifically for medical image classification. REN\nleverages anatomical priors to train seven specialized experts, each dedicated\nto distinct lung lobes and bilateral lung combinations, enabling precise\nmodeling of region-specific pathological variations. Multi-modal gating\nmechanisms dynamically integrate radiomics biomarkers and deep learning (DL)\nfeatures (CNN, ViT, Mamba) to weight expert contributions optimally. Applied to\ninterstitial lung disease (ILD) classification, REN achieves consistently\nsuperior performance: the radiomics-guided ensemble reached an average AUC of\n0.8646 +/- 0.0467, a +12.5 percent improvement over the SwinUNETR baseline (AUC\n0.7685, p = 0.031). Region-specific experts further revealed that lower-lobe\nmodels achieved AUCs of 0.88-0.90, surpassing DL counterparts (CNN: 0.76-0.79)\nand aligning with known disease progression patterns. Through rigorous\npatient-level cross-validation, REN demonstrates strong generalizability and\nclinical interpretability, presenting a scalable, anatomically-guided approach\nreadily extensible to other structured medical imaging applications.", "AI": {"tldr": "REN is an anatomically-informed Mixture-of-Experts framework for medical image classification that uses anatomical priors to train specialized experts for different lung regions, achieving superior performance in ILD classification with improved interpretability.", "motivation": "Traditional MoE systems lack domain-specific constraints needed for medical imaging where anatomical structure and regional disease heterogeneity strongly influence pathological patterns.", "method": "Leverages anatomical priors to train seven specialized experts for distinct lung lobes and bilateral lung combinations, with multi-modal gating mechanisms that integrate radiomics biomarkers and deep learning features (CNN, ViT, Mamba) to weight expert contributions.", "result": "Achieved average AUC of 0.8646 (+12.5% improvement over baseline), with lower-lobe experts reaching AUCs of 0.88-0.90, outperforming DL counterparts and aligning with known disease progression patterns.", "conclusion": "REN demonstrates strong generalizability and clinical interpretability, presenting a scalable, anatomically-guided approach extensible to other structured medical imaging applications."}}
{"id": "2510.04823", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04823", "abs": "https://arxiv.org/abs/2510.04823", "authors": ["Arnela Hadzic", "Simon Johannes Joham", "Martin Urschler"], "title": "Flow Matching for Conditional MRI-CT and CBCT-CT Image Synthesis", "comment": null, "summary": "Generating synthetic CT (sCT) from MRI or CBCT plays a crucial role in\nenabling MRI-only and CBCT-based adaptive radiotherapy, improving treatment\nprecision while reducing patient radiation exposure. To address this task, we\nadopt a fully 3D Flow Matching (FM) framework, motivated by recent work\ndemonstrating FM's efficiency in producing high-quality images. In our\napproach, a Gaussian noise volume is transformed into an sCT image by\nintegrating a learned FM velocity field, conditioned on features extracted from\nthe input MRI or CBCT using a lightweight 3D encoder. We evaluated the method\non the SynthRAD2025 Challenge benchmark, training separate models for MRI\n$\\rightarrow$ sCT and CBCT $\\rightarrow$ sCT across three anatomical regions:\nabdomen, head and neck, and thorax. Validation and testing were performed\nthrough the challenge submission system. The results indicate that the method\naccurately reconstructs global anatomical structures; however, preservation of\nfine details was limited, primarily due to the relatively low training\nresolution imposed by memory and runtime constraints. Future work will explore\npatch-based training and latent-space flow models to improve resolution and\nlocal structural fidelity.", "AI": {"tldr": "A 3D Flow Matching framework generates synthetic CT from MRI or CBCT for radiotherapy, achieving accurate global anatomical reconstruction but limited fine detail preservation due to resolution constraints.", "motivation": "To enable MRI-only and CBCT-based adaptive radiotherapy by generating synthetic CT images, improving treatment precision while reducing patient radiation exposure.", "method": "A fully 3D Flow Matching framework that transforms Gaussian noise into synthetic CT images by integrating a learned velocity field, conditioned on features from input MRI/CBCT using a lightweight 3D encoder.", "result": "The method accurately reconstructs global anatomical structures but has limited preservation of fine details due to low training resolution constraints from memory and runtime limitations.", "conclusion": "Future work will explore patch-based training and latent-space flow models to improve resolution and local structural fidelity for better synthetic CT generation."}}
{"id": "2510.04939", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04939", "abs": "https://arxiv.org/abs/2510.04939", "authors": ["Yuxi Liu", "Catherine Lalman", "Yimin Yang"], "title": "Unsupervised Active Learning via Natural Feature Progressive Framework", "comment": "Under review at IEEE TPAMI", "summary": "The effectiveness of modern deep learning models is predicated on the\navailability of large-scale, human-annotated datasets, a process that is\nnotoriously expensive and time-consuming. While Active Learning (AL) offers a\nstrategic solution by labeling only the most informative and representative\ndata, its iterative nature still necessitates significant human involvement.\nUnsupervised Active Learning (UAL) presents an alternative by shifting the\nannotation burden to a single, post-selection step. Unfortunately, prevailing\nUAL methods struggle to achieve state-of-the-art performance. These approaches\ntypically rely on local, gradient-based scoring for sample importance\nestimation, which not only makes them vulnerable to ambiguous and noisy data\nbut also hinders their capacity to select samples that adequately represent the\nfull data distribution. Moreover, their use of shallow, one-shot linear\nselection falls short of a true UAL paradigm. In this paper, we propose the\nNatural Feature Progressive Framework (NFPF), a UAL method that revolutionizes\nhow sample importance is measured. At its core, NFPF employs a Specific Feature\nLearning Machine (SFLM) to effectively quantify each sample's contribution to\nmodel performance. We further utilize the SFLM to define a powerful\nReconstruction Difference metric for initial sample selection. Our\ncomprehensive experiments show that NFPF significantly outperforms all\nestablished UAL methods and achieves performance on par with supervised AL\nmethods on vision datasets. Detailed ablation studies and qualitative\nvisualizations provide compelling evidence for NFPF's superior performance,\nenhanced robustness, and improved data distribution coverage.", "AI": {"tldr": "The paper proposes NFPF, a novel Unsupervised Active Learning method that uses Specific Feature Learning Machine to measure sample importance and achieves performance comparable to supervised AL methods.", "motivation": "Traditional Active Learning requires significant human involvement in iterative labeling, while existing Unsupervised AL methods struggle with performance and robustness due to reliance on local gradient-based scoring.", "method": "NFPF employs a Specific Feature Learning Machine (SFLM) to quantify sample importance and uses a Reconstruction Difference metric for initial sample selection, providing better data distribution coverage.", "result": "NFPF significantly outperforms all established UAL methods and achieves performance comparable to supervised AL methods on vision datasets, with enhanced robustness and improved data distribution coverage.", "conclusion": "The proposed NFPF framework revolutionizes sample importance measurement in UAL and demonstrates superior performance, making it a viable alternative to supervised active learning approaches."}}
{"id": "2510.04838", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04838", "abs": "https://arxiv.org/abs/2510.04838", "authors": ["Muquan Li", "Hang Gou", "Dongyang Zhang", "Shuang Liang", "Xiurui Xie", "Deqiang Ouyang", "Ke Qin"], "title": "Beyond Random: Automatic Inner-loop Optimization in Dataset Distillation", "comment": null, "summary": "The growing demand for efficient deep learning has positioned dataset\ndistillation as a pivotal technique for compressing training dataset while\npreserving model performance. However, existing inner-loop optimization methods\nfor dataset distillation typically rely on random truncation strategies, which\nlack flexibility and often yield suboptimal results. In this work, we observe\nthat neural networks exhibit distinct learning dynamics across different\ntraining stages-early, middle, and late-making random truncation ineffective.\nTo address this limitation, we propose Automatic Truncated Backpropagation\nThrough Time (AT-BPTT), a novel framework that dynamically adapts both\ntruncation positions and window sizes according to intrinsic gradient behavior.\nAT-BPTT introduces three key components: (1) a probabilistic mechanism for\nstage-aware timestep selection, (2) an adaptive window sizing strategy based on\ngradient variation, and (3) a low-rank Hessian approximation to reduce\ncomputational overhead. Extensive experiments on CIFAR-10, CIFAR-100,\nTiny-ImageNet, and ImageNet-1K show that AT-BPTT achieves state-of-the-art\nperformance, improving accuracy by an average of 6.16% over baseline methods.\nMoreover, our approach accelerates inner-loop optimization by 3.9x while saving\n63% memory cost.", "AI": {"tldr": "AT-BPTT is a novel framework for dataset distillation that dynamically adapts truncation positions and window sizes based on gradient behavior, achieving state-of-the-art performance with significant speed and memory improvements.", "motivation": "Existing inner-loop optimization methods for dataset distillation rely on random truncation strategies that lack flexibility and yield suboptimal results, as neural networks exhibit distinct learning dynamics across different training stages.", "method": "Proposes Automatic Truncated Backpropagation Through Time (AT-BPTT) with three key components: probabilistic stage-aware timestep selection, adaptive window sizing based on gradient variation, and low-rank Hessian approximation to reduce computational overhead.", "result": "Achieves state-of-the-art performance with 6.16% average accuracy improvement over baselines on CIFAR-10, CIFAR-100, Tiny-ImageNet, and ImageNet-1K, while accelerating inner-loop optimization by 3.9x and saving 63% memory cost.", "conclusion": "AT-BPTT effectively addresses the limitations of random truncation in dataset distillation by dynamically adapting to neural network learning dynamics, delivering superior performance and efficiency."}}
{"id": "2510.04947", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04947", "abs": "https://arxiv.org/abs/2510.04947", "authors": ["Xin Li", "Kaixiang Yang", "Qiang Li", "Zhiwei Wang"], "title": "Bidirectional Mammogram View Translation with Column-Aware and Implicit 3D Conditional Diffusion", "comment": "BIBM2025 accept, 8 pages, 4 figures", "summary": "Dual-view mammography, including craniocaudal (CC) and mediolateral oblique\n(MLO) projections, offers complementary anatomical views crucial for breast\ncancer diagnosis. However, in real-world clinical workflows, one view may be\nmissing, corrupted, or degraded due to acquisition errors or compression\nartifacts, limiting the effectiveness of downstream analysis. View-to-view\ntranslation can help recover missing views and improve lesion alignment. Unlike\nnatural images, this task in mammography is highly challenging due to large\nnon-rigid deformations and severe tissue overlap in X-ray projections, which\nobscure pixel-level correspondences. In this paper, we propose Column-Aware and\nImplicit 3D Diffusion (CA3D-Diff), a novel bidirectional mammogram view\ntranslation framework based on conditional diffusion model. To address\ncross-view structural misalignment, we first design a column-aware\ncross-attention mechanism that leverages the geometric property that\nanatomically corresponding regions tend to lie in similar column positions\nacross views. A Gaussian-decayed bias is applied to emphasize local column-wise\ncorrelations while suppressing distant mismatches. Furthermore, we introduce an\nimplicit 3D structure reconstruction module that back-projects noisy 2D latents\ninto a coarse 3D feature volume based on breast-view projection geometry. The\nreconstructed 3D structure is refined and injected into the denoising UNet to\nguide cross-view generation with enhanced anatomical awareness. Extensive\nexperiments demonstrate that CA3D-Diff achieves superior performance in\nbidirectional tasks, outperforming state-of-the-art methods in visual fidelity\nand structural consistency. Furthermore, the synthesized views effectively\nimprove single-view malignancy classification in screening settings,\ndemonstrating the practical value of our method in real-world diagnostics.", "AI": {"tldr": "CA3D-Diff is a novel bidirectional mammogram view translation framework that uses conditional diffusion models with column-aware cross-attention and implicit 3D structure reconstruction to address the challenge of missing or corrupted views in dual-view mammography.", "motivation": "In real-world clinical workflows, one mammography view (CC or MLO) may be missing, corrupted, or degraded due to acquisition errors or compression artifacts, limiting diagnostic effectiveness. View-to-view translation can help recover missing views and improve lesion alignment, but this is challenging due to large non-rigid deformations and severe tissue overlap in X-ray projections.", "method": "The proposed CA3D-Diff framework includes: 1) Column-aware cross-attention mechanism that leverages geometric properties where anatomically corresponding regions lie in similar column positions across views, with Gaussian-decayed bias to emphasize local correlations; 2) Implicit 3D structure reconstruction module that back-projects noisy 2D latents into a coarse 3D feature volume based on breast-view projection geometry, which is refined and injected into the denoising UNet to guide cross-view generation.", "result": "Extensive experiments demonstrate that CA3D-Diff achieves superior performance in bidirectional view translation tasks, outperforming state-of-the-art methods in both visual fidelity and structural consistency. The synthesized views effectively improve single-view malignancy classification in screening settings.", "conclusion": "CA3D-Diff provides an effective solution for mammogram view translation that addresses structural misalignment challenges through geometric-aware attention and 3D anatomical guidance, demonstrating practical value for real-world breast cancer diagnostics by improving classification performance when views are missing."}}
{"id": "2510.04840", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04840", "abs": "https://arxiv.org/abs/2510.04840", "authors": ["Viktor Koz\u00e1k", "Jan Chudoba", "Libor P\u0159eu\u010dil"], "title": "Detailed Aerial Mapping of Photovoltaic Power Plants Through Semantically Significant Keypoints", "comment": "10 pages, 18 figures", "summary": "An accurate and up-to-date model of a photovoltaic (PV) power plant is\nessential for its optimal operation and maintenance. However, such a model may\nnot be easily available. This work introduces a novel approach for PV power\nplant mapping based on aerial overview images. It enables the automation of the\nmapping process while removing the reliance on third-party data. The presented\nmapping method takes advantage of the structural layout of the power plants to\nachieve detailed modeling down to the level of individual PV modules. The\napproach relies on visual segmentation of PV modules in overview images and the\ninference of structural information in each image, assigning modules to\nindividual benches, rows, and columns. We identify visual keypoints related to\nthe layout and use these to merge detections from multiple images while\nmaintaining their structural integrity. The presented method was experimentally\nverified and evaluated on two different power plants. The final fusion of 3D\npositions and semantic structures results in a compact georeferenced model\nsuitable for power plant maintenance.", "AI": {"tldr": "A novel method for automated PV power plant mapping using aerial images to create detailed 3D models down to individual module level, eliminating reliance on third-party data.", "motivation": "Accurate and up-to-date PV power plant models are essential for optimal operation and maintenance but are often not readily available, creating a need for automated mapping solutions.", "method": "Uses visual segmentation of PV modules in aerial images, identifies structural layout keypoints, assigns modules to benches/rows/columns, and fuses 3D positions with semantic structures from multiple images.", "result": "Successfully tested on two power plants, producing compact georeferenced models suitable for maintenance applications.", "conclusion": "The approach enables automated PV power plant mapping with detailed structural modeling while removing dependency on external data sources."}}
{"id": "2510.04966", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04966", "abs": "https://arxiv.org/abs/2510.04966", "authors": ["Anna Chistyakova", "Mikhail Pautov"], "title": "ActiveMark: on watermarking of visual foundation models via massive activations", "comment": null, "summary": "Being trained on large and vast datasets, visual foundation models (VFMs) can\nbe fine-tuned for diverse downstream tasks, achieving remarkable performance\nand efficiency in various computer vision applications. The high computation\ncost of data collection and training motivates the owners of some VFMs to\ndistribute them alongside the license to protect their intellectual property\nrights. However, a dishonest user of the protected model's copy may illegally\nredistribute it, for example, to make a profit. As a consequence, the\ndevelopment of reliable ownership verification tools is of great importance\ntoday, since such methods can be used to differentiate between a redistributed\ncopy of the protected model and an independent model. In this paper, we propose\nan approach to ownership verification of visual foundation models by\nfine-tuning a small set of expressive layers of a VFM along with a small\nencoder-decoder network to embed digital watermarks into an internal\nrepresentation of a hold-out set of input images. Importantly, the watermarks\nembedded remain detectable in the functional copies of the protected model,\nobtained, for example, by fine-tuning the VFM for a particular downstream task.\nTheoretically and experimentally, we demonstrate that the proposed method\nyields a low probability of false detection of a non-watermarked model and a\nlow probability of false misdetection of a watermarked model.", "AI": {"tldr": "This paper proposes a digital watermarking method for visual foundation models (VFMs) to protect intellectual property by embedding detectable watermarks in internal representations that persist even after fine-tuning.", "motivation": "Protect VFM intellectual property rights by preventing illegal redistribution, as current methods lack reliable ownership verification tools that can distinguish between redistributed copies and independent models.", "method": "Fine-tune a small set of expressive VFM layers along with an encoder-decoder network to embed digital watermarks into internal representations of hold-out input images, ensuring watermarks remain detectable in functional copies.", "result": "The method achieves low probability of false detection of non-watermarked models and low probability of false misdetection of watermarked models, both theoretically and experimentally.", "conclusion": "The proposed watermarking approach provides effective ownership verification for VFMs that remains robust even after model fine-tuning for downstream tasks."}}
{"id": "2510.04844", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04844", "abs": "https://arxiv.org/abs/2510.04844", "authors": ["Cheyu Lin", "Katherine A. Flanigan"], "title": "From Actions to Kinesics: Extracting Human Psychological States through Bodily Movements", "comment": "The 15th International Workshop on Structural Health Monitoring\n  (IWSHM)", "summary": "Understanding the dynamic relationship between humans and the built\nenvironment is a key challenge in disciplines ranging from environmental\npsychology to reinforcement learning (RL). A central obstacle in modeling these\ninteractions is the inability to capture human psychological states in a way\nthat is both generalizable and privacy preserving. Traditional methods rely on\ntheoretical models or questionnaires, which are limited in scope, static, and\nlabor intensive. We present a kinesics recognition framework that infers the\ncommunicative functions of human activity -- known as kinesics -- directly from\n3D skeleton joint data. Combining a spatial-temporal graph convolutional\nnetwork (ST-GCN) with a convolutional neural network (CNN), the framework\nleverages transfer learning to bypass the need for manually defined mappings\nbetween physical actions and psychological categories. The approach preserves\nuser anonymity while uncovering latent structures in bodily movements that\nreflect cognitive and emotional states. Our results on the Dyadic User\nEngagemenT (DUET) dataset demonstrate that this method enables scalable,\naccurate, and human-centered modeling of behavior, offering a new pathway for\nenhancing RL-driven simulations of human-environment interaction.", "AI": {"tldr": "A kinesics recognition framework that infers psychological states from 3D skeleton data using ST-GCN and CNN, enabling privacy-preserving and scalable modeling of human-environment interactions.", "motivation": "To overcome limitations of traditional methods (theoretical models, questionnaires) in capturing human psychological states that are generalizable, privacy-preserving, and scalable for modeling human-built environment interactions.", "method": "Combines spatial-temporal graph convolutional network (ST-GCN) with convolutional neural network (CNN) using transfer learning to infer communicative functions (kinesics) from 3D skeleton joint data, eliminating need for manual mappings between actions and psychological categories.", "result": "Demonstrated on Dyadic User Engagement (DUET) dataset, the method enables scalable, accurate, and human-centered modeling of behavior while preserving user anonymity and uncovering latent structures in bodily movements reflecting cognitive/emotional states.", "conclusion": "Provides a new pathway for enhancing RL-driven simulations of human-environment interaction through privacy-preserving kinesics recognition from 3D skeleton data."}}
{"id": "2510.05096", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MA", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.05096", "abs": "https://arxiv.org/abs/2510.05096", "authors": ["Zeyu Zhu", "Kevin Qinghong Lin", "Mike Zheng Shou"], "title": "Paper2Video: Automatic Video Generation from Scientific Papers", "comment": "20 pages, 8 figures", "summary": "Academic presentation videos have become an essential medium for research\ncommunication, yet producing them remains highly labor-intensive, often\nrequiring hours of slide design, recording, and editing for a short 2 to 10\nminutes video. Unlike natural video, presentation video generation involves\ndistinctive challenges: inputs from research papers, dense multi-modal\ninformation (text, figures, tables), and the need to coordinate multiple\naligned channels such as slides, subtitles, speech, and human talker. To\naddress these challenges, we introduce PaperTalker, the first benchmark of 101\nresearch papers paired with author-created presentation videos, slides, and\nspeaker metadata. We further design four tailored evaluation metrics--Meta\nSimilarity, PresentArena, PresentQuiz, and IP Memory--to measure how videos\nconvey the paper's information to the audience. Building on this foundation, we\npropose PaperTalker, the first multi-agent framework for academic presentation\nvideo generation. It integrates slide generation with effective layout\nrefinement by a novel effective tree search visual choice, cursor grounding,\nsubtitling, speech synthesis, and talking-head rendering, while parallelizing\nslide-wise generation for efficiency. Experiments on Paper2Video demonstrate\nthat the presentation videos produced by our approach are more faithful and\ninformative than existing baselines, establishing a practical step toward\nautomated and ready-to-use academic video generation. Our dataset, agent, and\ncode are available at https://github.com/showlab/Paper2Video.", "AI": {"tldr": "PaperTalker is the first benchmark and multi-agent framework for automated academic presentation video generation, addressing challenges like dense multi-modal information coordination and reducing labor-intensive video production.", "motivation": "Academic presentation video production is highly labor-intensive, requiring hours of work for short videos, with distinctive challenges including research paper inputs, dense multi-modal information, and coordination of multiple aligned channels.", "method": "Proposes PaperTalker, a multi-agent framework that integrates slide generation with layout refinement using tree search visual choice, cursor grounding, subtitling, speech synthesis, and talking-head rendering, while parallelizing slide-wise generation for efficiency.", "result": "Experiments on Paper2Video show that the presentation videos produced are more faithful and informative than existing baselines, establishing a practical step toward automated academic video generation.", "conclusion": "PaperTalker provides the first benchmark and framework for academic presentation video generation, offering an effective solution to reduce the labor-intensive process while maintaining information fidelity and quality."}}
{"id": "2510.04854", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04854", "abs": "https://arxiv.org/abs/2510.04854", "authors": ["Cheyu Lin", "John Martins", "Katherine A. Flanigan", "Ph. D"], "title": "Read the Room: Inferring Social Context Through Dyadic Interaction Recognition in Cyber-physical-social Infrastructure Systems", "comment": "ASCE International Conference on Computing in Civil Engineering 2024", "summary": "Cyber-physical systems (CPS) integrate sensing, computing, and control to\nimprove infrastructure performance, focusing on economic goals like performance\nand safety. However, they often neglect potential human-centered (or\n''social'') benefits. Cyber-physical-social infrastructure systems (CPSIS) aim\nto address this by aligning CPS with social objectives. This involves defining\nsocial benefits, understanding human interactions with each other and\ninfrastructure, developing privacy-preserving measurement methods, modeling\nthese interactions for prediction, linking them to social benefits, and\nactuating the physical environment to foster positive social outcomes. This\npaper delves into recognizing dyadic human interactions using real-world data,\nwhich is the backbone to measuring social behavior. This lays a foundation to\naddress the need to enhance understanding of the deeper meanings and mutual\nresponses inherent in human interactions. While RGB cameras are informative for\ninteraction recognition, privacy concerns arise. Depth sensors offer a\nprivacy-conscious alternative by analyzing skeletal movements. This study\ncompares five skeleton-based interaction recognition algorithms on a dataset of\n12 dyadic interactions. Unlike single-person datasets, these interactions,\ncategorized into communication types like emblems and affect displays, offer\ninsights into the cultural and emotional aspects of human interactions.", "AI": {"tldr": "This paper compares five skeleton-based algorithms for recognizing dyadic human interactions using depth sensors as a privacy-preserving alternative to RGB cameras, focusing on 12 types of social interactions.", "motivation": "Cyber-physical systems traditionally focus on economic goals but neglect social benefits. Cyber-physical-social infrastructure systems aim to address this by aligning technology with social objectives, requiring better understanding of human interactions while preserving privacy.", "method": "The study compares five skeleton-based interaction recognition algorithms on a dataset of 12 dyadic interactions. Depth sensors are used instead of RGB cameras to address privacy concerns by analyzing skeletal movements. The interactions are categorized into communication types like emblems and affect displays.", "result": "The research provides a comparative analysis of different skeleton-based algorithms for recognizing dyadic human interactions, laying foundation for measuring social behavior through privacy-preserving methods.", "conclusion": "Skeleton-based interaction recognition using depth sensors offers a viable privacy-conscious approach for understanding human social interactions, which is crucial for developing cyber-physical-social infrastructure systems that can foster positive social outcomes."}}
{"id": "2510.04856", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04856", "abs": "https://arxiv.org/abs/2510.04856", "authors": ["Martial Guidez", "Stefan Duffner", "Yannick Alpou", "Oscar R\u00f6th", "Christophe Garcia"], "title": "ERDE: Entropy-Regularized Distillation for Early-exit", "comment": null, "summary": "Although deep neural networks and in particular Convolutional Neural Networks\nhave demonstrated state-of-the-art performance in image classification with\nrelatively high efficiency, they still exhibit high computational costs, often\nrendering them impractical for real-time and edge applications. Therefore, a\nmultitude of compression techniques have been developed to reduce these costs\nwhile maintaining accuracy. In addition, dynamic architectures have been\nintroduced to modulate the level of compression at execution time, which is a\ndesirable property in many resource-limited application scenarios. The proposed\nmethod effectively integrates two well-established optimization techniques:\nearly exits and knowledge distillation, where a reduced student early-exit\nmodel is trained from a more complex teacher early-exit model. The primary\ncontribution of this research lies in the approach for training the student\nearly-exit model. In comparison to the conventional Knowledge Distillation\nloss, our approach incorporates a new entropy-based loss for images where the\nteacher's classification was incorrect. The proposed method optimizes the\ntrade-off between accuracy and efficiency, thereby achieving significant\nreductions in computational complexity without compromising classification\nperformance. The validity of this approach is substantiated by experimental\nresults on image classification datasets CIFAR10, CIFAR100 and SVHN, which\nfurther opens new research perspectives for Knowledge Distillation in other\ncontexts.", "AI": {"tldr": "This paper proposes a method that combines early exits and knowledge distillation to reduce computational costs in neural networks while maintaining accuracy, using a new entropy-based loss for incorrectly classified images.", "motivation": "Deep neural networks have high computational costs that make them impractical for real-time and edge applications, necessitating compression techniques that maintain accuracy while reducing complexity.", "method": "Integrates early exits and knowledge distillation, training a reduced student early-exit model from a more complex teacher early-exit model with a new entropy-based loss for images where teacher classification was incorrect.", "result": "Achieves significant reductions in computational complexity without compromising classification performance on CIFAR10, CIFAR100, and SVHN datasets.", "conclusion": "The approach effectively optimizes the accuracy-efficiency trade-off and opens new research perspectives for knowledge distillation in other contexts."}}
{"id": "2510.04859", "categories": ["cs.CV", "physics.data-an", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2510.04859", "abs": "https://arxiv.org/abs/2510.04859", "authors": ["Elena Corbetta", "Thomas Bocklitz"], "title": "\u03bcDeepIQA: deep learning-based fast and robust image quality assessment with local predictions for optical microscopy", "comment": "16 pages, 6 figures. \\mu DeepIQA is publicly available at\n  https://git.photonicdata.science/elena.corbetta/udeepiqa", "summary": "Optical microscopy is one of the most widely used techniques in research\nstudies for life sciences and biomedicine. These applications require reliable\nexperimental pipelines to extract valuable knowledge from the measured samples\nand must be supported by image quality assessment (IQA) to ensure correct\nprocessing and analysis of the image data. IQA methods are implemented with\nvariable complexity. However, while most quality metrics have a straightforward\nimplementation, they might be time consuming and computationally expensive when\nevaluating a large dataset. In addition, quality metrics are often designed for\nwell-defined image features and may be unstable for images out of the ideal\ndomain.\n  To overcome these limitations, recent works have proposed deep learning-based\nIQA methods, which can provide superior performance, increased generalizability\nand fast prediction. Our method, named $\\mathrm{\\mu}$DeepIQA, is inspired by\nprevious studies and applies a deep convolutional neural network designed for\nIQA on natural images to optical microscopy measurements. We retrained the same\narchitecture to predict individual quality metrics and global quality scores\nfor optical microscopy data. The resulting models provide fast and stable\npredictions of image quality by generalizing quality estimation even outside\nthe ideal range of standard methods. In addition, $\\mathrm{\\mu}$DeepIQA\nprovides patch-wise prediction of image quality and can be used to visualize\nspatially varying quality in a single image. Our study demonstrates that\noptical microscopy-based studies can benefit from the generalizability of deep\nlearning models due to their stable performance in the presence of outliers,\nthe ability to assess small image patches, and rapid predictions.", "AI": {"tldr": "\u03bcDeepIQA is a deep learning-based image quality assessment method for optical microscopy that provides fast, stable quality predictions and patch-wise quality visualization, overcoming limitations of traditional quality metrics.", "motivation": "Traditional image quality assessment methods for optical microscopy are often time-consuming, computationally expensive for large datasets, and unstable for images outside ideal domains, requiring more robust and efficient solutions.", "method": "The method adapts a deep convolutional neural network originally designed for natural image quality assessment to optical microscopy data, retraining the architecture to predict both individual quality metrics and global quality scores.", "result": "\u03bcDeepIQA provides fast and stable quality predictions that generalize well even outside ideal ranges, enables patch-wise quality assessment, and visualizes spatially varying quality within single images.", "conclusion": "Deep learning models like \u03bcDeepIQA benefit optical microscopy studies through stable outlier performance, ability to assess small image patches, and rapid predictions, demonstrating superior generalizability over standard methods."}}
{"id": "2510.04864", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04864", "abs": "https://arxiv.org/abs/2510.04864", "authors": ["Ciem Cornelissen", "Sander De Coninck", "Axel Willekens", "Sam Leroux", "Pieter Simoens"], "title": "In-Field Mapping of Grape Yield and Quality with Illumination-Invariant Deep Learning", "comment": "Accepted manuscript for the IEEE Internet of Things Journal. The\n  final version will be available on IEEE Xplore. \\c{opyright} 2025 IEEE", "summary": "This paper presents an end-to-end, IoT-enabled robotic system for the\nnon-destructive, real-time, and spatially-resolved mapping of grape yield and\nquality (Brix, Acidity) in vineyards. The system features a comprehensive\nanalytical pipeline that integrates two key modules: a high-performance model\nfor grape bunch detection and weight estimation, and a novel deep learning\nframework for quality assessment from hyperspectral (HSI) data. A critical\nbarrier to in-field HSI is the ``domain shift\" caused by variable illumination.\nTo overcome this, our quality assessment is powered by the Light-Invariant\nSpectral Autoencoder (LISA), a domain-adversarial framework that learns\nillumination-invariant features from uncalibrated data. We validated the\nsystem's robustness on a purpose-built HSI dataset spanning three distinct\nillumination domains: controlled artificial lighting (lab), and variable\nnatural sunlight captured in the morning and afternoon. Results show the\ncomplete pipeline achieves a recall (0.82) for bunch detection and a $R^2$\n(0.76) for weight prediction, while the LISA module improves quality prediction\ngeneralization by over 20% compared to the baselines. By combining these robust\nmodules, the system successfully generates high-resolution, georeferenced data\nof both grape yield and quality, providing actionable, data-driven insights for\nprecision viticulture.", "AI": {"tldr": "An end-to-end IoT robotic system for real-time mapping of grape yield and quality in vineyards using hyperspectral imaging and deep learning, with a novel domain-adversarial framework to handle illumination variations.", "motivation": "To enable non-destructive, real-time monitoring of grape yield and quality in vineyards while overcoming the challenge of domain shift caused by variable illumination conditions in field settings.", "method": "Integrated pipeline with two modules: 1) High-performance model for grape bunch detection and weight estimation, 2) Light-Invariant Spectral Autoencoder (LISA) - a domain-adversarial deep learning framework for quality assessment from hyperspectral data that learns illumination-invariant features from uncalibrated data.", "result": "System achieves 0.82 recall for bunch detection and R\u00b2=0.76 for weight prediction. LISA module improves quality prediction generalization by over 20% compared to baselines across three illumination domains (lab, morning sunlight, afternoon sunlight).", "conclusion": "The system successfully generates high-resolution, georeferenced data of both grape yield and quality, providing actionable insights for precision viticulture by combining robust detection and illumination-invariant quality assessment modules."}}
{"id": "2510.04876", "categories": ["cs.CV", "cs.LG", "I.2.6; I.4.6; I.5.1; I.5.4"], "pdf": "https://arxiv.org/pdf/2510.04876", "abs": "https://arxiv.org/abs/2510.04876", "authors": ["Hayat Rajani", "Valerio Franchi", "Borja Martinez-Clavel Valles", "Raimon Ramos", "Rafael Garcia", "Nuno Gracias"], "title": "BenthiCat: An opti-acoustic dataset for advancing benthic classification and habitat mapping", "comment": "Article under review by IJRR", "summary": "Benthic habitat mapping is fundamental for understanding marine ecosystems,\nguiding conservation efforts, and supporting sustainable resource management.\nYet, the scarcity of large, annotated datasets limits the development and\nbenchmarking of machine learning models in this domain. This paper introduces a\nthorough multi-modal dataset, comprising about a million side-scan sonar (SSS)\ntiles collected along the coast of Catalonia (Spain), complemented by\nbathymetric maps and a set of co-registered optical images from targeted\nsurveys using an autonomous underwater vehicle (AUV). Approximately \\num{36000}\nof the SSS tiles have been manually annotated with segmentation masks to enable\nsupervised fine-tuning of classification models. All the raw sensor data,\ntogether with mosaics, are also released to support further exploration and\nalgorithm development. To address challenges in multi-sensor data fusion for\nAUVs, we spatially associate optical images with corresponding SSS tiles,\nfacilitating self-supervised, cross-modal representation learning. Accompanying\nopen-source preprocessing and annotation tools are provided to enhance\naccessibility and encourage research. This resource aims to establish a\nstandardized benchmark for underwater habitat mapping, promoting advancements\nin autonomous seafloor classification and multi-sensor integration.", "AI": {"tldr": "This paper introduces a large multi-modal dataset for benthic habitat mapping, including side-scan sonar tiles, bathymetric maps, and optical images, with manual annotations to support machine learning development in marine ecosystem research.", "motivation": "The scarcity of large, annotated datasets limits the development and benchmarking of machine learning models for benthic habitat mapping, which is crucial for understanding marine ecosystems, conservation efforts, and sustainable resource management.", "method": "The authors collected about a million side-scan sonar tiles along Catalonia's coast, complemented by bathymetric maps and co-registered optical images from AUV surveys. They manually annotated 36,000 SSS tiles with segmentation masks and spatially associated optical images with corresponding SSS tiles to enable cross-modal representation learning.", "result": "The paper presents a comprehensive multi-modal dataset with raw sensor data, mosaics, and annotations, along with open-source preprocessing and annotation tools to enhance accessibility and encourage research in underwater habitat mapping.", "conclusion": "This resource aims to establish a standardized benchmark for underwater habitat mapping, promoting advancements in autonomous seafloor classification and multi-sensor integration for marine ecosystem research."}}
{"id": "2510.04912", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04912", "abs": "https://arxiv.org/abs/2510.04912", "authors": ["Ngeyen Yinkfu", "Sunday Nwovu", "Jonathan Kayizzi", "Angelique Uwamahoro"], "title": "Comparative Analysis of YOLOv5, Faster R-CNN, SSD, and RetinaNet for Motorbike Detection in Kigali Autonomous Driving Context", "comment": "3 figures, 2 tables", "summary": "In Kigali, Rwanda, motorcycle taxis are a primary mode of transportation,\noften navigating unpredictably and disregarding traffic rules, posing\nsignificant challenges for autonomous driving systems. This study compares four\nobject detection models--YOLOv5, Faster R-CNN, SSD, and RetinaNet--for\nmotorbike detection using a custom dataset of 198 images collected in Kigali.\nImplemented in PyTorch with transfer learning, the models were evaluated for\naccuracy, localization, and inference speed to assess their suitability for\nreal-time navigation in resource-constrained settings. We identify\nimplementation challenges, including dataset limitations and model\ncomplexities, and recommend simplified architectures for future work to enhance\naccessibility for autonomous systems in developing countries like Rwanda.", "AI": {"tldr": "Comparison of four object detection models (YOLOv5, Faster R-CNN, SSD, RetinaNet) for motorbike detection in Kigali's challenging traffic environment, with recommendations for simplified architectures suitable for resource-constrained autonomous systems.", "motivation": "Motorcycle taxis in Kigali navigate unpredictably and disregard traffic rules, posing significant challenges for autonomous driving systems that need to detect and respond to these vehicles in real-time.", "method": "Used four object detection models (YOLOv5, Faster R-CNN, SSD, RetinaNet) implemented in PyTorch with transfer learning, evaluated on a custom dataset of 198 images collected in Kigali, assessing accuracy, localization, and inference speed.", "result": "The study identified implementation challenges including dataset limitations and model complexities, and evaluated the models' performance for real-time navigation in resource-constrained settings.", "conclusion": "Recommends simplified architectures to enhance accessibility for autonomous systems in developing countries like Rwanda, addressing the specific challenges of detecting motorbikes in chaotic urban environments."}}
{"id": "2510.04916", "categories": ["cs.CV", "I.4.6; I.4.8; I.4.10"], "pdf": "https://arxiv.org/pdf/2510.04916", "abs": "https://arxiv.org/abs/2510.04916", "authors": ["Giulio Weikmann", "Gianmarco Perantoni", "Lorenzo Bruzzone"], "title": "A Semantics-Aware Hierarchical Self-Supervised Approach to Classification of Remote Sensing Images", "comment": "12 pages, 6 figures", "summary": "Deep learning has become increasingly important in remote sensing image\nclassification due to its ability to extract semantic information from complex\ndata. Classification tasks often include predefined label hierarchies that\nrepresent the semantic relationships among classes. However, these hierarchies\nare frequently overlooked, and most approaches focus only on fine-grained\nclassification schemes. In this paper, we present a novel Semantics-Aware\nHierarchical Consensus (SAHC) method for learning hierarchical features and\nrelationships by integrating hierarchy-specific classification heads within a\ndeep network architecture, each specialized in different degrees of class\ngranularity. The proposed approach employs trainable hierarchy matrices, which\nguide the network through the learning of the hierarchical structure in a\nself-supervised manner. Furthermore, we introduce a hierarchical consensus\nmechanism to ensure consistent probability distributions across different\nhierarchical levels. This mechanism acts as a weighted ensemble being able to\neffectively leverage the inherent structure of the hierarchical classification\ntask. The proposed SAHC method is evaluated on three benchmark datasets with\ndifferent degrees of hierarchical complexity on different tasks, using distinct\nbackbone architectures to effectively emphasize its adaptability. Experimental\nresults show both the effectiveness of the proposed approach in guiding network\nlearning and the robustness of the hierarchical consensus for remote sensing\nimage classification tasks.", "AI": {"tldr": "A novel Semantics-Aware Hierarchical Consensus (SAHC) method that integrates hierarchy-specific classification heads with trainable hierarchy matrices and consensus mechanism for remote sensing image classification, leveraging semantic relationships in class hierarchies.", "motivation": "Most deep learning approaches for remote sensing image classification overlook predefined label hierarchies and focus only on fine-grained classification, missing the semantic relationships among classes.", "method": "Uses hierarchy-specific classification heads with trainable hierarchy matrices to learn hierarchical structure self-supervised, plus hierarchical consensus mechanism for consistent probability distributions across hierarchical levels.", "result": "Experimental results on three benchmark datasets show effectiveness in guiding network learning and robustness of hierarchical consensus for remote sensing image classification tasks.", "conclusion": "SAHC method effectively leverages hierarchical structure in classification tasks and demonstrates adaptability across different backbone architectures and hierarchical complexities."}}
{"id": "2510.04961", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04961", "abs": "https://arxiv.org/abs/2510.04961", "authors": ["Th\u00e9ophane Vallaeys", "Jakob Verbeek", "Matthieu Cord"], "title": "SSDD: Single-Step Diffusion Decoder for Efficient Image Tokenization", "comment": null, "summary": "Tokenizers are a key component of state-of-the-art generative image models,\nextracting the most important features from the signal while reducing data\ndimension and redundancy. Most current tokenizers are based on KL-regularized\nvariational autoencoders (KL-VAE), trained with reconstruction, perceptual and\nadversarial losses. Diffusion decoders have been proposed as a more principled\nalternative to model the distribution over images conditioned on the latent.\nHowever, matching the performance of KL-VAE still requires adversarial losses,\nas well as a higher decoding time due to iterative sampling. To address these\nlimitations, we introduce a new pixel diffusion decoder architecture for\nimproved scaling and training stability, benefiting from transformer components\nand GAN-free training. We use distillation to replicate the performance of the\ndiffusion decoder in an efficient single-step decoder. This makes SSDD the\nfirst diffusion decoder optimized for single-step reconstruction trained\nwithout adversarial losses, reaching higher reconstruction quality and faster\nsampling than KL-VAE. In particular, SSDD improves reconstruction FID from\n$0.87$ to $0.50$ with $1.4\\times$ higher throughput and preserve generation\nquality of DiTs with $3.8\\times$ faster sampling. As such, SSDD can be used as\na drop-in replacement for KL-VAE, and for building higher-quality and faster\ngenerative models.", "AI": {"tldr": "SSDD introduces a new diffusion decoder architecture with transformer components and GAN-free training, using distillation to create an efficient single-step decoder that outperforms KL-VAE tokenizers in reconstruction quality and speed.", "motivation": "Current KL-VAE tokenizers require adversarial losses and have limitations, while diffusion decoders need iterative sampling and still require adversarial training. There's a need for a more principled approach that eliminates adversarial losses while maintaining performance.", "method": "Developed a pixel diffusion decoder with transformer components and GAN-free training, then used distillation to create a single-step decoder (SSDD) that replicates the diffusion decoder's performance without iterative sampling.", "result": "SSDD improves reconstruction FID from 0.87 to 0.50 with 1.4x higher throughput than KL-VAE, and preserves DiT generation quality with 3.8x faster sampling. It achieves higher reconstruction quality without adversarial losses.", "conclusion": "SSDD can serve as a drop-in replacement for KL-VAE tokenizers, enabling higher-quality and faster generative models with improved reconstruction and sampling efficiency."}}
{"id": "2510.05006", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.05006", "abs": "https://arxiv.org/abs/2510.05006", "authors": ["Koen Vellenga", "H. Joe Steinhauer", "Jonas Andersson", "Anders Sj\u00f6gren"], "title": "Latent Uncertainty Representations for Video-based Driver Action and Intention Recognition", "comment": "16 pages, 8 figures, 7 tables, under submission", "summary": "Deep neural networks (DNNs) are increasingly applied to safety-critical tasks\nin resource-constrained environments, such as video-based driver action and\nintention recognition. While last layer probabilistic deep learning (LL-PDL)\nmethods can detect out-of-distribution (OOD) instances, their performance\nvaries. As an alternative to last layer approaches, we propose extending\npre-trained DNNs with transformation layers to produce multiple latent\nrepresentations to estimate the uncertainty. We evaluate our latent uncertainty\nrepresentation (LUR) and repulsively trained LUR (RLUR) approaches against\neight PDL methods across four video-based driver action and intention\nrecognition datasets, comparing classification performance, calibration, and\nuncertainty-based OOD detection. We also contribute 28,000 frame-level action\nlabels and 1,194 video-level intention labels for the NuScenes dataset. Our\nresults show that LUR and RLUR achieve comparable in-distribution\nclassification performance to other LL-PDL approaches. For uncertainty-based\nOOD detection, LUR matches top-performing PDL methods while being more\nefficient to train and easier to tune than approaches that require Markov-Chain\nMonte Carlo sampling or repulsive training procedures.", "AI": {"tldr": "Proposes latent uncertainty representation (LUR) and repulsively trained LUR (RLUR) methods for out-of-distribution detection in deep neural networks, achieving comparable performance to existing probabilistic deep learning methods with better efficiency.", "motivation": "Deep neural networks are increasingly used in safety-critical applications like driver action recognition, but existing last layer probabilistic deep learning methods for out-of-distribution detection have varying performance and can be inefficient.", "method": "Extends pre-trained DNNs with transformation layers to produce multiple latent representations for uncertainty estimation, comparing LUR and RLUR against eight probabilistic deep learning methods across four video datasets.", "result": "LUR and RLUR achieve comparable in-distribution classification performance to other approaches, with LUR matching top-performing methods for uncertainty-based OOD detection while being more efficient to train and easier to tune.", "conclusion": "The proposed LUR approach provides an effective and efficient alternative to existing probabilistic deep learning methods for out-of-distribution detection in safety-critical applications."}}
{"id": "2510.05015", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.05015", "abs": "https://arxiv.org/abs/2510.05015", "authors": ["Nabil Daiyan", "Md Rakibul Haque"], "title": "Exploring the Efficacy of Modified Transfer Learning in Identifying Parkinson's Disease Through Drawn Image Patterns", "comment": "5 pages, 11 figures, published on 2024 2nd International Conference\n  on Information and Communication Technology (ICICT 2024)", "summary": "Parkinson's disease (PD) is a progressive neurodegenerative condition\ncharacterized by the death of dopaminergic neurons, leading to various movement\ndisorder symptoms. Early diagnosis of PD is crucial to prevent adverse effects,\nyet traditional diagnostic methods are often cumbersome and costly. In this\nstudy, a machine learning-based approach is proposed using hand-drawn spiral\nand wave images as potential biomarkers for PD detection. Our methodology\nleverages convolutional neural networks (CNNs), transfer learning, and\nattention mechanisms to improve model performance and resilience against\noverfitting. To enhance the diversity and richness of both spiral and wave\ncategories, the training dataset undergoes augmentation to increase the number\nof images. The proposed architecture comprises three phases: utilizing\npre-trained CNNs, incorporating custom convolutional layers, and ensemble\nvoting. Employing hard voting further enhances performance by aggregating\npredictions from multiple models. Experimental results show promising accuracy\nrates. For spiral images, weighted average precision, recall, and F1-score are\n90%, and for wave images, they are 96.67%. After combining the predictions\nthrough ensemble hard voting, the overall accuracy is 93.3%. These findings\nunderscore the potential of machine learning in early PD diagnosis, offering a\nnon-invasive and cost-effective solution to improve patient outcomes.", "AI": {"tldr": "Machine learning approach using hand-drawn spiral and wave images achieves 93.3% accuracy for Parkinson's disease detection through CNN, transfer learning, and ensemble voting methods.", "motivation": "Early Parkinson's disease diagnosis is crucial but traditional methods are cumbersome and costly. This study aims to provide a non-invasive, cost-effective solution using machine learning.", "method": "Three-phase architecture: pre-trained CNNs, custom convolutional layers, and ensemble voting with hard voting. Dataset augmentation and attention mechanisms are used to improve performance and prevent overfitting.", "result": "Spiral images achieved 90% precision, recall, and F1-score; wave images achieved 96.67%. Combined ensemble voting achieved overall 93.3% accuracy.", "conclusion": "Machine learning using hand-drawn images shows strong potential for early Parkinson's disease diagnosis as a non-invasive and cost-effective alternative to traditional methods."}}
{"id": "2510.05034", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.05034", "abs": "https://arxiv.org/abs/2510.05034", "authors": ["Yunlong Tang", "Jing Bi", "Pinxin Liu", "Zhenyu Pan", "Zhangyun Tan", "Qianxiang Shen", "Jiani Liu", "Hang Hua", "Junjia Guo", "Yunzhong Xiao", "Chao Huang", "Zhiyuan Wang", "Susan Liang", "Xinyi Liu", "Yizhi Song", "Yuhe Nie", "Jia-Xing Zhong", "Bozheng Li", "Daiqing Qi", "Ziyun Zeng", "Ali Vosoughi", "Luchuan Song", "Zeliang Zhang", "Daiki Shimada", "Han Liu", "Jiebo Luo", "Chenliang Xu"], "title": "Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Models", "comment": "The 1st version", "summary": "Video understanding represents the most challenging frontier in computer\nvision, requiring models to reason about complex spatiotemporal relationships,\nlong-term dependencies, and multimodal evidence. The recent emergence of\nVideo-Large Multimodal Models (Video-LMMs), which integrate visual encoders\nwith powerful decoder-based language models, has demonstrated remarkable\ncapabilities in video understanding tasks. However, the critical phase that\ntransforms these models from basic perception systems into sophisticated\nreasoning engines, post-training, remains fragmented across the literature.\nThis survey provides the first comprehensive examination of post-training\nmethodologies for Video-LMMs, encompassing three fundamental pillars:\nsupervised fine-tuning (SFT) with chain-of-thought, reinforcement learning (RL)\nfrom verifiable objectives, and test-time scaling (TTS) through enhanced\ninference computation. We present a structured taxonomy that clarifies the\nroles, interconnections, and video-specific adaptations of these techniques,\naddressing unique challenges such as temporal localization, spatiotemporal\ngrounding, long video efficiency, and multimodal evidence integration. Through\nsystematic analysis of representative methods, we synthesize key design\nprinciples, insights, and evaluation protocols while identifying critical open\nchallenges in reward design, scalability, and cost-performance optimization. We\nfurther curate essential benchmarks, datasets, and metrics to facilitate\nrigorous assessment of post-training effectiveness. This survey aims to provide\nresearchers and practitioners with a unified framework for advancing Video-LMM\ncapabilities. Additional resources and updates are maintained at:\nhttps://github.com/yunlong10/Awesome-Video-LMM-Post-Training", "AI": {"tldr": "This survey provides the first comprehensive examination of post-training methodologies for Video-Large Multimodal Models (Video-LMMs), covering supervised fine-tuning, reinforcement learning, and test-time scaling techniques to transform basic perception systems into sophisticated reasoning engines.", "motivation": "Video understanding is the most challenging frontier in computer vision, requiring reasoning about complex spatiotemporal relationships and long-term dependencies. While Video-LMMs have shown remarkable capabilities, the critical post-training phase that transforms them into sophisticated reasoning engines remains fragmented across literature.", "method": "The survey examines three fundamental pillars of post-training methodologies: supervised fine-tuning (SFT) with chain-of-thought, reinforcement learning (RL) from verifiable objectives, and test-time scaling (TTS) through enhanced inference computation. It presents a structured taxonomy addressing video-specific challenges like temporal localization and spatiotemporal grounding.", "result": "The survey synthesizes key design principles, insights, and evaluation protocols while identifying critical open challenges in reward design, scalability, and cost-performance optimization. It also curates essential benchmarks, datasets, and metrics for rigorous assessment of post-training effectiveness.", "conclusion": "This survey aims to provide researchers and practitioners with a unified framework for advancing Video-LMM capabilities through systematic post-training methodologies."}}
{"id": "2510.05051", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.05051", "abs": "https://arxiv.org/abs/2510.05051", "authors": ["Rohit Jayanti", "Swayam Agrawal", "Vansh Garg", "Siddharth Tourani", "Muhammad Haris Khan", "Sourav Garg", "Madhava Krishna"], "title": "SegMASt3R: Geometry Grounded Segment Matching", "comment": "Accepted to The Thirty-Ninth Annual Conference on Neural Information\n  Processing Systems (NeurIPS 2025) as a Spotlight (top 3.5%)", "summary": "Segment matching is an important intermediate task in computer vision that\nestablishes correspondences between semantically or geometrically coherent\nregions across images. Unlike keypoint matching, which focuses on localized\nfeatures, segment matching captures structured regions, offering greater\nrobustness to occlusions, lighting variations, and viewpoint changes. In this\npaper, we leverage the spatial understanding of 3D foundation models to tackle\nwide-baseline segment matching, a challenging setting involving extreme\nviewpoint shifts. We propose an architecture that uses the inductive bias of\nthese 3D foundation models to match segments across image pairs with up to 180\ndegree view-point change. Extensive experiments show that our approach\noutperforms state-of-the-art methods, including the SAM2 video propagator and\nlocal feature matching methods, by upto 30% on the AUPRC metric, on ScanNet++\nand Replica datasets. We further demonstrate benefits of the proposed model on\nrelevant downstream tasks, including 3D instance segmentation and image-goal\nnavigation. Project Page: https://segmast3r.github.io/", "AI": {"tldr": "This paper proposes a segment matching method using 3D foundation models to handle extreme viewpoint changes up to 180 degrees, outperforming state-of-the-art methods by up to 30% on AUPRC metric.", "motivation": "Segment matching provides robustness to occlusions, lighting variations, and viewpoint changes compared to keypoint matching. The paper aims to tackle the challenging wide-baseline segment matching problem with extreme viewpoint shifts.", "method": "An architecture that leverages the spatial understanding and inductive bias of 3D foundation models to match segments across image pairs with extreme viewpoint changes.", "result": "Outperforms state-of-the-art methods including SAM2 video propagator and local feature matching methods by up to 30% on AUPRC metric on ScanNet++ and Replica datasets.", "conclusion": "The proposed approach successfully addresses wide-baseline segment matching and demonstrates benefits on downstream tasks including 3D instance segmentation and image-goal navigation."}}
{"id": "2510.05053", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.05053", "abs": "https://arxiv.org/abs/2510.05053", "authors": ["Mohammad-Ali Mahmoudpour", "Saeed Mahmoudpour"], "title": "No-reference Quality Assessment of Contrast-distorted Images using Contrast-enhanced Pseudo Reference", "comment": null, "summary": "Contrast change is an important factor that affects the quality of images.\nDuring image capturing, unfavorable lighting conditions can cause contrast\nchange and visual quality loss. While various methods have been proposed to\nassess the quality of images under different distortions such as blur and\nnoise, contrast distortion has been largely overlooked as its visual impact and\nproperties are different from other conventional types of distortions. In this\npaper, we propose a no-reference image quality assessment (NR-IQA) metric for\ncontrast-distorted images. Using a set of contrast enhancement algorithms, we\naim to generate pseudo-reference images that are visually close to the actual\nreference image, such that the NR problem is transformed to a Full-reference\n(FR) assessment with higher accuracy. To this end, a large dataset of\ncontrast-enhanced images is produced to train a classification network that can\nselect the most suitable contrast enhancement algorithm based on image content\nand distortion for pseudo-reference image generation. Finally, the evaluation\nis performed in the FR manner to assess the quality difference between the\ncontrast-enhanced (pseudoreference) and degraded images. Performance evaluation\nof the proposed method on three databases containing contrast distortions\n(CCID2014, TID2013, and CSIQ), indicates the promising performance of the\nproposed method.", "AI": {"tldr": "Proposes a no-reference image quality assessment metric for contrast-distorted images by generating pseudo-reference images through contrast enhancement algorithms and transforming the problem to full-reference assessment.", "motivation": "Contrast distortion has been largely overlooked in image quality assessment despite its significant visual impact, as existing methods mainly focus on other distortions like blur and noise.", "method": "Uses contrast enhancement algorithms to generate pseudo-reference images, trains a classification network to select the best algorithm based on image content and distortion, then performs full-reference assessment between contrast-enhanced and degraded images.", "result": "Performance evaluation on three databases (CCID2014, TID2013, and CSIQ) shows promising results for assessing contrast-distorted images.", "conclusion": "The proposed approach effectively addresses the challenge of contrast distortion assessment by transforming no-reference problems into full-reference scenarios through intelligent pseudo-reference generation."}}
{"id": "2510.05071", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.05071", "abs": "https://arxiv.org/abs/2510.05071", "authors": ["Debojyoti Ghosh", "Soumya K Ghosh", "Adrijit Goswami"], "title": "Neuroplastic Modular Framework: Cross-Domain Image Classification of Garbage and Industrial Surfaces", "comment": null, "summary": "Efficient and accurate classification of waste and industrial surface defects\nis essential for ensuring sustainable waste management and maintaining high\nstandards in quality control. This paper introduces the Neuroplastic Modular\nClassifier, a novel hybrid architecture designed for robust and adaptive image\nclassification in dynamic environments. The model combines a ResNet-50 backbone\nfor localized feature extraction with a Vision Transformer (ViT) to capture\nglobal semantic context. Additionally, FAISS-based similarity retrieval is\nincorporated to provide a memory-like reference to previously encountered data,\nenriching the model's feature space. A key innovation of our architecture is\nthe neuroplastic modular design composed of expandable, learnable blocks that\ndynamically grow during training when performance plateaus. Inspired by\nbiological learning systems, this mechanism allows the model to adapt to data\ncomplexity over time, improving generalization. Beyond garbage classification,\nwe validate the model on the Kolektor Surface Defect Dataset 2 (KolektorSDD2),\nwhich involves industrial defect detection on metal surfaces. Experimental\nresults across domains show that the proposed architecture outperforms\ntraditional static models in both accuracy and adaptability. The Neuroplastic\nModular Classifier offers a scalable, high-performance solution for real-world\nimage classification, with strong applicability in both environmental and\nindustrial domains.", "AI": {"tldr": "The paper introduces a Neuroplastic Modular Classifier that combines ResNet-50 and Vision Transformer for waste classification and industrial defect detection, featuring dynamic expansion during training to improve adaptability.", "motivation": "Efficient waste classification and industrial surface defect detection are crucial for sustainable waste management and quality control, requiring robust and adaptive models for dynamic environments.", "method": "Hybrid architecture using ResNet-50 for local features and Vision Transformer for global context, with FAISS-based similarity retrieval and neuroplastic modular blocks that dynamically expand during training when performance plateaus.", "result": "The model outperforms traditional static models in accuracy and adaptability, validated on waste classification and KolektorSDD2 industrial defect dataset.", "conclusion": "The Neuroplastic Modular Classifier provides a scalable, high-performance solution for real-world image classification with strong applicability in environmental and industrial domains."}}
{"id": "2510.05091", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.05091", "abs": "https://arxiv.org/abs/2510.05091", "authors": ["Le Zhuo", "Songhao Han", "Yuandong Pu", "Boxiang Qiu", "Sayak Paul", "Yue Liao", "Yihao Liu", "Jie Shao", "Xi Chen", "Si Liu", "Hongsheng Li"], "title": "Factuality Matters: When Image Generation and Editing Meet Structured Visuals", "comment": "Project page: https://structvisuals.github.io", "summary": "While modern visual generation models excel at creating aesthetically\npleasing natural images, they struggle with producing or editing structured\nvisuals like charts, diagrams, and mathematical figures, which demand\ncomposition planning, text rendering, and multimodal reasoning for factual\nfidelity. To address this, we present the first comprehensive, systematic\ninvestigation of this domain, encompassing data construction, model training,\nand an evaluation benchmark. First, we construct a large-scale dataset of 1.3\nmillion high-quality structured image pairs derived from executable drawing\nprograms and augmented with chain-of-thought reasoning annotations. Building on\nit, we train a unified model that integrates a VLM with FLUX.1 Kontext via a\nlightweight connector for enhanced multimodal understanding. A three-stage\ntraining curriculum enables progressive feature alignment, knowledge infusion,\nand reasoning-augmented generation, further boosted by an external reasoner at\ninference time. Finally, we introduce StructBench, a novel benchmark for\ngeneration and editing with over 1,700 challenging instances, and an\naccompanying evaluation metric, StructScore, which employs a multi-round Q\\&A\nprotocol to assess fine-grained factual accuracy. Evaluations of 15 models\nreveal that even leading closed-source systems remain far from satisfactory.\nOur model attains strong editing performance, and inference-time reasoning\nyields consistent gains across diverse architectures. By releasing the dataset,\nmodel, and benchmark, we aim to advance unified multimodal foundations for\nstructured visuals.", "AI": {"tldr": "This paper addresses the challenge of generating and editing structured visuals like charts and diagrams using AI models. It introduces a comprehensive approach including a large dataset, unified model training, and a new benchmark for evaluation.", "motivation": "Modern visual generation models struggle with structured visuals that require composition planning, text rendering, and multimodal reasoning for factual accuracy. There's a need for specialized approaches to handle charts, diagrams, and mathematical figures.", "method": "Constructed a 1.3M dataset from executable drawing programs with chain-of-thought annotations. Trained a unified model combining VLM with FLUX.1 Kontext via lightweight connector using three-stage curriculum training. Introduced StructBench benchmark with 1,700+ instances and StructScore metric using multi-round Q&A.", "result": "Evaluations of 15 models show even leading closed-source systems perform poorly on structured visuals. Their model achieves strong editing performance, with inference-time reasoning providing consistent improvements across architectures.", "conclusion": "The paper establishes foundations for structured visual generation by releasing dataset, model, and benchmark, enabling advancement in multimodal reasoning for structured visuals."}}
{"id": "2510.05093", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.05093", "abs": "https://arxiv.org/abs/2510.05093", "authors": ["Tingting Liao", "Chongjian Ge", "Guangyi Liu", "Hao Li", "Yi Zhou"], "title": "Character Mixing for Video Generation", "comment": null, "summary": "Imagine Mr. Bean stepping into Tom and Jerry--can we generate videos where\ncharacters interact naturally across different worlds? We study inter-character\ninteraction in text-to-video generation, where the key challenge is to preserve\neach character's identity and behaviors while enabling coherent cross-context\ninteraction. This is difficult because characters may never have coexisted and\nbecause mixing styles often causes style delusion, where realistic characters\nappear cartoonish or vice versa. We introduce a framework that tackles these\nissues with Cross-Character Embedding (CCE), which learns identity and\nbehavioral logic across multimodal sources, and Cross-Character Augmentation\n(CCA), which enriches training with synthetic co-existence and mixed-style\ndata. Together, these techniques allow natural interactions between previously\nuncoexistent characters without losing stylistic fidelity. Experiments on a\ncurated benchmark of cartoons and live-action series with 10 characters show\nclear improvements in identity preservation, interaction quality, and\nrobustness to style delusion, enabling new forms of generative\nstorytelling.Additional results and videos are available on our project page:\nhttps://tingtingliao.github.io/mimix/.", "AI": {"tldr": "A framework for generating videos where characters from different worlds naturally interact while preserving their original identities and styles, using Cross-Character Embedding and Cross-Character Augmentation techniques.", "motivation": "To enable natural interactions between characters from different contexts (like Mr. Bean and Tom & Jerry) while maintaining their original identities and avoiding style delusion issues where realistic characters appear cartoonish or vice versa.", "method": "Uses Cross-Character Embedding (CCE) to learn identity and behavioral logic across multimodal sources, and Cross-Character Augmentation (CCA) to enrich training with synthetic co-existence and mixed-style data.", "result": "Experiments on a curated benchmark with 10 characters from cartoons and live-action series show clear improvements in identity preservation, interaction quality, and robustness to style delusion.", "conclusion": "The framework enables new forms of generative storytelling by allowing natural interactions between previously uncoexistent characters without losing stylistic fidelity."}}
{"id": "2510.05094", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.05094", "abs": "https://arxiv.org/abs/2510.05094", "authors": ["Ziqi Huang", "Ning Yu", "Gordon Chen", "Haonan Qiu", "Paul Debevec", "Ziwei Liu"], "title": "VChain: Chain-of-Visual-Thought for Reasoning in Video Generation", "comment": "Project page: https://eyeline-labs.github.io/VChain Code:\n  https://github.com/Eyeline-Labs/VChain", "summary": "Recent video generation models can produce smooth and visually appealing\nclips, but they often struggle to synthesize complex dynamics with a coherent\nchain of consequences. Accurately modeling visual outcomes and state\ntransitions over time remains a core challenge. In contrast, large language and\nmultimodal models (e.g., GPT-4o) exhibit strong visual state reasoning and\nfuture prediction capabilities. To bridge these strengths, we introduce VChain,\na novel inference-time chain-of-visual-thought framework that injects visual\nreasoning signals from multimodal models into video generation. Specifically,\nVChain contains a dedicated pipeline that leverages large multimodal models to\ngenerate a sparse set of critical keyframes as snapshots, which are then used\nto guide the sparse inference-time tuning of a pre-trained video generator only\nat these key moments. Our approach is tuning-efficient, introduces minimal\noverhead and avoids dense supervision. Extensive experiments on complex,\nmulti-step scenarios show that VChain significantly enhances the quality of\ngenerated videos.", "AI": {"tldr": "VChain is a novel framework that uses multimodal models to generate keyframes for guiding video generation, improving complex dynamic synthesis with minimal tuning overhead.", "motivation": "Current video generation models struggle with complex dynamics and coherent state transitions, while multimodal models excel at visual reasoning and future prediction.", "method": "VChain leverages large multimodal models to generate sparse critical keyframes, then uses these to guide sparse inference-time tuning of pre-trained video generators only at key moments.", "result": "Extensive experiments show VChain significantly enhances video quality in complex, multi-step scenarios.", "conclusion": "VChain effectively bridges multimodal models' reasoning capabilities with video generation, achieving better dynamic synthesis with tuning efficiency."}}
