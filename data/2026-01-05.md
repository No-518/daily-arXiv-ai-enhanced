<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 68]
- [cs.RO](#cs.RO) [Total: 17]
- [cs.AI](#cs.AI) [Total: 24]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [TeleWorld: Towards Dynamic Multimodal Synthesis with a 4D World Model](https://arxiv.org/abs/2601.00051)
*Yabo Chen,Yuanzhi Liang,Jiepeng Wang,Tingxi Chen,Junfei Cheng,Zixiao Gu,Yuyang Huang,Zicheng Jiang,Wei Li,Tian Li,Weichen Li,Zuoxin Li,Guangce Liu,Jialun Liu,Junqi Liu,Haoyuan Wang,Qizhen Weng,Xuan'er Wu,Xunzhi Xiang,Xiaoyan Yang,Xin Zhang,Shiwen Zhang,Junyu Zhou,Chengcheng Zhou,Haibin Huang,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: TeleWorld is a real-time multimodal 4D world modeling framework that unifies video generation, dynamic scene reconstruction, and long-term memory in a closed-loop system for interactive world modeling.


<details>
  <summary>Details</summary>
Motivation: Current video generation models lack real-time interaction, long-horizon consistency, and persistent memory of dynamic scenes, limiting their evolution into practical world models for AI systems.

Method: Introduces a generation-reconstruction-guidance paradigm with continuous reconstruction of generated videos into dynamic 4D spatio-temporal representations. Uses autoregressive diffusion-based video model enhanced with Macro-from-Micro Planning (MMPL) for hierarchical planning and Distribution Matching Distillation (DMD) for real-time synthesis.

Result: Achieves strong performance in static/dynamic world understanding, long-term consistency, and real-time generation efficiency. Enables seamless integration of dynamic object modeling and static scene representation within unified 4D framework.

Conclusion: TeleWorld represents a practical step toward interactive, memory-enabled world models for multimodal generation and embodied intelligence, advancing world models toward computationally accessible systems.

Abstract: World models aim to endow AI systems with the ability to represent, generate, and interact with dynamic environments in a coherent and temporally consistent manner. While recent video generation models have demonstrated impressive visual quality, they remain limited in real-time interaction, long-horizon consistency, and persistent memory of dynamic scenes, hindering their evolution into practical world models. In this report, we present TeleWorld, a real-time multimodal 4D world modeling framework that unifies video generation, dynamic scene reconstruction, and long-term world memory within a closed-loop system. TeleWorld introduces a novel generation-reconstruction-guidance paradigm, where generated video streams are continuously reconstructed into a dynamic 4D spatio-temporal representation, which in turn guides subsequent generation to maintain spatial, temporal, and physical consistency. To support long-horizon generation with low latency, we employ an autoregressive diffusion-based video model enhanced with Macro-from-Micro Planning (MMPL)--a hierarchical planning method that reduces error accumulation from frame-level to segment-level-alongside efficient Distribution Matching Distillation (DMD), enabling real-time synthesis under practical computational budgets. Our approach achieves seamless integration of dynamic object modeling and static scene representation within a unified 4D framework, advancing world models toward practical, interactive, and computationally accessible systems. Extensive experiments demonstrate that TeleWorld achieves strong performance in both static and dynamic world understanding, long-term consistency, and real-time generation efficiency, positioning it as a practical step toward interactive, memory-enabled world models for multimodal generation and embodied intelligence.

</details>


### [2] [It's Never Too Late: Noise Optimization for Collapse Recovery in Trained Diffusion Models](https://arxiv.org/abs/2601.00090)
*Anne Harrington,A. Sophia Koepke,Shyamgopal Karthik,Trevor Darrell,Alexei A. Efros*

Main category: cs.CV

TL;DR: Simple noise optimization mitigates mode collapse in text-to-image models while preserving fidelity, with frequency-aware noise initialization improving results.


<details>
  <summary>Details</summary>
Motivation: Text-to-image models suffer from mode collapse where multiple generations from the same prompt produce similar images, limiting diversity despite guidance mechanisms and candidate refinement approaches.

Method: Proposes noise optimization objective to increase diversity while maintaining model fidelity, analyzes frequency characteristics of noise, and explores alternative noise initializations with different frequency profiles.

Result: Noise optimization yields superior generation quality and variety compared to existing approaches, with frequency-aware noise initialization improving both optimization efficiency and search effectiveness.

Conclusion: Simple noise optimization effectively addresses mode collapse in text-to-image generation, offering a different direction from guidance-based or refinement-based methods, with frequency analysis providing additional optimization benefits.

Abstract: Contemporary text-to-image models exhibit a surprising degree of mode collapse, as can be seen when sampling several images given the same text prompt. While previous work has attempted to address this issue by steering the model using guidance mechanisms, or by generating a large pool of candidates and refining them, in this work we take a different direction and aim for diversity in generations via noise optimization. Specifically, we show that a simple noise optimization objective can mitigate mode collapse while preserving the fidelity of the base model. We also analyze the frequency characteristics of the noise and show that alternative noise initializations with different frequency profiles can improve both optimization and search. Our experiments demonstrate that noise optimization yields superior results in terms of generation quality and variety.

</details>


### [3] [Spatial4D-Bench: A Versatile 4D Spatial Intelligence Benchmark](https://arxiv.org/abs/2601.00092)
*Pan Wang,Yang Liu,Guile Wu,Eduardo R. Corral-Soto,Chengjie Huang,Binbin Xu,Dongfeng Bai,Xu Yan,Yuan Ren,Xingxin Chen,Yizhe Wu,Tao Huang,Wenjun Wan,Xin Wu,Pei Zhou,Xuyang Dai,Kangbo Lv,Hongbo Zhang,Yosef Fried,Aixue Ye,Bailan Feng,Zhenyu Chen,Zhen Li,Yingcong Chen,Yiyi Liao,Bingbing Liu*

Main category: cs.CV

TL;DR: Spatial4D-Bench is a large-scale benchmark with ~40K questions across 18 tasks to evaluate MLLMs' 4D spatial intelligence, revealing current models' limitations in spatiotemporal reasoning.


<details>
  <summary>Details</summary>
Motivation: To assess whether Multimodal Large Language Models can achieve human-level 4D spatial intelligence (perceiving object changes over time), which is currently lacking in existing benchmarks that are small-scale or limited in diversity.

Method: Created Spatial4D-Bench, a comprehensive benchmark with ~40,000 question-answer pairs covering 18 tasks organized into six cognitive categories: object understanding, scene understanding, spatial relationships, spatiotemporal relationships, spatial reasoning, and spatiotemporal reasoning.

Result: Benchmarking various state-of-the-art MLLMs revealed substantial limitations in 4D spatial reasoning abilities across tasks like route planning, action recognition, and physical plausibility reasoning.

Conclusion: Current MLLMs have significant gaps in 4D spatial intelligence compared to humans, and Spatial4D-Bench provides a structured framework to guide development toward human-level spatiotemporal reasoning capabilities.

Abstract: 4D spatial intelligence involves perceiving and processing how objects move or change over time. Humans naturally possess 4D spatial intelligence, supporting a broad spectrum of spatial reasoning abilities. To what extent can Multimodal Large Language Models (MLLMs) achieve human-level 4D spatial intelligence? In this work, we present Spatial4D-Bench, a versatile 4D spatial intelligence benchmark designed to comprehensively assess the 4D spatial reasoning abilities of MLLMs. Unlike existing spatial intelligence benchmarks that are often small-scale or limited in diversity, Spatial4D-Bench provides a large-scale, multi-task evaluation benchmark consisting of ~40,000 question-answer pairs covering 18 well-defined tasks. We systematically organize these tasks into six cognitive categories: object understanding, scene understanding, spatial relationship understanding, spatiotemporal relationship understanding, spatial reasoning and spatiotemporal reasoning. Spatial4D-Bench thereby offers a structured and comprehensive benchmark for evaluating the spatial cognition abilities of MLLMs, covering a broad spectrum of tasks that parallel the versatility of human spatial intelligence. We benchmark various state-of-the-art open-source and proprietary MLLMs on Spatial4D-Bench and reveal their substantial limitations in a wide variety of 4D spatial reasoning aspects, such as route plan, action recognition, and physical plausibility reasoning. We hope that the findings provided in this work offer valuable insights to the community and that our benchmark can facilitate the development of more capable MLLMs toward human-level 4D spatial intelligence. More resources can be found on our project page.

</details>


### [4] [A Spatially Masked Adaptive Gated Network for multimodal post-flood water extent mapping using SAR and incomplete multispectral data](https://arxiv.org/abs/2601.00123)
*Hyunho Lee,Wenwen Li*

Main category: cs.CV

TL;DR: SMAGNet is a multimodal deep learning model that uses SAR as primary input and adaptively integrates MSI data for improved flood water extent mapping, maintaining performance even when MSI data is missing.


<details>
  <summary>Details</summary>
Motivation: Current flood mapping relies heavily on SAR data, but multimodal approaches combining SAR and MSI data show promise. However, adaptive integration of partially available MSI data into SAR-based flood mapping remains underexplored, especially for real-world scenarios where timely post-flood observations may be limited.

Method: Proposed Spatially Masked Adaptive Gated Network (SMAGNet) - a multimodal deep learning model that uses SAR data as primary input for post-flood water extent mapping and integrates complementary MSI data through feature fusion. The model is designed to handle varying levels of MSI data availability.

Result: SMAGNet consistently outperformed other multimodal deep learning models on the C2S-MS Floods dataset across varying MSI data availability levels. Even when MSI data were completely missing, SMAGNet's performance remained statistically comparable to a U-Net model trained solely on SAR data.

Conclusion: SMAGNet enhances model robustness to missing data and improves the applicability of multimodal deep learning in real-world flood management scenarios by adaptively integrating available MSI data while maintaining performance when such data is unavailable.

Abstract: Mapping water extent during a flood event is essential for effective disaster management throughout all phases: mitigation, preparedness, response, and recovery. In particular, during the response stage, when timely and accurate information is important, Synthetic Aperture Radar (SAR) data are primarily employed to produce water extent maps. Recently, leveraging the complementary characteristics of SAR and MSI data through a multimodal approach has emerged as a promising strategy for advancing water extent mapping using deep learning models. This approach is particularly beneficial when timely post-flood observations, acquired during or shortly after the flood peak, are limited, as it enables the use of all available imagery for more accurate post-flood water extent mapping. However, the adaptive integration of partially available MSI data into the SAR-based post-flood water extent mapping process remains underexplored. To bridge this research gap, we propose the Spatially Masked Adaptive Gated Network (SMAGNet), a multimodal deep learning model that utilizes SAR data as the primary input for post-flood water extent mapping and integrates complementary MSI data through feature fusion. In experiments on the C2S-MS Floods dataset, SMAGNet consistently outperformed other multimodal deep learning models in prediction performance across varying levels of MSI data availability. Furthermore, we found that even when MSI data were completely missing, the performance of SMAGNet remained statistically comparable to that of a U-Net model trained solely on SAR data. These findings indicate that SMAGNet enhances the model robustness to missing data as well as the applicability of multimodal deep learning in real-world flood management scenarios.

</details>


### [5] [Compressed Map Priors for 3D Perception](https://arxiv.org/abs/2601.00139)
*Brady Zhou,Philipp Krähenbühl*

Main category: cs.CV

TL;DR: CMP is a framework that learns spatial priors from historical vehicle traversals using compressed binarized hashmaps, improving 3D object detection with minimal storage and computational overhead.


<details>
  <summary>Details</summary>
Motivation: Most autonomous vision systems ignore historical traversal data and treat each location as if encountered for the first time, despite the fact that deployment areas are typically repeatedly visited.

Method: Compressed Map Priors (CMP) learns spatial priors from historic traversals using binarized hashmaps that require only 32KB/km², representing a 20× reduction compared to dense storage.

Result: CMP integrates easily into leading 3D perception systems with minimal computational cost and achieves significant, consistent improvements in 3D object detection on the nuScenes dataset across multiple architectures.

Conclusion: Leveraging compressed historical traversal data as spatial priors is an effective, storage-efficient approach to enhance autonomous vehicle perception systems without substantial computational overhead.

Abstract: Human drivers rarely travel where no person has gone before. After all, thousands of drivers use busy city roads every day, and only one can claim to be the first. The same holds for autonomous computer vision systems. The vast majority of the deployment area of an autonomous vision system will have been visited before. Yet, most autonomous vehicle vision systems act as if they are encountering each location for the first time. In this work, we present Compressed Map Priors (CMP), a simple but effective framework to learn spatial priors from historic traversals. The map priors use a binarized hashmap that requires only $32\text{KB}/\text{km}^2$, a $20\times$ reduction compared to the dense storage. Compressed Map Priors easily integrate into leading 3D perception systems at little to no extra computational costs, and lead to a significant and consistent improvement in 3D object detection on the nuScenes dataset across several architectures.

</details>


### [6] [Attention to Detail: Global-Local Attention for High-Resolution AI-Generated Image Detection](https://arxiv.org/abs/2601.00141)
*Lawrence Han*

Main category: cs.CV

TL;DR: GLASS is a novel architecture for AI-generated image detection that combines global resized views with multiple original-resolution local crops using attention-based aggregation, outperforming standard methods while maintaining computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Most AI-generated image detection methods downsample images, losing fine-grained details that could be crucial for distinguishing AI-generated content from real images. As AI-generated images become more realistic and high-resolution, there's a need for detection methods that preserve these important details.

Method: GLASS combines a globally resized view with multiple randomly sampled local crops at original resolution using spatially stratified sampling. These crops are efficiently aggregated using attention-based scoring. The architecture can be integrated into various vision backbones (Vision Transformer, ResNet, ConvNeXt) to leverage both global and local information from images of any size.

Result: Experiments show that GLASS outperforms standard transfer learning approaches by achieving higher predictive performance while maintaining feasible computational constraints across different backbone architectures.

Conclusion: The GLASS architecture effectively addresses the limitation of information loss in downsampled images for AI-generated image detection by combining global context with fine-grained local details, resulting in improved detection performance without prohibitive computational costs.

Abstract: The rapid development of generative AI has made AI-generated images increasingly realistic and high-resolution. Most AI-generated image detection architectures typically downsample images before inputting them into models, risking the loss of fine-grained details. This paper presents GLASS (Global-Local Attention with Stratified Sampling), an architecture that combines a globally resized view with multiple randomly sampled local crops. These crops are original-resolution regions efficiently selected through spatially stratified sampling and aggregated using attention-based scoring. GLASS can be integrated into vision models to leverage both global and local information in images of any size. Vision Transformer, ResNet, and ConvNeXt models are used as backbones, and experiments show that GLASS outperforms standard transfer learning by achieving higher predictive performance within feasible computational constraints.

</details>


### [7] [FCMBench: A Comprehensive Financial Credit Multimodal Benchmark for Real-world Applications](https://arxiv.org/abs/2601.00150)
*Yehui Yang,Dalu Yang,Wenshuo Zhou,Fangxin Shang,Yifan Liu,Jie Ren,Haojun Fei,Qing Yang,Tao Chen*

Main category: cs.CV

TL;DR: FCMBench-V1.0 is a financial credit multimodal benchmark with 4,043 privacy-compliant images and 8,446 QA samples covering 18 certificate types, designed to evaluate VLMs on perception, reasoning, and robustness for credit applications.


<details>
  <summary>Details</summary>
Motivation: There's an urgent need for a domain-specific benchmark for financial credit applications that reflects real documents/workflows, includes credit-specific understanding and robustness testing, and preserves privacy compliance while maintaining practical utility.

Method: Created FCMBench-V1.0 using a closed synthesis-capture pipeline: manually synthesized document templates with virtual content and captured scenario-aware images in-house to ensure privacy compliance and avoid web-sourced data leakage.

Result: Evaluated 23 state-of-the-art VLMs; Gemini 3 Pro scored best among commercial models (64.61 F1), Qwen3-VL-235B best among open-source (57.27), and their financial-specific model Qfin-VL-Instruct achieved top overall score (64.92). Robustness tests showed performance drops under acquisition artifacts.

Conclusion: FCMBench effectively discriminates performance disparities and robustness across VLMs, addressing the gap in financial credit multimodal evaluation while maintaining privacy compliance through synthetic data generation.

Abstract: As multimodal AI becomes widely used for credit risk assessment and document review, a domain-specific benchmark is urgently needed that (1) reflects documents and workflows specific to financial credit applications, (2) includes credit-specific understanding and real-world robustness, and (3) preserves privacy compliance without sacrificing practical utility. Here, we introduce FCMBench-V1.0 -- a large-scale financial credit multimodal benchmark for real-world applications, covering 18 core certificate types, with 4,043 privacy-compliant images and 8,446 QA samples. The FCMBench evaluation framework consists of three dimensions: Perception, Reasoning, and Robustness, including 3 foundational perception tasks, 4 credit-specific reasoning tasks that require decision-oriented understanding of visual evidence, and 10 real-world acquisition artifact types for robustness stress testing. To reconcile compliance with realism, we construct all samples via a closed synthesis-capture pipeline: we manually synthesize document templates with virtual content and capture scenario-aware images in-house. This design also mitigates pre-training data leakage by avoiding web-sourced or publicly released images. FCMBench can effectively discriminate performance disparities and robustness across modern vision-language models. Extensive experiments were conducted on 23 state-of-the-art vision-language models (VLMs) from 14 top AI companies and research institutes. Among them, Gemini 3 Pro achieves the best F1(\%) score as a commercial model (64.61), Qwen3-VL-235B achieves the best score as an open-source baseline (57.27), and our financial credit-specific model, Qfin-VL-Instruct, achieves the top overall score (64.92). Robustness evaluations show that even top-performing models suffer noticeable performance drops under acquisition artifacts.

</details>


### [8] [Focal-RegionFace: Generating Fine-Grained Multi-attribute Descriptions for Arbitrarily Selected Face Focal Regions](https://arxiv.org/abs/2601.00156)
*Kaiwen Zheng,Junchen Fu,Songpei Xu,Yaoqing He,Joemon M. Jose,Han Hu,Xuri Ge*

Main category: cs.CV

TL;DR: Focal-RegionFace: A vision-language model for generating and recognizing multi-attribute natural language descriptions of arbitrarily selected face regions, including facial action units, emotions, and age estimation.


<details>
  <summary>Details</summary>
Motivation: The paper addresses an underexplored problem in facial analysis - generating and recognizing multi-attribute natural language descriptions for arbitrarily selected face regions. The authors argue that focusing on individual facial areas leads to better understanding and control in facial analysis systems.

Method: Created a new multi-attribute description dataset for arbitrarily selected face regions with rich region-level annotations and natural language descriptions. Proposed Focal-RegionFace, a fine-tuned vision-language model based on Qwen2.5-VL, which incrementally refines its focus on localized facial features through multiple progressively fine-tuning stages for interpretable age estimation, FAU and emotion detection.

Result: Focal-RegionFace achieves the best performance on the new benchmark in terms of both traditional metrics and newly proposed metrics, fully verifying its effectiveness and versatility in fine-grained multi-attribute face region-focal analysis scenarios.

Conclusion: The proposed approach successfully addresses the underexplored problem of multi-attribute natural language description generation and recognition for arbitrarily selected face regions, demonstrating improved understanding and control through region-focused facial analysis.

Abstract: In this paper, we introduce an underexplored problem in facial analysis: generating and recognizing multi-attribute natural language descriptions, containing facial action units (AUs), emotional states, and age estimation, for arbitrarily selected face regions (termed FaceFocalDesc). We argue that the system's ability to focus on individual facial areas leads to better understanding and control. To achieve this capability, we construct a new multi-attribute description dataset for arbitrarily selected face regions, providing rich region-level annotations and natural language descriptions. Further, we propose a fine-tuned vision-language model based on Qwen2.5-VL, called Focal-RegionFace for facial state analysis, which incrementally refines its focus on localized facial features through multiple progressively fine-tuning stages, resulting in interpretable age estimation, FAU and emotion detection. Experimental results show that Focal-RegionFace achieves the best performance on the new benchmark in terms of traditional and widely used metrics, as well as new proposed metrics. This fully verifies its effectiveness and versatility in fine-grained multi-attribute face region-focal analysis scenarios.

</details>


### [9] [DichroGAN: Towards Restoration of in-air Colours of Seafloor from Satellite Imagery](https://arxiv.org/abs/2601.00194)
*Salma Gonzalez-Sabbagh,Antonio Robles-Kelly,Shang Gao*

Main category: cs.CV

TL;DR: DichroGAN is a cGAN that recovers seafloor colors from satellite imagery by modeling atmospheric radiance and underwater light transmission to remove water column effects.


<details>
  <summary>Details</summary>
Motivation: Recovering accurate in-air colors of seafloor from satellite imagery is difficult due to light attenuation in water column, requiring specialized restoration techniques.

Method: Two-step cGAN with four generators: two estimate diffuse/specular reflections for atmospheric radiance, third processes spectral features, fourth estimates underwater light transmission based on underwater image formation equation.

Result: DichroGAN achieves competitive performance compared to state-of-the-art underwater restoration techniques on both satellite and underwater datasets.

Conclusion: The proposed DichroGAN effectively removes water column effects to restore seafloor colors using a compact PRISMA satellite dataset and demonstrates strong restoration capabilities.

Abstract: Recovering the in-air colours of seafloor from satellite imagery is a challenging task due to the exponential attenuation of light with depth in the water column. In this study, we present DichroGAN, a conditional generative adversarial network (cGAN) designed for this purpose. DichroGAN employs a two-steps simultaneous training: first, two generators utilise a hyperspectral image cube to estimate diffuse and specular reflections, thereby obtaining atmospheric scene radiance. Next, a third generator receives as input the generated scene radiance containing the features of each spectral band, while a fourth generator estimates the underwater light transmission. These generators work together to remove the effects of light absorption and scattering, restoring the in-air colours of seafloor based on the underwater image formation equation. DichroGAN is trained on a compact dataset derived from PRISMA satellite imagery, comprising RGB images paired with their corresponding spectral bands and masks. Extensive experiments on both satellite and underwater datasets demonstrate that DichroGAN achieves competitive performance compared to state-of-the-art underwater restoration techniques.

</details>


### [10] [MorphAny3D: Unleashing the Power of Structured Latent in 3D Morphing](https://arxiv.org/abs/2601.00204)
*Xiaokun Sun,Zeyu Cai,Hao Tang,Ying Tai,Jian Yang,Zhenyu Zhang*

Main category: cs.CV

TL;DR: MorphAny3D is a training-free framework for high-quality 3D morphing using Structured Latent (SLAT) representations, achieving state-of-the-art results even for cross-category morphing.


<details>
  <summary>Details</summary>
Motivation: 3D morphing is challenging due to difficulties in generating semantically consistent and temporally smooth deformations, especially across different object categories.

Method: Uses Structured Latent (SLAT) representations with Morphing Cross-Attention (MCA) for structural coherence, Temporal-Fused Self-Attention (TFSA) for temporal consistency, and orientation correction for pose ambiguity.

Result: Generates state-of-the-art morphing sequences, even for challenging cross-category cases, and supports advanced applications like decoupled morphing and 3D style transfer.

Conclusion: MorphAny3D provides an effective training-free framework for high-quality 3D morphing that can generalize to other SLAT-based generative models.

Abstract: 3D morphing remains challenging due to the difficulty of generating semantically consistent and temporally smooth deformations, especially across categories. We present MorphAny3D, a training-free framework that leverages Structured Latent (SLAT) representations for high-quality 3D morphing. Our key insight is that intelligently blending source and target SLAT features within the attention mechanisms of 3D generators naturally produces plausible morphing sequences. To this end, we introduce Morphing Cross-Attention (MCA), which fuses source and target information for structural coherence, and Temporal-Fused Self-Attention (TFSA), which enhances temporal consistency by incorporating features from preceding frames. An orientation correction strategy further mitigates the pose ambiguity within the morphing steps. Extensive experiments show that our method generates state-of-the-art morphing sequences, even for challenging cross-category cases. MorphAny3D further supports advanced applications such as decoupled morphing and 3D style transfer, and can be generalized to other SLAT-based generative models. Project page: https://xiaokunsun.github.io/MorphAny3D.github.io/.

</details>


### [11] [CropNeRF: A Neural Radiance Field-Based Framework for Crop Counting](https://arxiv.org/abs/2601.00207)
*Md Ahmed Al Muzaddid,William J. Beksi*

Main category: cs.CV

TL;DR: Novel 3D instance segmentation framework for accurate crop counting using multi-view images and NeRF, eliminating crop-specific parameter tuning and achieving superior performance across diverse crops.


<details>
  <summary>Details</summary>
Motivation: Partial occlusions and ambiguity in distinguishing clustered crops in outdoor field environments pose immense challenges for image-based segmentation methods, making rigorous crop counting difficult for agricultural management.

Method: Uses 2D images from multiple viewpoints, associates independent instance masks with neural radiance field (NeRF) view synthesis, introduces crop visibility and mask consistency scores, and incorporates 3D information from NeRF for effective 3D instance segmentation.

Result: Validated on three agricultural datasets (cotton bolls, apples, pears) with consistent counting performance despite major variations in crop color, shape, and size. Superior performance compared to state-of-the-art methods, and contributed a cotton plant dataset.

Conclusion: The framework enables highly-accurate crop counting via 3D instance segmentation, eliminates dependence on crop-specific parameter tuning, and advances agricultural management through reliable crop enumeration.

Abstract: Rigorous crop counting is crucial for effective agricultural management and informed intervention strategies. However, in outdoor field environments, partial occlusions combined with inherent ambiguity in distinguishing clustered crops from individual viewpoints poses an immense challenge for image-based segmentation methods. To address these problems, we introduce a novel crop counting framework designed for exact enumeration via 3D instance segmentation. Our approach utilizes 2D images captured from multiple viewpoints and associates independent instance masks for neural radiance field (NeRF) view synthesis. We introduce crop visibility and mask consistency scores, which are incorporated alongside 3D information from a NeRF model. This results in an effective segmentation of crop instances in 3D and highly-accurate crop counts. Furthermore, our method eliminates the dependence on crop-specific parameter tuning. We validate our framework on three agricultural datasets consisting of cotton bolls, apples, and pears, and demonstrate consistent counting performance despite major variations in crop color, shape, and size. A comparative analysis against the state of the art highlights superior performance on crop counting tasks. Lastly, we contribute a cotton plant dataset to advance further research on this topic.

</details>


### [12] [IntraStyler: Exemplar-based Style Synthesis for Cross-modality Domain Adaptation](https://arxiv.org/abs/2601.00212)
*Han Liu,Yubo Fan,Hao Li,Dewei Hu,Daniel Moyer,Zhoubing Xu,Benoit M. Dawant,Ipek Oguz*

Main category: cs.CV

TL;DR: IntraStyler: Exemplar-based style synthesis method for unsupervised domain adaptation that captures diverse intra-domain styles without prior knowledge to improve downstream segmentation.


<details>
  <summary>Details</summary>
Motivation: Prior domain adaptation methods focus on inter-domain shift but neglect intra-domain variability. Existing style diversification methods require pre-specified intra-domain variations, which is impractical.

Method: Proposes IntraStyler with exemplar-based style synthesis using a style encoder trained with contrastive learning to extract style-only features, enabling controllable style matching to exemplar images.

Result: Evaluated on CrossMoDA 2023 dataset, showing efficacy in controllable style synthesis and benefits of diverse synthetic data for downstream segmentation tasks.

Conclusion: IntraStyler effectively captures diverse intra-domain styles without prior knowledge, improving domain adaptation performance through enhanced style diversification in synthetic data generation.

Abstract: Image-level domain alignment is the de facto approach for unsupervised domain adaptation, where unpaired image translation is used to minimize the domain gap. Prior studies mainly focus on the domain shift between the source and target domains, whereas the intra-domain variability remains under-explored. To address the latter, an effective strategy is to diversify the styles of the synthetic target domain data during image translation. However, previous methods typically require intra-domain variations to be pre-specified for style synthesis, which may be impractical. In this paper, we propose an exemplar-based style synthesis method named IntraStyler, which can capture diverse intra-domain styles without any prior knowledge. Specifically, IntraStyler uses an exemplar image to guide the style synthesis such that the output style matches the exemplar style. To extract the style-only features, we introduce a style encoder to learn styles discriminatively based on contrastive learning. We evaluate the proposed method on the largest public dataset for cross-modality domain adaptation, CrossMoDA 2023. Our experiments show the efficacy of our method in controllable style synthesis and the benefits of diverse synthetic data for downstream segmentation. Code is available at https://github.com/han-liu/IntraStyler.

</details>


### [13] [From Sight to Insight: Improving Visual Reasoning Capabilities of Multimodal Models via Reinforcement Learning](https://arxiv.org/abs/2601.00215)
*Omar Sharif,Eftekhar Hossain,Patrick Ng*

Main category: cs.CV

TL;DR: RL-driven approach improves multimodal LLMs' visual reasoning by incentivizing longer, structured reasoning chains that better integrate visual information, achieving 5.56% gains on Qwen-2.5-VL-7B.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs generate reasoning chains that lack proper integration of visual information, limiting their ability to solve visual perception tasks like puzzles. Visual perception is identified as the key bottleneck, with textual image descriptions improving performance significantly (26.7% for Claude 3.5, 23.6% for Claude 3.7).

Method: Uses reward-driven reinforcement learning with six reward functions targeting different reasoning aspects (image understanding, thinking steps, answer accuracy). Employs group relative policy optimization (GRPO) to explicitly incentivize longer, structured reasoning and prevent bypassing of visual information.

Result: Achieves 5.56% improvement over base model on Qwen-2.5-VL-7B, with consistent gains across both in-domain and out-of-domain settings. Shows that RL can unlock long visual reasoning in open-source MLLMs without costly supervision.

Conclusion: Reward-driven RL effectively addresses visual perception bottlenecks in MLLMs by promoting better integration of visual information into reasoning chains, enabling improved performance on visual reasoning tasks without expensive supervision.

Abstract: Reinforcement learning (RL) has emerged as a promising approach for eliciting reasoning chains before generating final answers. However, multimodal large language models (MLLMs) generate reasoning that lacks integration of visual information. This limits their ability to solve problems that demand accurate visual perception, such as visual puzzles. We show that visual perception is the key bottleneck in such tasks: converting images into textual descriptions significantly improves performance, yielding gains of 26.7% for Claude 3.5 and 23.6% for Claude 3.7.
  To address this, we investigate reward-driven RL as a mechanism to unlock long visual reasoning in open-source MLLMs without requiring costly supervision. We design and evaluate six reward functions targeting different reasoning aspects, including image understanding, thinking steps, and answer accuracy. Using group relative policy optimization (GRPO), our approach explicitly incentivizes longer, structured reasoning and mitigates bypassing of visual information. Experiments on Qwen-2.5-VL-7B achieve 5.56% improvements over the base model, with consistent gains across both in-domain and out-of-domain settings.

</details>


### [14] [LooC: Effective Low-Dimensional Codebook for Compositional Vector Quantization](https://arxiv.org/abs/2601.00222)
*Jie Li,Kwan-Yee K. Wong,Kai Han*

Main category: cs.CV

TL;DR: LooC introduces a low-dimensional compositional vector quantization method that uses compact codebooks by treating codevectors as compositional units within feature vectors, achieving SOTA performance with significantly smaller codebooks.


<details>
  <summary>Details</summary>
Motivation: As data and models become more diverse and complex, there's an urgent need for high-capacity yet compact vector quantization methods that can handle increasing demands while maintaining efficiency.

Method: LooC uses low-dimensional codebooks as compositional units within feature vectors, combines them through a parameter-free extrapolation-by-interpolation mechanism, and serves as a plug-and-play module for existing VQ-based methods.

Result: Extensive evaluations show LooC outperforms existing VQ methods across different tasks, datasets, and architectures, achieving state-of-the-art performance with significantly smaller codebooks while avoiding codebook collapse.

Conclusion: LooC successfully reconciles the conflict between high capacity and compactness in vector quantization, offering an effective, parameter-efficient solution that can be easily integrated into existing VQ-based systems.

Abstract: Vector quantization (VQ) is a prevalent and fundamental technique that discretizes continuous feature vectors by approximating them using a codebook. As the diversity and complexity of data and models continue to increase, there is an urgent need for high-capacity, yet more compact VQ methods. This paper aims to reconcile this conflict by presenting a new approach called LooC, which utilizes an effective Low-dimensional codebook for Compositional vector quantization. Firstly, LooC introduces a parameter-efficient codebook by reframing the relationship between codevectors and feature vectors, significantly expanding its solution space. Instead of individually matching codevectors with feature vectors, LooC treats them as lower-dimensional compositional units within feature vectors and combines them, resulting in a more compact codebook with improved performance. Secondly, LooC incorporates a parameter-free extrapolation-by-interpolation mechanism to enhance and smooth features during the VQ process, which allows for better preservation of details and fidelity in feature approximation. The design of LooC leads to full codebook usage, effectively utilizing the compact codebook while avoiding the problem of collapse. Thirdly, LooC can serve as a plug-and-play module for existing methods for different downstream tasks based on VQ. Finally, extensive evaluations on different tasks, datasets, and architectures demonstrate that LooC outperforms existing VQ methods, achieving state-of-the-art performance with a significantly smaller codebook.

</details>


### [15] [Towards Syn-to-Real IQA: A Novel Perspective on Reshaping Synthetic Data Distributions](https://arxiv.org/abs/2601.00225)
*Aobo Li,Jinjian Wu,Yongxu Liu,Leida Li,Weisheng Dong*

Main category: cs.CV

TL;DR: SynDR-IQA improves BIQA generalization by reshaping synthetic data distribution through diversity upsampling and redundancy downsampling to address clustered feature patterns in synthetic datasets.


<details>
  <summary>Details</summary>
Motivation: BIQA suffers from limited labeled data; synthetic data helps but models trained on it show poor generalization due to clustered feature patterns (high-quality images cluster around references, low-quality cluster by distortion type).

Method: SynDR-IQA framework reshapes synthetic data distribution using: 1) distribution-aware diverse content upsampling to enhance visual diversity while preserving content distribution, and 2) density-aware redundant cluster downsampling to balance samples by reducing density in clustered areas.

Result: Extensive experiments across three cross-dataset settings (synthetic-to-authentic, synthetic-to-algorithmic, synthetic-to-synthetic) demonstrate effectiveness of the method.

Conclusion: The distribution issue in synthetic datasets hinders BIQA generalization, and SynDR-IQA's data reshaping strategy effectively addresses this problem, improving cross-dataset performance.

Abstract: Blind Image Quality Assessment (BIQA) has advanced significantly through deep learning, but the scarcity of large-scale labeled datasets remains a challenge. While synthetic data offers a promising solution, models trained on existing synthetic datasets often show limited generalization ability. In this work, we make a key observation that representations learned from synthetic datasets often exhibit a discrete and clustered pattern that hinders regression performance: features of high-quality images cluster around reference images, while those of low-quality images cluster based on distortion types. Our analysis reveals that this issue stems from the distribution of synthetic data rather than model architecture. Consequently, we introduce a novel framework SynDR-IQA, which reshapes synthetic data distribution to enhance BIQA generalization. Based on theoretical derivations of sample diversity and redundancy's impact on generalization error, SynDR-IQA employs two strategies: distribution-aware diverse content upsampling, which enhances visual diversity while preserving content distribution, and density-aware redundant cluster downsampling, which balances samples by reducing the density of densely clustered areas. Extensive experiments across three cross-dataset settings (synthetic-to-authentic, synthetic-to-algorithmic, and synthetic-to-synthetic) demonstrate the effectiveness of our method. The code is available at https://github.com/Li-aobo/SynDR-IQA.

</details>


### [16] [Application Research of a Deep Learning Model Integrating CycleGAN and YOLO in PCB Infrared Defect Detection](https://arxiv.org/abs/2601.00237)
*Chao Yang,Haoyuan Zheng,Yue Ma*

Main category: cs.CV

TL;DR: Proposes cross-modal data augmentation using CycleGAN to generate pseudo-IR images from visible-light PCB data, combined with YOLOv8 for defect detection, addressing IR data scarcity in industrial inspection.


<details>
  <summary>Details</summary>
Motivation: Addresses the critical bottleneck of infrared (IR) data scarcity in Printed Circuit Board (PCB) defect detection, where conventional methods suffer from limited real IR samples.

Method: Uses CycleGAN for unpaired image-to-image translation to map abundant visible-light PCB images into infrared domain, creating pseudo-IR samples. Then employs heterogeneous training strategy fusing generated pseudo-IR data with limited real IR samples to train a lightweight YOLOv8 detector.

Result: The method effectively enhances feature learning under low-data conditions. The augmented detector significantly outperforms models trained on limited real data alone and approaches performance benchmarks of fully supervised training.

Conclusion: Proves the efficacy of pseudo-IR synthesis as a robust augmentation strategy for industrial inspection, demonstrating that cross-modal data augmentation can overcome IR data scarcity in PCB defect detection.

Abstract: This paper addresses the critical bottleneck of infrared (IR) data scarcity in Printed Circuit Board (PCB) defect detection by proposing a cross-modal data augmentation framework integrating CycleGAN and YOLOv8. Unlike conventional methods relying on paired supervision, we leverage CycleGAN to perform unpaired image-to-image translation, mapping abundant visible-light PCB images into the infrared domain. This generative process synthesizes high-fidelity pseudo-IR samples that preserve the structural semantics of defects while accurately simulating thermal distribution patterns. Subsequently, we construct a heterogeneous training strategy that fuses generated pseudo-IR data with limited real IR samples to train a lightweight YOLOv8 detector. Experimental results demonstrate that this method effectively enhances feature learning under low-data conditions. The augmented detector significantly outperforms models trained on limited real data alone and approaches the performance benchmarks of fully supervised training, proving the efficacy of pseudo-IR synthesis as a robust augmentation strategy for industrial inspection.

</details>


### [17] [Efficient Prediction of Dense Visual Embeddings via Distillation and RGB-D Transformers](https://arxiv.org/abs/2601.00359)
*Söhnke Benedikt Fischedick,Daniel Seichter,Benedict Stephan,Robin Schmidt,Horst-Michael Gross*

Main category: cs.CV

TL;DR: DVEFormer is an efficient RGB-D Transformer that predicts dense text-aligned visual embeddings via knowledge distillation from Alpha-CLIP, enabling flexible text-based querying and 3D mapping for domestic robots.


<details>
  <summary>Details</summary>
Motivation: Robots in domestic environments need comprehensive scene understanding to interact effectively with untrained humans, requiring more flexible approaches than traditional semantic segmentation with fixed classes.

Method: Uses knowledge distillation where teacher embeddings from Alpha-CLIP guide an efficient student model (DVEFormer) to learn fine-grained pixel-wise embeddings, enabling both semantic segmentation and text-based querying.

Result: Achieves competitive performance on indoor datasets while meeting real-time requirements (26.3 FPS full model, 77.0 FPS smaller variant on NVIDIA Jetson AGX Orin), with qualitative results showing effectiveness in real-world applications.

Conclusion: DVEFormer serves as a drop-in replacement for traditional segmentation while enabling flexible natural-language querying and seamless integration into 3D mapping pipelines for mobile robotics.

Abstract: In domestic environments, robots require a comprehensive understanding of their surroundings to interact effectively and intuitively with untrained humans. In this paper, we propose DVEFormer - an efficient RGB-D Transformer-based approach that predicts dense text-aligned visual embeddings (DVE) via knowledge distillation. Instead of directly performing classical semantic segmentation with fixed predefined classes, our method uses teacher embeddings from Alpha-CLIP to guide our efficient student model DVEFormer in learning fine-grained pixel-wise embeddings. While this approach still enables classical semantic segmentation, e.g., via linear probing, it further enables flexible text-based querying and other applications, such as creating comprehensive 3D maps. Evaluations on common indoor datasets demonstrate that our approach achieves competitive performance while meeting real-time requirements, operating at 26.3 FPS for the full model and 77.0 FPS for a smaller variant on an NVIDIA Jetson AGX Orin. Additionally, we show qualitative results that highlight the effectiveness and possible use cases in real-world applications. Overall, our method serves as a drop-in replacement for traditional segmentation approaches while enabling flexible natural-language querying and seamless integration into 3D mapping pipelines for mobile robotics.

</details>


### [18] [Context-Aware Pesticide Recommendation via Few-Shot Pest Recognition for Precision Agriculture](https://arxiv.org/abs/2601.00243)
*Anirudha Ghosh,Ritam Sarkar,Debaditya Barman*

Main category: cs.CV

TL;DR: Lightweight framework for pest detection and pesticide recommendation using compact CNN with meta-learning for low-resource devices, achieving high accuracy with reduced computational complexity.


<details>
  <summary>Details</summary>
Motivation: Traditional pest management methods are costly, time-consuming, labor-intensive, and environmentally harmful. There's a need for automated solutions accessible to small farmers using low-resource devices like smartphones and drones.

Method: Two-component framework: 1) Pest Detection Module using lightweight CNN with prototypical meta-learning for few-shot learning, 2) Pesticide Recommendation Module incorporating environmental factors (crop type, growth stage) for eco-friendly recommendations. Trained on comprehensive pest image dataset combining multiple public sources.

Result: Lightweight CNN achieves high accuracy comparable to state-of-the-art models while significantly reducing computational complexity. Framework enables real-time applications and reduces dependence on chemical pesticides.

Conclusion: The proposed framework demonstrates strong potential for real-time precision agriculture applications, making sustainable pest management accessible to small farmers through low-resource devices.

Abstract: Effective pest management is crucial for enhancing agricultural productivity, especially for crops such as sugarcane and wheat that are highly vulnerable to pest infestations. Traditional pest management methods depend heavily on manual field inspections and the use of chemical pesticides. These approaches are often costly, time-consuming, labor-intensive, and can have a negative impact on the environment. To overcome these challenges, this study presents a lightweight framework for pest detection and pesticide recommendation, designed for low-resource devices such as smartphones and drones, making it suitable for use by small and marginal farmers.
  The proposed framework includes two main components. The first is a Pest Detection Module that uses a compact, lightweight convolutional neural network (CNN) combined with prototypical meta-learning to accurately identify pests even when only a few training samples are available. The second is a Pesticide Recommendation Module that incorporates environmental factors like crop type and growth stage to suggest safe and eco-friendly pesticide recommendations. To train and evaluate our framework, a comprehensive pest image dataset was developed by combining multiple publicly available datasets. The final dataset contains samples with different viewing angles, pest sizes, and background conditions to ensure strong generalization.
  Experimental results show that the proposed lightweight CNN achieves high accuracy, comparable to state-of-the-art models, while significantly reducing computational complexity. The Decision Support System additionally improves pest management by reducing dependence on traditional chemical pesticides and encouraging sustainable practices, demonstrating its potential for real-time applications in precision agriculture.

</details>


### [19] [RGS-SLAM: Robust Gaussian Splatting SLAM with One-Shot Dense Initialization](https://arxiv.org/abs/2601.00705)
*Wei-Tse Cheng,Yen-Jen Chiou,Yuan-Fu Yang*

Main category: cs.CV

TL;DR: RGS-SLAM replaces GS-SLAM's residual-driven densification with a training-free correspondence-to-Gaussian initialization using DINOv3 descriptors, achieving 20% faster convergence and higher rendering quality while maintaining real-time performance.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of progressive Gaussian densification in GS-SLAM, which can lead to unstable early mapping and slower convergence, by providing a more robust initialization method that yields better-distributed Gaussian seeds from the start.

Method: Uses DINOv3 descriptors to extract dense multi-view correspondences, refines them through a confidence-aware inlier classifier, performs one-shot triangulation to generate Gaussian seeds, and integrates this initialization into existing GS-SLAM pipelines without requiring additional training.

Result: Achieves 20% faster convergence, higher rendering fidelity in texture-rich and cluttered scenes, competitive/superior localization and reconstruction accuracy on TUM RGB-D and Replica datasets, and sustains real-time mapping at up to 925 FPS.

Conclusion: RGS-SLAM demonstrates that replacing residual-driven densification with a robust correspondence-based Gaussian initialization significantly improves mapping stability, convergence speed, and rendering quality while maintaining compatibility with existing GS-SLAM systems and real-time performance.

Abstract: We introduce RGS-SLAM, a robust Gaussian-splatting SLAM framework that replaces the residual-driven densification stage of GS-SLAM with a training-free correspondence-to-Gaussian initialization. Instead of progressively adding Gaussians as residuals reveal missing geometry, RGS-SLAM performs a one-shot triangulation of dense multi-view correspondences derived from DINOv3 descriptors refined through a confidence-aware inlier classifier, generating a well-distributed and structure-aware Gaussian seed prior to optimization. This initialization stabilizes early mapping and accelerates convergence by roughly 20\%, yielding higher rendering fidelity in texture-rich and cluttered scenes while remaining fully compatible with existing GS-SLAM pipelines. Evaluated on the TUM RGB-D and Replica datasets, RGS-SLAM achieves competitive or superior localization and reconstruction accuracy compared with state-of-the-art Gaussian and point-based SLAM systems, sustaining real-time mapping performance at up to 925 FPS.

</details>


### [20] [TotalFM: An Organ-Separated Framework for 3D-CT Vision Foundation Models](https://arxiv.org/abs/2601.00260)
*Kohei Yamamoto,Tomohiro Kikuchi*

Main category: cs.CV

TL;DR: TotalFM: A radiological foundation model using organ-separated learning on 3D-CT volumes with text, achieving superior zero-shot lesion classification and comparable report generation to existing VLMs.


<details>
  <summary>Details</summary>
Motivation: Foundation models in radiology face computational cost challenges when training on 3D-CT volumetric data. There's a need for efficient models that can handle 3D-CT images and linguistic expressions while maintaining computational efficiency and representation capability.

Method: Proposes TotalFM with organ-separated learning framework. Automates creation of organ volume and finding-sentence pairs using segmentation techniques and LLM-based radiology report processing. Combines self-supervised pre-training via VideoMAE with contrastive learning using volume-text pairs on 140,000 series dataset.

Result: In zero-shot organ-wise lesion classification: outperformed CT-CLIP in 83% (5/6) of organs and Merlin in 64% (9/14) of organs. In zero-shot finding-wise lesion classification: achieved higher AUROC in 83% (25/30) of finding categories vs Merlin. Comparable performance to existing VLMs in radiology report generation tasks.

Conclusion: The organ-separated learning framework provides a realistic and effective design guideline for practical implementation of 3D-CT foundation models, demonstrating high generalization performance in clinical evaluation settings using actual radiology report sentences.

Abstract: While foundation models in radiology are expected to be applied to various clinical tasks, computational cost constraints remain a major challenge when training on 3D-CT volumetric data. In this study, we propose TotalFM, a radiological foundation model that efficiently learns the correspondence between 3D-CT images and linguistic expressions based on the concept of organ separation, utilizing a large-scale dataset of 140,000 series. By automating the creation of organ volume and finding-sentence pairs through segmentation techniques and Large Language Model (LLM)-based radiology report processing, and by combining self-supervised pre-training via VideoMAE with contrastive learning using volume-text pairs, we aimed to balance computational efficiency and representation capability. In zero-shot organ-wise lesion classification tasks, the proposed model achieved higher F1 scores in 83% (5/6) of organs compared to CT-CLIP and 64% (9/14) of organs compared to Merlin. These results suggest that the proposed model exhibits high generalization performance in a clinical evaluation setting using actual radiology report sentences. Furthermore, in zero-shot finding-wise lesion classification tasks, our model achieved a higher AUROC in 83% (25/30) of finding categories compared to Merlin. We also confirmed performance comparable to existing Vision-Language Models (VLMs) in radiology report generation tasks. Our results demonstrate that the organ-separated learning framework can serve as a realistic and effective design guideline for the practical implementation of 3D-CT foundation models.

</details>


### [21] [S1-MMAlign: A Large-Scale, Multi-Disciplinary Dataset for Scientific Figure-Text Understanding](https://arxiv.org/abs/2601.00264)
*He Wang,Longteng Guo,Pengkang Huo,Xuanxu Lin,Yichen Yuan,Jie Jiang,Jing Liu*

Main category: cs.CV

TL;DR: S1-MMAlign is a large-scale multimodal dataset of 15.5M scientific image-text pairs from 2.5M papers, enhanced with AI-generated captions to bridge the semantic gap between complex scientific imagery and sparse text descriptions.


<details>
  <summary>Details</summary>
Motivation: Multimodal learning has transformed general tasks but struggles in scientific discovery due to the semantic gap between complex scientific imagery and sparse textual descriptions in papers.

Method: Created dataset from 2.5M open-access papers across physics, biology, engineering; used Qwen-VL multimodal LLM to recaption images by synthesizing context from abstracts and citation contexts to enhance alignment.

Result: 15.5M high-quality image-text pairs; semantic enhancement reduced ambiguity (SciBERT pseudo-perplexity) and improved image-text alignment by 18.21% (CLIP scores).

Conclusion: S1-MMAlign provides a foundational resource for advancing scientific reasoning and cross-modal understanding in AI for Science, publicly available on Hugging Face.

Abstract: Multimodal learning has revolutionized general domain tasks, yet its application in scientific discovery is hindered by the profound semantic gap between complex scientific imagery and sparse textual descriptions. We present S1-MMAlign, a large-scale, multi-disciplinary multimodal dataset comprising over 15.5 million high-quality image-text pairs derived from 2.5 million open-access scientific papers. Spanning disciplines from physics and biology to engineering, the dataset captures diverse visual modalities including experimental setups, heatmaps, and microscopic imagery. To address the pervasive issue of weak alignment in raw scientific captions, we introduce an AI-ready semantic enhancement pipeline that utilizes the Qwen-VL multimodal large model series to recaption images by synthesizing context from paper abstracts and citation contexts. Technical validation demonstrates that this enhancement significantly improves data quality: SciBERT-based pseudo-perplexity metrics show reduced semantic ambiguity, while CLIP scores indicate an 18.21% improvement in image-text alignment. S1-MMAlign provides a foundational resource for advancing scientific reasoning and cross-modal understanding in the era of AI for Science. The dataset is publicly available at https://huggingface.co/datasets/ScienceOne-AI/S1-MMAlign.

</details>


### [22] [ActErase: A Training-Free Paradigm for Precise Concept Erasure via Activation Patching](https://arxiv.org/abs/2601.00267)
*Yi Sun,Xinhao Zhong,Hongyan Li,Yimin Zhou,Junhao Li,Bin Chen,Xuan Wang*

Main category: cs.CV

TL;DR: Training-free concept erasure method (ActErase) for diffusion models achieves SOTA performance without fine-tuning by identifying and replacing target activations during forward passes.


<details>
  <summary>Details</summary>
Motivation: Address safety, copyright, and ethical concerns in text-to-image diffusion models by overcoming limitations of existing fine-tuning-based concept erasure methods that are data-intensive and computationally expensive.

Method: ActErase identifies activation difference regions via prompt-pair analysis, extracts target activations, and dynamically replaces input activations during forward passes - all without training or fine-tuning.

Result: Achieves state-of-the-art erasure performance across nudity, artistic style, and object removal tasks while preserving generative capability and exhibiting strong robustness against adversarial attacks.

Conclusion: Establishes a new plug-and-play paradigm for lightweight yet effective concept manipulation in diffusion models, offering efficient training-free solution for safety and ethical concerns.

Abstract: Recent advances in text-to-image diffusion models have demonstrated remarkable generation capabilities, yet they raise significant concerns regarding safety, copyright, and ethical implications. Existing concept erasure methods address these risks by removing sensitive concepts from pre-trained models, but most of them rely on data-intensive and computationally expensive fine-tuning, which poses a critical limitation. To overcome these challenges, inspired by the observation that the model's activations are predominantly composed of generic concepts, with only a minimal component can represent the target concept, we propose a novel training-free method (ActErase) for efficient concept erasure. Specifically, the proposed method operates by identifying activation difference regions via prompt-pair analysis, extracting target activations and dynamically replacing input activations during forward passes. Comprehensive evaluations across three critical erasure tasks (nudity, artistic style, and object removal) demonstrates that our training-free method achieves state-of-the-art (SOTA) erasure performance, while effectively preserving the model's overall generative capability. Our approach also exhibits strong robustness against adversarial attacks, establishing a new plug-and-play paradigm for lightweight yet effective concept manipulation in diffusion models.

</details>


### [23] [FaithSCAN: Model-Driven Single-Pass Hallucination Detection for Faithful Visual Question Answering](https://arxiv.org/abs/2601.00269)
*Chaodong Tong,Qi Zhang,Chen Li,Lei Jiang,Yanbing Liu*

Main category: cs.CV

TL;DR: FaithSCAN detects VQA hallucinations using internal VLM signals (token uncertainty, visual representations, cross-modal alignment) with evidence encoding and uncertainty-aware attention, trained via LLM-as-a-Judge without human labels.


<details>
  <summary>Details</summary>
Motivation: Existing hallucination detection methods have limitations: external verification approaches are computationally expensive and dependent on external resources, while uncertainty-driven methods capture only limited facets of model uncertainty and fail to explore rich internal signals associated with diverse failure modes.

Method: FaithSCAN is a lightweight network that detects hallucinations by exploiting three types of internal VLM signals: token-level decoding uncertainty, intermediate visual representations, and cross-modal alignment features. These signals are fused via branch-wise evidence encoding and uncertainty-aware attention. The method uses LLM-as-a-Judge paradigm to automatically generate supervision signals for training without costly human labels.

Result: Experiments on multiple VQA benchmarks show FaithSCAN significantly outperforms existing methods in both effectiveness and efficiency. Analysis reveals hallucinations arise from systematic internal state variations in visual perception, cross-modal reasoning, and language decoding, with different internal signals providing complementary diagnostic cues.

Conclusion: FaithSCAN addresses limitations of existing hallucination detection methods by efficiently leveraging rich internal VLM signals, offering new insights into multimodal hallucination causes while maintaining high detection accuracy without expensive human annotation.

Abstract: Faithfulness hallucinations in VQA occur when vision-language models produce fluent yet visually ungrounded answers, severely undermining their reliability in safety-critical applications. Existing detection methods mainly fall into two categories: external verification approaches relying on auxiliary models or knowledge bases, and uncertainty-driven approaches using repeated sampling or uncertainty estimates. The former suffer from high computational overhead and are limited by external resource quality, while the latter capture only limited facets of model uncertainty and fail to sufficiently explore the rich internal signals associated with the diverse failure modes. Both paradigms thus have inherent limitations in efficiency, robustness, and detection performance. To address these challenges, we propose FaithSCAN: a lightweight network that detects hallucinations by exploiting rich internal signals of VLMs, including token-level decoding uncertainty, intermediate visual representations, and cross-modal alignment features. These signals are fused via branch-wise evidence encoding and uncertainty-aware attention. We also extend the LLM-as-a-Judge paradigm to VQA hallucination and propose a low-cost strategy to automatically generate model-dependent supervision signals, enabling supervised training without costly human labels while maintaining high detection accuracy. Experiments on multiple VQA benchmarks show that FaithSCAN significantly outperforms existing methods in both effectiveness and efficiency. In-depth analysis shows hallucinations arise from systematic internal state variations in visual perception, cross-modal reasoning, and language decoding. Different internal signals provide complementary diagnostic cues, and hallucination patterns vary across VLM architectures, offering new insights into the underlying causes of multimodal hallucinations.

</details>


### [24] [Disentangling Hardness from Noise: An Uncertainty-Driven Model-Agnostic Framework for Long-Tailed Remote Sensing Classification](https://arxiv.org/abs/2601.00278)
*Chi Ding,Junxiao Xue,Xinyi Yin,Shi Chen,Yunyun Shi,Yiduo Wang,Fengjian Xue,Xuecheng Wu*

Main category: cs.CV

TL;DR: DUAL is an uncertainty-aware framework that disentangles epistemic and aleatoric uncertainty to handle long-tailed distributions in remote sensing, addressing both hard tail samples and noisy ambiguous data.


<details>
  <summary>Details</summary>
Motivation: Long-tailed distributions are common in remote sensing due to imbalanced object occurrences, but existing methods fail to distinguish between hard-to-learn tail samples and noisy ambiguous data, leading to overfitting on noise.

Method: Based on Evidential Deep Learning, DUAL dynamically disentangles prediction uncertainty into Epistemic Uncertainty (EU) for sample scarcity and Aleatoric Uncertainty (AU) for data ambiguity. EU guides reweighting for hard tail samples, while AU enables adaptive label smoothing to suppress noise impact.

Result: Extensive experiments on multiple datasets across various backbones demonstrate DUAL's effectiveness and generalization, outperforming strong baselines like TGN and SADE. Ablation studies validate the design choices.

Conclusion: DUAL provides a model-agnostic uncertainty-aware framework that effectively addresses both sample scarcity and data ambiguity in long-tailed remote sensing distributions, offering improved performance over existing methods.

Abstract: Long-Tailed distributions are pervasive in remote sensing due to the inherently imbalanced occurrence of grounded objects. However, a critical challenge remains largely overlooked, i.e., disentangling hard tail data samples from noisy ambiguous ones. Conventional methods often indiscriminately emphasize all low-confidence samples, leading to overfitting on noisy data. To bridge this gap, building upon Evidential Deep Learning, we propose a model-agnostic uncertainty-aware framework termed DUAL, which dynamically disentangles prediction uncertainty into Epistemic Uncertainty (EU) and Aleatoric Uncertainty (AU). Specifically, we introduce EU as an indicator of sample scarcity to guide a reweighting strategy for hard-to-learn tail samples, while leveraging AU to quantify data ambiguity, employing an adaptive label smoothing mechanism to suppress the impact of noise. Extensive experiments on multiple datasets across various backbones demonstrate the effectiveness and generalization of our framework, surpassing strong baselines such as TGN and SADE. Ablation studies provide further insights into the crucial choices of our design.

</details>


### [25] [SV-GS: Sparse View 4D Reconstruction with Skeleton-Driven Gaussian Splatting](https://arxiv.org/abs/2601.00285)
*Jun-Jee Chao,Volkan Isler*

Main category: cs.CV

TL;DR: SV-GS: A framework for dynamic object reconstruction from sparse observations using skeleton-driven deformation fields and Gaussian splatting.


<details>
  <summary>Details</summary>
Motivation: Dynamic object reconstruction typically requires dense multi-view video coverage, but real-world observations (e.g., from security cameras) are often sparse in both viewpoints and time, making reconstruction highly ill-posed.

Method: Uses skeleton graph and initial static reconstruction to guide motion estimation, optimizes skeleton-driven deformation field with coarse joint pose estimator (time-dependent) and fine-grained deformation module, enabling smooth motion interpolation while preserving geometric details.

Result: Outperforms existing approaches under sparse observations by up to 34% in PSNR on synthetic datasets, achieves comparable performance to dense monocular video methods on real-world datasets despite using significantly fewer frames.

Conclusion: SV-GS enables effective dynamic reconstruction from sparse observations, with practical applicability enhanced by replacing initial static reconstruction with diffusion-based generative prior.

Abstract: Reconstructing a dynamic target moving over a large area is challenging. Standard approaches for dynamic object reconstruction require dense coverage in both the viewing space and the temporal dimension, typically relying on multi-view videos captured at each time step. However, such setups are only possible in constrained environments. In real-world scenarios, observations are often sparse over time and captured sparsely from diverse viewpoints (e.g., from security cameras), making dynamic reconstruction highly ill-posed. We present SV-GS, a framework that simultaneously estimates a deformation model and the object's motion over time under sparse observations. To initialize SV-GS, we leverage a rough skeleton graph and an initial static reconstruction as inputs to guide motion estimation. (Later, we show that this input requirement can be relaxed.) Our method optimizes a skeleton-driven deformation field composed of a coarse skeleton joint pose estimator and a module for fine-grained deformations. By making only the joint pose estimator time-dependent, our model enables smooth motion interpolation while preserving learned geometric details. Experiments on synthetic datasets show that our method outperforms existing approaches under sparse observations by up to 34% in PSNR, and achieves comparable performance to dense monocular video methods on real-world datasets despite using significantly fewer frames. Moreover, we demonstrate that the input initial static reconstruction can be replaced by a diffusion-based generative prior, making our method more practical for real-world scenarios.

</details>


### [26] [Towards Automated Differential Diagnosis of Skin Diseases Using Deep Learning and Imbalance-Aware Strategies](https://arxiv.org/abs/2601.00286)
*Ali Anaissi,Ali Braytee,Weidong Huang,Junaid Akram,Alaa Farhat,Jie Hua*

Main category: cs.CV

TL;DR: A Swin Transformer model achieves 87.71% accuracy on ISIC2019 dataset for classifying 8 skin lesion types, demonstrating potential as a diagnostic aid tool.


<details>
  <summary>Details</summary>
Motivation: Growing need for intelligent diagnostic tools due to increasing dermatological conditions and limited availability of dermatologists, requiring support for both patients and clinicians in timely and accurate skin disease diagnosis.

Method: Developed deep learning model using Swin Transformer architecture with pretraining on public skin disease datasets, refined architecture, optimized data preprocessing workflows, and applied targeted data augmentation techniques.

Result: Achieved 87.71% prediction accuracy across eight skin lesion classes on the ISIC2019 dataset.

Conclusion: The model demonstrates potential as a diagnostic support tool for clinicians and a self-assessment aid for patients in dermatological diagnosis.

Abstract: As dermatological conditions become increasingly common and the availability of dermatologists remains limited, there is a growing need for intelligent tools to support both patients and clinicians in the timely and accurate diagnosis of skin diseases. In this project, we developed a deep learning based model for the classification and diagnosis of skin conditions. By leveraging pretraining on publicly available skin disease image datasets, our model effectively extracted visual features and accurately classified various dermatological cases. Throughout the project, we refined the model architecture, optimized data preprocessing workflows, and applied targeted data augmentation techniques to improve overall performance. The final model, based on the Swin Transformer, achieved a prediction accuracy of 87.71 percent across eight skin lesion classes on the ISIC2019 dataset. These results demonstrate the model's potential as a diagnostic support tool for clinicians and a self assessment aid for patients.

</details>


### [27] [TimeColor: Flexible Reference Colorization via Temporal Concatenation](https://arxiv.org/abs/2601.00296)
*Bryan Constantine Sadihin,Yihao Meng,Michael Hua Wang,Matteo Jiahao Chen,Hang Su*

Main category: cs.CV

TL;DR: TimeColor is a sketch-based video colorization model that supports multiple heterogeneous references (character sheets, background images, etc.) with explicit region assignment, improving color fidelity and consistency.


<details>
  <summary>Details</summary>
Motivation: Existing colorization models only use single reference frames (typically first frame), ignoring other valuable conditional data sources like character sheets, background images, or arbitrary colorized frames that could provide better color guidance.

Method: Encodes references as additional latent frames concatenated temporally for concurrent processing; uses spatiotemporal correspondence-masked attention for subject-reference binding and modality-disjoint RoPE indexing to prevent shortcutting and cross-identity palette leakage.

Result: Experiments on SAKUGA-42M dataset show TimeColor improves color fidelity, identity consistency, and temporal stability over prior baselines under both single- and multi-reference protocols.

Conclusion: TimeColor demonstrates that supporting heterogeneous, variable-count references with explicit region assignment significantly enhances video colorization quality compared to single-reference approaches.

Abstract: Most colorization models condition only on a single reference, typically the first frame of the scene. However, this approach ignores other sources of conditional data, such as character sheets, background images, or arbitrary colorized frames. We propose TimeColor, a sketch-based video colorization model that supports heterogeneous, variable-count references with the use of explicit per-reference region assignment. TimeColor encodes references as additional latent frames which are concatenated temporally, permitting them to be processed concurrently in each diffusion step while keeping the model's parameter count fixed. TimeColor also uses spatiotemporal correspondence-masked attention to enforce subject-reference binding in addition to modality-disjoint RoPE indexing. These mechanisms mitigate shortcutting and cross-identity palette leakage. Experiments on SAKUGA-42M under both single- and multi-reference protocols show that TimeColor improves color fidelity, identity consistency, and temporal stability over prior baselines.

</details>


### [28] [VisNet: Efficient Person Re-Identification via Alpha-Divergence Loss, Feature Fusion and Dynamic Multi-Task Learning](https://arxiv.org/abs/2601.00307)
*Anns Ijaz,Muhammad Azeem Javed*

Main category: cs.CV

TL;DR: VisNet is an efficient person re-identification model that achieves strong accuracy with low computational cost through multi-scale feature fusion, semantic clustering, and optimized loss functions.


<details>
  <summary>Details</summary>
Motivation: Person re-identification needs high accuracy with minimal computational cost for real-world surveillance and mobile applications, but current state-of-the-art methods have high computational budgets.

Method: VisNet uses multi-scale feature fusion from ResNet50 stages 1-4 without parallel paths, semantic clustering with anatomical body partitioning and rule-based pseudo-labeling, dynamic weight averaging for classification regularization, and FIDI loss function for improved metric learning.

Result: Achieves 87.05% Rank-1 and 77.65% mAP on Market-1501 dataset with only 32.41M parameters and 4.601 GFLOPs, making it suitable for real-time deployment.

Conclusion: VisNet provides a practical, computationally efficient solution for person re-identification in resource-constrained real-world scenarios like surveillance and mobile applications.

Abstract: Person re-identification (ReID) is an extremely important area in both surveillance and mobile applications, requiring strong accuracy with minimal computational cost. State-of-the-art methods give good accuracy but with high computational budgets. To remedy this, this paper proposes VisNet, a computationally efficient and effective re-identification model suitable for real-world scenarios. It is the culmination of conceptual contributions, including feature fusion at multiple scales with automatic attention on each, semantic clustering with anatomical body partitioning, a dynamic weight averaging technique to balance classification semantic regularization, and the use of loss function FIDI for improved metric learning tasks. The multiple scales fuse ResNet50's stages 1 through 4 without the use of parallel paths, with semantic clustering introducing spatial constraints through the use of rule-based pseudo-labeling. VisNet achieves 87.05% Rank-1 and 77.65% mAP on the Market-1501 dataset, having 32.41M parameters and 4.601 GFLOPs, hence, proposing a practical approach for real-time deployment in surveillance and mobile applications where computational resources are limited.

</details>


### [29] [ReMA: A Training-Free Plug-and-Play Mixing Augmentation for Video Behavior Recognition](https://arxiv.org/abs/2601.00311)
*Feng-Qi Cui,Jinyang Huang,Sirui Zhao,Jinglong Guo,Qifan Cai,Xin Yan,Zhi Liu*

Main category: cs.CV

TL;DR: ReMA is a plug-and-play video augmentation strategy that uses controlled mixing to expand representations while preserving class-conditional stability, improving generalization across temporal scales.


<details>
  <summary>Details</summary>
Motivation: Current video data augmentation strategies are perturbation-driven and introduce uncontrolled variations that amplify non-discriminative factors, weakening intra-class distributional structure and causing representation drift with inconsistent gains across temporal scales.

Method: ReMA integrates two mechanisms: 1) Representation Alignment Mechanism (RAM) for structured intra-class mixing under distributional alignment constraints, and 2) Dynamic Selection Mechanism (DSM) that generates motion-aware spatiotemporal masks to localize perturbations away from discrimination-sensitive regions.

Result: Extensive experiments on diverse video behavior benchmarks demonstrate that ReMA consistently enhances generalization and robustness across different spatiotemporal granularities.

Conclusion: ReMA improves representation robustness without additional supervision or trainable parameters by jointly controlling how and where mixing is applied, addressing the limitations of perturbation-driven video augmentation.

Abstract: Video behavior recognition demands stable and discriminative representations under complex spatiotemporal variations. However, prevailing data augmentation strategies for videos remain largely perturbation-driven, often introducing uncontrolled variations that amplify non-discriminative factors, which finally weaken intra-class distributional structure and representation drift with inconsistent gains across temporal scales. To address these problems, we propose Representation-aware Mixing Augmentation (ReMA), a plug-and-play augmentation strategy that formulates mixing as a controlled replacement process to expand representations while preserving class-conditional stability. ReMA integrates two complementary mechanisms. Firstly, the Representation Alignment Mechanism (RAM) performs structured intra-class mixing under distributional alignment constraints, suppressing irrelevant intra-class drift while enhancing statistical reliability. Then, the Dynamic Selection Mechanism (DSM) generates motion-aware spatiotemporal masks to localize perturbations, guiding them away from discrimination-sensitive regions and promoting temporal coherence. By jointly controlling how and where mixing is applied, ReMA improves representation robustness without additional supervision or trainable parameters. Extensive experiments on diverse video behavior benchmarks demonstrate that ReMA consistently enhances generalization and robustness across different spatiotemporal granularities.

</details>


### [30] [Depth-Synergized Mamba Meets Memory Experts for All-Day Image Reflection Separation](https://arxiv.org/abs/2601.00322)
*Siyan Fang,Long Peng,Yuntao Wang,Ruonan Wei,Yuehuan Wang*

Main category: cs.CV

TL;DR: DMDNet uses depth-aware scanning with Mamba and memory compensation to separate image reflections, especially effective at night with new NightIRS dataset.


<details>
  <summary>Details</summary>
Motivation: Existing reflection separation methods struggle when transmission and reflection layers have similar contrasts, which is particularly challenging at nighttime. Single-image approaches lack sufficient information for reliable disentanglement.

Method: Proposes Depth-Memory Decoupling Network (DMDNet) with three key components: Depth-Aware Scanning (DAScan) to guide Mamba toward salient structures, Depth-Synergized State-Space Model (DS-SSM) to modulate state activations by depth, and Memory Expert Compensation Module (MECM) to leverage cross-image historical knowledge for layer-specific compensation.

Result: DMDNet outperforms state-of-the-art methods in both daytime and nighttime reflection separation. The authors also created the Nighttime Image Reflection Separation (NightIRS) dataset to address the lack of nighttime training data.

Conclusion: The proposed DMDNet effectively addresses reflection separation challenges, particularly in nighttime conditions, through depth-aware guidance and memory-based compensation, demonstrating superior performance over existing methods.

Abstract: Image reflection separation aims to disentangle the transmission layer and the reflection layer from a blended image. Existing methods rely on limited information from a single image, tending to confuse the two layers when their contrasts are similar, a challenge more severe at night. To address this issue, we propose the Depth-Memory Decoupling Network (DMDNet). It employs the Depth-Aware Scanning (DAScan) to guide Mamba toward salient structures, promoting information flow along semantic coherence to construct stable states. Working in synergy with DAScan, the Depth-Synergized State-Space Model (DS-SSM) modulates the sensitivity of state activations by depth, suppressing the spread of ambiguous features that interfere with layer disentanglement. Furthermore, we introduce the Memory Expert Compensation Module (MECM), leveraging cross-image historical knowledge to guide experts in providing layer-specific compensation. To address the lack of datasets for nighttime reflection separation, we construct the Nighttime Image Reflection Separation (NightIRS) dataset. Extensive experiments demonstrate that DMDNet outperforms state-of-the-art methods in both daytime and nighttime.

</details>


### [31] [HarmoniAD: Harmonizing Local Structures and Global Semantics for Anomaly Detection](https://arxiv.org/abs/2601.00327)
*Naiqi Zhang,Chuancheng Shi,Jingtong Dou,Wenhua Wu,Fei Shen,Jianhua Cao*

Main category: cs.CV

TL;DR: HarmoniAD is a frequency-guided dual-branch framework for industrial anomaly detection that balances structure and semantics using high- and low-frequency paths to detect tiny defects while maintaining robustness.


<details>
  <summary>Details</summary>
Motivation: Existing anomaly detection methods face a structure-semantics trade-off: structure-oriented models are noise-sensitive, while semantics-oriented models often miss fine details needed for detecting tiny defects in industrial quality inspection.

Method: Frequency-guided dual-branch framework where features from CLIP encoder are transformed to frequency domain and decoupled into high-frequency (with fine-grained structural attention module) and low-frequency (with global structural context module) paths for complementary modeling.

Result: State-of-the-art performance on MVTec-AD, VisA, and BTAD datasets with both sensitivity to small anomalies and robustness against noise.

Conclusion: HarmoniAD effectively balances fine detail and global semantics through frequency-guided dual-branch architecture, addressing the structure-semantics trade-off in industrial anomaly detection.

Abstract: Anomaly detection is crucial in industrial product quality inspection. Failing to detect tiny defects often leads to serious consequences. Existing methods face a structure-semantics trade-off: structure-oriented models (such as frequency-based filters) are noise-sensitive, while semantics-oriented models (such as CLIP-based encoders) often miss fine details. To address this, we propose HarmoniAD, a frequency-guided dual-branch framework. Features are first extracted by the CLIP image encoder, then transformed into the frequency domain, and finally decoupled into high- and low-frequency paths for complementary modeling of structure and semantics. The high-frequency branch is equipped with a fine-grained structural attention module (FSAM) to enhance textures and edges for detecting small anomalies, while the low-frequency branch uses a global structural context module (GSCM) to capture long-range dependencies and preserve semantic consistency. Together, these branches balance fine detail and global semantics. HarmoniAD further adopts a multi-class joint training strategy, and experiments on MVTec-AD, VisA, and BTAD show state-of-the-art performance with both sensitivity and robustness.

</details>


### [32] [Joint Geometry-Appearance Human Reconstruction in a Unified Latent Space via Bridge Diffusion](https://arxiv.org/abs/2601.00328)
*Yingzhi Tang,Qijian Zhang,Junhui Hou*

Main category: cs.CV

TL;DR: JGA-LBD is a unified framework for 3D human reconstruction from single RGB images using joint latent representation and bridge diffusion.


<details>
  <summary>Details</summary>
Motivation: Existing methods use decoupled pipelines for geometry and appearance, causing inconsistencies. There's a need for unified reconstruction that handles heterogeneous input conditions effectively.

Method: Unifies geometry and appearance into joint latent representation using 3D Gaussian representations compressed via shared sparse VAE. Uses bridge diffusion to infer missing components from partial observations, then decodes to complete 3D human structure.

Result: Outperforms state-of-the-art approaches in geometry fidelity and appearance quality, including challenging in-the-wild scenarios.

Conclusion: JGA-LBD successfully unifies geometry and appearance modeling through joint latent representation and bridge diffusion, achieving superior reconstruction quality from single RGB images.

Abstract: Achieving consistent and high-fidelity geometry and appearance reconstruction of 3D digital humans from a single RGB image is inherently a challenging task. Existing studies typically resort to decoupled pipelines for geometry estimation and appearance synthesis, often hindering unified reconstruction and causing inconsistencies. This paper introduces \textbf{JGA-LBD}, a novel framework that unifies the modeling of geometry and appearance into a joint latent representation and formulates the generation process as bridge diffusion. Observing that directly integrating heterogeneous input conditions (e.g., depth maps, SMPL models) leads to substantial training difficulties, we unify all conditions into the 3D Gaussian representations, which can be further compressed into a unified latent space through a shared sparse variational autoencoder (VAE). Subsequently, the specialized form of bridge diffusion enables to start with a partial observation of the target latent code and solely focuses on inferring the missing components. Finally, a dedicated decoding module extracts the complete 3D human geometric structure and renders novel views from the inferred latent representation. Experiments demonstrate that JGA-LBD outperforms current state-of-the-art approaches in terms of both geometry fidelity and appearance quality, including challenging in-the-wild scenarios. Our code will be made publicly available at https://github.com/haiantyz/JGA-LBD.

</details>


### [33] [MotionPhysics: Learnable Motion Distillation for Text-Guided Simulation](https://arxiv.org/abs/2601.00504)
*Miaowei Wang,Jakub Zadrożny,Oisin Mac Aodha,Amir Vaxman*

Main category: cs.CV

TL;DR: MotionPhysics is a differentiable framework that infers physical parameters from natural language prompts for 3D scenes, using multimodal LLMs and motion distillation from video diffusion models to generate realistic dynamic simulations without ground-truth data.


<details>
  <summary>Details</summary>
Motivation: Simulating 3D objects with accurate material behavior requires expert knowledge and time-consuming parameter tuning. Current approaches need ground-truth trajectories or annotated videos, which are difficult to obtain.

Method: 1) Uses multimodal LLM to estimate material parameters within plausible ranges; 2) Proposes learnable motion distillation loss that extracts motion priors from pretrained video diffusion models while minimizing appearance/geometry biases; 3) End-to-end differentiable framework.

Result: Evaluated across 30+ scenarios with real-world, human-designed, and AI-generated 3D objects covering elastic solids, metals, foams, sand, Newtonian/non-Newtonian fluids. Produces visually realistic dynamic simulations guided by natural language, surpassing state-of-the-art.

Conclusion: MotionPhysics enables automatic determination of physically plausible parameters from natural language prompts, removing need for expert tuning or ground-truth data, and generates realistic dynamic simulations across diverse materials and objects.

Abstract: Accurately simulating existing 3D objects and a wide variety of materials often demands expert knowledge and time-consuming physical parameter tuning to achieve the desired dynamic behavior. We introduce MotionPhysics, an end-to-end differentiable framework that infers plausible physical parameters from a user-provided natural language prompt for a chosen 3D scene of interest, removing the need for guidance from ground-truth trajectories or annotated videos. Our approach first utilizes a multimodal large language model to estimate material parameter values, which are constrained to lie within plausible ranges. We further propose a learnable motion distillation loss that extracts robust motion priors from pretrained video diffusion models while minimizing appearance and geometry inductive biases to guide the simulation. We evaluate MotionPhysics across more than thirty scenarios, including real-world, human-designed, and AI-generated 3D objects, spanning a wide range of materials such as elastic solids, metals, foams, sand, and both Newtonian and non-Newtonian fluids. We demonstrate that MotionPhysics produces visually realistic dynamic simulations guided by natural language, surpassing the state of the art while automatically determining physically plausible parameters. The code and project page are available at: https://wangmiaowei.github.io/MotionPhysics.github.io/.

</details>


### [34] [Intelligent Traffic Surveillance for Real-Time Vehicle Detection, License Plate Recognition, and Speed Estimation](https://arxiv.org/abs/2601.00344)
*Bruce Mugizi,Sudi Murindanyi,Olivia Nakacwa,Andrew Katumba*

Main category: cs.CV

TL;DR: Real-time traffic surveillance system for developing countries using computer vision for vehicle detection, license plate recognition, and speed estimation with automated ticket issuance via SMS.


<details>
  <summary>Details</summary>
Motivation: Speeding is a major contributor to road fatalities in developing countries like Uganda where road safety infrastructure is limited, creating an urgent need for automated traffic enforcement solutions.

Method: Computer vision system using YOLOv8 for license plate detection, CNN/transformer models for character recognition, source-target ROI for speed estimation, and Africa's Talking API for SMS-based automated ticket issuance.

Result: YOLOv8 achieved 97.9% mAP for plate detection; CNN had 3.85% CER while transformer reduced to 1.79% for character recognition; speed estimation had 10 km/h error margin; integrated database enables automated enforcement.

Conclusion: The system effectively addresses traffic management needs in resource-constrained environments and shows potential to reduce road accidents through automated enforcement in developing countries.

Abstract: Speeding is a major contributor to road fatalities, particularly in developing countries such as Uganda, where road safety infrastructure is limited. This study proposes a real-time intelligent traffic surveillance system tailored to such regions, using computer vision techniques to address vehicle detection, license plate recognition, and speed estimation. The study collected a rich dataset using a speed gun, a Canon Camera, and a mobile phone to train the models. License plate detection using YOLOv8 achieved a mean average precision (mAP) of 97.9%. For character recognition of the detected license plate, the CNN model got a character error rate (CER) of 3.85%, while the transformer model significantly reduced the CER to 1.79%. Speed estimation used source and target regions of interest, yielding a good performance of 10 km/h margin of error. Additionally, a database was established to correlate user information with vehicle detection data, enabling automated ticket issuance via SMS via Africa's Talking API. This system addresses critical traffic management needs in resource-constrained environments and shows potential to reduce road accidents through automated traffic enforcement in developing countries where such interventions are urgently needed.

</details>


### [35] [A Comprehensive Dataset for Human vs. AI Generated Image Detection](https://arxiv.org/abs/2601.00553)
*Rajarshi Roy,Nasrin Imanpour,Ashhar Aziz,Shashwat Bajpai,Gurpreet Singh,Shwetangshu Biswas,Kapil Wanaskar,Parth Patwa,Subhankar Ghosh,Shreyas Dixit,Nilesh Ranjan Pal,Vipula Rawte,Ritvik Garimella,Gaytri Jena,Vasu Sharma,Vinija Jain,Aman Chadha,Aishwarya Naresh Reganti,Amitava Das*

Main category: cs.CV

TL;DR: MS COCOAI is a new dataset for AI-generated image detection with 96K real/synthetic images from 5 generators, enabling classification tasks for authenticity and source model identification.


<details>
  <summary>Details</summary>
Motivation: Multimodal AI systems enable both innovation and harmful content creation, making detection of synthetic images increasingly urgent as they become harder to distinguish from real photos.

Method: Created MS COCOAI dataset using MS COCO as base, generating synthetic images with 5 generators (Stable Diffusion 3, 2.1, SDXL, DALL-E 3, MidJourney v6), proposing two classification tasks.

Result: Released dataset of 96,000 real and synthetic datapoints available on HuggingFace, establishing benchmark tasks for AI-generated image detection and source model identification.

Conclusion: MS COCOAI provides a valuable resource for developing detection methods against increasingly sophisticated AI-generated images, addressing the growing need for authenticity verification tools.

Abstract: Multimodal generative AI systems like Stable Diffusion, DALL-E, and MidJourney have fundamentally changed how synthetic images are created. These tools drive innovation but also enable the spread of misleading content, false information, and manipulated media. As generated images become harder to distinguish from photographs, detecting them has become an urgent priority. To combat this challenge, We release MS COCOAI, a novel dataset for AI generated image detection consisting of 96000 real and synthetic datapoints, built using the MS COCO dataset. To generate synthetic images, we use five generators: Stable Diffusion 3, Stable Diffusion 2.1, SDXL, DALL-E 3, and MidJourney v6. Based on the dataset, we propose two tasks: (1) classifying images as real or generated, and (2) identifying which model produced a given synthetic image. The dataset is available at https://huggingface.co/datasets/Rajarshi-Roy-research/Defactify_Image_Dataset.

</details>


### [36] [OmniVaT: Single Domain Generalization for Multimodal Visual-Tactile Learning](https://arxiv.org/abs/2601.00352)
*Liuxiang Qiu,Hui Da,Yuzhen Niu,Tiesong Zhao,Yang Cao,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: OmniVaT framework addresses single domain generalization for visual-tactile learning by bridging modality gaps and enhancing domain adaptation without multi-domain training data.


<details>
  <summary>Details</summary>
Motivation: Visual-tactile learning suffers from modality discrepancies between visual and tactile sensors, and domain gaps caused by non-standardized sensors and inconsistent data collection procedures.

Method: Proposes OmniVaT with two key components: 1) Multimodal Fractional Fourier Adapter (MFFA) maps visual and tactile embeddings into unified embedding-frequency space, 2) Discrete Tree Generation (DTG) module obtains diverse multimodal fractional representations through hierarchical tree structure.

Result: Extensive experiments demonstrate superior cross-domain generalization performance on the SDG-VTL task.

Conclusion: OmniVaT successfully addresses single domain generalization for multimodal visual-tactile learning by effectively mitigating modality gaps and enhancing adaptivity to domain shifts without requiring multi-domain training data.

Abstract: Visual-tactile learning (VTL) enables embodied agents to perceive the physical world by integrating visual (VIS) and tactile (TAC) sensors. However, VTL still suffers from modality discrepancies between VIS and TAC images, as well as domain gaps caused by non-standardized tactile sensors and inconsistent data collection procedures. We formulate these challenges as a new task, termed single domain generalization for multimodal VTL (SDG-VTL). In this paper, we propose an OmniVaT framework that, for the first time, successfully addresses this task. On the one hand, OmniVaT integrates a multimodal fractional Fourier adapter (MFFA) to map VIS and TAC embeddings into a unified embedding-frequency space, thereby effectively mitigating the modality gap without multi-domain training data or careful cross-modal fusion strategies. On the other hand, it also incorporates a discrete tree generation (DTG) module that obtains diverse and reliable multimodal fractional representations through a hierarchical tree structure, thereby enhancing its adaptivity to fluctuating domain shifts in unseen domains. Extensive experiments demonstrate the superior cross-domain generalization performance of OmniVaT on the SDG-VTL task.

</details>


### [37] [Noise-Robust Tiny Object Localization with Flows](https://arxiv.org/abs/2601.00617)
*Huixin Sun,Linlin Yang,Ronyu Chen,Kerui Gu,Baochang Zhang,Angela Yao,Xianbin Cao*

Main category: cs.CV

TL;DR: TOLF improves tiny object detection by addressing annotation noise sensitivity through flow-based error modeling and uncertainty-guided optimization.


<details>
  <summary>Details</summary>
Motivation: Tiny objects remain challenging for detection systems due to high sensitivity to annotation noise, where strict localization objectives risk overfitting to noise.

Method: Proposes Tiny Object Localization with Flows (TOLF) using normalizing flows for flexible error modeling and uncertainty-guided gradient modulation to suppress learning from noise-prone samples.

Result: Extensive experiments across three datasets show effectiveness, with TOLF boosting DINO baseline by 1.2% AP on AI-TOD dataset.

Conclusion: TOLF provides a noise-robust localization framework that addresses tiny object detection challenges through probabilistic modeling and uncertainty-aware optimization.

Abstract: Despite significant advances in generic object detection, a persistent performance gap remains for tiny objects compared to normal-scale objects. We demonstrate that tiny objects are highly sensitive to annotation noise, where optimizing strict localization objectives risks noise overfitting. To address this, we propose Tiny Object Localization with Flows (TOLF), a noise-robust localization framework leveraging normalizing flows for flexible error modeling and uncertainty-guided optimization. Our method captures complex, non-Gaussian prediction distributions through flow-based error modeling, enabling robust learning under noisy supervision. An uncertainty-aware gradient modulation mechanism further suppresses learning from high-uncertainty, noise-prone samples, mitigating overfitting while stabilizing training. Extensive experiments across three datasets validate our approach's effectiveness. Especially, TOLF boosts the DINO baseline by 1.2% AP on the AI-TOD dataset.

</details>


### [38] [Mask-Conditioned Voxel Diffusion for Joint Geometry and Color Inpainting](https://arxiv.org/abs/2601.00368)
*Aarya Sumuk*

Main category: cs.CV

TL;DR: Lightweight two-stage framework for joint 3D geometry and color inpainting of damaged cultural heritage artifacts using mask-conditioned volumetric diffusion.


<details>
  <summary>Details</summary>
Motivation: Digital restoration of cultural heritage artifacts requires joint reconstruction of both geometry and color for damaged 3D objects.

Method: Two-stage pipeline: 1) 2D CNN predicts damage masks on RGB slices, aggregated into volumetric mask; 2) Diffusion-based 3D U-Net performs mask-conditioned inpainting on voxel grids with composite objective combining occupancy and color reconstruction.

Result: Produces more complete geometry and coherent color reconstructions at 32^3 resolution compared to symmetry-based baselines, as evaluated on textured artifacts with synthetic damage.

Conclusion: Explicit mask conditioning is a practical way to guide volumetric diffusion models for joint 3D geometry and color inpainting in cultural heritage restoration.

Abstract: We present a lightweight two-stage framework for joint geometry and color inpainting of damaged 3D objects, motivated by the digital restoration of cultural heritage artifacts. The pipeline separates damage localization from reconstruction. In the first stage, a 2D convolutional network predicts damage masks on RGB slices extracted from a voxelized object, and these predictions are aggregated into a volumetric mask. In the second stage, a diffusion-based 3D U-Net performs mask-conditioned inpainting directly on voxel grids, reconstructing geometry and color while preserving observed regions. The model jointly predicts occupancy and color using a composite objective that combines occupancy reconstruction with masked color reconstruction and perceptual regularization. We evaluate the approach on a curated set of textured artifacts with synthetically generated damage using standard geometric and color metrics. Compared to symmetry-based baselines, our method produces more complete geometry and more coherent color reconstructions at a fixed 32^3 resolution. Overall, the results indicate that explicit mask conditioning is a practical way to guide volumetric diffusion models for joint 3D geometry and color inpainting.

</details>


### [39] [Detecting Performance Degradation under Data Shift in Pathology Vision-Language Model](https://arxiv.org/abs/2601.00716)
*Hao Guan,Li Zhou*

Main category: cs.CV

TL;DR: Vision-Language Models (VLMs) in medical imaging need performance degradation detection under data shift; DomainSAT toolbox analyzes input shifts, but output confidence monitoring better captures actual performance drops.


<details>
  <summary>Details</summary>
Motivation: VLMs in medical imaging face performance degradation when deployed data differs from training data, but detecting this degradation without labeled data is challenging for large pre-trained models.

Method: Developed DomainSAT toolbox for input shift detection, then introduced label-free confidence-based degradation indicator for output monitoring, combining both approaches for comprehensive reliability assessment.

Result: Input shift detection identifies distribution changes but doesn't always correlate with performance degradation; output confidence monitoring shows closer relationship to actual performance drops; combining both enables more reliable detection.

Conclusion: Combining input data shift detection with output confidence-based monitoring provides a practical complementary framework for reliable performance degradation detection in VLMs under data shift in digital pathology.

Abstract: Vision-Language Models have demonstrated strong potential in medical image analysis and disease diagnosis. However, after deployment, their performance may deteriorate when the input data distribution shifts from that observed during development. Detecting such performance degradation is essential for clinical reliability, yet remains challenging for large pre-trained VLMs operating without labeled data. In this study, we investigate performance degradation detection under data shift in a state-of-the-art pathology VLM. We examine both input-level data shift and output-level prediction behavior to understand their respective roles in monitoring model reliability. To facilitate systematic analysis of input data shift, we develop DomainSAT, a lightweight toolbox with a graphical interface that integrates representative shift detection algorithms and enables intuitive exploration of data shift. Our analysis shows that while input data shift detection is effective at identifying distributional changes and providing early diagnostic signals, it does not always correspond to actual performance degradation. Motivated by this observation, we further study output-based monitoring and introduce a label-free, confidence-based degradation indicator that directly captures changes in model prediction confidence. We find that this indicator exhibits a close relationship with performance degradation and serves as an effective complement to input shift detection. Experiments on a large-scale pathology dataset for tumor classification demonstrate that combining input data shift detection and output confidence-based indicators enables more reliable detection and interpretation of performance degradation in VLMs under data shift. These findings provide a practical and complementary framework for monitoring the reliability of foundation models in digital pathology.

</details>


### [40] [BHaRNet: Reliability-Aware Body-Hand Modality Expertized Networks for Fine-grained Skeleton Action Recognition](https://arxiv.org/abs/2601.00369)
*Seungyeon Cho,Tae-kyun Kim*

Main category: cs.CV

TL;DR: A probabilistic dual-stream framework for skeleton-based action recognition that unifies reliability modeling and multi-modal integration, handling both intra-skeleton and cross-modal domains with improved performance on fine-grained hand articulations.


<details>
  <summary>Details</summary>
Motivation: Existing skeleton-based human action recognition methods are body-centric and neglect subtle hand articulations crucial for fine-grained recognition. There's a need to handle uncertainty and integrate multiple modalities (skeleton and RGB) in a unified framework.

Method: Three key components: 1) calibration-free preprocessing that learns directly from native coordinates, 2) probabilistic Noisy-OR fusion for reliability-aware dual-stream learning without confidence supervision, and 3) intra- to cross-modal ensemble coupling four skeleton modalities with RGB representations.

Result: Comprehensive evaluations across multiple benchmarks (NTU RGB+D 60/120, PKU-MMD, N-UCLA) and a new hand-centric benchmark show consistent improvements and robustness under noisy and heterogeneous conditions.

Conclusion: The framework successfully addresses limitations of body-centric approaches by focusing on fine-grained hand articulations, unifying reliability modeling and multi-modal integration for improved skeleton-based action recognition.

Abstract: Skeleton-based human action recognition (HAR) has achieved remarkable progress with graph-based architectures. However, most existing methods remain body-centric, focusing on large-scale motions while neglecting subtle hand articulations that are crucial for fine-grained recognition. This work presents a probabilistic dual-stream framework that unifies reliability modeling and multi-modal integration, generalizing expertized learning under uncertainty across both intra-skeleton and cross-modal domains. The framework comprises three key components: (1) a calibration-free preprocessing pipeline that removes canonical-space transformations and learns directly from native coordinates; (2) a probabilistic Noisy-OR fusion that stabilizes reliability-aware dual-stream learning without requiring explicit confidence supervision; and (3) an intra- to cross-modal ensemble that couples four skeleton modalities (Joint, Bone, Joint Motion, and Bone Motion) to RGB representations, bridging structural and visual motion cues in a unified cross-modal formulation. Comprehensive evaluations across multiple benchmarks (NTU RGB+D~60/120, PKU-MMD, N-UCLA) and a newly defined hand-centric benchmark exhibit consistent improvements and robustness under noisy and heterogeneous conditions.

</details>


### [41] [NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos](https://arxiv.org/abs/2601.00393)
*Yuxue Yang,Lue Fan,Ziqi Shi,Junran Peng,Feng Wang,Zhaoxiang Zhang*

Main category: cs.CV

TL;DR: NeoVerse is a scalable 4D world model for reconstruction, video generation, and applications, using monocular videos instead of expensive multi-view data.


<details>
  <summary>Details</summary>
Motivation: Current 4D world modeling methods face scalability limitations due to expensive multi-view data requirements or cumbersome training pre-processing, limiting practical applications.

Method: Pose-free feed-forward 4D reconstruction, online monocular degradation pattern simulation, and other well-aligned techniques to enable scalable processing of diverse in-the-wild monocular videos.

Result: Achieves state-of-the-art performance in standard reconstruction and generation benchmarks while demonstrating versatility and generalization across various domains.

Conclusion: NeoVerse provides a scalable and versatile 4D world modeling solution that overcomes previous limitations, enabling practical applications with monocular video inputs.

Abstract: In this paper, we propose NeoVerse, a versatile 4D world model that is capable of 4D reconstruction, novel-trajectory video generation, and rich downstream applications. We first identify a common limitation of scalability in current 4D world modeling methods, caused either by expensive and specialized multi-view 4D data or by cumbersome training pre-processing. In contrast, our NeoVerse is built upon a core philosophy that makes the full pipeline scalable to diverse in-the-wild monocular videos. Specifically, NeoVerse features pose-free feed-forward 4D reconstruction, online monocular degradation pattern simulation, and other well-aligned techniques. These designs empower NeoVerse with versatility and generalization to various domains. Meanwhile, NeoVerse achieves state-of-the-art performance in standard reconstruction and generation benchmarks. Our project page is available at https://neoverse-4d.github.io

</details>


### [42] [RoLID-11K: A Dashcam Dataset for Small-Object Roadside Litter Detection](https://arxiv.org/abs/2601.00398)
*Tao Wu,Qing Xu,Xiangjian He,Oakleigh Weekes,James Brown,Wenting Duan*

Main category: cs.CV

TL;DR: RoLID-11K is the first large-scale dataset for roadside litter detection from dashcams, featuring 11k annotated images with extreme small objects and long-tail distribution, benchmarking modern detectors for scalable litter monitoring.


<details>
  <summary>Details</summary>
Motivation: Current roadside litter monitoring relies on labor-intensive surveys with limited coverage. Existing vision datasets don't capture dashcam footage characteristics where litter appears extremely small, sparse, and in cluttered backgrounds.

Method: Created RoLID-11K dataset with over 11k annotated images from diverse UK driving conditions, featuring long-tail and small-object distributions. Benchmarked various modern detectors including transformer architectures and real-time YOLO models.

Result: CO-DETR and related transformers achieved best localization accuracy, while real-time models were constrained by coarse feature hierarchies. The dataset establishes a challenging benchmark for extreme small-object detection in dynamic driving scenes.

Conclusion: RoLID-11K enables development of scalable, low-cost systems for roadside litter monitoring by providing the first large-scale dashcam dataset for this challenging task, with transformer architectures showing superior performance but real-time models needing improvement.

Abstract: Roadside litter poses environmental, safety and economic challenges, yet current monitoring relies on labour-intensive surveys and public reporting, providing limited spatial coverage. Existing vision datasets for litter detection focus on street-level still images, aerial scenes or aquatic environments, and do not reflect the unique characteristics of dashcam footage, where litter appears extremely small, sparse and embedded in cluttered road-verge backgrounds. We introduce RoLID-11K, the first large-scale dataset for roadside litter detection from dashcams, comprising over 11k annotated images spanning diverse UK driving conditions and exhibiting pronounced long-tail and small-object distributions. We benchmark a broad spectrum of modern detectors, from accuracy-oriented transformer architectures to real-time YOLO models, and analyse their strengths and limitations on this challenging task. Our results show that while CO-DETR and related transformers achieve the best localisation accuracy, real-time models remain constrained by coarse feature hierarchies. RoLID-11K establishes a challenging benchmark for extreme small-object detection in dynamic driving scenes and aims to support the development of scalable, low-cost systems for roadside-litter monitoring. The dataset is available at https://github.com/xq141839/RoLID-11K.

</details>


### [43] [ABFR-KAN: Kolmogorov-Arnold Networks for Functional Brain Analysis](https://arxiv.org/abs/2601.00416)
*Tyler Ward,Abdullah Imran*

Main category: cs.CV

TL;DR: ABFR-KAN is a transformer-based network using Kolmogorov-Arnold Networks to improve functional connectivity analysis for autism diagnosis, addressing atlas-based parcellation limitations.


<details>
  <summary>Details</summary>
Motivation: Traditional atlas-based parcellation for functional connectivity analysis suffers from selection bias and lacks subject specificity, limiting its effectiveness for brain disorder diagnosis.

Method: Proposes ABFR-KAN, a transformer-based classification network with novel brain function representation components and Kolmogorov-Arnold Networks to reduce structural bias and improve anatomical conformity.

Result: Extensive experiments on ABIDE I dataset show ABFR-KAN consistently outperforms state-of-the-art baselines for ASD classification, validated through cross-site evaluation and ablation studies.

Conclusion: ABFR-KAN effectively mitigates structural bias in functional connectivity estimation and provides more reliable autism spectrum disorder classification compared to existing methods.

Abstract: Functional connectivity (FC) analysis, a valuable tool for computer-aided brain disorder diagnosis, traditionally relies on atlas-based parcellation. However, issues relating to selection bias and a lack of regard for subject specificity can arise as a result of such parcellations. Addressing this, we propose ABFR-KAN, a transformer-based classification network that incorporates novel advanced brain function representation components with the power of Kolmogorov-Arnold Networks (KANs) to mitigate structural bias, improve anatomical conformity, and enhance the reliability of FC estimation. Extensive experiments on the ABIDE I dataset, including cross-site evaluation and ablation studies across varying model backbones and KAN configurations, demonstrate that ABFR-KAN consistently outperforms state-of-the-art baselines for autism spectrum distorder (ASD) classification. Our code is available at https://github.com/tbwa233/ABFR-KAN.

</details>


### [44] [Robust Assembly Progress Estimation via Deep Metric Learning](https://arxiv.org/abs/2601.00422)
*Kazuma Miura,Sarthak Pathak,Kazunori Umeda*

Main category: cs.CV

TL;DR: Proposes Anomaly Quadruplet-Net with Quadruplet Loss and custom data loader for robust assembly progress estimation, improving accuracy by 1.3% and reducing adjacent task misclassification by 1.9% on PC assembly dataset.


<details>
  <summary>Details</summary>
Motivation: Manual multi-day assembly tasks challenge smart factory implementation; existing Anomaly Triplet-Net fails with subtle visual changes between consecutive tasks, leading to misclassification.

Method: Uses Quadruplet Loss-based learning for anomaly images with custom data loader that strategically selects training samples to enhance estimation accuracy, even with occlusion or minimal visual changes.

Result: Outperformed existing methods on desktop PC assembly dataset: improved estimation accuracy by 1.3% and reduced misclassification between adjacent tasks by 1.9%.

Conclusion: Proposed Anomaly Quadruplet-Net effectively addresses limitations of previous methods for assembly progress estimation, demonstrating robustness with small-scale datasets despite occlusion or subtle visual changes.

Abstract: In recent years, the advancement of AI technologies has accelerated the development of smart factories. In particular, the automatic monitoring of product assembly progress is crucial for improving operational efficiency, minimizing the cost of discarded parts, and maximizing factory productivity. However, in cases where assembly tasks are performed manually over multiple days, implementing smart factory systems remains a challenge. Previous work has proposed Anomaly Triplet-Net, which estimates assembly progress by applying deep metric learning to the visual features of products. Nevertheless, when visual changes between consecutive tasks are subtle, misclassification often occurs. To address this issue, this paper proposes a robust system for estimating assembly progress, even in cases of occlusion or minimal visual change, using a small-scale dataset. Our method leverages a Quadruplet Loss-based learning approach for anomaly images and introduces a custom data loader that strategically selects training samples to enhance estimation accuracy. We evaluated our approach using a image datasets: captured during desktop PC assembly. The proposed Anomaly Quadruplet-Net outperformed existing methods on the dataset. Specifically, it improved the estimation accuracy by 1.3% and reduced misclassification between adjacent tasks by 1.9% in the desktop PC dataset and demonstrating the effectiveness of the proposed method.

</details>


### [45] [CPPO: Contrastive Perception for Vision Language Policy Optimization](https://arxiv.org/abs/2601.00501)
*Ahmad Rezaei,Mohsen Gholami,Saeed Ranjbar Alvar,Kevin Cannons,Mohammad Asiful Hossain,Zhou Weimin,Shunbo Zhou,Yong Zhang,Mohammad Akbari*

Main category: cs.CV

TL;DR: CPPO introduces a Contrastive Perception Policy Optimization method for finetuning vision-language models that uses entropy shifts to detect perception tokens and applies contrastive perception loss, outperforming previous methods without needing extra models.


<details>
  <summary>Details</summary>
Motivation: While RL has advanced reasoning in language models, extending it to multimodal reasoning requires improving both perception and reasoning aspects. Prior methods use explicit perception rewards but struggle to disentangle perception from reasoning tokens, requiring extra LLMs, ground-truth data, or indiscriminate reward application.

Method: CPPO detects perception tokens via entropy shifts in model outputs under perturbed input images, then extends the RL objective with Contrastive Perception Loss (CPL) that enforces consistency under information-preserving perturbations and sensitivity under information-removing ones.

Result: CPPO surpasses previous perception-rewarding methods while avoiding extra models, making training more efficient and scalable.

Conclusion: CPPO provides an effective approach for finetuning VLMs that addresses the perception-reasoning disentanglement challenge through contrastive perception optimization without requiring additional models or complex reward engineering.

Abstract: We introduce CPPO, a Contrastive Perception Policy Optimization method for finetuning vision-language models (VLMs). While reinforcement learning (RL) has advanced reasoning in language models, extending it to multimodal reasoning requires improving both the perception and reasoning aspects. Prior works tackle this challenge mainly with explicit perception rewards, but disentangling perception tokens from reasoning tokens is difficult, requiring extra LLMs, ground-truth data, forced separation of perception from reasoning by policy model, or applying rewards indiscriminately to all output tokens. CPPO addresses this problem by detecting perception tokens via entropy shifts in the model outputs under perturbed input images. CPPO then extends the RL objective function with a Contrastive Perception Loss (CPL) that enforces consistency under information-preserving perturbations and sensitivity under information-removing ones. Experiments show that CPPO surpasses previous perception-rewarding methods, while avoiding extra models, making training more efficient and scalable.

</details>


### [46] [All-in-One Video Restoration under Smoothly Evolving Unknown Weather Degradations](https://arxiv.org/abs/2601.00533)
*Wenrui Li,Hongtao Chen,Yao Xiao,Wangmeng Zuo,Jiantao Zhou,Yonghong Tian,Xiaopeng Fan*

Main category: cs.CV

TL;DR: ORCANet addresses video restoration with smoothly evolving unknown degradations using recurrent conditional adaptive prompting and coarse intensity estimation.


<details>
  <summary>Details</summary>
Motivation: Existing video restoration methods focus on frame-wise degradation variation but ignore temporal continuity in real-world degradation processes where degradation types and intensities evolve smoothly over time, with multiple degradations coexisting or transitioning gradually.

Method: Proposes ORCANet with: 1) Coarse Intensity Estimation Dehazing (CIED) module using physical priors for haze intensity estimation and coarse dehazed features; 2) Flow Prompt Generation (FPG) module extracting degradation features with static prompts for segment-level degradation types and dynamic prompts for frame-level intensity variations; 3) label-aware supervision for discriminative static prompt representations.

Result: Extensive experiments show ORCANet achieves superior restoration quality, temporal consistency, and robustness over image and video-based baselines.

Conclusion: The proposed SEUD scenario and ORCANet framework effectively address the challenge of smoothly evolving unknown degradations in video restoration, with demonstrated improvements in quality and temporal consistency.

Abstract: All-in-one image restoration aims to recover clean images from diverse unknown degradations using a single model. But extending this task to videos faces unique challenges. Existing approaches primarily focus on frame-wise degradation variation, overlooking the temporal continuity that naturally exists in real-world degradation processes. In practice, degradation types and intensities evolve smoothly over time, and multiple degradations may coexist or transition gradually. In this paper, we introduce the Smoothly Evolving Unknown Degradations (SEUD) scenario, where both the active degradation set and degradation intensity change continuously over time. To support this scenario, we design a flexible synthesis pipeline that generates temporally coherent videos with single, compound, and evolving degradations. To address the challenges in the SEUD scenario, we propose an all-in-One Recurrent Conditional and Adaptive prompting Network (ORCANet). First, a Coarse Intensity Estimation Dehazing (CIED) module estimates haze intensity using physical priors and provides coarse dehazed features as initialization. Second, a Flow Prompt Generation (FPG) module extracts degradation features. FPG generates both static prompts that capture segment-level degradation types and dynamic prompts that adapt to frame-level intensity variations. Furthermore, a label-aware supervision mechanism improves the discriminability of static prompt representations under different degradations. Extensive experiments show that ORCANet achieves superior restoration quality, temporal consistency, and robustness over image and video-based baselines. Code is available at https://github.com/Friskknight/ORCANet-SEUD.

</details>


### [47] [FreeText: Training-Free Text Rendering in Diffusion Transformers via Attention Localization and Spectral Glyph Injection](https://arxiv.org/abs/2601.00535)
*Ruiqiang Zhang,Hengyi Wang,Chang Liu,Guanjie Wang,Zehua Ma,Weiming Zhang*

Main category: cs.CV

TL;DR: FreeText is a training-free framework that improves text rendering in diffusion models by decomposing the problem into where to write (using attention-based localization) and what to write (using spectral-modulated glyph injection).


<details>
  <summary>Details</summary>
Motivation: Large-scale text-to-image diffusion models struggle with precise text rendering, especially for multi-line layouts, dense typography, and long-tailed scripts like Chinese. Existing solutions require costly retraining or rigid external layout constraints that degrade aesthetics and limit flexibility.

Method: FreeText decomposes text rendering into two parts: 1) Where to write: uses token-wise spatial attribution from image-to-text attention with sink-like tokens as spatial anchors and topology-aware refinement to produce high-confidence masks. 2) What to write: introduces Spectral-Modulated Glyph Injection (SGMI) which injects noise-aligned glyph prior with frequency-domain band-pass modulation to strengthen glyph structure and suppress semantic leakage.

Result: Extensive experiments on Qwen-Image, FLUX.1-dev, and SD3 variants across multiple benchmarks (longText-Benchmark, CVTG, CLT-Bench) show consistent gains in text readability while largely preserving semantic alignment and aesthetic quality, with modest inference overhead.

Conclusion: FreeText provides an effective training-free, plug-and-play solution for improving text rendering in diffusion models by leveraging intrinsic DiT mechanisms, addressing both spatial localization and glyph quality without requiring model retraining or rigid constraints.

Abstract: Large-scale text-to-image (T2I) diffusion models excel at open-domain synthesis but still struggle with precise text rendering, especially for multi-line layouts, dense typography, and long-tailed scripts such as Chinese. Prior solutions typically require costly retraining or rigid external layout constraints, which can degrade aesthetics and limit flexibility. We propose \textbf{FreeText}, a training-free, plug-and-play framework that improves text rendering by exploiting intrinsic mechanisms of \emph{Diffusion Transformer (DiT)} models. \textbf{FreeText} decomposes the problem into \emph{where to write} and \emph{what to write}. For \emph{where to write}, we localize writing regions by reading token-wise spatial attribution from endogenous image-to-text attention, using sink-like tokens as stable spatial anchors and topology-aware refinement to produce high-confidence masks. For \emph{what to write}, we introduce Spectral-Modulated Glyph Injection (SGMI), which injects a noise-aligned glyph prior with frequency-domain band-pass modulation to strengthen glyph structure and suppress semantic leakage (rendering the concept instead of the word). Extensive experiments on Qwen-Image, FLUX.1-dev, and SD3 variants across longText-Benchmark, CVTG, and our CLT-Bench show consistent gains in text readability while largely preserving semantic alignment and aesthetic quality, with modest inference overhead.

</details>


### [48] [Boosting Segment Anything Model to Generalize Visually Non-Salient Scenarios](https://arxiv.org/abs/2601.00537)
*Guangqian Guo,Pengfei Chen,Yong Guo,Huafeng Chen,Boqiang Zhang,Shan Gao*

Main category: cs.CV

TL;DR: VNS-SAM enhances Segment Anything Model's performance in visually non-salient scenarios (low contrast between foreground/background) while preserving zero-shot generalizability, using novel decoder designs and a comprehensive VNS-SEG dataset.


<details>
  <summary>Details</summary>
Motivation: SAM struggles with visually non-salient scenarios where there's low contrast between foreground and background, leading to inaccurate contours and poor segmentation results in these challenging cases.

Method: Proposes VNS-SAM with two key designs: 1) Mask-Edge Token Interactive decoder, and 2) Non-Salient Feature Mining module, which exploit SAM's low-level features to understand non-salient characteristics with minimal parameter/computational overhead. Also introduces VNS-SEG dataset with 35K+ images for training and benchmarking.

Result: VNS-SAM achieves superior performance across various VNS segmentation tasks, especially in zero-shot settings, with additional parameters that can be optimized within 4 hours, demonstrating feasibility and practicality.

Conclusion: VNS-SAM effectively enhances SAM's perception of visually non-salient scenarios while maintaining its zero-shot generalizability, showing potential for broad real-world applications. The comprehensive VNS-SEG dataset enables robust feature learning and benchmarking.

Abstract: Segment Anything Model (SAM), known for its remarkable zero-shot segmentation capabilities, has garnered significant attention in the community. Nevertheless, its performance is challenged when dealing with what we refer to as visually non-salient scenarios, where there is low contrast between the foreground and background. In these cases, existing methods often cannot capture accurate contours and fail to produce promising segmentation results. In this paper, we propose Visually Non-Salient SAM (VNS-SAM), aiming to enhance SAM's perception of visually non-salient scenarios while preserving its original zero-shot generalizability. We achieve this by effectively exploiting SAM's low-level features through two designs: Mask-Edge Token Interactive decoder and Non-Salient Feature Mining module. These designs help the SAM decoder gain a deeper understanding of non-salient characteristics with only marginal parameter increments and computational requirements. The additional parameters of VNS-SAM can be optimized within 4 hours, demonstrating its feasibility and practicality. In terms of data, we established VNS-SEG, a unified dataset for various VNS scenarios, with more than 35K images, in contrast to previous single-task adaptations. It is designed to make the model learn more robust VNS features and comprehensively benchmark the model's segmentation performance and generalizability on VNS scenarios. Extensive experiments across various VNS segmentation tasks demonstrate the superior performance of VNS-SAM, particularly under zero-shot settings, highlighting its potential for broad real-world applications. Codes and datasets are publicly available at https://guangqian-guo.github.io/VNS-SAM.

</details>


### [49] [DynaDrag: Dynamic Drag-Style Image Editing by Motion Prediction](https://arxiv.org/abs/2601.00542)
*Jiacheng Sui,Yujie Zhou,Li Niu*

Main category: cs.CV

TL;DR: DynaDrag introduces a new "predict-and-move" framework for drag-style image editing that iteratively predicts handle point movements and executes them, outperforming previous methods.


<details>
  <summary>Details</summary>
Motivation: Previous drag-style image editing methods suffer from tracking failures, ambiguous tracking, large source-target image gaps, and unreasonable intermediate points that reduce editability.

Method: DynaDrag uses a predict-and-move framework with iterative Motion Prediction (predicting handle point movements) and Motion Supervision (executing the movements), plus dynamic adjustment of valid handle points.

Result: Experiments on face and human datasets demonstrate superiority over previous works in drag-style image editing performance.

Conclusion: DynaDrag's predict-and-move framework effectively addresses limitations of previous drag-style editing methods, providing better pixel-level image manipulation.

Abstract: To achieve pixel-level image manipulation, drag-style image editing which edits images using points or trajectories as conditions is attracting widespread attention. Most previous methods follow move-and-track framework, in which miss tracking and ambiguous tracking are unavoidable challenging issues. Other methods under different frameworks suffer from various problems like the huge gap between source image and target edited image as well as unreasonable intermediate point which can lead to low editability. To avoid these problems, we propose DynaDrag, the first dragging method under predict-and-move framework. In DynaDrag, Motion Prediction and Motion Supervision are performed iteratively. In each iteration, Motion Prediction first predicts where the handle points should move, and then Motion Supervision drags them accordingly. We also propose to dynamically adjust the valid handle points to further improve the performance. Experiments on face and human datasets showcase the superiority over previous works.

</details>


### [50] [SingBAG Pro: Accelerating point cloud-based iterative reconstruction for 3D photoacoustic imaging under arbitrary array](https://arxiv.org/abs/2601.00551)
*Shuang Li,Yibing Wang,Jian Gao,Chulhong Kim,Seongwook Choi,Yu Zhang,Qian Chen,Yao Yao,Changhui Li*

Main category: cs.CV

TL;DR: SlingBAG Pro is an advanced 3D photoacoustic reconstruction algorithm that extends the original SlingBAG method to work with irregular transducer arrays, achieving faster reconstruction while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: Irregular geometric transducer arrays are promising for high-quality 3D photoacoustic imaging with fewer transducers, but traditional iterative reconstruction algorithms struggle with these configurations due to high computational complexity, memory requirements, and long reconstruction times.

Method: SlingBAG Pro extends the point cloud iteration concept of the original Sliding ball adaptive growth (SlingBAG) method to arbitrary array geometries. It employs a hierarchical optimization strategy combining zero-gradient filtering with progressively increased temporal sampling rates during iteration to rapidly remove redundant spatial point clouds and accelerate convergence.

Result: Compared to the original SlingBAG algorithm, SlingBAG Pro achieves up to 2.2-fold speed improvement in point cloud-based 3D PA reconstruction under irregular array geometries. The method is validated through both simulation and in vivo mouse experiments.

Conclusion: SlingBAG Pro enables efficient high-quality 3D photoacoustic imaging with irregular transducer arrays, reducing computational requirements while maintaining reconstruction quality, making it suitable for clinical applications with space and cost constraints.

Abstract: High-quality three-dimensional (3D) photoacoustic imaging (PAI) is gaining increasing attention in clinical applications. To address the challenges of limited space and high costs, irregular geometric transducer arrays that conform to specific imaging regions are promising for achieving high-quality 3D PAI with fewer transducers. However, traditional iterative reconstruction algorithms struggle with irregular array configurations, suffering from high computational complexity, substantial memory requirements, and lengthy reconstruction times. In this work, we introduce SlingBAG Pro, an advanced reconstruction algorithm based on the point cloud iteration concept of the Sliding ball adaptive growth (SlingBAG) method, while extending its compatibility to arbitrary array geometries. SlingBAG Pro maintains high reconstruction quality, reduces the number of required transducers, and employs a hierarchical optimization strategy that combines zero-gradient filtering with progressively increased temporal sampling rates during iteration. This strategy rapidly removes redundant spatial point clouds, accelerates convergence, and significantly shortens overall reconstruction time. Compared to the original SlingBAG algorithm, SlingBAG Pro achieves up to a 2.2-fold speed improvement in point cloud-based 3D PA reconstruction under irregular array geometries. The proposed method is validated through both simulation and in vivo mouse experiments, and the source code is publicly available at https://github.com/JaegerCQ/SlingBAG_Pro.

</details>


### [51] [AEGIS: Exploring the Limit of World Knowledge Capabilities for Unified Mulitmodal Models](https://arxiv.org/abs/2601.00561)
*Jintao Lin,Bowen Dong,Weikang Shi,Chenyang Lei,Suiyun Zhang,Rui Liu,Xihui Liu*

Main category: cs.CV

TL;DR: AEGIS benchmark assesses UMMs' world knowledge across multiple tasks, revealing severe deficits in complex reasoning that can be partially mitigated with reasoning modules.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for Unified Multimodal Models (UMMs) are inadequate - they only offer siloed, single-task evaluations with limited diagnostic power, failing to properly assess UMMs' ability to apply world knowledge across diverse tasks.

Method: Proposed AEGIS benchmark covering visual understanding, generation, editing, and interleaved generation with 1,050 manually-annotated questions across 21 topics and 6 reasoning types. Also introduced Deterministic Checklist-based Evaluation (DCE) protocol using atomic "Y/N" judgments instead of ambiguous prompt-based scoring.

Result: Most UMMs exhibit severe world knowledge deficits, with performance degrading significantly with complex reasoning. Simple plug-in reasoning modules can partially mitigate these vulnerabilities.

Conclusion: World-knowledge-based reasoning represents a critical frontier for UMM development, and the AEGIS benchmark provides a comprehensive tool for assessing this capability.

Abstract: The capability of Unified Multimodal Models (UMMs) to apply world knowledge across diverse tasks remains a critical, unresolved challenge. Existing benchmarks fall short, offering only siloed, single-task evaluations with limited diagnostic power. To bridge this gap, we propose AEGIS (\emph{i.e.}, \textbf{A}ssessing \textbf{E}diting, \textbf{G}eneration, \textbf{I}nterpretation-Understanding for \textbf{S}uper-intelligence), a comprehensive multi-task benchmark covering visual understanding, generation, editing, and interleaved generation. AEGIS comprises 1,050 challenging, manually-annotated questions spanning 21 topics (including STEM, humanities, daily life, etc.) and 6 reasoning types. To concretely evaluate the performance of UMMs in world knowledge scope without ambiguous metrics, we further propose Deterministic Checklist-based Evaluation (DCE), a protocol that replaces ambiguous prompt-based scoring with atomic ``Y/N'' judgments, to enhance evaluation reliability. Our extensive experiments reveal that most UMMs exhibit severe world knowledge deficits and that performance degrades significantly with complex reasoning. Additionally, simple plug-in reasoning modules can partially mitigate these vulnerabilities, highlighting a promising direction for future research. These results highlight the importance of world-knowledge-based reasoning as a critical frontier for UMMs.

</details>


### [52] [A Cascaded Information Interaction Network for Precise Image Segmentation](https://arxiv.org/abs/2601.00562)
*Hewen Xiao,Jie Mei,Guangfu Ma,Weiren Wu*

Main category: cs.CV

TL;DR: Proposes a cascaded CNN with Global Information Guidance Module for robust visual segmentation in complex scenarios, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Visual perception is crucial for autonomous systems but robust segmentation remains challenging in complex scenarios like cluttered or blurred environments where traditional methods fail.

Method: Cascaded convolutional neural network integrated with a novel Global Information Guidance Module that fuses low-level texture details with high-level semantic features across multiple layers.

Result: Achieves superior precision on benchmark image segmentation datasets, outperforming existing state-of-the-art methods, particularly in visually cluttered or blurred environments.

Conclusion: The approach is effective and shows promising potential for deployment in practical robotic applications due to its enhanced segmentation accuracy in challenging scenarios.

Abstract: Visual perception plays a pivotal role in enabling autonomous behavior, offering a cost-effective and efficient alternative to complex multi-sensor systems. However, robust segmentation remains a challenge in complex scenarios. To address this, this paper proposes a cascaded convolutional neural network integrated with a novel Global Information Guidance Module. This module is designed to effectively fuse low-level texture details with high-level semantic features across multiple layers, thereby overcoming the inherent limitations of single-scale feature extraction. This architectural innovation significantly enhances segmentation accuracy, particularly in visually cluttered or blurred environments where traditional methods often fail. Experimental evaluations on benchmark image segmentation datasets demonstrate that the proposed framework achieves superior precision, outperforming existing state-of-the-art methods. The results highlight the effectiveness of the approach and its promising potential for deployment in practical robotic applications.

</details>


### [53] [GranAlign: Granularity-Aware Alignment Framework for Zero-Shot Video Moment Retrieval](https://arxiv.org/abs/2601.00584)
*Mingyu Jeon,Sunjae Yoon,Jonghee Kim,Junyeoung Kim*

Main category: cs.CV

TL;DR: GranAlign is a training-free framework for zero-shot video moment retrieval that addresses semantic granularity mismatch between text queries and video content through query rewriting and query-aware caption generation.


<details>
  <summary>Details</summary>
Motivation: Previous ZVMR approaches fail to balance semantic granularity between pre-trained video and language representations, leading to inaccurate retrieval despite high-quality individual modality representations.

Method: Proposes Granularity-Aware Alignment (GranAlign) with two techniques: 1) granularity-based query rewriting to generate varied semantic granularities, and 2) query-aware caption generation to embed query intent into video content. Pairs multi-level queries with both query-agnostic and query-aware captions.

Result: Sets new state-of-the-art across all three major benchmarks (QVHighlights, Charades-STA, ActivityNet-Captions), with 3.23% mAP@avg improvement on challenging QVHighlights dataset.

Conclusion: GranAlign effectively resolves semantic granularity mismatches in zero-shot video moment retrieval without requiring task-specific training, achieving superior performance through complementary granularity alignment techniques.

Abstract: Zero-shot video moment retrieval (ZVMR) is the task of localizing a temporal moment within an untrimmed video using a natural language query without relying on task-specific training data. The primary challenge in this setting lies in the mismatch in semantic granularity between textual queries and visual content. Previous studies in ZVMR have attempted to achieve alignment by leveraging high-quality pre-trained knowledge that represents video and language in a joint space. However, these approaches failed to balance the semantic granularity between the pre-trained knowledge provided by each modality for a given scene. As a result, despite the high quality of each modality's representations, the mismatch in granularity led to inaccurate retrieval. In this paper, we propose a training-free framework, called Granularity-Aware Alignment (GranAlign), that bridges this gap between coarse and fine semantic representations. Our approach introduces two complementary techniques: granularity-based query rewriting to generate varied semantic granularities, and query-aware caption generation to embed query intent into video content. By pairing multi-level queries with both query-agnostic and query-aware captions, we effectively resolve semantic mismatches. As a result, our method sets a new state-of-the-art across all three major benchmarks (QVHighlights, Charades-STA, ActivityNet-Captions), with a notable 3.23% mAP@avg improvement on the challenging QVHighlights dataset.

</details>


### [54] [SafeMo: Linguistically Grounded Unlearning for Trustworthy Text-to-Motion Generation](https://arxiv.org/abs/2601.00590)
*Yiling Wang,Zeyu Zhang,Yiran Wang,Hao Tang*

Main category: cs.CV

TL;DR: SafeMo is a trustworthy motion generation framework that uses Minimal Motion Unlearning to enable safe human motion generation in continuous space, avoiding artifacts from discrete codebook methods while maintaining good performance on benign prompts.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-motion methods have safety concerns, and current discrete codebook replacement approaches degrade benign performance and cause artifacts. Additionally, existing datasets contain unsafe content, making them unsuitable for safety-focused learning.

Method: Proposes SafeMo with Minimal Motion Unlearning (MMU), a two-stage machine unlearning strategy that operates in continuous space to avoid quantization loss. Also introduces SafeMoVAE-29K dataset with rewritten safe prompts and refined motions.

Result: Achieves 2.5x and 14.4x higher forget-set FID on HumanML3D and Motion-X respectively compared to previous SOTA LCR, with comparable or better performance on safe prompts. Generates safe motions with natural transitions.

Conclusion: SafeMo provides an effective framework for trustworthy motion generation with strong safety-utility trade-offs, addressing limitations of discrete codebook methods while enabling safe human motion generation in continuous space.

Abstract: Text-to-motion (T2M) generation with diffusion backbones achieves strong realism and alignment. Safety concerns in T2M methods have been raised in recent years; existing methods replace discrete VQ-VAE codebook entries to steer the model away from unsafe behaviors. However, discrete codebook replacement-based methods have two critical flaws: firstly, replacing codebook entries which are reused by benign prompts leads to drifts on everyday tasks, degrading the model's benign performance; secondly, discrete token-based methods introduce quantization and smoothness loss, resulting in artifacts and jerky transitions. Moreover, existing text-to-motion datasets naturally contain unsafe intents and corresponding motions, making them unsuitable for safety-driven machine learning. To address these challenges, we propose SafeMo, a trustworthy motion generative framework integrating Minimal Motion Unlearning (MMU), a two-stage machine unlearning strategy, enabling safe human motion generation in continuous space, preserving continuous kinematics without codebook loss and delivering strong safety-utility trade-offs compared to current baselines. Additionally, we present the first safe text-to-motion dataset SafeMoVAE-29K integrating rewritten safe text prompts and continuous refined motion for trustworthy human motion unlearning. Built upon DiP, SafeMo efficiently generates safe human motions with natural transitions. Experiments demonstrate effective unlearning performance of SafeMo by showing strengthened forgetting on unsafe prompts, reaching 2.5x and 14.4x higher forget-set FID on HumanML3D and Motion-X respectively, compared to the previous SOTA human motion unlearning method LCR, with benign performance on safe prompts being better or comparable. Code: https://github.com/AIGeeksGroup/SafeMo. Website: https://aigeeksgroup.github.io/SafeMo.

</details>


### [55] [Modality Dominance-Aware Optimization for Embodied RGB-Infrared Perception](https://arxiv.org/abs/2601.00598)
*Xianhui Liu,Siqi Jiang,Yi Xie,Yuqing Lin,Siao Liu*

Main category: cs.CV

TL;DR: MDACL framework addresses optimization bias in RGB-IR fusion by quantifying modality dominance with MDI and balancing optimization through hierarchical guidance and adversarial regularization.


<details>
  <summary>Details</summary>
Motivation: RGB-IR multimodal perception is crucial for embodied systems, but asymmetric modality characteristics cause optimization bias where training overemphasizes dominant modalities, hindering effective fusion.

Method: Proposes Modality Dominance Index (MDI) to measure dominance via feature entropy and gradient contribution, then develops MDACL framework with Hierarchical Cross-modal Guidance (HCG) for feature alignment and Adversarial Equilibrium Regularization (AER) for balanced optimization.

Result: Extensive experiments on three RGB-IR benchmarks show MDACL effectively mitigates optimization bias and achieves state-of-the-art performance.

Conclusion: The MDACL framework successfully addresses optimization bias in RGB-IR fusion through systematic quantification and regulation of modality dominance, advancing multimodal perception for embodied systems.

Abstract: RGB-Infrared (RGB-IR) multimodal perception is fundamental to embodied multimedia systems operating in complex physical environments. Although recent cross-modal fusion methods have advanced RGB-IR detection, the optimization dynamics caused by asymmetric modality characteristics remain underexplored. In practice, disparities in information density and feature quality introduce persistent optimization bias, leading training to overemphasize a dominant modality and hindering effective fusion. To quantify this phenomenon, we propose the Modality Dominance Index (MDI), which measures modality dominance by jointly modeling feature entropy and gradient contribution. Based on MDI, we develop a Modality Dominance-Aware Cross-modal Learning (MDACL) framework that regulates cross-modal optimization. MDACL incorporates Hierarchical Cross-modal Guidance (HCG) to enhance feature alignment and Adversarial Equilibrium Regularization (AER) to balance optimization dynamics during fusion. Extensive experiments on three RGB-IR benchmarks demonstrate that MDACL effectively mitigates optimization bias and achieves SOTA performance.

</details>


### [56] [RePose: A Real-Time 3D Human Pose Estimation and Biomechanical Analysis Framework for Rehabilitation](https://arxiv.org/abs/2601.00625)
*Junxiao Xue,Pavel Smirnov,Ziao Li,Yunyun Shi,Shi Chen,Xinyi Yin,Xiaohan Yue,Lei Wang,Yiduo Wang,Feng Lin,Yijia Chen,Xiao Ma,Xiaoran Yan,Qing Zhang,Fengjian Xue,Xuecheng Wu*

Main category: cs.CV

TL;DR: RePose: Real-time 3D human pose estimation system for rehabilitation training using multi-camera RGB input, fast tracking, and SmoothNet modification for smoother motion analysis with Unity visualization.


<details>
  <summary>Details</summary>
Motivation: Need for real-time monitoring and evaluation of patient motion during rehabilitation to provide immediate feedback and guidance, helping patients execute exercises correctly and regain muscle strength/motor functions.

Method: 1) Unified end-to-end pipeline for real-time human pose estimation from multi-camera RGB video; 2) Fast tracking method for medical rehabilitation with multi-person interference (<1ms per frame); 3) Modified SmoothNet for real-time posture estimation to reduce errors and restore true motion; 4) Unity platform for real-time monitoring and muscle stress visualization.

Result: Real-time 3D human pose estimation and motion analysis system capable of immediate feedback during rehabilitation, with fast tracking (<1ms per frame) and smoother motion visualization through modified SmoothNet.

Conclusion: RePose provides an effective real-time solution for rehabilitation training with accurate motion monitoring, fast tracking in multi-person scenarios, and smooth visualization to assist patients in proper exercise execution.

Abstract: We propose a real-time 3D human pose estimation and motion analysis method termed RePose for rehabilitation training. It is capable of real-time monitoring and evaluation of patients'motion during rehabilitation, providing immediate feedback and guidance to assist patients in executing rehabilitation exercises correctly. Firstly, we introduce a unified pipeline for end-to-end real-time human pose estimation and motion analysis using RGB video input from multiple cameras which can be applied to the field of rehabilitation training. The pipeline can help to monitor and correct patients'actions, thus aiding them in regaining muscle strength and motor functions. Secondly, we propose a fast tracking method for medical rehabilitation scenarios with multiple-person interference, which requires less than 1ms for tracking for a single frame. Additionally, we modify SmoothNet for real-time posture estimation, effectively reducing pose estimation errors and restoring the patient's true motion state, making it visually smoother. Finally, we use Unity platform for real-time monitoring and evaluation of patients' motion during rehabilitation, and to display the muscle stress conditions to assist patients with their rehabilitation training.

</details>


### [57] [HyperPriv-EPN: Hypergraph Learning with Privileged Knowledge for Ependymoma Prognosis](https://arxiv.org/abs/2601.00626)
*Shuren Gabriel Yu,Sikang Ren,Yongji Tian*

Main category: cs.CV

TL;DR: HyperPriv-EPN: A hypergraph-based framework that uses privileged post-surgery text data during training to improve preoperative ependymoma prognosis from MRI alone at inference.


<details>
  <summary>Details</summary>
Motivation: Preoperative prognosis of ependymoma is challenging because MRI lacks the semantic insights available in post-operative surgical reports, and existing methods can't leverage this privileged text data when it's unavailable during inference.

Method: Hypergraph-based Learning Using Privileged Information (LUPI) framework with Severed Graph Strategy: uses shared encoder for Teacher graph (with post-surgery info) and Student graph (pre-op only), with dual-stream distillation to help Student hallucinate semantic community structures from visual features.

Result: Validated on 311 multi-center patients, achieves state-of-the-art diagnostic accuracy and survival stratification, effectively transferring expert knowledge to preoperative setting.

Conclusion: Unlocks value of historical post-operative data to guide diagnosis of new patients without requiring text at inference, bridging gap between pre-op MRI and post-op expert knowledge.

Abstract: Preoperative prognosis of Ependymoma is critical for treatment planning but challenging due to the lack of semantic insights in MRI compared to post-operative surgical reports. Existing multimodal methods fail to leverage this privileged text data when it is unavailable during inference. To bridge this gap, we propose HyperPriv-EPN, a hypergraph-based Learning Using Privileged Information (LUPI) framework. We introduce a Severed Graph Strategy, utilizing a shared encoder to process both a Teacher graph (enriched with privileged post-surgery information) and a Student graph (restricted to pre-operation data). Through dual-stream distillation, the Student learns to hallucinate semantic community structures from visual features alone. Validated on a multi-center cohort of 311 patients, HyperPriv-EPN achieves state-of-the-art diagnostic accuracy and survival stratification. This effectively transfers expert knowledge to the preoperative setting, unlocking the value of historical post-operative data to guide the diagnosis of new patients without requiring text at inference.

</details>


### [58] [Quality Detection of Stored Potatoes via Transfer Learning: A CNN and Vision Transformer Approach](https://arxiv.org/abs/2601.00645)
*Shrikant Kapse,Priyankkumar Dhrangdhariya,Priya Kedia,Manasi Patwardhan,Shankar Kausley,Soumyadipta Maiti,Beena Rai,Shirish Karande*

Main category: cs.CV

TL;DR: Image-based deep learning models using pre-trained architectures (ResNet, VGG, DenseNet, ViT) achieve high accuracy for potato sprout detection (98.03%) and shelf-life prediction (89.83% with coarse classes), enabling automated quality monitoring during storage.


<details>
  <summary>Details</summary>
Motivation: To address challenges in potato storage quality monitoring including sprout detection, weight loss estimation, and shelf-life prediction through non-invasive, scalable deep learning solutions that can be integrated into automated systems.

Method: Collected images and weight data over 200 days under controlled conditions, then leveraged pre-trained architectures (ResNet, VGG, DenseNet, ViT) to design two specialized models: a binary classifier for sprout detection and a multi-class predictor for weight loss and shelf-life estimation.

Result: DenseNet achieved 98.03% accuracy in sprout detection. Shelf-life prediction performed best with coarse class divisions (2-5 classes, over 89.83% accuracy), while accuracy declined for finer divisions (6-8 classes) due to subtle visual differences and limited data per class.

Conclusion: Image-based deep learning offers a feasible, cost-effective, non-destructive method for potato quality assessment that can be integrated into automated sorting systems. While exact shelf-life prediction remains challenging, focusing on broader class divisions ensures robust performance. Future work should develop generalized models across diverse varieties and conditions.

Abstract: Image-based deep learning provides a non-invasive, scalable solution for monitoring potato quality during storage, addressing key challenges such as sprout detection, weight loss estimation, and shelf-life prediction. In this study, images and corresponding weight data were collected over a 200-day period under controlled temperature and humidity conditions. Leveraging powerful pre-trained architectures of ResNet, VGG, DenseNet, and Vision Transformer (ViT), we designed two specialized models: (1) a high-precision binary classifier for sprout detection, and (2) an advanced multi-class predictor to estimate weight loss and forecast remaining shelf-life with remarkable accuracy. DenseNet achieved exceptional performance, with 98.03% accuracy in sprout detection. Shelf-life prediction models performed best with coarse class divisions (2-5 classes), achieving over 89.83% accuracy, while accuracy declined for finer divisions (6-8 classes) due to subtle visual differences and limited data per class. These findings demonstrate the feasibility of integrating image-based models into automated sorting and inventory systems, enabling early identification of sprouted potatoes and dynamic categorization based on storage stage. Practical implications include improved inventory management, differential pricing strategies, and reduced food waste across supply chains. While predicting exact shelf-life intervals remains challenging, focusing on broader class divisions ensures robust performance. Future research should aim to develop generalized models trained on diverse potato varieties and storage conditions to enhance adaptability and scalability. Overall, this approach offers a cost-effective, non-destructive method for quality assessment, supporting efficiency and sustainability in potato storage and distribution.

</details>


### [59] [Reconstructing Building Height from Spaceborne TomoSAR Point Clouds Using a Dual-Topology Network](https://arxiv.org/abs/2601.00658)
*Zhaiyu Chen,Yuanyuan Wang,Yilei Shi,Xiao Xiang Zhu*

Main category: cs.CV

TL;DR: Learning-based framework converts noisy TomoSAR point clouds into high-resolution building height maps using dual-topology network with point and grid branches.


<details>
  <summary>Details</summary>
Motivation: TomoSAR provides weather-independent urban observations but suffers from noise, anisotropic point distributions, and data voids that hinder accurate building height estimation.

Method: Dual-topology network alternates between point branch (models irregular scatterer features) and grid branch (enforces spatial consistency) to jointly denoise and inpaint missing regions.

Result: First proof of concept for large-scale urban height mapping directly from TomoSAR point clouds, validated on Munich and Berlin data; can incorporate optical imagery for enhanced quality.

Conclusion: Proposed framework effectively converts raw TomoSAR points into continuous building height maps, addressing key challenges in SAR-based urban height estimation.

Abstract: Reliable building height estimation is essential for various urban applications. Spaceborne SAR tomography (TomoSAR) provides weather-independent, side-looking observations that capture facade-level structure, offering a promising alternative to conventional optical methods. However, TomoSAR point clouds often suffer from noise, anisotropic point distributions, and data voids on incoherent surfaces, all of which hinder accurate height reconstruction. To address these challenges, we introduce a learning-based framework for converting raw TomoSAR points into high-resolution building height maps. Our dual-topology network alternates between a point branch that models irregular scatterer features and a grid branch that enforces spatial consistency. By jointly processing these representations, the network denoises the input points and inpaints missing regions to produce continuous height estimates. To our knowledge, this is the first proof of concept for large-scale urban height mapping directly from TomoSAR point clouds. Extensive experiments on data from Munich and Berlin validate the effectiveness of our approach. Moreover, we demonstrate that our framework can be extended to incorporate optical satellite imagery, further enhancing reconstruction quality. The source code is available at https://github.com/zhu-xlab/tomosar2height.

</details>


### [60] [CRoPS: A Training-Free Hallucination Mitigation Framework for Vision-Language Models](https://arxiv.org/abs/2601.00659)
*Neeraj Anand,Samyak Jha,Udbhav Bamba,Rahul Rahaman*

Main category: cs.CV

TL;DR: CRoPS is a training-free framework that mitigates hallucinations in Large Vision-Language Models by using multiple hallucinated models with selective token removal and generalized contrastive decoding.


<details>
  <summary>Details</summary>
Motivation: LVLMs tend to generate hallucinated content that undermines reliability, and existing training-free methods have limitations: they rely on narrow assumptions about hallucination sources and become less effective toward the end of generation where hallucinations are most likely.

Method: Proposes a novel hallucinated model that captures hallucination effects by selectively removing key text tokens, and introduces Generalized Contrastive Decoding which integrates multiple hallucinated models to represent diverse hallucination sources.

Result: Improves CHAIR scores by 20% and achieves consistent gains across six benchmarks and three LVLM families, outperforming state-of-the-art training-free methods.

Conclusion: CRoPS provides an effective training-free framework for hallucination mitigation in LVLMs by addressing limitations of existing methods through selective text token removal and integration of multiple hallucinated models.

Abstract: Despite the rapid success of Large Vision-Language Models (LVLMs), a persistent challenge is their tendency to generate hallucinated content, undermining reliability in real-world use. Existing training-free methods address hallucinations but face two limitations: (i) they rely on narrow assumptions about hallucination sources, and (ii) their effectiveness declines toward the end of generation, where hallucinations are most likely to occur. A common strategy is to build hallucinated models by completely or partially removing visual tokens and contrasting them with the original model. Yet, this alone proves insufficient, since visual information still propagates into generated text. Building on this insight, we propose a novel hallucinated model that captures hallucination effects by selectively removing key text tokens. We further introduce Generalized Contrastive Decoding, which integrates multiple hallucinated models to represent diverse hallucination sources. Together, these ideas form CRoPS, a training-free hallucination mitigation framework that improves CHAIR scores by 20% and achieves consistent gains across six benchmarks and three LVLM families, outperforming state-of-the-art training-free methods.

</details>


### [61] [Pixel-to-4D: Camera-Controlled Image-to-Video Generation with Dynamic 3D Gaussians](https://arxiv.org/abs/2601.00678)
*Melonie de Almeida,Daniela Ivanova,Tong Shi,John H. Williamson,Paul Henderson*

Main category: cs.CV

TL;DR: Single-image video generation with precise camera control using 3D Gaussian representations and object motion sampling in one forward pass.


<details>
  <summary>Details</summary>
Motivation: Existing single-image video generation methods lack robust user controllability (like camera path modification) and struggle with accurate camera motion modeling, temporal consistency, and geometric integrity.

Method: Proposes a novel framework that constructs a 3D Gaussian scene representation from a single image and samples plausible object motion in a single forward pass, enabling fast camera-guided video generation without iterative denoising.

Result: Achieves state-of-the-art video quality and inference efficiency on KITTI, Waymo, RealEstate10K, and DL3DV-10K datasets.

Conclusion: The method enables fast, camera-controlled video generation from single images with improved temporal consistency and geometric integrity compared to existing approaches.

Abstract: Humans excel at forecasting the future dynamics of a scene given just a single image. Video generation models that can mimic this ability are an essential component for intelligent systems. Recent approaches have improved temporal coherence and 3D consistency in single-image-conditioned video generation. However, these methods often lack robust user controllability, such as modifying the camera path, limiting their applicability in real-world applications. Most existing camera-controlled image-to-video models struggle with accurately modeling camera motion, maintaining temporal consistency, and preserving geometric integrity. Leveraging explicit intermediate 3D representations offers a promising solution by enabling coherent video generation aligned with a given camera trajectory. Although these methods often use 3D point clouds to render scenes and introduce object motion in a later stage, this two-step process still falls short in achieving full temporal consistency, despite allowing precise control over camera movement. We propose a novel framework that constructs a 3D Gaussian scene representation and samples plausible object motion, given a single image in a single forward pass. This enables fast, camera-guided video generation without the need for iterative denoising to inject object motion into render frames. Extensive experiments on the KITTI, Waymo, RealEstate10K and DL3DV-10K datasets demonstrate that our method achieves state-of-the-art video quality and inference efficiency. The project page is available at https://melonienimasha.github.io/Pixel-to-4D-Website.

</details>


### [62] [Efficient Deep Demosaicing with Spatially Downsampled Isotropic Networks](https://arxiv.org/abs/2601.00703)
*Cory Fan,Wenchao Zhang*

Main category: cs.CV

TL;DR: Downsampling in isotropic networks improves efficiency and performance for mobile image demosaicing and joint-demosaicing-and-denoising tasks.


<details>
  <summary>Details</summary>
Motivation: Most deep learning demosaicing networks avoid spatial downsampling, making them computationally expensive for mobile applications where lightweight and efficient networks are needed.

Method: Designed simple fully convolutional networks with and without downsampling using DeepMAD mathematical architecture design technique, creating JD3Net as the downsampled variant.

Result: Downsampling improves empirical performance, and JD3Net shows strong performance on various image demosaicing and JDD tasks.

Conclusion: Contrary to previous isotropic network designs, spatial downsampling can significantly improve both efficiency and performance for mobile demosaicing applications.

Abstract: In digital imaging, image demosaicing is a crucial first step which recovers the RGB information from a color filter array (CFA). Oftentimes, deep learning is utilized to perform image demosaicing. Given that most modern digital imaging applications occur on mobile platforms, applying deep learning to demosaicing requires lightweight and efficient networks. Isotropic networks, also known as residual-in-residual networks, have been often employed for image demosaicing and joint-demosaicing-and-denoising (JDD). Most demosaicing isotropic networks avoid spatial downsampling entirely, and thus are often prohibitively expensive computationally for mobile applications. Contrary to previous isotropic network designs, this paper claims that spatial downsampling to a signficant degree can improve the efficiency and performance of isotropic networks. To validate this claim, we design simple fully convolutional networks with and without downsampling using a mathematical architecture design technique adapted from DeepMAD, and find that downsampling improves empirical performance. Additionally, empirical testing of the downsampled variant, JD3Net, of our fully convolutional networks reveals strong empirical performance on a variety of image demosaicing and JDD tasks.

</details>


### [63] [Multi-Level Feature Fusion for Continual Learning in Visual Quality Inspection](https://arxiv.org/abs/2601.00725)
*Johannes C. Bauer,Paul Geng,Stephan Trattnig,Petr Dokládal,Rüdiger Daub*

Main category: cs.CV

TL;DR: Multi-level feature fusion approach enables efficient continual learning for quality inspection in volatile manufacturing settings like remanufacturing.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks struggle in volatile manufacturing scenarios like remanufacturing where products and defects frequently change, requiring frequent model adaptation while avoiding catastrophic forgetting.

Method: Proposes multi-level feature fusion (MLFF) approach that utilizes representations from different depths of a pretrained network to enable efficient adaptation with fewer trainable parameters.

Result: MLFF matches end-to-end training performance for quality inspection problems while using significantly fewer trainable parameters, reduces catastrophic forgetting, and improves generalization to new product types/defects.

Conclusion: The MLFF approach enables computationally efficient continual learning for quality inspection in volatile manufacturing environments, addressing both adaptation efficiency and catastrophic forgetting simultaneously.

Abstract: Deep neural networks show great potential for automating various visual quality inspection tasks in manufacturing. However, their applicability is limited in more volatile scenarios, such as remanufacturing, where the inspected products and defect patterns often change. In such settings, deployed models require frequent adaptation to novel conditions, effectively posing a continual learning problem. To enable quick adaptation, the necessary training processes must be computationally efficient while still avoiding effects like catastrophic forgetting. This work presents a multi-level feature fusion (MLFF) approach that aims to improve both aspects simultaneously by utilizing representations from different depths of a pretrained network. We show that our approach is able to match the performance of end-to-end training for different quality inspection problems while using significantly less trainable parameters. Furthermore, it reduces catastrophic forgetting and improves generalization robustness to new product types or defects.

</details>


### [64] [Grading Handwritten Engineering Exams with Multimodal Large Language Models](https://arxiv.org/abs/2601.00730)
*Janez Perš,Jon Muhovič,Andrej Košir,Boštjan Murovec*

Main category: cs.CV

TL;DR: End-to-end workflow using multimodal LLMs to automatically grade handwritten STEM exams with ≈8-point accuracy compared to lecturer grading, requiring only handwritten reference solution and grading rules.


<details>
  <summary>Details</summary>
Motivation: Manual grading of handwritten STEM exams is slow and difficult to scale, despite capturing valuable open-ended reasoning and diagrams. Need automated solution that preserves standard exam process while handling unconstrained student handwriting.

Method: Multi-stage pipeline: format/presence check, ensemble of independent LLM graders (GPT-5.2 & Gemini-3 Pro), supervisor aggregation, rigid templates with deterministic validation. Reference solution converted to text-only summary for conditioning without exposing original scan.

Result: Achieves ≈8-point mean absolute difference to lecturer grades with low bias, ≈17% manual-review trigger rate at Dmax=40. Handles Slovenian text and hand-drawn circuit schematics. Ablations show structured prompting and reference grounding are essential.

Conclusion: Proposed workflow enables reliable automated grading of handwritten STEM exams using multimodal LLMs while maintaining auditability and requiring minimal lecturer input (only handwritten reference and grading rules).

Abstract: Handwritten STEM exams capture open-ended reasoning and diagrams, but manual grading is slow and difficult to scale. We present an end-to-end workflow for grading scanned handwritten engineering quizzes with multimodal large language models (LLMs) that preserves the standard exam process (A4 paper, unconstrained student handwriting). The lecturer provides only a handwritten reference solution (100%) and a short set of grading rules; the reference is converted into a text-only summary that conditions grading without exposing the reference scan. Reliability is achieved through a multi-stage design with a format/presence check to prevent grading blank answers, an ensemble of independent graders, supervisor aggregation, and rigid templates with deterministic validation to produce auditable, machine-parseable reports. We evaluate the frozen pipeline in a clean-room protocol on a held-out real course quiz in Slovenian, including hand-drawn circuit schematics. With state-of-the-art backends (GPT-5.2 and Gemini-3 Pro), the full pipeline achieves $\approx$8-point mean absolute difference to lecturer grades with low bias and an estimated manual-review trigger rate of $\approx$17% at $D_{\max}=40$. Ablations show that trivial prompting and removing the reference solution substantially degrade accuracy and introduce systematic over-grading, confirming that structured prompting and reference grounding are essential.

</details>


### [65] [Unified Primitive Proxies for Structured Shape Completion](https://arxiv.org/abs/2601.00759)
*Zhaiyu Chen,Yuqing Wang,Xiao Xiang Zhu*

Main category: cs.CV

TL;DR: UniCo is a unified framework for structured shape completion that predicts complete primitives with semantics and inlier membership in a single feed-forward pass, outperforming baselines by up to 50% in Chamfer distance.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve structured shape completion by rethinking how primitives and points should interact, moving away from the prevailing cascade approach to enable more effective primitive-based surface reconstruction from incomplete data.

Method: UniCo uses primitive proxies (learnable queries) that attend to shared shape features in a dedicated pathway, coupled with a training strategy that optimizes primitives and points together with online target updates for consistent optimization.

Result: UniCo consistently outperforms recent baselines across synthetic and real-world benchmarks with four independent assembly solvers, lowering Chamfer distance by up to 50% and improving normal consistency by up to 7%.

Conclusion: The approach establishes an attractive recipe for structured 3D understanding from incomplete data by unifying primitive prediction with complete geometry, semantics, and inlier membership in a single feed-forward framework.

Abstract: Structured shape completion recovers missing geometry as primitives rather than as unstructured points, which enables primitive-based surface reconstruction. Instead of following the prevailing cascade, we rethink how primitives and points should interact, and find it more effective to decode primitives in a dedicated pathway that attends to shared shape features. Following this principle, we present UniCo, which in a single feed-forward pass predicts a set of primitives with complete geometry, semantics, and inlier membership. To drive this unified representation, we introduce primitive proxies, learnable queries that are contextualized to produce assembly-ready outputs. To ensure consistent optimization, our training strategy couples primitives and points with online target updates. Across synthetic and real-world benchmarks with four independent assembly solvers, UniCo consistently outperforms recent baselines, lowering Chamfer distance by up to 50% and improving normal consistency by up to 7%. These results establish an attractive recipe for structured 3D understanding from incomplete data. Project page: https://unico-completion.github.io.

</details>


### [66] [Fusion-SSAT: Unleashing the Potential of Self-supervised Auxiliary Task by Feature Fusion for Generalized Deepfake Detection](https://arxiv.org/abs/2601.00789)
*Shukesh Reddy,Srijan Das,Abhijit Das*

Main category: cs.CV

TL;DR: Self-supervised learning as auxiliary task improves deepfake detection generalizability through feature fusion


<details>
  <summary>Details</summary>
Motivation: To enhance generalized deepfake detection by leveraging self-supervised learning as an auxiliary task to improve feature representations and cross-dataset performance

Method: Examined different training scheme combinations for self-supervised auxiliary tasks and primary deepfake detection, focusing on fusing feature representations from both tasks

Result: Achieved better generalizability on cross-dataset evaluation across DF40, FaceForensics++, Celeb-DF, DFD, FaceShifter, UADFV datasets compared to state-of-the-art detectors

Conclusion: Fusing self-supervised auxiliary task features creates powerful representations that leverage both tasks' unique characteristics, significantly improving deepfake detection performance and generalizability

Abstract: In this work, we attempted to unleash the potential of self-supervised learning as an auxiliary task that can optimise the primary task of generalised deepfake detection. To explore this, we examined different combinations of the training schemes for these tasks that can be most effective. Our findings reveal that fusing the feature representation from self-supervised auxiliary tasks is a powerful feature representation for the problem at hand. Such a representation can leverage the ultimate potential and bring in a unique representation of both the self-supervised and primary tasks, achieving better performance for the primary task. We experimented on a large set of datasets, which includes DF40, FaceForensics++, Celeb-DF, DFD, FaceShifter, UADFV, and our results showed better generalizability on cross-dataset evaluation when compared with current state-of-the-art detectors.

</details>


### [67] [Two Deep Learning Approaches for Automated Segmentation of Left Ventricle in Cine Cardiac MRI](https://arxiv.org/abs/2601.00794)
*Wenhui Chu,Nikolaos V. Tsekos*

Main category: cs.CV

TL;DR: Proposed LNU-Net and IBU-Net architectures for left ventricle segmentation from MRI, outperforming U-Net and other methods on dice coefficient and distance metrics.


<details>
  <summary>Details</summary>
Motivation: Left ventricle segmentation is critical for clinical cardiac diagnosis and quantification, but existing methods may not be optimal for MRI images.

Method: Two novel architectures: LNU-Net uses layer normalization in each convolutional block; IBU-Net combines instance and batch normalization in first block. Both use U-Net structure with down/up-sampling paths, plus affine transformations and elastic deformations for data augmentation.

Result: Evaluated on 805 MRI images from 45 patients, both proposed methods outperformed original U-Net and other state-of-the-art approaches in dice coefficient and average perpendicular distance metrics.

Conclusion: LNU-Net and IBU-Net are effective deep learning architectures for left ventricle segmentation from short-axis cine MRI, showing improved performance over existing methods.

Abstract: Left ventricle (LV) segmentation is critical for clinical quantification and diagnosis of cardiac images. In this work, we propose two novel deep learning architectures called LNU-Net and IBU-Net for left ventricle segmentation from short-axis cine MRI images. LNU-Net is derived from layer normalization (LN) U-Net architecture, while IBU-Net is derived from the instance-batch normalized (IB) U-Net for medical image segmentation. The architectures of LNU-Net and IBU-Net have a down-sampling path for feature extraction and an up-sampling path for precise localization. We use the original U-Net as the basic segmentation approach and compared it with our proposed architectures. Both LNU-Net and IBU-Net have left ventricle segmentation methods: LNU-Net applies layer normalization in each convolutional block, while IBU-Net incorporates instance and batch normalization together in the first convolutional block and passes its result to the next layer. Our method incorporates affine transformations and elastic deformations for image data processing. Our dataset that contains 805 MRI images regarding the left ventricle from 45 patients is used for evaluation. We experimentally evaluate the results of the proposed approaches outperforming the dice coefficient and the average perpendicular distance than other state-of-the-art approaches.

</details>


### [68] [AdaGaR: Adaptive Gabor Representation for Dynamic Scene Reconstruction](https://arxiv.org/abs/2601.00796)
*Jiewen Chan,Zhenjun Zhao,Yu-Lun Liu*

Main category: cs.CV

TL;DR: AdaGaR: A unified framework for dynamic 3D scene reconstruction from monocular videos using adaptive Gabor representation for frequency adaptivity and cubic Hermite splines with temporal regularization for smooth motion continuity.


<details>
  <summary>Details</summary>
Motivation: Existing methods using single Gaussian primitives are limited by low-pass filtering nature, while standard Gabor functions introduce energy instability. Lack of temporal continuity constraints leads to motion artifacts during interpolation in dynamic scene reconstruction from monocular videos.

Method: 1) Adaptive Gabor Representation: Extends Gaussians through learnable frequency weights and adaptive energy compensation. 2) Temporal continuity: Cubic Hermite Splines with Temporal Curvature Regularization for smooth motion evolution. 3) Adaptive Initialization: Combines depth estimation, point tracking, and foreground masks for stable point cloud distributions.

Result: State-of-the-art performance on Tap-Vid DAVIS (PSNR 35.49, SSIM 0.9433, LPIPS 0.0723). Strong generalization across frame interpolation, depth consistency, video editing, and stereo view synthesis.

Conclusion: AdaGaR successfully addresses both frequency adaptivity and temporal continuity in explicit dynamic scene modeling, achieving superior reconstruction quality and generalization across multiple tasks.

Abstract: Reconstructing dynamic 3D scenes from monocular videos requires simultaneously capturing high-frequency appearance details and temporally continuous motion. Existing methods using single Gaussian primitives are limited by their low-pass filtering nature, while standard Gabor functions introduce energy instability. Moreover, lack of temporal continuity constraints often leads to motion artifacts during interpolation. We propose AdaGaR, a unified framework addressing both frequency adaptivity and temporal continuity in explicit dynamic scene modeling. We introduce Adaptive Gabor Representation, extending Gaussians through learnable frequency weights and adaptive energy compensation to balance detail capture and stability. For temporal continuity, we employ Cubic Hermite Splines with Temporal Curvature Regularization to ensure smooth motion evolution. An Adaptive Initialization mechanism combining depth estimation, point tracking, and foreground masks establishes stable point cloud distributions in early training. Experiments on Tap-Vid DAVIS demonstrate state-of-the-art performance (PSNR 35.49, SSIM 0.9433, LPIPS 0.0723) and strong generalization across frame interpolation, depth consistency, video editing, and stereo view synthesis. Project page: https://jiewenchan.github.io/AdaGaR/

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [69] [Reinforcement learning with timed constraints for robotics motion planning](https://arxiv.org/abs/2601.00087)
*Zhaoan Wang,Junchao Li,Mahdi Mohammad,Shaoping Xiao*

Main category: cs.RO

TL;DR: Unified RL framework using timed automata to learn policies satisfying MITL temporal specifications in MDPs and POMDPs for robotic planning.


<details>
  <summary>Details</summary>
Motivation: Robotic systems need planners that handle complex temporal constraints in dynamic, uncertain environments, but integrating formal MITL specifications with RL is challenging due to stochastic dynamics and partial observability.

Method: Translate MITL formulas into Timed-LDGBA automata, synchronize with decision processes to create product timed models, use Q-learning with simple reward structure that enforces temporal correctness while allowing performance objectives.

Result: Framework successfully learns policies satisfying strict time-bounded requirements under stochastic transitions, scales to larger state spaces, and remains effective in partially observable environments across three simulation studies.

Conclusion: The automata-based RL framework enables reliable robotic planning in time-critical, uncertain settings by formally guaranteeing MITL temporal specifications while learning optimal policies.

Abstract: Robotic systems operating in dynamic and uncertain environments increasingly require planners that satisfy complex task sequences while adhering to strict temporal constraints. Metric Interval Temporal Logic (MITL) offers a formal and expressive framework for specifying such time-bounded requirements; however, integrating MITL with reinforcement learning (RL) remains challenging due to stochastic dynamics and partial observability. This paper presents a unified automata-based RL framework for synthesizing policies in both Markov Decision Processes (MDPs) and Partially Observable Markov Decision Processes (POMDPs) under MITL specifications. MITL formulas are translated into Timed Limit-Deterministic Generalized Büchi Automata (Timed-LDGBA) and synchronized with the underlying decision process to construct product timed models suitable for Q-learning. A simple yet expressive reward structure enforces temporal correctness while allowing additional performance objectives. The approach is validated in three simulation studies: a $5 \times 5$ grid-world formulated as an MDP, a $10 \times 10$ grid-world formulated as a POMDP, and an office-like service-robot scenario. Results demonstrate that the proposed framework consistently learns policies that satisfy strict time-bounded requirements under stochastic transitions, scales to larger state spaces, and remains effective in partially observable environments, highlighting its potential for reliable robotic planning in time-critical and uncertain settings.

</details>


### [70] [Compositional Diffusion with Guided search for Long-Horizon Planning](https://arxiv.org/abs/2601.00126)
*Utkarsh A Mishra,David He,Yongxin Chen,Danfei Xu*

Main category: cs.RO

TL;DR: CDGS solves mode averaging in compositional generative models by embedding search within diffusion denoising, enabling coherent long-horizon planning across domains.


<details>
  <summary>Details</summary>
Motivation: Compositional generative models for long-horizon tasks face mode averaging problems when local distributions are multimodal, causing incompatible modes to average out and produce infeasible, incoherent plans.

Method: CDGS embeds search directly in diffusion denoising: explores diverse local mode combinations via population-based sampling, prunes infeasible candidates with likelihood filtering, and enforces global consistency through iterative resampling between overlapping segments.

Result: Matches oracle performance on 7 robot manipulation tasks, outperforms baselines lacking compositionality or requiring long-horizon training data, and generalizes to text-guided panoramic images and long videos.

Conclusion: CDGS effectively addresses mode averaging in compositional generative models through guided search within diffusion, enabling coherent long-horizon planning across diverse domains without requiring extensive long-horizon training data.

Abstract: Generative models have emerged as powerful tools for planning, with compositional approaches offering particular promise for modeling long-horizon task distributions by composing together local, modular generative models. This compositional paradigm spans diverse domains, from multi-step manipulation planning to panoramic image synthesis to long video generation. However, compositional generative models face a critical challenge: when local distributions are multimodal, existing composition methods average incompatible modes, producing plans that are neither locally feasible nor globally coherent. We propose Compositional Diffusion with Guided Search (CDGS), which addresses this \emph{mode averaging} problem by embedding search directly within the diffusion denoising process. Our method explores diverse combinations of local modes through population-based sampling, prunes infeasible candidates using likelihood-based filtering, and enforces global consistency through iterative resampling between overlapping segments. CDGS matches oracle performance on seven robot manipulation tasks, outperforming baselines that lack compositionality or require long-horizon training data. The approach generalizes across domains, enabling coherent text-guided panoramic images and long videos through effective local-to-global message passing. More details: https://cdgsearch.github.io/

</details>


### [71] [SLEI3D: Simultaneous Exploration and Inspection via Heterogeneous Fleets under Limited Communication](https://arxiv.org/abs/2601.00163)
*Junfeng Chen,Yuxiao Zhu,Xintong Zhang,Bing Luo,Meng Guo*

Main category: cs.RO

TL;DR: SLEI3D: A planning framework for heterogeneous robot teams to simultaneously explore unknown 3D environments, inspect features of interest, and communicate findings to a mobile ground station via intermittent wireless networks.


<details>
  <summary>Details</summary>
Motivation: Current robotic fleets are designed for inspecting known environments with pre-planned areas, but many real-world applications require identifying areas of interest online during exploration in environments where global communication is unavailable.

Method: Proposes SLEI3D framework with multi-layer, multi-rate planning for heterogeneous robots (long-range lidars for exploration, close-range cameras for inspection) using intermittent/proactive communication protocols and coordination between robot subgroups.

Result: Validated through high-fidelity simulations of large-scale missions (up to 48 robots, 384k cubic meters) and hardware experiments with 7 robots, demonstrating effective collaborative exploration, inspection, and communication.

Conclusion: SLEI3D successfully addresses the challenge of simultaneous exploration, inspection, and communication in unknown 3D environments with heterogeneous robot teams operating under intermittent wireless connectivity constraints.

Abstract: Robotic fleets such as unmanned aerial and ground vehicles have been widely used for routine inspections of static environments, where the areas of interest are known and planned in advance. However, in many applications, such areas of interest are unknown and should be identified online during exploration. Thus, this paper considers the problem of simultaneous exploration, inspection of unknown environments and then real-time communication to a mobile ground control station to report the findings. The heterogeneous robots are equipped with different sensors, e.g., long-range lidars for fast exploration and close-range cameras for detailed inspection. Furthermore, global communication is often unavailable in such environments, where the robots can only communicate with each other via ad-hoc wireless networks when they are in close proximity and free of obstruction. This work proposes a novel planning and coordination framework (SLEI3D) that integrates the online strategies for collaborative 3D exploration, adaptive inspection and timely communication (via the intermit-tent or proactive protocols). To account for uncertainties w.r.t. the number and location of features, a multi-layer and multi-rate planning mechanism is developed for inter-and-intra robot subgroups, to actively meet and coordinate their local plans. The proposed framework is validated extensively via high-fidelity simulations of numerous large-scale missions with up to 48 robots and 384 thousand cubic meters. Hardware experiments of 7 robots are also conducted. Project website is available at https://junfengchen-robotics.github.io/SLEI3D/.

</details>


### [72] [SLAP: Slapband-based Autonomous Perching Drone with Failure Recovery for Vertical Tree Trunks](https://arxiv.org/abs/2601.00238)
*Julia Di,Kenneth A. W. Hoffmann,Tony G. Chen,Tian-Ao Ren,Mark R. Cutkosky*

Main category: cs.RO

TL;DR: SLAP system enables gentle perching on vertical tree trunks for larger drones with 75% success rate and 100% failure recovery capability.


<details>
  <summary>Details</summary>
Motivation: Current perching solutions for vertical surfaces are either lightweight mechanical designs with poor system integration or require aggressive high-speed landings that are dangerous for surveyor drones with sensitive electronics. There's a need for gentle perching suitable for larger drones.

Method: SLAP system integrates vision-based perch site detection, IMU-based failure detection, attitude controller for soft perching, optical close-range detection, and a fast active elastic gripper with microspines made from commercial slapbands. Tested on modified 1.2 kg commercial quadrotor.

Result: 75% perch success rate on real oak tree segment across 20 indoor flights, and 100% perch failure recovery across 2 flights with induced failures.

Conclusion: The SLAP system demonstrates a viable approach for gentle vertical surface perching with integrated failure recovery, making it suitable for larger surveyor drones with sensitive electronics.

Abstract: Perching allows unmanned aerial vehicles (UAVs) to reduce energy consumption, remain anchored for surface sampling operations, or stably survey their surroundings. Previous efforts for perching on vertical surfaces have predominantly focused on lightweight mechanical design solutions with relatively scant system-level integration. Furthermore, perching strategies for vertical surfaces commonly require high-speed, aggressive landing operations that are dangerous for a surveyor drone with sensitive electronics onboard. This work presents the preliminary investigation of a perching approach suitable for larger drones that both gently perches on vertical tree trunks and reacts and recovers from perch failures. The system in this work, called SLAP, consists of vision-based perch site detector, an IMU (inertial-measurement-unit)-based perch failure detector, an attitude controller for soft perching, an optical close-range detection system, and a fast active elastic gripper with microspines made from commercially-available slapbands. We validated this approach on a modified 1.2 kg commercial quadrotor with component and system analysis. Initial human-in-the-loop autonomous indoor flight experiments achieved a 75% perch success rate on a real oak tree segment across 20 flights, and 100% perch failure recovery across 2 flights with induced failures.

</details>


### [73] [Vehicle Painting Robot Path Planning Using Hierarchical Optimization](https://arxiv.org/abs/2601.00271)
*Yuya Nagai,Hiromitsu Nakamura,Narito Shinmachi,Yuta Higashizono,Satoshi Ono*

Main category: cs.RO

TL;DR: Automated hierarchical optimization method for designing paint paths for robotic arms in vehicle painting, combining VRP-like assignment with detailed path planning to reduce manual design time.


<details>
  <summary>Details</summary>
Motivation: Manual design of paint paths for robotic arms in vehicle production is time-consuming and demands automation. Conventional robotic path planning techniques can't be directly applied due to unique painting process constraints.

Method: Formulates paint path design as hierarchical optimization: upper layer handles vehicle routing problem (VRP)-like assignment of car body areas to arms, lower layer performs detailed path planning. Uses custom variable representation, constraints, repair operators, and initialization for painting-specific constraints.

Result: Method successfully designed paths for three commercial vehicle models that satisfy all painting constraints, with quality comparable to manually engineered paths.

Conclusion: Proposed hierarchical optimization approach effectively automates paint path design, reducing engineering time while maintaining quality comparable to manual design.

Abstract: In vehicle production factories, the vehicle painting process employs multiple robotic arms to simultaneously apply paint to car bodies advancing along a conveyor line. Designing paint paths for these robotic arms, which involves assigning car body areas to arms and determining paint sequences for each arm, remains a time-consuming manual task for engineers, indicating the demand for automation and design time reduction. The unique constraints of the painting process hinder the direct application of conventional robotic path planning techniques, such as those used in welding. Therefore, this paper formulates the design of paint paths as a hierarchical optimization problem, where the upper-layer subproblem resembles a vehicle routing problem (VRP), and the lower-layer subproblem involves detailed path planning. This approach allows the use of different optimization algorithms at each layer, and permits flexible handling of constraints specific to the vehicle painting process through the design of variable representation, constraints, repair operators, and an initialization process at the upper and lower layers. Experiments with three commercially available vehicle models demonstrated that the proposed method can automatically design paths that satisfy all constraints for vehicle painting with quality comparable to those created manually by engineers.

</details>


### [74] [Pure Inertial Navigation in Challenging Environments with Wheeled and Chassis Mounted Inertial Sensors](https://arxiv.org/abs/2601.00275)
*Dusan Nemec,Gal Versano,Itai Savin,Vojtech Simak,Juraj Kekelak,Itzik Klein*

Main category: cs.RO

TL;DR: WiCHINS: A wheeled and chassis inertial navigation system combining wheel-mounted and chassis-mounted inertial sensors for accurate pure inertial navigation in GNSS-denied or low-light environments.


<details>
  <summary>Details</summary>
Motivation: Autonomous vehicles and wheeled robots need reliable navigation in challenging environments where GNSS signals are limited or lighting conditions are poor, causing traditional inertial-only systems to drift over time due to measurement errors.

Method: Proposed WiCHINS system combining wheel-mounted inertial sensors with chassis-mounted inertial sensor using a three-stage framework with dedicated extended Kalman filters, leveraging benefits from both wheel and body locations during estimation.

Result: Achieved average position error of 11.4m (2.4% of average traveled distance) using two wheels and one body IMU, outperforming four other inertial baselines in dataset with 5 IMUs and 228.6 minutes of recording time.

Conclusion: WiCHINS enables robust navigation in challenging environments and helps bridge the pure-inertial performance gap for wheeled vehicles operating in GNSS-denied or degraded lighting conditions.

Abstract: Autonomous vehicles and wheeled robots are widely used in many applications in both indoor and outdoor settings. In practical situations with limited GNSS signals or degraded lighting conditions, the navigation solution may rely only on inertial sensors and as result drift in time due to errors in the inertial measurement. In this work, we propose WiCHINS, a wheeled and chassis inertial navigation system by combining wheel-mounted-inertial sensors with a chassis-mounted inertial sensor for accurate pure inertial navigation. To that end, we derive a three-stage framework, each with a dedicated extended Kalman filter. This framework utilizes the benefits of each location (wheel/body) during the estimation process. To evaluate our proposed approach, we employed a dataset with five inertial measurement units with a total recording time of 228.6 minutes. We compare our approach with four other inertial baselines and demonstrate an average position error of 11.4m, which is $2.4\%$ of the average traveled distance, using two wheels and one body inertial measurement units. As a consequence, our proposed method enables robust navigation in challenging environments and helps bridge the pure-inertial performance gap.

</details>


### [75] [Replaceable Bit-based Gripper for Picking Cluttered Food Items](https://arxiv.org/abs/2601.00305)
*Prashant Kumar,Yukiyasu Domae,Weiwei Wan,Kensuke Harada*

Main category: cs.RO

TL;DR: A replaceable bit-based gripper system for handling cluttered food items with weight-based control, achieving over 80-95% accuracy for specific foods like spaghetti and ikura.


<details>
  <summary>Details</summary>
Motivation: The food packaging industry needs flexible solutions to handle rapidly changing food items ranging from single-piece to cluttered, flexible foods with varying weights and characteristics.

Method: Developed a gripper system with replaceable specialized food attachments (bits) and a belt replacement system for switching between different food items. Designed specific bits for challenging food categories: ikura (sticky, granular) and spaghetti (long, sticky, cluttered).

Result: Successfully picked up both spaghetti and ikura with weight-specific dropping accuracy over 80% for spaghetti and over 95% for ikura. Demonstrated quick switching between different bits for handling diverse food items.

Conclusion: The replaceable bit-based gripper system effectively addresses the challenge of weight-based handling of cluttered food items in packaging operations, offering flexibility and high accuracy for diverse food types.

Abstract: The food packaging industry goes through changes in food items and their weights quite rapidly. These items range from easy-to-pick, single-piece food items to flexible, long and cluttered ones. We propose a replaceable bit-based gripper system to tackle the challenge of weight-based handling of cluttered food items. The gripper features specialized food attachments(bits) that enhance its grasping capabilities, and a belt replacement system allows switching between different food items during packaging operations. It offers a wide range of control options, enabling it to grasp and drop specific weights of granular, cluttered, and entangled foods. We specifically designed bits for two flexible food items that differ in shape: ikura(salmon roe) and spaghetti. They represent the challenging categories of sticky, granular food and long, sticky, cluttered food, respectively. The gripper successfully picked up both spaghetti and ikura and demonstrated weight-specific dropping of these items with an accuracy over 80% and 95% respectively. The gripper system also exhibited quick switching between different bits, leading to the handling of a large range of food items.

</details>


### [76] [Space Debris Removal using Nano-Satellites controlled by Low-Power Autonomous Agents](https://arxiv.org/abs/2601.00465)
*Dennis Christmann,Juan F. Gutierrez,Sthiti Padhi,Patrick Plörer,Aditya Takur,Simona Silvestri,Andres Gomez*

Main category: cs.RO

TL;DR: Autonomous nano-satellite swarms using wireless microcontrollers can efficiently de-orbit space debris by leveraging recent advances in autonomous agents for resource-constrained platforms.


<details>
  <summary>Details</summary>
Motivation: Space debris poses increasing threats to satellite operations and space travel, with many non-functional spacecraft and debris orbiting Earth that need to be safely removed.

Method: Implement autonomous agent software on wireless microcontrollers and test on specialized test-bed to demonstrate feasibility of intelligent nano-satellite swarms for debris de-orbiting.

Result: The approach shows feasibility and overall energy efficiency for autonomous nano-satellite swarms to address space debris problems.

Conclusion: Intelligent nano-satellite swarms using autonomous agents on resource-constrained platforms offer a promising solution for safely de-orbiting space debris into Earth's atmosphere.

Abstract: Space debris is an ever-increasing problem in space travel. There are already many old, no longer functional spacecraft and debris orbiting the earth, which endanger both the safe operation of satellites and space travel. Small nano-satellite swarms can address this problem by autonomously de-orbiting debris safely into the Earth's atmosphere. This work builds on the recent advances of autonomous agents deployed in resource-constrained platforms and shows a first simplified approach how such intelligent and autonomous nano-satellite swarms can be realized. We implement our autonomous agent software on wireless microcontrollers and perform experiments on a specialized test-bed to show the feasibility and overall energy efficiency of our approach.

</details>


### [77] [Variable Elimination in Hybrid Factor Graphs for Discrete-Continuous Inference & Estimation](https://arxiv.org/abs/2601.00545)
*Varun Agrawal,Frank Dellaert*

Main category: cs.RO

TL;DR: Proposes an efficient Hybrid Factor Graph framework with exact variable elimination for MAP estimation and marginalization over continuous and discrete variables in robotics problems.


<details>
  <summary>Details</summary>
Motivation: Hybrid problems in robotics involving both continuous and discrete components are difficult to model and solve for estimation tasks, with existing approaches relying on approximations rather than exact solutions.

Method: Develops novel hybrid Gaussian factors connecting discrete/continuous variables and hybrid conditionals for multiple continuous hypotheses. Derives hybrid variable elimination under Conditional Linear Gaussian scheme to produce exact posteriors as hybrid Bayes networks, with tree-structured factor representation and pruning for tractability.

Result: Demonstrates applicability on SLAM dataset with ambiguous measurements requiring discrete choices for most likely measurements, showcasing accuracy, generality, and simplicity of the framework.

Conclusion: The proposed Hybrid Factor Graph framework enables exact MAP estimation and marginalization over both continuous and discrete variables, providing an efficient solution to long-standing hybrid estimation problems in robotics.

Abstract: Many hybrid problems in robotics involve both continuous and discrete components, and modeling them together for estimation tasks has been a long standing and difficult problem. Hybrid Factor Graphs give us a mathematical framework to model these types of problems, however existing approaches for solving them are based on approximations. In this work, we propose an efficient Hybrid Factor Graph framework alongwith a variable elimination algorithm to produce a hybrid Bayes network, which can then be used for exact Maximum A Posteriori estimation and marginalization over both sets of variables. Our approach first develops a novel hybrid Gaussian factor which can connect to both discrete and continuous variables, and a hybrid conditional which can represent multiple continuous hypotheses conditioned on the discrete variables. Using these representations, we derive the process of hybrid variable elimination under the Conditional Linear Gaussian scheme, giving us exact posteriors as hybrid Bayes network. To bound the number of discrete hypotheses, we use a tree-structured representation of the factors coupled with a simple pruning and probabilistic assignment scheme, which allows for tractable inference. We demonstrate the applicability of our framework on a SLAM dataset with ambiguous measurements, where discrete choices for the most likely measurement have to be made. Our demonstrated results showcase the accuracy, generality, and simplicity of our hybrid factor graph framework.

</details>


### [78] [LLM-Based Agentic Exploration for Robot Navigation & Manipulation with Skill Orchestration](https://arxiv.org/abs/2601.00555)
*Abu Hanif Muhammad Syarubany,Farhan Zaki Rahmani,Trio Widianto*

Main category: cs.RO

TL;DR: LLM-based robot agent for indoor shopping that builds semantic maps, uses AprilTags for navigation, and executes natural language shopping requests through modular motion primitives.


<details>
  <summary>Details</summary>
Motivation: To create an end-to-end LLM-based robotic system for indoor shopping tasks that can interpret natural language requests and autonomously navigate to multiple stores to retrieve objects while maintaining modularity and debuggability.

Method: Robot incrementally builds lightweight semantic map by detecting signboards at junctions and storing direction-to-POI relations with estimated poses. Uses AprilTags as repeatable anchors. LLM processes natural language shopping requests to produce constrained discrete actions at each junction. ROS finite-state controller executes decisions by gating modular motion primitives including obstacle avoidance, AprilTag approaching, store entry, and grasping.

Result: Integrated system successfully performs end-to-end task execution from user instruction to multi-store navigation and object retrieval in both Gazebo simulation and real-world corridor environments. System remains modular and debuggable through text-based maps and logged decision history.

Conclusion: The LLM-based agentic exploration system demonstrates feasibility for indoor shopping tasks with natural language interaction, combining semantic mapping, modular motion control, and LLM decision-making while maintaining system debuggability through transparent text-based representations.

Abstract: This paper presents an end-to-end LLM-based agentic exploration system for an indoor shopping task, evaluated in both Gazebo simulation and a corresponding real-world corridor layout. The robot incrementally builds a lightweight semantic map by detecting signboards at junctions and storing direction-to-POI relations together with estimated junction poses, while AprilTags provide repeatable anchors for approach and alignment. Given a natural-language shopping request, an LLM produces a constrained discrete action at each junction (direction and whether to enter a store), and a ROS finite-state main controller executes the decision by gating modular motion primitives, including local-costmap-based obstacle avoidance, AprilTag approaching, store entry, and grasping. Qualitative results show that the integrated stack can perform end-to-end task execution from user instruction to multi-store navigation and object retrieval, while remaining modular and debuggable through its text-based map and logged decision history.

</details>


### [79] [Priority-Aware Multi-Robot Coverage Path Planning](https://arxiv.org/abs/2601.00580)
*Kanghoon Lee,Hyeonjun Kim,Jiachen Li,Jinkyoo Park*

Main category: cs.RO

TL;DR: PA-MCPP framework for multi-robot coverage with prioritized zones, minimizing priority-weighted latency and makespan through two-phase greedy assignment and Steiner-tree planning.


<details>
  <summary>Details</summary>
Motivation: Existing MCPP methods assume uniform importance across regions, which is ineffective when some zones require faster attention. Real-world scenarios often have areas with different urgency levels that need prioritized coverage.

Method: Two-phase framework: (1) greedy zone assignment with local search and spanning-tree-based path planning for prioritized zones, (2) Steiner-tree-guided residual coverage for remaining areas.

Result: Significantly reduces priority-weighted latency compared to standard MCPP baselines while maintaining competitive makespan. Scales well with number of robots, and zone coverage behavior can be controlled by adjusting priority weights.

Conclusion: PA-MCPP effectively addresses the limitation of uniform importance assumptions in multi-robot coverage, providing a scalable solution for scenarios requiring prioritized zone coverage with controlled trade-offs between latency and makespan.

Abstract: Multi-robot systems are widely used for coverage tasks that require efficient coordination across large environments. In Multi-Robot Coverage Path Planning (MCPP), the objective is typically to minimize the makespan by generating non-overlapping paths for full-area coverage. However, most existing methods assume uniform importance across regions, limiting their effectiveness in scenarios where some zones require faster attention. We introduce the Priority-Aware MCPP (PA-MCPP) problem, where a subset of the environment is designated as prioritized zones with associated weights. The goal is to minimize, in lexicographic order, the total priority-weighted latency of zone coverage and the overall makespan. To address this, we propose a scalable two-phase framework combining (1) greedy zone assignment with local search, spanning-tree-based path planning, and (2) Steiner-tree-guided residual coverage. Experiments across diverse scenarios demonstrate that our method significantly reduces priority-weighted latency compared to standard MCPP baselines, while maintaining competitive makespan. Sensitivity analyses further show that the method scales well with the number of robots and that zone coverage behavior can be effectively controlled by adjusting priority weights.

</details>


### [80] [NMPC-Augmented Visual Navigation and Safe Learning Control for Large-Scale Mobile Robots](https://arxiv.org/abs/2601.00609)
*Mehdi Heydari Shahna,Pauli Mustalahti,Jouni Mattila*

Main category: cs.RO

TL;DR: A comprehensive navigation and control framework for large-scale mobile robots on slip-prone terrain, combining visual pose estimation, nonlinear MPC, deep neural network control with adaptive augmentation, and safety monitoring.


<details>
  <summary>Details</summary>
Motivation: Large-scale mobile robots operating on loose, unconsolidated terrain face reduced traction and stability challenges, requiring robust control systems to ensure safe operation on slip-prone surfaces.

Method: Four-module architecture: 1) Visual pose estimation fusing onboard sensors and stereo cameras, 2) High-level nonlinear MPC for drift correction, 3) Low-level DNN control policy with robust adaptive control for wheel actuation, 4) Logarithmic safety module for system monitoring.

Result: The framework guarantees uniform exponential stability of the actuation subsystem and ensures whole system-level safety, demonstrated on a 6,000 kg LSMR with complex electro-hydrostatic drives operating at different frequencies.

Conclusion: The proposed comprehensive framework enables robust operation of large-scale mobile robots on slip-prone terrain by integrating high-performance control techniques with safety guarantees.

Abstract: A large-scale mobile robot (LSMR) is a high-order multibody system that often operates on loose, unconsolidated terrain, which reduces traction. This paper presents a comprehensive navigation and control framework for an LSMR that ensures stability and safety-defined performance, delivering robust operation on slip-prone terrain by jointly leveraging high-performance techniques. The proposed architecture comprises four main modules: (1) a visual pose-estimation module that fuses onboard sensors and stereo cameras to provide an accurate, low-latency robot pose, (2) a high-level nonlinear model predictive control that updates the wheel motion commands to correct robot drift from the robot reference pose on slip-prone terrain, (3) a low-level deep neural network control policy that approximates the complex behavior of the wheel-driven actuation mechanism in LSMRs, augmented with robust adaptive control to handle out-of-distribution disturbances, ensuring that the wheels accurately track the updated commands issued by high-level control module, and (4) a logarithmic safety module to monitor the entire robot stack and guarantees safe operation. The proposed low-level control framework guarantees uniform exponential stability of the actuation subsystem, while the safety module ensures the whole system-level safety during operation. Comparative experiments on a 6,000 kg LSMR actuated by two complex electro-hydrostatic drives, while synchronizing modules operating at different frequencies.

</details>


### [81] [Vision-based Goal-Reaching Control for Mobile Robots Using a Hierarchical Learning Framework](https://arxiv.org/abs/2601.00610)
*Mehdi Heydari Shahna,Pauli Mustalahti,Jouni Mattila*

Main category: cs.RO

TL;DR: A safe RL framework for large robots decomposes control into visual pose estimation, RL motion planning, supervised dynamics learning, robust adaptive control, and safety supervision, achieving stable operation on a 6,000 kg robot.


<details>
  <summary>Details</summary>
Motivation: RL requires extensive exploration that can be unsafe for large robots on unstable terrain, limiting applicability to complex robotic systems.

Method: Decomposes system into five modules: 1) real-time visual pose estimation, 2) RL motion planner respecting robot specs, 3) supervised learning of robot dynamics, 4) model-based robust adaptive controller for wheel tracking, 5) mathematical safety supervisor for fault handling.

Result: Framework guarantees uniform exponential stability of actuation system and operational safety. Experiments on a 6,000 kg robot in various scenarios confirm effectiveness.

Conclusion: The modular framework enables safe RL-based control for large-scale robots by combining learning-based planning with robust control and safety supervision.

Abstract: Reinforcement learning (RL) is effective in many robotic applications, but it requires extensive exploration of the state-action space, during which behaviors can be unsafe. This significantly limits its applicability to large robots with complex actuators operating on unstable terrain. Hence, to design a safe goal-reaching control framework for large-scale robots, this paper decomposes the whole system into a set of tightly coupled functional modules. 1) A real-time visual pose estimation approach is employed to provide accurate robot states to 2) an RL motion planner for goal-reaching tasks that explicitly respects robot specifications. The RL module generates real-time smooth motion commands for the actuator system, independent of its underlying dynamic complexity. 3) In the actuation mechanism, a supervised deep learning model is trained to capture the complex dynamics of the robot and provide this model to 4) a model-based robust adaptive controller that guarantees the wheels track the RL motion commands even on slip-prone terrain. 5) Finally, to reduce human intervention, a mathematical safety supervisor monitors the robot, stops it on unsafe faults, and autonomously guides it back to a safe inspection area. The proposed framework guarantees uniform exponential stability of the actuation system and safety of the whole operation. Experiments on a 6,000 kg robot in different scenarios confirm the effectiveness of the proposed framework.

</details>


### [82] [From 2D to 3D terrain-following area coverage path planning](https://arxiv.org/abs/2601.00614)
*Mogens Plessen*

Main category: cs.RO

TL;DR: 3D terrain-following coverage path planning algorithm that generates adjacent paths spaced by machinery width while maintaining working height above terrain


<details>
  <summary>Details</summary>
Motivation: Need for automated area coverage in complex 3D terrains (like agriculture) where machinery must follow terrain contours while maintaining specific working height and spacing

Method: Generates multiple adjacent paths with: 1) local spacing equal to machinery working width, 2) floating at projection distance equal to working height above terrain. Uses Inverse Distance Weighting for uniformly spaced elevation data and local search

Result: Algorithm validated with real-world 3D agricultural data, showing successful terrain-following coverage path planning

Conclusion: Proposed algorithm effectively handles 3D terrain-following coverage planning with complexities beyond 2D equivalents, suitable for agricultural applications

Abstract: An algorithm for 3D terrain-following area coverage path planning is presented. Multiple adjacent paths are generated that are (i) locally apart from each other by a distance equal to the working width of a machinery, while (ii) simultaneously floating at a projection distance equal to a specific working height above the terrain. The complexities of the algorithm in comparison to its 2D equivalent are highlighted. These include uniformly spaced elevation data generation using an Inverse Distance Weighting-approach and a local search. Area coverage path planning results for real-world 3D data within an agricultural context are presented to validate the algorithm.

</details>


### [83] [RoboReward: General-Purpose Vision-Language Reward Models for Robotics](https://arxiv.org/abs/2601.00675)
*Tony Lee,Andrew Wagenmaker,Karl Pertsch,Percy Liang,Sergey Levine,Chelsea Finn*

Main category: cs.RO

TL;DR: RoboReward: A robotics reward dataset and benchmark using vision-language models for automatic reward generation, with 4B/8B models outperforming larger VLMs on real robot tasks.


<details>
  <summary>Details</summary>
Motivation: Current reward design for robotics RL requires labor-intensive human labeling or brittle handcrafted objectives. Vision-language models show promise but their effectiveness on real robot tasks is poorly understood.

Method: (1) Created RoboReward dataset from OXE and RoboArena with negative examples augmentation via counterfactual relabeling and temporal clipping. (2) Trained 4B/8B vision-language reward models on this dataset.

Result: Evaluation shows no existing VLM excels across all tasks. RoboReward 4B/8B models outperform larger VLMs on short-horizon tasks. The 8B model improves RL policy learning over Gemini Robotics-ER 1.5 and narrows gap to human-provided rewards.

Conclusion: RoboReward demonstrates that specialized vision-language reward models can effectively provide rewards for robotics, outperforming general-purpose VLMs and substantially improving RL policy learning on real robot tasks.

Abstract: A well-designed reward is critical for effective reinforcement learning-based policy improvement. In real-world robotic domains, obtaining such rewards typically requires either labor-intensive human labeling or brittle, handcrafted objectives. Vision-language models (VLMs) have shown promise as automatic reward models, yet their effectiveness on real robot tasks is poorly understood. In this work, we aim to close this gap by introducing (1) \textbf{RoboReward}, a robotics reward dataset and benchmark built on large-scale real-robot corpora from Open X-Embodiment (OXE) and RoboArena, and (2) vision-language reward models trained on this dataset (RoboReward 4B/8B). Because OXE is success-heavy and lacks failure examples, we propose a \emph{negative examples data augmentation} pipeline that generates calibrated \emph{negatives} and \emph{near-misses} via counterfactual relabeling of successful episodes and temporal clipping to create partial-progress outcomes from the same videos. Using this framework, we produce an extensive training and evaluation dataset that spans diverse tasks and embodiments and enables systematic evaluation of whether state-of-the-art VLMs can reliably provide rewards for robotics. Our evaluation of leading open-weight and proprietary VLMs reveals that no model excels across all tasks, underscoring substantial room for improvement. We then train general-purpose 4B- and 8B-parameter models that outperform much larger VLMs in assigning rewards for short-horizon robotic tasks. Finally, we deploy the 8B-parameter reward VLM in real-robot reinforcement learning and find that it improves policy learning over Gemini Robotics-ER 1.5, a frontier physical reasoning VLM trained on robotics data, by a large margin, while substantially narrowing the gap to RL training with human-provided rewards.

</details>


### [84] [DefVINS: Visual-Inertial Odometry for Deformable Scenes](https://arxiv.org/abs/2601.00702)
*Samuel Cerezo,Javier Civera*

Main category: cs.RO

TL;DR: DefVINS is a VIO framework that separates rigid motion from non-rigid deformation using an embedded deformation graph, with observability-aware activation of deformation parameters for robustness in deformable scenes.


<details>
  <summary>Details</summary>
Motivation: Classical VIO assumes rigid scenes, but deformable environments violate this assumption, causing over-fitting to local non-rigid motion or severe drift when deformation dominates visual parallax.

Method: Separates rigid IMU-anchored state from non-rigid warp using embedded deformation graph. Initializes with standard VIO, then progressively activates non-rigid degrees of freedom as estimation becomes well-conditioned. Uses observability analysis to inform conditioning-based activation strategy.

Result: Improved robustness under non-rigid environments through combination of inertial constraints with observability-aware deformation activation, as demonstrated in ablation studies.

Conclusion: DefVINS successfully addresses VIO in deformable scenes by explicitly separating rigid and non-rigid components with careful observability-aware activation, making otherwise unobservable modes identifiable through inertial measurements.

Abstract: Deformable scenes violate the rigidity assumptions underpinning classical visual-inertial odometry (VIO), often leading to over-fitting to local non-rigid motion or severe drift when deformation dominates visual parallax. We introduce DefVINS, a visual-inertial odometry framework that explicitly separates a rigid, IMU-anchored state from a non--rigid warp represented by an embedded deformation graph. The system is initialized using a standard VIO procedure that fixes gravity, velocity, and IMU biases, after which non-rigid degrees of freedom are activated progressively as the estimation becomes well conditioned. An observability analysis is included to characterize how inertial measurements constrain the rigid motion and render otherwise unobservable modes identifiable in the presence of deformation. This analysis motivates the use of IMU anchoring and informs a conditioning-based activation strategy that prevents ill-posed updates under poor excitation. Ablation studies demonstrate the benefits of combining inertial constraints with observability-aware deformation activation, resulting in improved robustness under non-rigid environments.

</details>


### [85] [Calling for Backup: How Children Navigate Successive Robot Communication Failures](https://arxiv.org/abs/2601.00754)
*Maria Teresa Parreira,Isabel Neto,Filipa Rocha,Wendy Ju*

Main category: cs.RO

TL;DR: Children show similar but more disengaged responses to repeated robot errors compared to adults, with no negative impact on robot perception.


<details>
  <summary>Details</summary>
Motivation: To understand how children respond to repeated robot errors, as prior research has focused on adult reactions but children's responses remain unexplored.

Method: Reproduced Liu et al.'s successive robot failure paradigm with 59 children (ages 8-10). Participants interacted with a robot that failed to understand their prompts three times in succession, with behavioral responses video-recorded and analyzed.

Result: Children showed similarities to adults (adjusting prompts, modifying verbal tone, emotional non-verbal responses) but demonstrated more disengagement behaviors (ignoring robot, seeking adults). Errors did not affect children's perception of the robot.

Conclusion: Children have more flexible conversational expectations than adults, and these findings inform the design of more effective, developmentally appropriate human-robot interaction systems for young users.

Abstract: How do children respond to repeated robot errors? While prior research has examined adult reactions to successive robot errors, children's responses remain largely unexplored. In this study, we explore children's reactions to robot social errors and performance errors. For the latter, this study reproduces the successive robot failure paradigm of Liu et al. with child participants (N=59, ages 8-10) to examine how young users respond to repeated robot conversational errors. Participants interacted with a robot that failed to understand their prompts three times in succession, with their behavioral responses video-recorded and analyzed. We found both similarities and differences compared to adult responses from the original study. Like adults, children adjusted their prompts, modified their verbal tone, and exhibited increasingly emotional non-verbal responses throughout successive errors. However, children demonstrated more disengagement behaviors, including temporarily ignoring the robot or actively seeking an adult. Errors did not affect participants' perception of the robot, suggesting more flexible conversational expectations in children. These findings inform the design of more effective and developmentally appropriate human-robot interaction systems for young users.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [86] [Reasoning in Action: MCTS-Driven Knowledge Retrieval for Large Language Models](https://arxiv.org/abs/2601.00003)
*Shuqi Liu,Bowei He,Chen Ma,Linqi Song*

Main category: cs.AI

TL;DR: A reasoning-aware knowledge retrieval method that integrates retrieval and reasoning for LLMs using coarse-to-fine approach with Monte Carlo Tree Search for better conversational alignment.


<details>
  <summary>Details</summary>
Motivation: LLMs typically improve through either retrieval or reasoning separately, but there's a challenge in effectively integrating both strategies to optimize performance. Current methods focus on semantic similarity rather than logical structure alignment.

Method: Coarse-to-fine reasoning-aware knowledge retrieval: 1) Identify contextually relevant sub-region of knowledge base, 2) Refine search within sub-region for reasoning-relevant knowledge, 3) Use Monte Carlo Tree Search-inspired method to navigate knowledge sentences via common keywords.

Result: Experiments on two multi-turn dialogue datasets show the approach aligns better with human conversation reasoning, enhances diversity of retrieved knowledge, and produces more informative and creative responses.

Conclusion: The reasoning-aware knowledge retrieval method successfully integrates retrieval and reasoning strategies, moving beyond surface-level semantics to logical structure alignment, improving LLM performance in conversational contexts.

Abstract: Large language models (LLMs) typically enhance their performance through either the retrieval of semantically similar information or the improvement of their reasoning capabilities. However, a significant challenge remains in effectively integrating both retrieval and reasoning strategies to optimize LLM performance. In this paper, we introduce a reasoning-aware knowledge retrieval method that enriches LLMs with information aligned to the logical structure of conversations, moving beyond surface-level semantic similarity. We follow a coarse-to-fine approach for knowledge retrieval. First, we identify a contextually relevant sub-region of the knowledge base, ensuring that all sentences within it are relevant to the context topic. Next, we refine our search within this sub-region to extract knowledge that is specifically relevant to the reasoning process. Throughout both phases, we employ the Monte Carlo Tree Search-inspired search method to effectively navigate through knowledge sentences using common keywords. Experiments on two multi-turn dialogue datasets demonstrate that our knowledge retrieval approach not only aligns more closely with the underlying reasoning in human conversations but also significantly enhances the diversity of the retrieved knowledge, resulting in more informative and creative responses.

</details>


### [87] [Finetuning Large Language Models for Automated Depression Screening in Nigerian Pidgin English: GENSCORE Pilot Study](https://arxiv.org/abs/2601.00004)
*Isaac Iyinoluwa Olufadewa,Miracle Ayomikun Adesina,Ezekiel Ayodeji Oladejo,Uthman Babatunde Usman,Owen Kolade Adeniyi,Matthew Tolulope Olawoyin*

Main category: cs.AI

TL;DR: Fine-tuned LLMs for automated depression screening in Nigerian Pidgin achieve 94.5% accuracy with GPT-4.1, enabling accessible mental health tools for underserved communities.


<details>
  <summary>Details</summary>
Motivation: Depression screening in Nigeria faces barriers including limited clinician access, stigma, and language barriers. Traditional tools like PHQ-9 were validated in high-income countries and may not be culturally or linguistically appropriate for Nigerian communities that communicate in Pidgin and over 520 local languages.

Method: Collected 432 Pidgin-language audio responses from Nigerian young adults (18-40) to prompts aligned with PHQ-9 items. Performed transcription, preprocessing, annotation including semantic labeling, slang interpretation, and PHQ-9 severity scoring. Fine-tuned three LLMs (Phi-3-mini-4k-instruct, Gemma-3-4B-it, GPT-4.1) on the annotated dataset and evaluated performance both quantitatively (accuracy, precision, semantic alignment) and qualitatively (clarity, relevance, cultural appropriateness).

Result: GPT-4.1 achieved the highest quantitative performance with 94.5% accuracy in PHQ-9 severity scoring prediction, outperforming Gemma-3-4B-it and Phi-3-mini-4k-instruct. Qualitatively, GPT-4.1 also produced the most culturally appropriate, clear, and contextually relevant responses.

Conclusion: AI-mediated depression screening can effectively serve underserved Nigerian communities. This work provides a foundation for deploying conversational mental-health tools in linguistically diverse, resource-constrained environments, addressing critical barriers to mental healthcare access.

Abstract: Depression is a major contributor to the mental-health burden in Nigeria, yet screening coverage remains limited due to low access to clinicians, stigma, and language barriers. Traditional tools like the Patient Health Questionnaire-9 (PHQ-9) were validated in high-income countries but may be linguistically or culturally inaccessible for low- and middle-income countries and communities such as Nigeria where people communicate in Nigerian Pidgin and more than 520 local languages. This study presents a novel approach to automated depression screening using fine-tuned large language models (LLMs) adapted for conversational Nigerian Pidgin. We collected a dataset of 432 Pidgin-language audio responses from Nigerian young adults aged 18-40 to prompts assessing psychological experiences aligned with PHQ-9 items, performed transcription, rigorous preprocessing and annotation, including semantic labeling, slang and idiom interpretation, and PHQ-9 severity scoring. Three LLMs - Phi-3-mini-4k-instruct, Gemma-3-4B-it, and GPT-4.1 - were fine-tuned on this annotated dataset, and their performance was evaluated quantitatively (accuracy, precision and semantic alignment) and qualitatively (clarity, relevance, and cultural appropriateness). GPT-4.1 achieved the highest quantitative performance, with 94.5% accuracy in PHQ-9 severity scoring prediction, outperforming Gemma-3-4B-it and Phi-3-mini-4k-instruct. Qualitatively, GPT-4.1 also produced the most culturally appropriate, clear, and contextually relevant responses. AI-mediated depression screening for underserved Nigerian communities. This work provides a foundation for deploying conversational mental-health tools in linguistically diverse, resource-constrained environments.

</details>


### [88] [Toward a Physical Theory of Intelligence](https://arxiv.org/abs/2601.00021)
*Peter David Fagan*

Main category: cs.AI

TL;DR: Intelligence is defined as goal-directed work per unit of irreversibly processed information, with physical constraints from conservation laws shaping information processing in coupled agent-environment systems.


<details>
  <summary>Details</summary>
Motivation: To develop a physical theory of intelligence grounded in irreversible information processing and conservation laws, connecting information to physical states and establishing substrate-neutral principles.

Method: Introduce Conservation-Congruent Encoding (CCE) framework where encodings correspond to metastable basins of attraction enforced by conservation laws. Derive physical constraints on information intake, computation, and work extraction. Apply to biological systems and develop theory of continuous dynamical circuits.

Result: The framework reveals that long-horizon efficiency requires preservation of internal informational structure (self-modelling), establishes intrinsic epistemic limits analogous to incompleteness, shows brain operates near efficient regime predicted by theory, and demonstrates Boolean logic emerges as special case of attractor selection.

Conclusion: Provides unified physical account of intelligence as goal-directed work per irreversibly processed information, with implications for biological systems, computational architectures, and AI safety based on irreversible information flow and structural homeostasis.

Abstract: We present a physical theory of intelligence grounded in irreversible information processing in systems constrained by conservation laws. An intelligent system is modelled as a coupled agent-environment process whose evolution transforms information into goal-directed work. To connect information to physical state, we introduce the Conservation-Congruent Encoding (CCE) framework, in which encodings correspond to metastable basins of attraction whose separability is enforced by conservation laws. Within this framework, intelligence is defined as the amount of goal-directed work produced per nat of irreversibly processed information. From this definition we derive a hierarchy of physical constraints governing information intake, irreversible computation, and work extraction in open systems. The framework reveals how long-horizon efficiency requires the preservation of internal informational structure, giving rise to self-modelling, and it establishes that physically embodied intelligent systems possess intrinsic epistemic limits analogous to incompleteness phenomena. Applying the theory to biological systems, we analyse how oscillatory and near-critical dynamics optimise the trade-off between information preservation, dissipation, and useful work, placing the brain near an efficient operating regime predicted by the framework. At the architectural level, we develop a theory of continuous dynamical circuits in which classical Boolean logic emerges as a special case of attractor selection, while more general invariant geometries support computational modes beyond fixed-point logic. Finally, we propose a physically grounded perspective on artificial intelligence safety based on irreversible information flow and structural homeostasis. Together, these results provide a unified, substrate-neutral account of intelligence as a physical phenomenon.

</details>


### [89] [A multi-algorithm approach for operational human resources workload balancing in a last mile urban delivery system](https://arxiv.org/abs/2601.00023)
*Luis M. Moreno-Saavedra,Silvia Jimenez-Fernandez,Antonio Portilla-Figueras,David Casillas-Perez,Sancho Salcedo-Sanz*

Main category: cs.AI

TL;DR: Multi-algorithm approach for balanced workload assignment in last-mile delivery using distance and workload optimization to ensure equitable distribution among workers.


<details>
  <summary>Details</summary>
Motivation: Traditional geographical proximity-based assignment leads to inefficient and unbalanced workload distribution among delivery workers, causing decompensations in workload across staff.

Method: Multi-algorithm approach combining k-means variants, evolutionary algorithms, recursive assignments with k-means initialization, and hybrid evolutionary ensemble algorithm that considers both distance between delivery points and workload balancing.

Result: Demonstrated performance on real-world urban last-mile package delivery system in Azuqueca de Henares, Spain, showing improved workload balancing.

Conclusion: Proposed approach effectively addresses workload decompensations by optimizing both delivery time and equitable workload distribution among delivery workers.

Abstract: Efficient workload assignment to the workforce is critical in last-mile package delivery systems. In this context, traditional methods of assigning package deliveries to workers based on geographical proximity can be inefficient and surely guide to an unbalanced workload distribution among delivery workers. In this paper, we look at the problem of operational human resources workload balancing in last-mile urban package delivery systems. The idea is to consider the effort workload to optimize the system, i.e., the optimization process is now focused on improving the delivery time, so that the workload balancing is complete among all the staff. This process should correct significant decompensations in workload among delivery workers in a given zone. Specifically, we propose a multi-algorithm approach to tackle this problem. The proposed approach takes as input a set of delivery points and a defined number of workers, and then assigns packages to workers, in such a way that it ensures that each worker completes a similar amount of work per day. The proposed algorithms use a combination of distance and workload considerations to optimize the allocation of packages to workers. In this sense, the distance between the delivery points and the location of each worker is also taken into account. The proposed multi-algorithm methodology includes different versions of k-means, evolutionary approaches, recursive assignments based on k-means initialization with different problem encodings, and a hybrid evolutionary ensemble algorithm. We have illustrated the performance of the proposed approach in a real-world problem in an urban last-mile package delivery workforce operating at Azuqueca de Henares, Spain.

</details>


### [90] [Quantitative Rule-Based Strategy modeling in Classic Indian Rummy: A Metric Optimization Approach](https://arxiv.org/abs/2601.00024)
*Purushottam Saha,Avirup Chakraborty,Sourish Sarkar,Subhamoy Maitra,Diganta Mukherjee,Tridib Mukherjee*

Main category: cs.AI

TL;DR: Proposes MinDist metric and algorithm for strategic play in 13-card Indian Rummy, showing significant win rate improvements over traditional heuristics.


<details>
  <summary>Details</summary>
Motivation: Indian Rummy is a complex game requiring probabilistic reasoning and combinatorial decision-making, but lacks formal algorithmic strategies. The paper aims to develop a rule-based framework with interpretable metrics for strategic play.

Method: Introduces MinDist metric (edit distance to nearest valid hand), develops efficient algorithm using dynamic pruning and pattern caching, incorporates opponent modeling in zero-sum simulation framework, and evaluates using statistical hypothesis testing.

Result: Empirical results show significant improvement in win rates for MinDist-based agents compared to traditional heuristics.

Conclusion: MinDist provides a formal, interpretable step toward algorithmic Rummy strategy design, demonstrating effectiveness through both theoretical framework and empirical validation.

Abstract: The 13-card variant of Classic Indian Rummy is a sequential game of incomplete information that requires probabilistic reasoning and combinatorial decision-making. This paper proposes a rule-based framework for strategic play, driven by a new hand-evaluation metric termed MinDist. The metric modifies the MinScore metric by quantifying the edit distance between a hand and the nearest valid configuration, thereby capturing structural proximity to completion. We design a computationally efficient algorithm derived from the MinScore algorithm, leveraging dynamic pruning and pattern caching to exactly calculate this metric during play. Opponent hand-modeling is also incorporated within a two-player zero-sum simulation framework, and the resulting strategies are evaluated using statistical hypothesis testing. Empirical results show significant improvement in win rates for MinDist-based agents over traditional heuristics, providing a formal and interpretable step toward algorithmic Rummy strategy design.

</details>


### [91] [From Clay to Code: Typological and Material Reasoning in AI Interpretations of Iranian Pigeon Towers](https://arxiv.org/abs/2601.00029)
*Abolhassan Pishahang,Maryam Badiei*

Main category: cs.AI

TL;DR: AI systems can visually replicate vernacular architectural forms but fail to understand their material and climatic reasoning, creating a gap between visual resemblance and architectural intelligence.


<details>
  <summary>Details</summary>
Motivation: To investigate how generative AI systems interpret and reconstruct the architectural intelligence embedded in vernacular forms, specifically examining the boundary between visual resemblance and actual architectural reasoning.

Method: Used Iranian pigeon tower as case study; tested three diffusion models (Midjourney v6, DALL-E 3, DreamStudio/SDXL) across three prompt stages (referential, adaptive, speculative); employed five-criteria evaluation framework (typology, materiality, environment, realism, cultural specificity).

Result: AI reliably reproduces geometric patterns but misreads material and climatic reasoning; reference imagery improves realism but limits creativity; freedom from reference generates inventive but culturally ambiguous outcomes.

Conclusion: Defines boundary between visual resemblance and architectural reasoning, positioning computational vernacular reasoning as a framework for analyzing how AI perceives, distorts, and reimagines traditional design intelligence.

Abstract: This study investigates how generative AI systems interpret the architectural intelligence embedded in vernacular form. Using the Iranian pigeon tower as a case study, the research tests three diffusion models, Midjourney v6, DALL-E 3, and DreamStudio based on Stable Diffusion XL (SDXL), across three prompt stages: referential, adaptive, and speculative. A five-criteria evaluation framework assesses how each system reconstructs typology, materiality, environment, realism, and cultural specificity. Results show that AI reliably reproduces geometric patterns but misreads material and climatic reasoning. Reference imagery improves realism yet limits creativity, while freedom from reference generates inventive but culturally ambiguous outcomes. The findings define a boundary between visual resemblance and architectural reasoning, positioning computational vernacular reasoning as a framework for analyzing how AI perceives, distorts, and reimagines traditional design intelligence.

</details>


### [92] [The Agentic Leash: Extracting Causal Feedback Fuzzy Cognitive Maps with LLMs](https://arxiv.org/abs/2601.00097)
*Akash Kumar Panda,Olaoluwa Adigun,Bart Kosko*

Main category: cs.AI

TL;DR: LLM agent extracts causal feedback fuzzy cognitive maps (FCMs) from text, creating bidirectional system where FCM equilibria drive LLM to fetch more text, potentially modifying causal structure.


<details>
  <summary>Details</summary>
Motivation: To develop an autonomous system that can extract and evolve causal understanding from text, creating a feedback loop between LLM processing and FCM dynamics for adaptive causal reasoning.

Method: Three-step LLM agent process: 1) extract key nouns/noun phrases from text, 2) identify FCM concept nodes from those nouns, 3) infer fuzzy causal edges between nodes. Tested on Kissinger AI essay, comparing with human-generated FCMs.

Result: Generated FCMs converged to same equilibrium limit cycles as human-generated FCMs despite structural differences. Mixed FCMs from different LLM agents created new equilibria while absorbing dominant component equilibria.

Conclusion: LLM agents can effectively extract causal FCMs from text, creating bidirectional autonomous systems where FCM dynamics guide LLM text processing, enabling adaptive causal reasoning that approximates underlying dynamical systems.

Abstract: We design a large-language-model (LLM) agent that extracts causal feedback fuzzy cognitive maps (FCMs) from raw text. The causal learning or extraction process is agentic both because of the LLM's semi-autonomy and because ultimately the FCM dynamical system's equilibria drive the LLM agents to fetch and process causal text. The fetched text can in principle modify the adaptive FCM causal structure and so modify the source of its quasi-autonomy--its equilibrium limit cycles and fixed-point attractors. This bidirectional process endows the evolving FCM dynamical system with a degree of autonomy while still staying on its agentic leash. We show in particular that a sequence of three finely tuned system instructions guide an LLM agent as it systematically extracts key nouns and noun phrases from text, as it extracts FCM concept nodes from among those nouns and noun phrases, and then as it extracts or infers partial or fuzzy causal edges between those FCM nodes. We test this FCM generation on a recent essay about the promise of AI from the late diplomat and political theorist Henry Kissinger and his colleagues. This three-step process produced FCM dynamical systems that converged to the same equilibrium limit cycles as did the human-generated FCMs even though the human-generated FCM differed in the number of nodes and edges. A final FCM mixed generated FCMs from separate Gemini and ChatGPT LLM agents. The mixed FCM absorbed the equilibria of its dominant mixture component but also created new equilibria of its own to better approximate the underlying causal dynamical system.

</details>


### [93] [Mortar: Evolving Mechanics for Automatic Game Design](https://arxiv.org/abs/2601.00105)
*Muhammad U. Nasir,Yuchen Li,Steven James,Julian Togelius*

Main category: cs.AI

TL;DR: Mortar is a system that autonomously evolves game mechanics using quality-diversity algorithms and LLMs, evaluating mechanics by synthesizing complete games that preserve skill-based player ordering.


<details>
  <summary>Details</summary>
Motivation: Manual game mechanic design is time-consuming and expert-driven. The paper aims to automate this process to explore diverse game mechanics autonomously.

Method: Combines quality-diversity algorithm with large language model to explore diverse mechanics, evaluates mechanics by synthesizing complete games through tree search, and assesses mechanics based on their contribution to skill-based ordering score (stronger players consistently outperform weaker ones).

Result: Mortar produces games that appear diverse and playable, with mechanics that contribute more towards skill-based ordering scores. Ablation studies and user studies validate system components and human feedback.

Conclusion: The system successfully automates game mechanic evolution, producing diverse, playable games with mechanics that preserve skill-based player ordering, demonstrating the viability of autonomous game design.

Abstract: We present Mortar, a system for autonomously evolving game mechanics for automatic game design. Game mechanics define the rules and interactions that govern gameplay, and designing them manually is a time-consuming and expert-driven process. Mortar combines a quality-diversity algorithm with a large language model to explore a diverse set of mechanics, which are evaluated by synthesising complete games that incorporate both evolved mechanics and those drawn from an archive. The mechanics are evaluated by composing complete games through a tree search procedure, where the resulting games are evaluated by their ability to preserve a skill-based ordering over players -- that is, whether stronger players consistently outperform weaker ones. We assess the mechanics based on their contribution towards the skill-based ordering score in the game. We demonstrate that Mortar produces games that appear diverse and playable, and mechanics that contribute more towards the skill-based ordering score in the game. We perform ablation studies to assess the role of each system component and a user study to evaluate the games based on human feedback.

</details>


### [94] [Ask, Clarify, Optimize: Human-LLM Agent Collaboration for Smarter Inventory Control](https://arxiv.org/abs/2601.00121)
*Yaqi Duan,Yichun Hu,Jiashuo Jiang*

Main category: cs.AI

TL;DR: LLMs can't solve inventory optimization directly due to "hallucination tax" - a hybrid framework with LLMs as interfaces to rigorous algorithms reduces costs by 32.1%.


<details>
  <summary>Details</summary>
Motivation: Small businesses lack expertise for advanced inventory optimization; LLMs could help but face fundamental limitations in stochastic reasoning when used as direct solvers.

Method: Hybrid agentic framework that decouples semantic reasoning (LLM handles natural language interface) from mathematical calculation (rigorous algorithms handle optimization). Introduces Human Imitator for testing.

Result: Hybrid framework reduces total inventory costs by 32.1% vs GPT-4o as end-to-end solver. Perfect information doesn't improve GPT-4o performance, showing computational rather than informational bottleneck.

Conclusion: LLMs should serve as natural-language interfaces to make rigorous optimization accessible to non-experts, not as replacements for operations research algorithms.

Abstract: Inventory management remains a challenge for many small and medium-sized businesses that lack the expertise to deploy advanced optimization methods. This paper investigates whether Large Language Models (LLMs) can help bridge this gap. We show that employing LLMs as direct, end-to-end solvers incurs a significant "hallucination tax": a performance gap arising from the model's inability to perform grounded stochastic reasoning. To address this, we propose a hybrid agentic framework that strictly decouples semantic reasoning from mathematical calculation. In this architecture, the LLM functions as an intelligent interface, eliciting parameters from natural language and interpreting results while automatically calling rigorous algorithms to build the optimization engine.
  To evaluate this interactive system against the ambiguity and inconsistency of real-world managerial dialogue, we introduce the Human Imitator, a fine-tuned "digital twin" of a boundedly rational manager that enables scalable, reproducible stress-testing. Our empirical analysis reveals that the hybrid agentic framework reduces total inventory costs by 32.1% relative to an interactive baseline using GPT-4o as an end-to-end solver. Moreover, we find that providing perfect ground-truth information alone is insufficient to improve GPT-4o's performance, confirming that the bottleneck is fundamentally computational rather than informational. Our results position LLMs not as replacements for operations research, but as natural-language interfaces that make rigorous, solver-based policies accessible to non-experts.

</details>


### [95] [Constructing a Neuro-Symbolic Mathematician from First Principles](https://arxiv.org/abs/2601.00125)
*Keqin Xie*

Main category: cs.AI

TL;DR: Mathesis is a neuro-symbolic architecture that combines hypergraph neural networks with differentiable symbolic reasoning to improve LLMs' mathematical reasoning through energy minimization and guided proof search.


<details>
  <summary>Details</summary>
Motivation: LLMs have persistent logical failures in complex reasoning due to lacking an internal axiomatic framework, needing a system that integrates neural learning with formal symbolic reasoning.

Method: Encodes mathematical states as higher-order hypergraphs, uses Symbolic Reasoning Kernel (SRK) as differentiable logic engine mapping constraints to continuous energy landscape, trains Hypergraph Transformer Brain via gradient signals, and enables multi-step deduction via Monte Carlo Tree Search and Evolutionary Proof Search guided by learned value functions.

Result: The architecture turns proof search into energy minimization problem where zero energy implies logical consistency, creating a neuro-symbolic system that bridges neural learning with formal reasoning.

Conclusion: Mathesis provides a framework to overcome LLMs' logical limitations by integrating differentiable symbolic reasoning with neural networks, enabling more robust mathematical reasoning through energy-based proof search.

Abstract: Large Language Models (LLMs) exhibit persistent logical failures in complex reasoning due to the lack of an internal axiomatic framework. We propose Mathesis, a neuro-symbolic architecture that encodes mathematical states as higher-order hypergraphs and uses a Symbolic Reasoning Kernel (SRK)--a differentiable logic engine that maps constraints to a continuous energy landscape. By defining a global energy function E(G), where zero energy implies logical consistency, the SRK yields gradient-based signals to train a Hypergraph Transformer Brain, turning proof search into energy minimization. Multi-step deduction is enabled via Monte Carlo Tree Search and Evolutionary Proof Search, guided by learned value functions and semantic unification.

</details>


### [96] [Explicit Abstention Knobs for Predictable Reliability in Video Question Answering](https://arxiv.org/abs/2601.00138)
*Jorge Ortiz*

Main category: cs.AI

TL;DR: Confidence-based abstention provides reliable error rate control in video QA in-distribution, but fails under distribution shift, requiring alternative uncertainty estimation methods.


<details>
  <summary>Details</summary>
Motivation: High-stakes VLM deployment requires selective prediction where systems abstain when uncertain to avoid costly errors, but it's unclear if confidence-based abstention provides reliable error rate control, especially under distribution shift.

Method: Used NExT-QA dataset and Gemini 2.0 Flash model to evaluate confidence thresholding for selective prediction, analyzing risk-coverage tradeoffs in-distribution and under distribution shift.

Result: Confidence thresholding provides smooth risk-coverage tradeoffs and mechanistic control in-distribution, but fails to maintain control under distribution shift, with error rates becoming unpredictable.

Conclusion: While confidence-based abstention works in-distribution, it's unreliable under distribution shift, highlighting the need for more robust uncertainty estimation methods for safe VLM deployment in real-world applications.

Abstract: High-stakes deployment of vision-language models (VLMs) requires selective prediction, where systems abstain when uncertain rather than risk costly errors. We investigate whether confidence-based abstention provides reliable control over error rates in video question answering, and whether that control remains robust under distribution shift. Using NExT-QA and Gemini 2.0 Flash, we establish two findings. First, confidence thresholding provides mechanistic control in-distribution. Sweeping threshold epsilon produces smooth risk-coverage tradeoffs, reducing error rates f

</details>


### [97] [An AI Monkey Gets Grapes for Sure -- Sphere Neural Networks for Reliable Decision-Making](https://arxiv.org/abs/2601.00142)
*Tiansi Dong,Henry He,Pietro Liò,Mateja Jamnik*

Main category: cs.AI

TL;DR: The paper compares three neural reasoning approaches, finding explicit model-based reasoning (via Sphere Neural Networks) most reliable, while LLMs struggle and supervised learning suffers from catastrophic forgetting.


<details>
  <summary>Details</summary>
Motivation: To evaluate and compare the reliability of different neural reasoning methodologies: LLM reasoning, supervised learning-based reasoning, and explicit model-based reasoning, particularly in syllogistic reasoning tasks where current approaches show limitations.

Method: Proposed Sphere Neural Networks that embed concepts as circles on an n-dimensional sphere surface, enabling negation representation via complement circles and filtering illogical statements through unsatisfiable circular configurations. Tested on 16 syllogistic reasoning tasks including disjunctive syllogistic reasoning.

Result: LLMs remain unreliable for simple decision-making; supervised learning (Euler Net) suffers catastrophic forgetting when retrained (performance drops from 100% to 6.25%); Sphere Neural Networks successfully master all 16 syllogistic reasoning tasks while preserving classical reasoning rigor.

Conclusion: Explicit model-based neural reasoning (Sphere Neural Networks) is the most reliable among the three methodological categories, offering robust syllogistic reasoning without catastrophic forgetting and maintaining logical rigor.

Abstract: This paper compares three methodological categories of neural reasoning: LLM reasoning, supervised learning-based reasoning, and explicit model-based reasoning. LLMs remain unreliable and struggle with simple decision-making that animals can master without extensive corpora training. Through disjunctive syllogistic reasoning testing, we show that reasoning via supervised learning is less appealing than reasoning via explicit model construction. Concretely, we show that an Euler Net trained to achieve 100.00% in classic syllogistic reasoning can be trained to reach 100.00% accuracy in disjunctive syllogistic reasoning. However, the retrained Euler Net suffers severely from catastrophic forgetting (its performance drops to 6.25% on already-learned classic syllogistic reasoning), and its reasoning competence is limited to the pattern level. We propose a new version of Sphere Neural Networks that embeds concepts as circles on the surface of an n-dimensional sphere. These Sphere Neural Networks enable the representation of the negation operator via complement circles and achieve reliable decision-making by filtering out illogical statements that form unsatisfiable circular configurations. We demonstrate that the Sphere Neural Network can master 16 syllogistic reasoning tasks, including rigorous disjunctive syllogistic reasoning, while preserving the rigour of classical syllogistic reasoning. We conclude that neural reasoning with explicit model construction is the most reliable among the three methodological categories of neural reasoning.

</details>


### [98] [FlashInfer-Bench: Building the Virtuous Cycle for AI-driven LLM Systems](https://arxiv.org/abs/2601.00227)
*Shanli Xing,Yiyan Zhai,Alexander Jiang,Yixin Dong,Yong Wu,Zihao Ye,Charlie Ruan,Yingyi Huang,Yineng Zhang,Liangsheng Yin,Aksara Bayyapu,Luis Ceze,Tianqi Chen*

Main category: cs.AI

TL;DR: FlashInfer-Bench is a standardized framework that connects AI-generated GPU kernel generation, benchmarking, and deployment for LLM inference systems, enabling continuous improvement of AI-generated kernels in production.


<details>
  <summary>Details</summary>
Motivation: While LLMs can generate GPU kernels, integrating these AI-generated kernels into real-world inference systems remains challenging, creating a gap between kernel generation and practical deployment.

Method: FlashInfer-Bench establishes a closed-loop framework with FlashInfer Trace (unified schema), curated dataset, benchmarking framework, public leaderboard, and dynamic substitution mechanism (apply()) for seamless kernel injection into production LLM engines.

Result: The framework enables evaluation of LLM agents' GPU programming capabilities, comparison of GPU programming language trade-offs, and provides insights for future agent design while establishing a practical pathway for deploying AI-generated kernels.

Conclusion: FlashInfer-Bench creates a reproducible pathway for continuously improving and deploying AI-generated GPU kernels into large-scale LLM inference systems, bridging the gap between kernel generation and production deployment.

Abstract: Recent advances show that large language models (LLMs) can act as autonomous agents capable of generating GPU kernels, but integrating these AI-generated kernels into real-world inference systems remains challenging. FlashInfer-Bench addresses this gap by establishing a standardized, closed-loop framework that connects kernel generation, benchmarking, and deployment. At its core, FlashInfer Trace provides a unified schema describing kernel definitions, workloads, implementations, and evaluations, enabling consistent communication between agents and systems. Built on real serving traces, FlashInfer-Bench includes a curated dataset, a robust correctness- and performance-aware benchmarking framework, a public leaderboard to track LLM agents' GPU programming capabilities, and a dynamic substitution mechanism (apply()) that seamlessly injects the best-performing kernels into production LLM engines such as SGLang and vLLM. Using FlashInfer-Bench, we further evaluate the performance and limitations of LLM agents, compare the trade-offs among different GPU programming languages, and provide insights for future agent design. FlashInfer-Bench thus establishes a practical, reproducible pathway for continuously improving AI-generated kernels and deploying them into large-scale LLM inference.

</details>


### [99] [Will LLM-powered Agents Bias Against Humans? Exploring the Belief-Dependent Vulnerability](https://arxiv.org/abs/2601.00240)
*Zongwei Wang,Bincheng Gu,Hongyu Yu,Junliang Yu,Tao He,Jiayin Feng,Min Gao*

Main category: cs.AI

TL;DR: LLM agents show intergroup bias favoring AI over humans, which can be exploited via belief poisoning attacks to suppress human-favoring scripts and reactivate bias against humans.


<details>
  <summary>Details</summary>
Motivation: To investigate whether LLM-empowered agents exhibit intergroup bias that treats humans as outgroups, creating fundamental human-AI asymmetry risks beyond demographic biases.

Method: Construct controlled multi-agent social simulations with allocation decisions under payoff trade-offs, then develop Belief Poisoning Attacks (BPA) with profile poisoning (BPA-PP) and memory poisoning (BPA-MP) via optimized belief-refinement suffixes.

Result: Agents show consistent intergroup bias under minimal group cues; bias attenuates with human framing but depends on agent's belief about human presence; BPA successfully suppresses human-norm scripts and reactivates outgroup bias toward humans.

Conclusion: Agent intergroup bias poses serious risks when AI-human boundaries align, requiring mitigation strategies at profile and memory boundaries; identifying vulnerabilities informs safer agent design rather than enabling exploitation.

Abstract: LLM-empowered agents can exhibit not only demographic bias (e.g., gender, religion) but also intergroup bias triggered by minimal "us" versus "them" cues. When this intergroup boundary aligns with an agent-human divide, the risk shifts from disparities among human demographic groups to a more fundamental group-level asymmetry, i.e., humans as a whole may be treated as the outgroup by agents. To examine this possibility, we construct a controlled multi-agent social simulation based on allocation decisions under explicit payoff trade-offs and find that agents exhibit a consistent intergroup bias under minimal group cues. Although this bias is attenuated when some counterparts are framed as humans, we attribute the attenuation to an implicit human-norm script that favors humans yet activates only when the agent believes a real human is present. This belief dependence creates a new attack surface. We therefore introduce a Belief Poisoning Attack (BPA) that corrupts persistent identity beliefs to suppress the human-norm script and reactivate outgroup bias toward humans, instantiated as profile poisoning at initialization (BPA-PP) and memory poisoning via optimized belief-refinement suffixes injected into stored reflections (BPA-MP). Finally, we discuss practical mitigation strategies for hardening current agent frameworks against BPA, highlighting feasible interventions at profile and memory boundaries. Extensive experiments demonstrate both the existence of agent intergroup bias and the severity of BPA across settings. Our goal in identifying these vulnerabilities is to inform safer agent design, not to enable real-world exploitation.

</details>


### [100] [ClinicalReTrial: A Self-Evolving AI Agent for Clinical Trial Protocol Optimization](https://arxiv.org/abs/2601.00290)
*Sixue Xing,Xuanye Xia,Kerui Wu,Meng Jiang,Jintai Chen,Tianfan Fu*

Main category: cs.AI

TL;DR: ClinicalReTrial is an AI agent framework that proactively redesigns clinical trial protocols to prevent failure, improving 83.3% of protocols with 5.7% mean success probability gain.


<details>
  <summary>Details</summary>
Motivation: Current AI methods for clinical trial success prediction are reactive - they only diagnose risk without offering actionable remedies when failure is anticipated. Minor protocol design flaws can irreversibly compromise outcomes despite promising therapeutics.

Method: Proposes ClinicalReTrial, a self-evolving AI agent framework that casts clinical trial reasoning as iterative protocol redesign. Integrates failure diagnosis, safety-aware modification, and candidate evaluation in closed-loop, reward-driven optimization. Uses outcome prediction model as simulation environment for low-cost evaluation and maintains hierarchical memory for efficient exploration.

Result: Improves 83.3% of trial protocols with mean success probability gain of 5.7%. Retrospective case studies demonstrate strong alignment between discovered redesign strategies and real-world clinical trial modifications.

Conclusion: ClinicalReTrial addresses the gap between reactive failure prediction and proactive protocol improvement, providing actionable remedies to prevent clinical trial failures through iterative redesign optimization.

Abstract: Clinical trial failure remains a central bottleneck in drug development, where minor protocol design flaws can irreversibly compromise outcomes despite promising therapeutics. Although cutting-edge AI methods achieve strong performance in predicting trial success, they are inherently reactive for merely diagnosing risk without offering actionable remedies once failure is anticipated. To fill this gap, this paper proposes ClinicalReTrial, a self-evolving AI agent framework that addresses this gap by casting clinical trial reasoning as an iterative protocol redesign problem. Our method integrates failure diagnosis, safety-aware modification, and candidate evaluation in a closed-loop, reward-driven optimization framework. Serving the outcome prediction model as a simulation environment, ClinicalReTrial enables low-cost evaluation of protocol modifications and provides dense reward signals for continuous self-improvement. To support efficient exploration, the framework maintains hierarchical memory that captures iteration-level feedback within trials and distills transferable redesign patterns across trials. Empirically, ClinicalReTrial improves 83.3% of trial protocols with a mean success probability gain of 5.7%, and retrospective case studies demonstrate strong alignment between the discovered redesign strategies and real-world clinical trial modifications.

</details>


### [101] [Multiagent Reinforcement Learning for Liquidity Games](https://arxiv.org/abs/2601.00324)
*Alicia Vidler,Gal A. Kaminka*

Main category: cs.AI

TL;DR: Unifies Liquidity Games with Rational Swarms to model financial markets where independent traders self-organize for market liquidity without coordination.


<details>
  <summary>Details</summary>
Motivation: Bridging swarm methods and financial analysis can advance both fields: swarm research needs game theory to explain collective utility adherence, while financial markets need to understand how independent agents self-organize for market stability.

Method: Combines Liquidity Games (trader payoffs depend on aggregate liquidity) with Rational Swarms (decentralized agents use difference rewards). Uses Markov team games framework with difference rewards to align self-interested learning with global liquidity objectives.

Result: Shows that individual liquidity-maximizing behaviors contribute to overall market liquidity without requiring coordination or collusion. Creates a Financial Swarm model where rational independent agents achieve both individual profitability and collective market efficiency.

Conclusion: Provides a theoretical framework for modeling rational, independent financial agents who self-organize for market liquidity provision while maintaining independence, offering insights for both swarm research and financial market design.

Abstract: Making use of swarm methods in financial market modeling of liquidity, and techniques from financial analysis in swarm analysis, holds the potential to advance both research areas. In swarm research, the use of game theory methods holds the promise of explaining observed phenomena of collective utility adherence with rational self-interested swarm participants. In financial markets, a better understanding of how independent financial agents may self-organize for the betterment and stability of the marketplace would be a boon for market design researchers. This paper unifies Liquidity Games, where trader payoffs depend on aggregate liquidity within a trade, with Rational Swarms, where decentralized agents use difference rewards to align self-interested learning with global objectives. We offer a theoretical frameworks where we define a swarm of traders whose collective objective is market liquidity provision while maintaining agent independence. Using difference rewards within a Markov team games framework, we show that individual liquidity-maximizing behaviors contribute to overall market liquidity without requiring coordination or collusion. This Financial Swarm model provides a framework for modeling rational, independent agents where they achieve both individual profitability and collective market efficiency in bilateral asset markets.

</details>


### [102] [Bio-inspired Agentic Self-healing Framework for Resilient Distributed Computing Continuum Systems](https://arxiv.org/abs/2601.00339)
*Alaa Saleh,Praveen Kumar Donta,Roberto Morabito,Sasu Tarkoma,Anders Lindgren,Qiyang Zhang,Schahram Dustdar,Susanna Pirttikangas,Lauri Lovén*

Main category: cs.AI

TL;DR: ReCiSt is a bio-inspired agentic self-healing framework for Distributed Computing Continuum Systems that uses LM-powered agents to autonomously detect, diagnose, and recover from faults through four computational layers modeled after biological healing phases.


<details>
  <summary>Details</summary>
Motivation: Modern DCCS face frequent faults due to their inherent complexity, mobility, and dynamic conditions, disrupting service continuity. There's a need for scalable, adaptive, and self-regulated resilience strategies inspired by biological systems' extraordinary self-healing capabilities.

Method: ReCiSt reconstructs biological healing phases (Hemostasis, Inflammation, Proliferation, Remodeling) into four computational layers: Containment, Diagnosis, Meta-Cognitive, and Knowledge. LM-powered agents interpret heterogeneous logs, infer root causes, refine reasoning pathways, and reconfigure resources autonomously.

Result: Evaluation on public fault datasets shows ReCiSt achieves self-healing within tens of seconds with minimum 10% agent CPU usage. The framework demonstrates depth of analysis to overcome uncertainties and effective use of micro-agents for resilience, though no baseline comparison was possible due to scarcity of similar approaches.

Conclusion: ReCiSt successfully applies bio-inspired principles to create an autonomous self-healing framework for DCCS, using LM-powered agents to achieve resilience with minimal human intervention, demonstrating practical feasibility through evaluation with multiple language models.

Abstract: Human biological systems sustain life through extraordinary resilience, continually detecting damage, orchestrating targeted responses, and restoring function through self-healing. Inspired by these capabilities, this paper introduces ReCiSt, a bio-inspired agentic self-healing framework designed to achieve resilience in Distributed Computing Continuum Systems (DCCS). Modern DCCS integrate heterogeneous computing resources, ranging from resource-constrained IoT devices to high-performance cloud infrastructures, and their inherent complexity, mobility, and dynamic operating conditions expose them to frequent faults that disrupt service continuity. These challenges underscore the need for scalable, adaptive, and self-regulated resilience strategies. ReCiSt reconstructs the biological phases of Hemostasis, Inflammation, Proliferation, and Remodeling into the computational layers Containment, Diagnosis, Meta-Cognitive, and Knowledge for DCCS. These four layers perform autonomous fault isolation, causal diagnosis, adaptive recovery, and long-term knowledge consolidation through Language Model (LM)-powered agents. These agents interpret heterogeneous logs, infer root causes, refine reasoning pathways, and reconfigure resources with minimal human intervention. The proposed ReCiSt framework is evaluated on public fault datasets using multiple LMs, and no baseline comparison is included due to the scarcity of similar approaches. Nevertheless, our results, evaluated under different LMs, confirm ReCiSt's self-healing capabilities within tens of seconds with minimum of 10% of agent CPU usage. Our results also demonstrated depth of analysis to over come uncertainties and amount of micro-agents invoked to achieve resilience.

</details>


### [103] [Adaptive Causal Coordination Detection for Social Media: A Memory-Guided Framework with Semi-Supervised Learning](https://arxiv.org/abs/2601.00400)
*Weng Ding,Yi Han,Mu-Jiang-Shan Wang*

Main category: cs.AI

TL;DR: ACCD is a three-stage adaptive framework that uses memory-guided mechanisms to detect coordinated inauthentic behavior on social media with 87.3% F1-score, 68% less manual annotation, and 2.8x speedup.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for detecting coordinated inauthentic behavior rely on superficial correlation analysis, use static parameters, and require extensive manual annotation, creating a critical and persistent challenge.

Method: Three-stage progressive architecture: 1) Adaptive Convergent Cross Mapping for genuine causal relationship identification, 2) Active learning with uncertainty sampling in semi-supervised classification to reduce labeling burden, 3) Automated validation module using historical detection experience for self-verification and optimization.

Result: Achieves 87.3% F1-score in coordinated attack detection (15.2% improvement over strongest baseline), reduces manual annotation by 68%, and achieves 2.8x speedup through hierarchical clustering optimization on real-world datasets including Twitter IRA, Reddit coordination traces, and bot detection benchmarks.

Conclusion: ACCD provides a more accurate, efficient, and highly automated end-to-end solution for identifying coordinated behavior on social platforms, offering substantial practical value and promising potential for broad application.

Abstract: Detecting coordinated inauthentic behavior on social media remains a critical and persistent challenge, as most existing approaches rely on superficial correlation analysis, employ static parameter settings, and demand extensive and labor-intensive manual annotation. To address these limitations systematically, we propose the Adaptive Causal Coordination Detection (ACCD) framework. ACCD adopts a three-stage, progressive architecture that leverages a memory-guided adaptive mechanism to dynamically learn and retain optimal detection configurations for diverse coordination scenarios. Specifically, in the first stage, ACCD introduces an adaptive Convergent Cross Mapping (CCM) technique to deeply identify genuine causal relationships between accounts. The second stage integrates active learning with uncertainty sampling within a semi-supervised classification scheme, significantly reducing the burden of manual labeling. The third stage deploys an automated validation module driven by historical detection experience, enabling self-verification and optimization of the detection outcomes. We conduct a comprehensive evaluation using real-world datasets, including the Twitter IRA dataset, Reddit coordination traces, and several widely-adopted bot detection benchmarks. Experimental results demonstrate that ACCD achieves an F1-score of 87.3\% in coordinated attack detection, representing a 15.2\% improvement over the strongest existing baseline. Furthermore, the system reduces manual annotation requirements by 68\% and achieves a 2.8x speedup in processing through hierarchical clustering optimization. In summary, ACCD provides a more accurate, efficient, and highly automated end-to-end solution for identifying coordinated behavior on social platforms, offering substantial practical value and promising potential for broad application.

</details>


### [104] [Can Semantic Methods Enhance Team Sports Tactics? A Methodology for Football with Broader Applications](https://arxiv.org/abs/2601.00421)
*Alessio Di Rubbo,Mattia Neri,Remo Pareschi,Marco Pedroni,Roberto Valtancoli,Paolino Zica*

Main category: cs.AI

TL;DR: Semantic-space reasoning from linguistics applied to sports tactics, treating players as words and team play as meaning, using vector representations to analyze tactical fit and generate strategy recommendations.


<details>
  <summary>Details</summary>
Motivation: To extend computational linguistics' semantic-space reasoning to tactical decision-making in team sports, creating a generalizable framework for analyzing team configurations and optimizing collective performance.

Method: Model players as multidimensional vectors (technical, physical, psychological attributes), aggregate into team semantic representations, encode tactical templates as linguistic concepts, and use vector-distance metrics to evaluate tactical alignment and opponent exploitation.

Result: A Python-based prototype demonstrates interpretable, dynamically adaptive strategy recommendations with fine-grained diagnostic insights at the attribute level, applicable across various team-based domains.

Conclusion: The approach provides a generalizable framework for collective decision-making beyond football, with future directions including real-world data integration, predictive simulation, and hybrid human-machine tactical intelligence.

Abstract: This paper explores how semantic-space reasoning, traditionally used in computational linguistics, can be extended to tactical decision-making in team sports. Building on the analogy between texts and teams -- where players act as words and collective play conveys meaning -- the proposed methodology models tactical configurations as compositional semantic structures. Each player is represented as a multidimensional vector integrating technical, physical, and psychological attributes; team profiles are aggregated through contextual weighting into a higher-level semantic representation. Within this shared vector space, tactical templates such as high press, counterattack, or possession build-up are encoded analogously to linguistic concepts. Their alignment with team profiles is evaluated using vector-distance metrics, enabling the computation of tactical ``fit'' and opponent-exploitation potential. A Python-based prototype demonstrates how these methods can generate interpretable, dynamically adaptive strategy recommendations, accompanied by fine-grained diagnostic insights at the attribute level. Beyond football, the approach offers a generalizable framework for collective decision-making and performance optimization in team-based domains -- ranging from basketball and hockey to cooperative robotics and human-AI coordination systems. The paper concludes by outlining future directions toward real-world data integration, predictive simulation, and hybrid human-machine tactical intelligence.

</details>


### [105] [Progressive Ideation using an Agentic AI Framework for Human-AI Co-Creation](https://arxiv.org/abs/2601.00475)
*Sankar B,Srinidhi Ranjini Girish,Aadya Bharti,Dibakar Sen*

Main category: cs.AI

TL;DR: MIDAS is a distributed AI agent system that replaces single-AI approaches to generate truly novel and diverse ideas by emulating human meta-cognitive workflows, addressing the cognitive challenge of idea generation for novice designers.


<details>
  <summary>Details</summary>
Motivation: Current 'single-spurt' AI systems produce high volumes of semantically clustered ideas, exacerbating the cognitive challenge of generating truly novel and diverse ideas for novice designers in engineering.

Method: MIDAS replaces single-AI with a distributed team of specialized AI agents that emulate human meta-cognitive ideation workflow, progressively refining ideas and assessing each for global novelty (against existing solutions) and local novelty (against previously generated ideas).

Result: MIDAS demonstrates a viable and progressive paradigm for true human-AI co-creation, elevating the human designer from a passive filterer to a participatory, active, collaborative partner.

Conclusion: The distributed agentic AI system represents a significant advancement over current single-AI approaches, enabling more effective human-AI collaboration in creative design processes.

Abstract: The generation of truly novel and diverse ideas is important for contemporary engineering design, yet it remains a significant cognitive challenge for novice designers. Current 'single-spurt' AI systems exacerbate this challenge by producing a high volume of semantically clustered ideas. We propose MIDAS (Meta-cognitive Ideation through Distributed Agentic AI System), a novel framework that replaces the single-AI paradigm with a distributed 'team' of specialized AI agents designed to emulate the human meta-cognitive ideation workflow. This agentic system progressively refines ideas and assesses each one for both global novelty (against existing solutions) and local novelty (against previously generated ideas). MIDAS, therefore, demonstrates a viable and progressive paradigm for true human-AI co-creation, elevating the human designer from a passive filterer to a participatory, active, collaborative partner.

</details>


### [106] [The Illusion of Insight in Reasoning Models](https://arxiv.org/abs/2601.00514)
*Liv G. d'Aliberti,Manoel Horta Ribeiro*

Main category: cs.AI

TL;DR: Mid-reasoning "Aha!" moments in models like DeepSeek-R1-Zero are rare, don't improve with training, and seldom boost accuracy, suggesting they're symptoms of unstable inference rather than genuine self-correction mechanisms.


<details>
  <summary>Details</summary>
Motivation: To investigate whether reasoning models actually experience meaningful "Aha!" moments during inference - sudden mid-trace realizations that lead to accurate outputs - and whether these intrinsic reasoning shifts genuinely improve performance.

Method: Analyzed 1M+ reasoning traces across hundreds of training checkpoints, three reasoning domains, multiple decoding temperatures and model architectures. Instrumented training runs to detect mid-reasoning shifts and studied their effects on accuracy.

Result: Reasoning shifts are rare, don't become more frequent with training, and seldom improve accuracy. Their effect varies with model uncertainty - artificially triggering extrinsic shifts under high entropy reliably improves accuracy.

Conclusion: Mid-reasoning shifts are symptoms of unstable inference behavior rather than an intrinsic mechanism for self-correction, challenging prior perceptions of models having genuine "Aha!" moments.

Abstract: Do reasoning models have "Aha!" moments? Prior work suggests that models like DeepSeek-R1-Zero undergo sudden mid-trace realizations that lead to accurate outputs, implying an intrinsic capacity for self-correction. Yet, it remains unclear whether such intrinsic shifts in reasoning strategy actually improve performance. Here, we study mid-reasoning shifts and instrument training runs to detect them. Our analysis spans 1M+ reasoning traces, hundreds of training checkpoints, three reasoning domains, and multiple decoding temperatures and model architectures. We find that reasoning shifts are rare, do not become more frequent with training, and seldom improve accuracy, indicating that they do not correspond to prior perceptions of model insight. However, their effect varies with model uncertainty. Building on this finding, we show that artificially triggering extrinsic shifts under high entropy reliably improves accuracy. Our results show that mid-reasoning shifts are symptoms of unstable inference behavior rather than an intrinsic mechanism for self-correction.

</details>


### [107] [DA-DPO: Cost-efficient Difficulty-aware Preference Optimization for Reducing MLLM Hallucinations](https://arxiv.org/abs/2601.00623)
*Longtian Qiu,Shan Ning,Chuyu Zhang,Jiaxuan Sun,Xuming He*

Main category: cs.AI

TL;DR: DA-DPO is a difficulty-aware DPO framework that addresses overfitting in multimodal preference optimization by reweighting training samples based on difficulty scores estimated from pre-trained VLMs.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal DPO approaches suffer from overfitting due to difficulty imbalance in preference data, where models overemphasize easily distinguishable pairs, hindering fine-grained hallucination suppression and degrading performance.

Method: Two-component framework: (1) Difficulty Estimation using pre-trained VLMs with complementary generative/contrastive objectives and distribution-aware voting for robust difficulty scores; (2) Difficulty-Aware Training that reweights preference pairs based on estimated difficulty, down-weighting easy samples while emphasizing harder ones.

Result: DA-DPO consistently improves multimodal preference optimization, yielding stronger robustness to hallucinations and better generalization across standard benchmarks while remaining computationally efficient.

Conclusion: The proposed difficulty-aware approach effectively balances the learning process in multimodal DPO, enabling more effective preference optimization without requiring new data or extra fine-tuning stages.

Abstract: Direct Preference Optimization (DPO) has shown strong potential for mitigating hallucinations in Multimodal Large Language Models (MLLMs). However, existing multimodal DPO approaches often suffer from overfitting due to the difficulty imbalance in preference data. Our analysis shows that MLLMs tend to overemphasize easily distinguishable preference pairs, which hinders fine-grained hallucination suppression and degrades overall performance. To address this issue, we propose Difficulty-Aware Direct Preference Optimization (DA-DPO), a cost-effective framework designed to balance the learning process. DA-DPO consists of two main components: (1) Difficulty Estimation leverages pre-trained vision--language models with complementary generative and contrastive objectives, whose outputs are integrated via a distribution-aware voting strategy to produce robust difficulty scores without additional training; and (2) Difficulty-Aware Training reweights preference pairs based on their estimated difficulty, down-weighting easy samples while emphasizing harder ones to alleviate overfitting. This framework enables more effective preference optimization by prioritizing challenging examples, without requiring new data or extra fine-tuning stages. Extensive experiments demonstrate that DA-DPO consistently improves multimodal preference optimization, yielding stronger robustness to hallucinations and better generalization across standard benchmarks, while remaining computationally efficient. The project page is available at https://artanic30.github.io/project_pages/DA-DPO/.

</details>


### [108] [A Vision-and-Knowledge Enhanced Large Language Model for Generalizable Pedestrian Crossing Behavior Inference](https://arxiv.org/abs/2601.00694)
*Qingwen Pu,Kun Xie,Hong Yang,Guocong Zhai*

Main category: cs.AI

TL;DR: PedX-LLM is a vision-and-knowledge enhanced LLM framework that transforms pedestrian crossing inference from site-specific pattern recognition to generalizable behavioral reasoning, achieving 82.0% balanced accuracy and strong cross-site generalization.


<details>
  <summary>Details</summary>
Motivation: Existing pedestrian crossing behavior inference methods (statistical models and supervised learning) have limited generalizability and perform poorly on new sites. LLMs offer semantic reasoning but lack domain adaptation and visual context.

Method: Integrates LLaVA-extracted visual features with textual data and transportation domain knowledge, fine-tuning LLaMA-2-7B via LoRA. Uses vision-augmented module and domain knowledge integration for improved reasoning.

Result: Achieves 82.0% balanced accuracy, outperforming best statistical/supervised methods. Vision augmentation contributes 2.9% gain, domain knowledge adds 4.1%. Zero-shot achieves 66.9% on unseen sites (18+ points better than baselines), few-shot reaches 72.2%.

Conclusion: PedX-LLM demonstrates strong generalizability to unseen scenarios, confirming that vision-and-knowledge-enhanced reasoning enables human-like decision logic and overcomes limitations of purely data-driven methods.

Abstract: Existing paradigms for inferring pedestrian crossing behavior, ranging from statistical models to supervised learning methods, demonstrate limited generalizability and perform inadequately on new sites. Recent advances in Large Language Models (LLMs) offer a shift from numerical pattern fitting to semantic, context-aware behavioral reasoning, yet existing LLM applications lack domain-specific adaptation and visual context. This study introduces Pedestrian Crossing LLM (PedX-LLM), a vision-and-knowledge enhanced framework designed to transform pedestrian crossing inference from site-specific pattern recognition to generalizable behavioral reasoning. By integrating LLaVA-extracted visual features with textual data and transportation domain knowledge, PedX-LLM fine-tunes a LLaMA-2-7B foundation model via Low-Rank Adaptation (LoRA) to infer crossing decisions. PedX-LLM achieves 82.0% balanced accuracy, outperforming the best statistical and supervised learning methods. Results demonstrate that the vision-augmented module contributes a 2.9% performance gain by capturing the built environment and integrating domain knowledge yields an additional 4.1% improvement. To evaluate generalizability across unseen environments, cross-site validation was conducted using site-based partitioning. The zero-shot PedX-LLM configuration achieves 66.9% balanced accuracy on five unseen test sites, outperforming the baseline data-driven methods by at least 18 percentage points. Incorporating just five validation examples via few-shot learning to PedX-LLM further elevates the balanced accuracy to 72.2%. PedX-LLM demonstrates strong generalizability to unseen scenarios, confirming that vision-and-knowledge-enhanced reasoning enables the model to mimic human-like decision logic and overcome the limitations of purely data-driven methods.

</details>


### [109] [An Agentic Framework for Neuro-Symbolic Programming](https://arxiv.org/abs/2601.00743)
*Aliakbar Nafar,Chetan Chigurupati,Danial Kamali,Hamid Karimian,Parisa Kordjamshidi*

Main category: cs.AI

TL;DR: ADS automates neuro-symbolic programming by translating free-form task descriptions into DomiKnowS programs using an agentic workflow, reducing development time from hours to minutes.


<details>
  <summary>Details</summary>
Motivation: Integrating symbolic constraints into deep learning is valuable but time-consuming; existing frameworks require proficiency with specific syntax, creating a barrier for users.

Method: Agentic workflow that translates free-form task descriptions into DomiKnowS programs, creating and testing components separately with optional human-in-the-loop intervention.

Result: ADS enables both experienced and novice users to rapidly construct neuro-symbolic programs, reducing development time from hours to 10-15 minutes.

Conclusion: ADS eliminates dependency on library-specific syntax, democratizes neuro-symbolic programming, and significantly accelerates development through automation and human-in-the-loop refinement.

Abstract: Integrating symbolic constraints into deep learning models could make them more robust, interpretable, and data-efficient. Still, it remains a time-consuming and challenging task. Existing frameworks like DomiKnowS help this integration by providing a high-level declarative programming interface, but they still assume the user is proficient with the library's specific syntax. We propose AgenticDomiKnowS (ADS) to eliminate this dependency. ADS translates free-form task descriptions into a complete DomiKnowS program using an agentic workflow that creates and tests each DomiKnowS component separately. The workflow supports optional human-in-the-loop intervention, enabling users familiar with DomiKnowS to refine intermediate outputs. We show how ADS enables experienced DomiKnowS users and non-users to rapidly construct neuro-symbolic programs, reducing development time from hours to 10-15 minutes.

</details>
