<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 276]
- [cs.AI](#cs.AI) [Total: 153]
- [cs.RO](#cs.RO) [Total: 86]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [EDU-CIRCUIT-HW: Evaluating Multimodal Large Language Models on Real-World University-Level STEM Student Handwritten Solutions](https://arxiv.org/abs/2602.00095)
*Weiyu Sun,Liangliang Chen,Yongnuo Cai,Huiru Xie,Yi Zeng,Ying Zhang*

Main category: cs.CV

TL;DR: EDU-CIRCUIT-HW dataset reveals MLLMs' significant recognition failures on authentic student handwritten STEM solutions, limiting their reliability for auto-grading despite potential for improvement with minimal human intervention.


<details>
  <summary>Details</summary>
Motivation: MLLMs show promise for education but struggle with authentic student handwritten solutions containing mixed math, diagrams, and text. Current benchmarks lack domain-specific data and evaluation focuses only on downstream tasks, missing holistic understanding assessment.

Method: Released EDU-CIRCUIT-HW dataset with 1,300+ authentic student handwritten solutions from university STEM course. Used expert-verified transcriptions and grading reports to evaluate MLLMs' upstream recognition fidelity and downstream auto-grading performance simultaneously.

Result: Uncovered astonishing scale of latent failures in MLLM-recognized content, showing insufficient reliability for high-stakes educational applications. Case study demonstrated that using error patterns to detect/rectify errors with only ~4% human intervention significantly improves grading system robustness.

Conclusion: Current MLLMs lack reliability for auto-grading authentic student handwritten STEM solutions, but targeted error detection with minimal human oversight can substantially enhance system robustness for educational deployment.

Abstract: Multimodal Large Language Models (MLLMs) hold significant promise for revolutionizing traditional education and reducing teachers' workload. However, accurately interpreting unconstrained STEM student handwritten solutions with intertwined mathematical formulas, diagrams, and textual reasoning poses a significant challenge due to the lack of authentic and domain-specific benchmarks. Additionally, current evaluation paradigms predominantly rely on the outcomes of downstream tasks (e.g., auto-grading), which often probe only a subset of the recognized content, thereby failing to capture the MLLMs' understanding of complex handwritten logic as a whole. To bridge this gap, we release EDU-CIRCUIT-HW, a dataset consisting of 1,300+ authentic student handwritten solutions from a university-level STEM course. Utilizing the expert-verified verbatim transcriptions and grading reports of student solutions, we simultaneously evaluate various MLLMs' upstream recognition fidelity and downstream auto-grading performance. Our evaluation uncovers an astonishing scale of latent failures within MLLM-recognized student handwritten content, highlighting the models' insufficient reliability for auto-grading and other understanding-oriented applications in high-stakes educational settings. In solution, we present a case study demonstrating that leveraging identified error patterns to preemptively detect and rectify recognition errors, with only minimal human intervention (approximately 4% of the total solutions), can significantly enhance the robustness of the deployed AI-enabled grading system on unseen student solutions.

</details>


### [2] [Mirage2Matter: A Physically Grounded Gaussian World Model from Video](https://arxiv.org/abs/2602.00096)
*Zhengqing Gao,Ziwen Li,Xin Wang,Jiaxin Huang,Zhenyang Ren,Mingkai Shao,Hanlue Zhang,Tianyu Huang,Yongkang Cheng,Yandong Guo,Runqi Lin,Yuanyuan Wang,Tongliang Liu,Kun Zhang,Mingming Gong*

Main category: cs.CV

TL;DR: Simulate Anything is a graphics-driven framework that reconstructs real-world environments from multi-view videos using 3D Gaussian Splatting, creates physically realistic simulations, and trains VLA models that perform as well as those trained on real data.


<details>
  <summary>Details</summary>
Motivation: Real-world interaction data is scarce for embodied AI training, and existing simulation approaches suffer from visual/physical gaps, expensive sensors, calibration requirements, and depth measurement dependencies, limiting scalability.

Method: Uses 3D Gaussian Splatting to reconstruct photorealistic scenes from multi-view videos, employs generative models to recover physically realistic representations, integrates into simulation via precision calibration targets for accurate scale alignment, creating unified editable world models.

Result: Vision Language Action models trained on simulated data achieve strong zero-shot performance on downstream tasks, matching or even surpassing results obtained with real-world data.

Conclusion: Reconstruction-driven world modeling enables scalable and practical embodied intelligence training, demonstrating the potential of simulation frameworks to overcome real-world data scarcity.

Abstract: The scalability of embodied intelligence is fundamentally constrained by the scarcity of real-world interaction data. While simulation platforms provide a promising alternative, existing approaches often suffer from a substantial visual and physical gap to real environments and rely on expensive sensors, precise robot calibration, or depth measurements, limiting their practicality at scale. We present Simulate Anything, a graphics-driven world modeling and simulation framework that enables efficient generation of high-fidelity embodied training data using only multi-view environment videos and off-the-shelf assets. Our approach reconstructs real-world environments into a photorealistic scene representation using 3D Gaussian Splatting (3DGS), seamlessly capturing fine-grained geometry and appearance from video. We then leverage generative models to recover a physically realistic representation and integrate it into a simulation environment via a precision calibration target, enabling accurate scale alignment between the reconstructed scene and the real world. Together, these components provide a unified, editable, and physically grounded world model. Vision Language Action (VLA) models trained on our simulated data achieve strong zero-shot performance on downstream tasks, matching or even surpassing results obtained with real-world data, highlighting the potential of reconstruction-driven world modeling for scalable and practical embodied intelligence training.

</details>


### [3] [R3G: A Reasoning--Retrieval--Reranking Framework for Vision-Centric Answer Generation](https://arxiv.org/abs/2602.00104)
*Zhuohong Chen,Zhengxian Wu,Zirui Liao,Shenao Jiang,Hangrui Xu,Yang Chen,Chaokui Su,Xiaoyu Liu,Haoqian Wang*

Main category: cs.CV

TL;DR: R3G is a modular Reasoning-Retrieval-Reranking framework for vision-centric VQA that uses reasoning plans to guide two-stage image retrieval and achieves SOTA performance on MRAG-Bench.


<details>
  <summary>Details</summary>
Motivation: Vision-centric VQA requires retrieving relevant images to supply missing visual cues, but selecting the right images and effectively integrating them into reasoning remains challenging.

Method: R3G uses a modular framework: 1) produces reasoning plan specifying required visual cues, 2) two-stage retrieval with coarse retrieval followed by fine-grained reranking to select evidence images.

Result: Improves accuracy across six MLLM backbones and nine sub-scenarios on MRAG-Bench, achieving state-of-the-art overall performance. Ablations show sufficiency-aware reranking and reasoning steps are complementary.

Conclusion: R3G effectively addresses the challenge of selecting and integrating visual evidence in VQA through its modular reasoning-retrieval-reranking approach, with code and data released publicly.

Abstract: Vision-centric retrieval for VQA requires retrieving images to supply missing visual cues and integrating them into the reasoning process. However, selecting the right images and integrating them effectively into the model's reasoning remains challenging.To address this challenge, we propose R3G, a modular Reasoning-Retrieval-Reranking framework.It first produces a brief reasoning plan that specifies the required visual cues, then adopts a two-stage strategy, with coarse retrieval followed by fine-grained reranking, to select evidence images.On MRAG-Bench, R3G improves accuracy across six MLLM backbones and nine sub-scenarios, achieving state-of-the-art overall performance. Ablations show that sufficiency-aware reranking and reasoning steps are complementary, helping the model both choose the right images and use them well. We release code and data at https://github.com/czh24/R3G.

</details>


### [4] [HYPE-EDIT-1: Benchmark for Measuring Reliability in Frontier Image Editing Models](https://arxiv.org/abs/2602.00105)
*Wing Chan,Richard Allen*

Main category: cs.CV

TL;DR: HYPE-EDIT-1 is a 100-task benchmark for evaluating image editing models on real-world marketing/design workflows, measuring per-attempt success rates and effective costs including retries and human review time.


<details>
  <summary>Details</summary>
Motivation: Public demos of image editing models show best-case samples, but real workflows involve retries and review time costs that aren't captured in traditional evaluations.

Method: Created a 100-task benchmark with reference-based marketing/design edits, binary pass/fail judging, generating 10 independent outputs per task to estimate per-attempt pass rates, pass@10, expected attempts under retry caps, and effective cost per successful edit combining model price with human review time.

Result: Across evaluated models, per-attempt pass rates ranged 34-83%, and effective cost per success ranged USD 0.66-1.42. Models with low per-image pricing became more expensive when considering total effective costs of retries and human reviews.

Conclusion: The benchmark reveals that traditional per-image pricing metrics are misleading for real workflows; effective costs including retries and human review time provide more accurate evaluation of image editing models for practical applications.

Abstract: Public demos of image editing models are typically best-case samples; real workflows pay for retries and review time. We introduce HYPE-EDIT-1, a 100-task benchmark of reference-based marketing/design edits with binary pass/fail judging. For each task we generate 10 independent outputs to estimate per-attempt pass rate, pass@10, expected attempts under a retry cap, and an effective cost per successful edit that combines model price with human review time. We release 50 public tasks and maintain a 50-task held-out private split for server-side evaluation, plus a standardized JSON schema and tooling for VLM and human-based judging. Across the evaluated models, per-attempt pass rates span 34-83 percent and effective cost per success spans USD 0.66-1.42. Models that have low per-image pricing are more expensive when you consider the total effective cost of retries and human reviews.

</details>


### [5] [Efficient UAV trajectory prediction: A multi-modal deep diffusion framework](https://arxiv.org/abs/2602.00107)
*Yuan Gao,Xinyu Guo,Wenjing Xie,Zifan Wang,Hongwen Yu,Gongyang Li,Shugong Xu*

Main category: cs.CV

TL;DR: Multi-modal UAV trajectory prediction method fuses LiDAR and radar data using deep fusion network with cross-attention, achieving 40% accuracy improvement over baseline.


<details>
  <summary>Details</summary>
Motivation: To meet requirements for managing unauthorized UAVs in the low-altitude economy by improving trajectory prediction accuracy through multi-modal sensor fusion.

Method: Proposes Multi-Modal Deep Fusion Framework with two modality-specific feature extraction networks for LiDAR and radar, followed by bidirectional cross-attention fusion module to exploit complementary spatial geometric and dynamic reflection characteristics.

Result: Experimental results show 40% improvement in trajectory prediction accuracy compared to baseline model on MMAUD dataset from CVPR 2024 UG2+ challenge; ablation studies validate effectiveness of loss functions and post-processing strategies.

Conclusion: The proposed model effectively utilizes multi-modal data and provides an efficient solution for unauthorized UAV trajectory prediction in low-altitude economy applications.

Abstract: To meet the requirements for managing unauthorized UAVs in the low-altitude economy, a multi-modal UAV trajectory prediction method based on the fusion of LiDAR and millimeter-wave radar information is proposed. A deep fusion network for multi-modal UAV trajectory prediction, termed the Multi-Modal Deep Fusion Framework, is designed. The overall architecture consists of two modality-specific feature extraction networks and a bidirectional cross-attention fusion module, aiming to fully exploit the complementary information of LiDAR and radar point clouds in spatial geometric structure and dynamic reflection characteristics. In the feature extraction stage, the model employs independent but structurally identical feature encoders for LiDAR and radar. After feature extraction, the model enters the Bidirectional Cross-Attention Mechanism stage to achieve information complementarity and semantic alignment between the two modalities. To verify the effectiveness of the proposed model, the MMAUD dataset used in the CVPR 2024 UG2+ UAV Tracking and Pose-Estimation Challenge is adopted as the training and testing dataset. Experimental results show that the proposed multi-modal fusion model significantly improves trajectory prediction accuracy, achieving a 40% improvement compared to the baseline model. In addition, ablation experiments are conducted to demonstrate the effectiveness of different loss functions and post-processing strategies in improving model performance. The proposed model can effectively utilize multi-modal data and provides an efficient solution for unauthorized UAV trajectory prediction in the low-altitude economy.

</details>


### [6] [SITUATE -- Synthetic Object Counting Dataset for VLM training](https://arxiv.org/abs/2602.00108)
*René Peinl,Vincent Tischler,Patrick Schröder,Christian Groth*

Main category: cs.CV

TL;DR: SITUATE is a new dataset for training VLMs on counting tasks with spatial constraints, bridging the gap between simple 2D datasets and ambiguous real-world datasets.


<details>
  <summary>Details</summary>
Motivation: Existing counting datasets have limitations: simple 2D datasets like VLMCountBench lack real-world complexity, while real-world datasets like TallyQA lack control over occlusions and spatial composition, making them ambiguous for training.

Method: Created SITUATE dataset specifically designed for counting with spatial constraints. Evaluated by finetuning Qwen VL 2.5 7B model on SITUATE and comparing performance against Pixmo count test data and other established counting benchmarks.

Result: Finetuning on SITUATE improves accuracy on Pixmo count test data, but finetuning on Pixmo count does not improve performance on SITUATE. SITUATE helps improve generalization for out-of-distribution images.

Conclusion: SITUATE dataset effectively bridges the gap between controlled 2D datasets and ambiguous real-world datasets, enabling better training of VLMs for counting tasks with spatial constraints and improving out-of-distribution generalization.

Abstract: We present SITUATE, a novel dataset designed for training and evaluating Vision Language Models on counting tasks with spatial constraints. The dataset bridges the gap between simple 2D datasets like VLMCountBench and often ambiguous real-life datasets like TallyQA, which lack control over occlusions and spatial composition. Experiments show that our dataset helps to improve generalization for out-of-distribution images, since a finetune of Qwen VL 2.5 7B on SITUATE improves accuracy on the Pixmo count test data, but not vice versa. We cross validate this by comparing the model performance across established other counting benchmarks and against an equally sized fine-tuning set derived from Pixmo count.

</details>


### [7] [Robustness of Presentation Attack Detection in Remote Identity Validation Scenarios](https://arxiv.org/abs/2602.00109)
*John J. Howard,Richard O. Plesh,Yevgeniy B. Sirotin,Jerry L. Tipton,Arun R. Vemury*

Main category: cs.CV

TL;DR: Commercial PAD systems show significant performance degradation in low-light and auto-capture scenarios, with error rates increasing 4x and 2x respectively.


<details>
  <summary>Details</summary>
Motivation: Presentation attack detection (PAD) is crucial for remote identity validation systems, but ensuring robust performance across diverse environmental conditions (especially low-light and automated workflows) remains a critical challenge.

Method: The paper investigates commercial PAD systems using scenario testing of remote identity validation, specifically examining performance under low-light conditions and automated image acquisition workflows.

Result: PAD systems experience significant performance decline in low-light (4x error rate increase) and auto-capture (2x error rate increase) scenarios. Only one tested system maintained robust performance with <3% error rate across all scenarios.

Conclusion: Testing across diverse environmental conditions is essential to ensure robust and reliable PAD performance in real-world applications, as most commercial systems fail under challenging but realistic scenarios.

Abstract: Presentation attack detection (PAD) subsystems are an important part of effective and user-friendly remote identity validation (RIV) systems. However, ensuring robust performance across diverse environmental and procedural conditions remains a critical challenge. This paper investigates the impact of low-light conditions and automated image acquisition on the robustness of commercial PAD systems using a scenario test of RIV. Our results show that PAD systems experience a significant decline in performance when utilized in low-light or auto-capture scenarios, with a model-predicted increase in error rates by a factor of about four under low-light conditions and a doubling of those odds under auto-capture workflows. Specifically, only one of the tested systems was robust to these perturbations, maintaining a maximum bona fide presentation classification error rate below 3% across all scenarios. Our findings emphasize the importance of testing across diverse environments to ensure robust and reliable PAD performance in real-world applications.

</details>


### [8] [Observing Health Outcomes Using Remote Sensing Imagery and Geo-Context Guided Visual Transformer](https://arxiv.org/abs/2602.00110)
*Yu Li,Guilherme N. DeSouza,Praveen Rao,Chi-Ren Shyu*

Main category: cs.CV

TL;DR: A novel vision transformer model that enhances remote sensing image analysis by incorporating geospatial data through aligned embeddings and guided attention mechanisms, outperforming existing geospatial foundation models in disease prevalence prediction.


<details>
  <summary>Details</summary>
Motivation: Current vision-language and multimodal models for remote sensing focus on semantic alignment between visual and textual content but lack proper geospatial understanding. They are not designed to handle structured geospatial layers, limiting their effectiveness for tasks requiring geospatial reasoning.

Method: Proposes a geospatial embedding mechanism that transforms diverse geospatial data into embedding patches spatially aligned with image patches. Introduces a guided attention module that dynamically integrates multimodal information by computing attention weights based on correlations with auxiliary data, directing the model toward relevant regions. The module assigns distinct roles to individual attention heads to capture complementary aspects of guidance information.

Result: The proposed framework outperforms existing pretrained geospatial foundation models in predicting disease prevalence, demonstrating its effectiveness in multimodal geospatial understanding.

Conclusion: The model successfully enhances remote sensing imagery processing by incorporating auxiliary geospatial information through novel embedding and attention mechanisms, enabling better geospatial reasoning and interpretability compared to existing approaches.

Abstract: Visual transformers have driven major progress in remote sensing image analysis, particularly in object detection and segmentation. Recent vision-language and multimodal models further extend these capabilities by incorporating auxiliary information, including captions, question and answer pairs, and metadata, which broadens applications beyond conventional computer vision tasks. However, these models are typically optimized for semantic alignment between visual and textual content rather than geospatial understanding, and therefore are not suited for representing or reasoning with structured geospatial layers. In this study, we propose a novel model that enhances remote sensing imagery processing with guidance from auxiliary geospatial information. Our approach introduces a geospatial embedding mechanism that transforms diverse geospatial data into embedding patches that are spatially aligned with image patches. To facilitate cross-modal interaction, we design a guided attention module that dynamically integrates multimodal information by computing attention weights based on correlations with auxiliary data, thereby directing the model toward the most relevant regions. In addition, the module assigns distinct roles to individual attention heads, allowing the model to capture complementary aspects of the guidance information and improving the interpretability of its predictions. Experimental results demonstrate that the proposed framework outperforms existing pretrained geospatial foundation models in predicting disease prevalence, highlighting its effectiveness in multimodal geospatial understanding.

</details>


### [9] [From Manual Observation to Automated Monitoring: Space Allowance Effects on Play Behaviour in Group-Housed Dairy Calves](https://arxiv.org/abs/2602.00111)
*Haiyu Yang,Heidi Lesscher,Enhong Liu,Miel Hostens*

Main category: cs.CV

TL;DR: Space allowance affects calf play behavior non-linearly, with optimal play at 8-10 m² per calf; automated computer vision achieves 97.6% accuracy for scalable welfare monitoring.


<details>
  <summary>Details</summary>
Motivation: Play behavior indicates positive welfare in dairy calves, but the influence of space allowance under commercial conditions (especially at 6-20 m² per calf) remains poorly characterized, limiting practical welfare guidelines.

Method: Studied 60 group-housed dairy calves across 14 Dutch farms with space range 2.66-17.98 m² per calf; used detailed ethogram analysis of video observations; developed automated computer vision pipeline trained on 108 hours of manual annotations from 6 farms; employed linear mixed models with farm as random effect.

Result: Calves spent average 1.0% of observation period playing (~10 minutes per 17-hour period); space-play relationship was non-linear with highest play at 8-10 m² per calf (1.6% OP) and lowest at 6-8 m² and 12-14 m² (<0.6% OP); space remained significant after controlling for age, health, and group size; computer vision classifier achieved 97.6% accuracy with 99.4% recall for active play detection.

Conclusion: 8-10 m² per calf represents optimal space allowance balancing welfare benefits with economic feasibility; automated computer vision monitoring enables scaling small annotation projects to continuous welfare assessment systems for commercial dairy operations.

Abstract: Play behaviour serves as a positive welfare indicator in dairy calves, yet the influence of space allowance under commercial conditions remains poorly characterized, particularly at intermediate-to-high allowances (6-20 m2 per calf). This study investigated the relationship between space allowance and play behaviour in 60 group-housed dairy calves across 14 commercial farms in the Netherlands (space range: 2.66-17.98 m2 per calf), and developed an automated computer vision pipeline for scalable monitoring. Video observations were analyzed using a detailed ethogram, with play expressed as percentage of observation period (%OP). Statistical analysis employed linear mixed models with farm as a random effect. A computer vision pipeline was trained on manual annotations from 108 hours on 6 farms and validated on held-out test data. The computer vision classifier achieved 97.6% accuracy with 99.4% recall for active play detection. Calves spent on average 1.0% of OP playing reflecting around 10 minutes per 17-hour period. The space-play relationship was non-linear, with highest play levels at 8-10 m2 per calf (1.6% OP) and lowest at 6-8 m2 and 12-14 m2 (<0.6% OP). Space remained significant after controlling for age, health, and group size. In summary, these findings suggest that 8-10 m2 per calf represents a practical target balancing welfare benefits with economic feasibility, and demonstrate that automated monitoring can scale small annotation projects to continuous welfare assessment systems.

</details>


### [10] [AI-Driven Three-Dimensional Reconstruction and Quantitative Analysis for Burn Injury Assessment](https://arxiv.org/abs/2602.00113)
*S. Kalaycioglu,C. Hong,K. Zhai,H. Xie,J. N. Wong*

Main category: cs.CV

TL;DR: AI platform combines multi-view photogrammetry, 3D reconstruction, and deep learning for objective burn assessment with quantitative metrics and longitudinal tracking.


<details>
  <summary>Details</summary>
Motivation: Current burn assessment methods (visual inspection, 2D photography) are subjective and inadequate for longitudinal comparison, creating challenges for treatment planning, healing monitoring, and medico-legal documentation.

Method: AI-enabled platform integrates multi-view photogrammetry from consumer-grade cameras, 3D surface reconstruction, and deep learning-based segmentation within structured clinical workflow to create patient-specific 3D burn surfaces.

Result: System computes objective metrics (surface area, TBSA, depth proxies, volumetric change), enables spatial alignment of successive reconstructions for healing progression tracking, and supports automated report generation with clinically plausible longitudinal trends.

Conclusion: Platform provides scalable, non-invasive approach to objective, geometry-aware burn assessment and decision support for both acute and outpatient care settings.

Abstract: Accurate, reproducible burn assessment is critical for treatment planning, healing monitoring, and medico-legal documentation, yet conventional visual inspection and 2D photography are subjective and limited for longitudinal comparison. This paper presents an AI-enabled burn assessment and management platform that integrates multi-view photogrammetry, 3D surface reconstruction, and deep learning-based segmentation within a structured clinical workflow. Using standard multi-angle images from consumer-grade cameras, the system reconstructs patient-specific 3D burn surfaces and maps burn regions onto anatomy to compute objective metrics in real-world units, including surface area, TBSA, depth-related geometric proxies, and volumetric change. Successive reconstructions are spatially aligned to quantify healing progression over time, enabling objective tracking of wound contraction and depth reduction. The platform also supports structured patient intake, guided image capture, 3D analysis and visualization, treatment recommendations, and automated report generation. Simulation-based evaluation demonstrates stable reconstructions, consistent metric computation, and clinically plausible longitudinal trends, supporting a scalable, non-invasive approach to objective, geometry-aware burn assessment and decision support in acute and outpatient care.

</details>


### [11] [1S-DAug: One-Shot Data Augmentation for Robust Few-Shot Generalization](https://arxiv.org/abs/2602.00114)
*Yunwei Bai,Ying Kiat Tan,Yao Shu,Tsuhan Chen*

Main category: cs.CV

TL;DR: 1S-DAug is a one-shot generative augmentation method that synthesizes diverse yet faithful image variants from just one example at test time, improving few-shot learning performance without model updates.


<details>
  <summary>Details</summary>
Motivation: Traditional test-time augmentations fail in few-shot learning scenarios where only a few labeled examples are available for novel classes. There's a need for effective augmentation methods that can work with minimal data at test time to improve model generalization.

Method: 1S-DAug combines geometric perturbations with controlled noise injection and a denoising diffusion process conditioned on the original image. It generates diverse yet faithful variants from just one example, encodes them, and aggregates the representations alongside the original image for more robust predictions.

Result: The method consistently improves few-shot learning across 4 standard benchmark datasets without model parameter updates, achieving over 10% proportional accuracy improvement on miniImagenet 5-way-1-shot benchmark.

Conclusion: 1S-DAug is an effective training-free, model-agnostic plugin for few-shot learning that addresses the limitations of traditional test-time augmentations by generating diverse yet faithful image variants from minimal data.

Abstract: Few-shot learning (FSL) challenges model generalization to novel classes based on just a few shots of labeled examples, a testbed where traditional test-time augmentations fail to be effective. We introduce 1S-DAug, a one-shot generative augmentation operator that synthesizes diverse yet faithful variants from just one example image at test time. 1S-DAug couples traditional geometric perturbations with controlled noise injection and a denoising diffusion process conditioned on the original image. The generated images are then encoded and aggregated, alongside the original image, into a combined representation for more robust FSL predictions. Integrated as a training-free model-agnostic plugin, 1S-DAug consistently improves FSL across standard benchmarks of 4 different datasets without any model parameter update, including achieving over 10% proportional accuracy improvement on the miniImagenet 5-way-1-shot benchmark. Codes will be released.

</details>


### [12] [Event Driven Clustering Algorithm](https://arxiv.org/abs/2602.00115)
*David El-Chai Ben-Ezra,Adar Tal,Daniel Brisk*

Main category: cs.CV

TL;DR: Novel O(n) asynchronous event-driven clustering algorithm for real-time detection of small event clusters in event camera data


<details>
  <summary>Details</summary>
Motivation: Need for efficient real-time detection of small event clusters in event camera data, leveraging the asynchronous nature of event cameras for improved performance

Method: Asynchronous, event-driven hierarchical agglomerative clustering algorithm based on tempo-spatial distance with sophisticated, efficient decision-making

Result: Achieves linear O(n) complexity independent of pixel array dimensions, enabling real-time performance

Conclusion: Proposed algorithm efficiently detects small event clusters in real-time by leveraging event camera's asynchronous data structure with linear complexity

Abstract: This paper introduces a novel asynchronous, event-driven algorithm for real-time detection of small event clusters in event camera data. Like other hierarchical agglomerative clustering algorithms, the algorithm detects the event clusters based on their tempo-spatial distance. However, the algorithm leverages the special asynchronous data structure of event camera, and by a sophisticated, efficient and simple decision-making, enjoys a linear complexity of $O(n)$ where $n$ is the events amount. In addition, the run-time of the algorithm is independent with the dimensions of the pixels array.

</details>


### [13] [IC-EO: Interpretable Code-based assistant for Earth Observation](https://arxiv.org/abs/2602.00117)
*Lamia Lahouel,Laurynas Lopata,Simon Gruening,Gabriele Meoni,Gaetan Petit,Sylvain Lobry*

Main category: cs.CV

TL;DR: A conversational code-generating agent for Earth Observation analysis that transforms natural language queries into executable Python workflows, outperforming general-purpose LLMs while providing transparent, auditable results.


<details>
  <summary>Details</summary>
Motivation: Earth Observation analysis remains difficult for non-experts, requiring technical expertise, and existing systems often provide black-box predictions that are hard to audit or reproduce.

Method: Proposes a conversational agent that uses tool LLMs to generate executable Python code from natural language queries, operating over a unified API for classification, segmentation, detection, spectral indices, and geospatial operators.

Result: The agent outperforms GPT-4o and LLaVA baselines, achieving 64.2% vs. 51.7% accuracy on land-composition mapping and 50% vs. 0% on post-wildfire damage assessment, while producing transparent, interpretable results.

Conclusion: The approach transforms Earth Observation analysis into a transparent, reproducible process by outputting verifiable code, making EO analysis more accessible while maintaining auditability.

Abstract: Despite recent advances in computer vision, Earth Observation (EO) analysis remains difficult to perform for the laymen, requiring expert knowledge and technical capabilities. Furthermore, many systems return black-box predictions that are difficult to audit or reproduce. Leveraging recent advances in tool LLMs, this study proposes a conversational, code-generating agent that transforms natural-language queries into executable, auditable Python workflows. The agent operates over a unified easily extendable API for classification, segmentation, detection (oriented bounding boxes), spectral indices, and geospatial operators. With our proposed framework, it is possible to control the results at three levels: (i) tool-level performance on public EO benchmarks; (ii) at the agent-level to understand the capacity to generate valid, hallucination-free code; and (iii) at the task-level on specific use cases. In this work, we select two use-cases of interest: land-composition mapping and post-wildfire damage assessment. The proposed agent outperforms general-purpose LLM/VLM baselines (GPT-4o, LLaVA), achieving 64.2% vs. 51.7% accuracy on land-composition and 50% vs. 0% on post-wildfire analysis, while producing results that are transparent and easy to interpret. By outputting verifiable code, the approach turns EO analysis into a transparent, reproducible process.

</details>


### [14] [VDE Bench: Evaluating The Capability of Image Editing Models to Modify Visual Documents](https://arxiv.org/abs/2602.00122)
*Hongzhu Yi,Yujia Yang,Yuanxiang Wang,Zhenyu Guan,Jiahuan Chen,Chenxi Bao,Tiankun Yang,Yixuan Yuan,Tianyu Zong,Xinming Wang,Tao Yu,Ruiwen Tao,Haijin Liang,Jin Ma,Jinwen Luo,Yeshani Xinyu Zuo,Jungang Xu*

Main category: cs.CV

TL;DR: VDE Bench is the first systematic benchmark for evaluating image editing models on multilingual and densely textual visual documents, addressing gaps in existing approaches that focus on English and sparse layouts.


<details>
  <summary>Details</summary>
Motivation: Current multimodal image editing models lack adequate evaluation for visual document image editing, especially for dense, structurally complex documents and non-Latin scripts like Chinese. Existing approaches (AnyText, GlyphControl, TextCtrl) focus on English and sparse layouts, leaving a research gap.

Method: Proposes VDE Bench: 1) A human-annotated benchmark with high-quality dataset of dense textual documents in English and Chinese (academic papers, posters, slides, exams, newspapers), 2) A decoupled evaluation framework that systematically quantifies editing performance at OCR parsing level for fine-grained assessment.

Result: Comprehensive evaluation of state-of-the-art image editing models shows strong consistency between human judgments and automated evaluation metrics. VDE Bench enables systematic assessment of multilingual and dense document editing capabilities.

Conclusion: VDE Bench is the first systematic benchmark for evaluating image editing models on multilingual and densely textual visual documents, providing a rigorous framework to address current limitations in visual document editing research.

Abstract: In recent years, multimodal image editing models have achieved substantial progress, enabling users to manipulate visual content through natural language in a flexible and interactive manner. Nevertheless, an important yet insufficiently explored research direction remains visual document image editing, which involves modifying textual content within images while faithfully preserving the original text style and background context. Existing approaches, including AnyText, GlyphControl, and TextCtrl, predominantly focus on English-language scenarios and documents with relatively sparse textual layouts, thereby failing to adequately address dense, structurally complex documents or non-Latin scripts such as Chinese. To bridge this gap, we propose \textbf{V}isual \textbf{D}oc \textbf{E}dit Bench(VDE Bench), a rigorously human-annotated and evaluated benchmark specifically designed to assess image editing models on multilingual and complex visual document editing tasks. The benchmark comprises a high-quality dataset encompassing densely textual documents in both English and Chinese, including academic papers, posters, presentation slides, examination materials, and newspapers. Furthermore, we introduce a decoupled evaluation framework that systematically quantifies editing performance at the OCR parsing level, enabling fine-grained assessment of text modification accuracy. Based on this benchmark, we conduct a comprehensive evaluation of representative state-of-the-art image editing models. Manual verification demonstrates a strong consistency between human judgments and automated evaluation metrics. VDE Bench constitutes the first systematic benchmark for evaluating image editing models on multilingual and densely textual visual documents.

</details>


### [15] [Context-Aware Autoencoders for Anomaly Detection in Maritime Surveillance](https://arxiv.org/abs/2602.00124)
*Divya Acharya,Pierre Bernab'e,Antoine Chevrot,Helge Spieker,Arnaud Gotlieb,Bruno Legeard*

Main category: cs.CV

TL;DR: Context-aware autoencoder improves anomaly detection in maritime surveillance by using vessel-specific thresholds, outperforming conventional methods for fishing status anomalies.


<details>
  <summary>Details</summary>
Motivation: Autoencoders have limitations in detecting collective and contextual anomalies in maritime surveillance, where anomalies depend on vessel-specific contexts from AIS messages.

Method: Proposed context-aware autoencoder with context-specific thresholds; compared four variants against conventional autoencoder for fishing status anomaly detection.

Result: Context-aware autoencoder outperforms others in time series anomaly detection, showing significant impact of context on reconstruction loss and detection accuracy.

Conclusion: Incorporating context-specific thresholds offers promising solution to improve accuracy in maritime vessel traffic surveillance systems by recognizing importance of context.

Abstract: The detection of anomalies is crucial to ensuring the safety and security of maritime vessel traffic surveillance. Although autoencoders are popular for anomaly detection, their effectiveness in identifying collective and contextual anomalies is limited, especially in the maritime domain, where anomalies depend on vessel-specific contexts derived from self-reported AIS messages. To address these limitations, we propose a novel solution: the context-aware autoencoder. By integrating context-specific thresholds, our method improves detection accuracy and reduces computational cost. We compare four context-aware autoencoder variants and a conventional autoencoder using a case study focused on fishing status anomalies in maritime surveillance. Results demonstrate the significant impact of context on reconstruction loss and anomaly detection. The context-aware autoencoder outperforms others in detecting anomalies in time series data. By incorporating context-specific thresholds and recognizing the importance of context in anomaly detection, our approach offers a promising solution to improve accuracy in maritime vessel traffic surveillance systems.

</details>


### [16] [D3R-Net: Dual-Domain Denoising Reconstruction Network for Robust Industrial Anomaly Detection](https://arxiv.org/abs/2602.00126)
*Dmytro Filatov,Valentyn Fedorov,Vira Filatova,Andrii Zelenchuk*

Main category: cs.CV

TL;DR: D3R-Net improves unsupervised anomaly detection by combining spatial and frequency domain reconstruction with self-supervised healing, achieving better defect segmentation while maintaining speed.


<details>
  <summary>Details</summary>
Motivation: Existing reconstruction-based UAD methods produce oversmoothed results that fail to highlight subtle defects, limiting segmentation accuracy for automated visual inspection in manufacturing.

Method: Dual-Domain Denoising Reconstruction framework with self-supervised healing task and frequency-aware regularization using FFT magnitude loss, plus optional SSIM term, built on lightweight convolutional autoencoder.

Result: On MVTec AD Hazelnut: PRO AUC improved from 0.603 to 0.687; Across 15 MVTec categories: pixel ROC AUC from 0.733 to 0.751, PRO AUC from 0.417 to 0.468, while maintaining ~20 FPS on single GPU.

Conclusion: D3R-Net provides a practical, lightweight alternative to heavy pre-trained methods by effectively combining spatial and frequency domain reconstruction for improved anomaly detection in manufacturing inspection.

Abstract: Unsupervised anomaly detection (UAD) is a key ingredient of automated visual inspection in modern manufacturing. The reconstruction-based methods appeal because they have basic architectural design and they process data quickly but they produce oversmoothed results for high-frequency details. As a result, subtle defects are partially reconstructed rather than highlighted, which limits segmentation accuracy. We build on this line of work and introduce D3R-Net, a Dual-Domain Denoising Reconstruction framework that couples a self-supervised 'healing' task with frequency-aware regularization. During training, the network receives synthetically corrupted normal images and is asked to reconstruct the clean targets, which prevents trivial identity mapping and pushes the model to learn the manifold of defect-free textures. In addition to the spatial mean squared error, we employ a Fast Fourier Transform (FFT) magnitude loss that encourages consistency in the frequency domain. The implementation also allows an optional structural similarity (SSIM) term, which we study in an ablation. On the MVTec AD Hazelnut benchmark, D3R-Net with the FFT loss improves localization consistency over a spatial-only baseline: PRO AUC increases from 0.603 to 0.687, while image-level ROC AUC remains robust. Evaluated across fifteen MVTec categories, the FFT variant raises the average pixel ROC AUC from 0.733 to 0.751 and PRO AUC from 0.417 to 0.468 compared to the MSE-only baseline, at roughly 20 FPS on a single GPU. The network is trained from scratch and uses a lightweight convolutional autoencoder backbone, providing a practical alternative to heavy pre-trained feature embedding methods.

</details>


### [17] [PovNet+: A Deep Learning Architecture for Socially Assistive Robots to Learn and Assist with Multiple Activities of Daily Living](https://arxiv.org/abs/2602.00131)
*Fraser Robinson,Souren Pashangpour,Matthew Lisondra,Goldie Nejat*

Main category: cs.CV

TL;DR: POVNet+ is a multimodal deep learning architecture for socially assistive robots that recognizes multiple activities of daily living (ADLs), distinguishes between known/unknown/atypical ADLs, and proactively initiates appropriate assistive interactions.


<details>
  <summary>Details</summary>
Motivation: Current socially assistive robots lack the ability to perceive and assist with multiple ADLs, which is a significant barrier to long-term deployment. There's a need for robots that can recognize various daily activities and proactively initiate appropriate assistance.

Method: POVNet+ introduces a novel multimodal deep learning architecture using both ADL and motion embedding spaces. It distinguishes between known ADLs, unseen ADLs, and atypically performed ADLs. The method also includes user state estimation applied to motion embeddings to recognize new ADLs while monitoring user performance.

Result: POVNet+ achieves higher ADL classification accuracy compared to state-of-the-art human activity recognition methods. Human-robot interaction experiments in a cluttered living environment with multiple users and the robot Leia demonstrate successful identification of seen/unseen ADLs and atypically performed ADLs, with appropriate assistive interactions.

Conclusion: The POVNet+ architecture enables socially assistive robots to effectively perceive multiple ADLs, distinguish between different activity scenarios, and proactively initiate appropriate assistive behaviors, addressing a key barrier to long-term deployment of autonomous assistive robots.

Abstract: A significant barrier to the long-term deployment of autonomous socially assistive robots is their inability to both perceive and assist with multiple activities of daily living (ADLs). In this paper, we present the first multimodal deep learning architecture, POVNet+, for multi-activity recognition for socially assistive robots to proactively initiate assistive behaviors. Our novel architecture introduces the use of both ADL and motion embedding spaces to uniquely distinguish between a known ADL being performed, a new unseen ADL, or a known ADL being performed atypically in order to assist people in real scenarios. Furthermore, we apply a novel user state estimation method to the motion embedding space to recognize new ADLs while monitoring user performance. This ADL perception information is used to proactively initiate robot assistive interactions. Comparison experiments with state-of-the-art human activity recognition methods show our POVNet+ method has higher ADL classification accuracy. Human-robot interaction experiments in a cluttered living environment with multiple users and the socially assistive robot Leia using POVNet+ demonstrate the ability of our multi-modal ADL architecture in successfully identifying different seen and unseen ADLs, and ADLs being performed atypically, while initiating appropriate assistive human-robot interactions.

</details>


### [18] [Shedding the Facades, Connecting the Domains: Detecting Shifting Multimodal Hate Video with Test-Time Adaptation](https://arxiv.org/abs/2602.00132)
*Jiao Li,Jian Lang,Xikai Tang,Wenzheng Shu,Ting Zhong,Qiang Gao,Yong Wang,Leiting Chen,Fan Zhou*

Main category: cs.CV

TL;DR: SCANNER is a novel Test-Time Adaptation framework for Hate Video Detection that addresses severe semantic drift by leveraging stable hate cores (gender, race, etc.) as domain bridges, using centroid-guided alignment with outlier handling and intra-cluster diversity regularization.


<details>
  <summary>Details</summary>
Motivation: Hateful content evolves into irregular forms to evade censorship, causing semantic drift that makes existing models ineffective. Conventional TTA methods target mild distribution shifts and struggle with severe semantic drift in HVD.

Method: SCANNER uses stable hate cores as domain bridges with: 1) centroid-guided alignment to reveal cores from ambiguous content, 2) sample-level adaptive centroid alignment for outlier handling, and 3) intra-cluster diversity regularization to prevent semantic collapse.

Result: SCANNER outperforms all baselines with an average gain of 4.69% in Macro-F1 over the best existing method.

Conclusion: SCANNER effectively addresses severe semantic drift in Hate Video Detection through stable core bridging, adaptive alignment, and diversity regularization, making it the first successful TTA framework for this challenging problem.

Abstract: Hate Video Detection (HVD) is crucial for online ecosystems. Existing methods assume identical distributions between training (source) and inference (target) data. However, hateful content often evolves into irregular and ambiguous forms to evade censorship, resulting in substantial semantic drift and rendering previously trained models ineffective. Test-Time Adaptation (TTA) offers a solution by adapting models during inference to narrow the cross-domain gap, while conventional TTA methods target mild distribution shifts and struggle with the severe semantic drift in HVD. To tackle these challenges, we propose SCANNER, the first TTA framework tailored for HVD. Motivated by the insight that, despite the evolving nature of hateful manifestations, their underlying cores remain largely invariant (i.e., targeting is still based on characteristics like gender, race, etc), we leverage these stable cores as a bridge to connect the source and target domains. Specifically, SCANNER initially reveals the stable cores from the ambiguous layout in evolving hateful content via a principled centroid-guided alignment mechanism. To alleviate the impact of outlier-like samples that are weakly correlated with centroids during the alignment process, SCANNER enhances the prior by incorporating a sample-level adaptive centroid alignment strategy, promoting more stable adaptation. Furthermore, to mitigate semantic collapse from overly uniform outputs within clusters, SCANNER introduces an intra-cluster diversity regularization that encourages the cluster-wise semantic richness. Experiments show that SCANNER outperforms all baselines, with an average gain of 4.69% in Macro-F1 over the best.

</details>


### [19] [LLaVA-FA: Learning Fourier Approximation for Compressing Large Multimodal Models](https://arxiv.org/abs/2602.00135)
*Pengcheng Zheng,Chaoning Zhang,Jiarong Mo,GuoHui Li,Jiaquan Zhang,Jiahao Zhang,Sihan Cao,Sheng Zheng,Caiyan Qin,Guoqing Wang,Yang Yang*

Main category: cs.CV

TL;DR: LLaVA-FA is a novel efficient large multimodal model that uses joint low-rank plus quantization approximation in the frequency domain to compress vision-language models while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Large multimodal models have high computational and memory costs that hinder practical deployment. Existing compression methods often decouple low-rank decomposition and quantization, leading to compounded reconstruction errors, especially in multimodal architectures with cross-modal redundancy.

Method: LLaVA-FA performs joint low-rank plus quantization approximation in the frequency domain using Fourier transform properties (de-correlation and conjugate symmetry). It introduces PolarQuant for complex matrix quantization and an optional diagonal calibration scheme that doesn't require large-scale calibration data.

Result: Extensive experiments show LLaVA-FA outperforms existing efficient multimodal models across multiple benchmarks while maintaining minimal activated parameters and low computational costs.

Conclusion: LLaVA-FA provides an effective solution for compressing large multimodal models, validating that joint frequency-domain approximation with specialized quantization methods can achieve both efficiency and performance.

Abstract: Large multimodal models (LMMs) have achieved impressive performance on various vision-language tasks, but their substantial computational and memory costs hinder their practical deployment. Existing compression methods often decouple low-rank decomposition and quantization, leading to compounded reconstruction errors, especially in multimodal architectures with cross-modal redundancy. To address this issue, we propose LLaVA-FA, a novel efficient LMM that performs joint low-rank plus quantization approximation in the frequency domain. By leveraging the de-correlation and conjugate symmetry properties of Fourier transform, LLaVA-FA achieves more compact and accurate weight representations. Furthermore, we introduce PolarQuant, a polar-coordinate quantization method tailored for complex matrices, and an optional diagonal calibration (ODC) scheme that eliminates the need for large-scale calibration data. Extensive experimental results demonstrate that our proposed LLaVA-FA outperforms existing efficient multimodal models across multiple benchmarks while maintaining minimal activated parameters and low computational costs, validating its effectiveness as a powerful solution for compressing LMMs.

</details>


### [20] [Scalable Analytic Classifiers with Associative Drift Compensation for Class-Incremental Learning of Vision Transformers](https://arxiv.org/abs/2602.00144)
*Xuan Rao,Mingming Ha,Bo Zhao,Derong Liu,Cesare Alippi*

Main category: cs.CV

TL;DR: LR-RGDA + HopDC: Scalable class-incremental learning for ViTs using low-rank factorized RGDA for efficient classifier reconstruction and Hopfield networks to compensate for representation drift.


<details>
  <summary>Details</summary>
Motivation: Class-incremental learning with Vision Transformers faces computational bottlenecks in classifier reconstruction using iterative SGD. While analytic RGDA offers comparable accuracy, its quadratic inference complexity limits scalability for large-scale CIL.

Method: Proposes Low-Rank Factorized RGDA (LR-RGDA) that exploits low-rank covariance structure via Woodbury identity to reduce inference complexity from O(Cd²) to O(d² + Crd²). Also introduces Hopfield-based Distribution Compensator (HopDC) to mitigate representation drift by recalibrating historical class statistics using continuous Hopfield Networks on unlabeled anchors.

Result: Extensive experiments on diverse CIL benchmarks demonstrate state-of-the-art performance, providing a scalable solution for large-scale class-incremental learning with ViTs.

Conclusion: The framework combines RGDA's expressivity with linear classifier efficiency through LR-RGDA, while HopDC addresses representation drift, offering an effective and scalable approach for ViT-based class-incremental learning.

Abstract: Class-incremental learning (CIL) with Vision Transformers (ViTs) faces a major computational bottleneck during the classifier reconstruction phase, where most existing methods rely on costly iterative stochastic gradient descent (SGD). We observe that analytic Regularized Gaussian Discriminant Analysis (RGDA) provides a Bayes-optimal alternative with accuracy comparable to SGD-based classifiers; however, its quadratic inference complexity limits its use in large-scale CIL scenarios. To overcome this, we propose Low-Rank Factorized RGDA (LR-RGDA), a scalable classifier that combines RGDA's expressivity with the efficiency of linear classifiers. By exploiting the low-rank structure of the covariance via the Woodbury matrix identity, LR-RGDA decomposes the discriminant function into a global affine term refined by a low-rank quadratic perturbation, reducing the inference complexity from $\mathcal{O}(Cd^2)$ to $\mathcal{O}(d^2 + Crd^2)$, where $C$ is the class number, $d$ the feature dimension, and $r \ll d$ the subspace rank. To mitigate representation drift caused by backbone updates, we further introduce Hopfield-based Distribution Compensator (HopDC), a training-free mechanism that uses modern continuous Hopfield Networks to recalibrate historical class statistics through associative memory dynamics on unlabeled anchors, accompanied by a theoretical bound on the estimation error. Extensive experiments on diverse CIL benchmarks demonstrate that our framework achieves state-of-the-art performance, providing a scalable solution for large-scale class-incremental learning with ViTs. Code: https://github.com/raoxuan98-hash/lr_rgda_hopdc.

</details>


### [21] [DensiThAI, A Multi-View Deep Learning Framework for Breast Density Estimation using Infrared Images](https://arxiv.org/abs/2602.00145)
*Siva Teja Kakileti,Geetha Manjunath*

Main category: cs.CV

TL;DR: DensiThAI uses multi-view deep learning on thermal images to estimate breast density non-invasively, achieving 0.73 AUROC comparable to mammography-based assessment.


<details>
  <summary>Details</summary>
Motivation: Current breast density assessment relies on ionizing X-ray mammography. The study aims to develop a non-ionizing alternative using thermal imaging, leveraging distinct thermophysical properties of breast tissues.

Method: DensiThAI - a multi-view deep learning framework that analyzes five standard thermal views of breasts, trained on a multi-center dataset of 3,500 women using mammography-derived density labels as ground truth.

Result: Achieved mean AUROC of 0.73 across 10 random splits with statistically significant separation between density classes (p << 0.05). Consistent performance across age cohorts.

Conclusion: Thermal imaging shows promise as a non-ionizing approach for breast density assessment, potentially improving patient experience and workflow optimization in clinical settings.

Abstract: Breast tissue density is a key biomarker of breast cancer risk and a major factor affecting mammographic sensitivity. However, density assessment currently relies almost exclusively on X-ray mammography, an ionizing imaging modality. This study investigates the feasibility of estimating breast density using artificial intelligence over infrared thermal images, offering a non-ionizing imaging approach. The underlying hypothesis is that fibroglandular and adipose tissues exhibit distinct thermophysical and physiological properties, leading to subtle but spatially coherent temperature variations on the breast surface. In this paper, we propose DensiThAI, a multi-view deep learning framework for breast density classification from thermal images. The framework was evaluated on a multi-center dataset of 3,500 women using mammography-derived density labels as reference. Using five standard thermal views, DensiThAI achieved a mean AUROC of 0.73 across 10 random splits, with statistically significant separation between density classes across all splits (p << 0.05). Consistent performance across age cohorts supports the potential of thermal imaging as a non-ionizing approach for breast density assessment with implications for improved patient experience and workflow optimization.

</details>


### [22] [Learning Physics-Grounded 4D Dynamics with Neural Gaussian Force Fields](https://arxiv.org/abs/2602.00148)
*Shiqian Li,Ruihong Shen,Junfeng Ni,Chang Pan,Chi Zhang,Yixin Zhu*

Main category: cs.CV

TL;DR: NGFF integrates 3D Gaussian perception with physics modeling to generate physically realistic 4D videos 100x faster than prior methods, using a new dataset GSCollision for training.


<details>
  <summary>Details</summary>
Motivation: Current video generation models lack physical plausibility, while physics-based approaches are computationally expensive and not robust for complex real-world scenarios.

Method: Neural Gaussian Force Field (NGFF) combines 3D Gaussian perception with physics-based dynamic modeling in an end-to-end neural framework for 4D video generation from multi-view RGB inputs.

Result: NGFF achieves two orders of magnitude faster generation than prior Gaussian simulators, shows strong generalization and robustness in physical reasoning on synthetic and real 3D scenarios.

Conclusion: NGFF advances video prediction towards physics-grounded world models by integrating perception with physics for interactive, physically realistic 4D video generation.

Abstract: Predicting physical dynamics from raw visual data remains a major challenge in AI. While recent video generation models have achieved impressive visual quality, they still cannot consistently generate physically plausible videos due to a lack of modeling of physical laws. Recent approaches combining 3D Gaussian splatting and physics engines can produce physically plausible videos, but are hindered by high computational costs in both reconstruction and simulation, and often lack robustness in complex real-world scenarios. To address these issues, we introduce Neural Gaussian Force Field (NGFF), an end-to-end neural framework that integrates 3D Gaussian perception with physics-based dynamic modeling to generate interactive, physically realistic 4D videos from multi-view RGB inputs, achieving two orders of magnitude faster than prior Gaussian simulators. To support training, we also present GSCollision, a 4D Gaussian dataset featuring diverse materials, multi-object interactions, and complex scenes, totaling over 640k rendered physical videos (~4 TB). Evaluations on synthetic and real 3D scenarios show NGFF's strong generalization and robustness in physical reasoning, advancing video prediction towards physics-grounded world models.

</details>


### [23] [SDCM: Simulated Densifying and Compensatory Modeling Fusion for Radar-Vision 3-D Object Detection in Internet of Vehicles](https://arxiv.org/abs/2602.00149)
*Shucong Li,Xiaoluo Zhou,Yuqian He,Zhenyu Liu*

Main category: cs.CV

TL;DR: SDCM framework improves 4-D radar-vision 3D object detection for IoV by densifying sparse radar point clouds, compensating for vision degradation, and using Mamba modeling for effective multimodal fusion.


<details>
  <summary>Details</summary>
Motivation: Two main challenges in 4-D radar-vision 3D object detection for Internet of Vehicles: 1) Sparse 4-D radar point clouds lead to poor 3D representation, and 2) Vision data suffers representation degradation under low-light, long-distance, and dense occlusion conditions, providing unreliable texture information during fusion.

Method: SDCM framework with three modules: 1) Simulated Densifying (SimDen) generates dense radar point clouds using Gaussian simulation of key points from 3-D KDE and curvature-based outline generation; 2) Radar Compensatory Mapping (RCM) reduces effects of vision data degradation by leveraging radar's all-weather capabilities; 3) Mamba Modeling Interactive Fusion (MMIF) extracts and models feature tensor differences to reduce heterogeneity and enable interactive multimodal fusion.

Result: SDCM achieves best performance on VoD, TJ4DRadSet, and Astyx HiRes 2019 datasets with lower parameter quantity and faster inference speed compared to other methods.

Conclusion: The proposed SDCM framework effectively addresses sparse radar data and vision degradation challenges in 4-D radar-vision 3D object detection for IoV, achieving superior performance with efficient computation.

Abstract: 3-D object detection based on 4-D radar-vision is an important part in Internet of Vehicles (IoV). However, there are two challenges which need to be faced. First, the 4-D radar point clouds are sparse, leading to poor 3-D representation. Second, vision datas exhibit representation degradation under low-light, long distance detection and dense occlusion scenes, which provides unreliable texture information during fusion stage. To address these issues, a framework named SDCM is proposed, which contains Simulated Densifying and Compensatory Modeling Fusion for radar-vision 3-D object detection in IoV. Firstly, considering point generation based on Gaussian simulation of key points obtained from 3-D Kernel Density Estimation (3-D KDE), and outline generation based on curvature simulation, Simulated Densifying (SimDen) module is designed to generate dense radar point clouds. Secondly, considering that radar data could provide more real time information than vision data, due to the all-weather property of 4-D radar. Radar Compensatory Mapping (RCM) module is designed to reduce the affects of vision datas' representation degradation. Thirdly, considering that feature tensor difference values contain the effective information of every modality, which could be extracted and modeled for heterogeneity reduction and modalities interaction, Mamba Modeling Interactive Fusion (MMIF) module is designed for reducing heterogeneous and achieving interactive Fusion. Experiment results on the VoD, TJ4DRadSet and Astyx HiRes 2019 dataset show that SDCM achieves best performance with lower parameter quantity and faster inference speed. Our code will be available.

</details>


### [24] [Investigating the Impact of Histopathological Foundation Models on Regressive Prediction of Homologous Recombination Deficiency](https://arxiv.org/abs/2602.00151)
*Alexander Blezinger,Wolfgang Nejdl,Ming Tang*

Main category: cs.CV

TL;DR: Foundation models pretrained on large-scale histopathology data outperform contrastive learning baselines for predicting continuous HRD scores across multiple cancer types, with proposed upsampling strategy improving performance for underrepresented patient populations.


<details>
  <summary>Details</summary>
Motivation: While foundation models have shown success in computational pathology, their impact on regressive biomarker prediction (like continuous HRD scores) remains underexplored, despite HRD being a critical biomarker for personalized cancer treatment.

Method: Used multiple instance learning frameworks to extract patch-level features from whole slide images using five state-of-the-art foundation models, compared to contrastive learning-based features. Proposed distribution-based upsampling to mitigate target imbalance, and investigated different sampling strategies and instance bag sizes through ablation studies.

Result: Foundation model features consistently outperformed baseline in predictive accuracy and generalization across breast, endometrial, and lung cancer cohorts. Different foundation models showed systematic performance differences. The upsampling strategy significantly improved recall and balanced accuracy for underrepresented patient populations.

Conclusion: Large-scale histopathological pretraining enables more precise and transferable regressive biomarker prediction, showcasing potential to advance AI-driven precision oncology through improved biomarker quantification.

Abstract: Foundation models pretrained on large-scale histopathology data have found great success in various fields of computational pathology, but their impact on regressive biomarker prediction remains underexplored. In this work, we systematically evaluate histopathological foundation models for regression-based tasks, demonstrated through the prediction of homologous recombination deficiency (HRD) score - a critical biomarker for personalized cancer treatment. Within multiple instance learning frameworks, we extract patch-level features from whole slide images (WSI) using five state-of-the-art foundation models, and evaluate their impact compared to contrastive learning-based features. Models are trained to predict continuous HRD scores based on these extracted features across breast, endometrial, and lung cancer cohorts from two public medical data collections. Extensive experiments demonstrate that models trained on foundation model features consistently outperform the baseline in terms of predictive accuracy and generalization capabilities while exhibiting systematic differences among the foundation models. Additionally, we propose a distribution-based upsampling strategy to mitigate target imbalance in these datasets, significantly improving the recall and balanced accuracy for underrepresented but clinically important patient populations. Furthermore, we investigate the impact of different sampling strategies and instance bagsizes by ablation studies. Our results highlight the benefits of large-scale histopathological pretraining for more precise and transferable regressive biomarker prediction, showcasing its potential to advance AI-driven precision oncology.

</details>


### [25] [Real-Time Human Activity Recognition on Edge Microcontrollers: Dynamic Hierarchical Inference with Multi-Spectral Sensor Fusion](https://arxiv.org/abs/2602.00152)
*Boyu Li,Kuangji Zuo,Lincong Li,Yonghui Wu*

Main category: cs.CV

TL;DR: HPPI-Net is a resource-efficient hierarchical network for on-device human activity recognition that achieves 96.70% accuracy with minimal memory usage (22.3 KiB RAM, 439.5 KiB ROM) on ARM Cortex-M4 microcontrollers.


<details>
  <summary>Details</summary>
Motivation: There's growing demand for accurate on-device pattern recognition in edge applications, but existing approaches struggle to balance accuracy with computational constraints, especially for memory-constrained edge platforms in wearable, industrial, and smart home applications.

Method: A two-layer hierarchical architecture: first layer extracts preliminary features using FFT spectrograms; second layer selectively activates either a stationary activity module or a parallel LSTM-MobileNet network (PLMN) for dynamic states. PLMN fuses FFT, Wavelet, and Gabor spectrograms through parallel LSTM encoders, then refines features using Efficient Channel Attention and Depthwise Separable Convolution for interpretability and efficiency.

Result: Achieves 96.70% accuracy with only 22.3 KiB RAM and 439.5 KiB ROM usage on ARM Cortex-M4. Compared to MobileNetV3, improves accuracy by 1.22% while reducing RAM usage by 71.2% and ROM usage by 42.1%.

Conclusion: HPPI-Net achieves a favorable accuracy-efficiency trade-off with explainable predictions, establishing a practical solution for human activity recognition on memory-constrained edge platforms in wearable, industrial, and smart home applications.

Abstract: The demand for accurate on-device pattern recognition in edge applications is intensifying, yet existing approaches struggle to reconcile accuracy with computational constraints. To address this challenge, a resource-aware hierarchical network based on multi-spectral fusion and interpretable modules, namely the Hierarchical Parallel Pseudo-image Enhancement Fusion Network (HPPI-Net), is proposed for real-time, on-device Human Activity Recognition (HAR). Deployed on an ARM Cortex-M4 microcontroller for low-power real-time inference, HPPI-Net achieves 96.70% accuracy while utilizing only 22.3 KiB of RAM and 439.5 KiB of ROM after optimization. HPPI-Net employs a two-layer architecture. The first layer extracts preliminary features using Fast Fourier Transform (FFT) spectrograms, while the second layer selectively activates either a dedicated module for stationary activity recognition or a parallel LSTM-MobileNet network (PLMN) for dynamic states. PLMN fuses FFT, Wavelet, and Gabor spectrograms through three parallel LSTM encoders and refines the concatenated features using Efficient Channel Attention (ECA) and Depthwise Separable Convolution (DSC), thereby offering channel-level interpretability while substantially reducing multiply-accumulate operations. Compared with MobileNetV3, HPPI-Net improves accuracy by 1.22% and reduces RAM usage by 71.2% and ROM usage by 42.1%. These results demonstrate that HPPI-Net achieves a favorable accuracy-efficiency trade-off and provides explainable predictions, establishing a practical solution for wearable, industrial, and smart home HAR on memory-constrained edge platforms.

</details>


### [26] [See Without Decoding: Motion-Vector-Based Tracking in Compressed Video](https://arxiv.org/abs/2602.00153)
*Axel Duché,Clément Chatelain,Gilles Gasso*

Main category: cs.CV

TL;DR: Lightweight compressed-domain tracking model uses motion vectors and transform coefficients from video streams without full RGB decoding, achieving 3.7x speed-up with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: To enable real-time analytics in large monitoring systems by reducing computational overhead through direct processing of compressed video streams without full RGB decoding.

Method: Deep model that operates directly on compressed video data using motion vectors and transform coefficients, propagating object bounding boxes across frames without requiring full video decoding.

Result: Achieves computational speed-up of up to 3.7x with only 4% mAP@0.5 drop compared to RGB baseline on MOTS15/17/20 datasets.

Conclusion: Codec-domain motion modeling is efficient for real-time analytics, demonstrating significant speed improvements with minimal accuracy trade-offs for large-scale monitoring applications.

Abstract: We propose a lightweight compressed-domain tracking model that operates directly on video streams, without requiring full RGB video decoding. Using motion vectors and transform coefficients from compressed data, our deep model propagates object bounding boxes across frames, achieving a computational speed-up of order up to 3.7 with only a slight 4% mAP@0.5 drop vs RGB baseline on MOTS15/17/20 datasets. These results highlight codec-domain motion modeling efficiency for real-time analytics in large monitoring systems.

</details>


### [27] [Deep Learning Pose Estimation for Multi-Label Recognition of Combined Hyperkinetic Movement Disorders](https://arxiv.org/abs/2602.00163)
*Laura Cif,Diane Demailly,Gabriella A. Horvàth,Juan Dario Ortigoza Escobar,Nathalie Dorison,Mayté Castro Jiménez,Cécile A. Hubsch,Thomas Wirth,Gun-Marie Hariz,Sophie Huby,Morgan Dornadic,Zohra Souei,Muhammad Mushhood Ur Rehman,Simone Hemm,Mehdi Boulayme,Eduardo M. Moraud,Jocelyne Bloch,Xavier Vasques*

Main category: cs.CV

TL;DR: Pose-based ML framework converts clinical videos into keypoint time series to objectively analyze hyperkinetic movement disorders using kinematic features


<details>
  <summary>Details</summary>
Motivation: Hyperkinetic movement disorders (dystonia, tremor, chorea, myoclonus, tics) are disabling but difficult to clinically recognize and monitor due to their fluctuating, intermittent nature and overlapping symptoms, with current methods being subjective and prone to inter-rater variability.

Method: Developed a pose-based machine-learning framework that converts standard outpatient clinical videos into anatomically meaningful keypoint time series and computes comprehensive kinematic descriptors including statistical, temporal, spectral, and higher-order irregularity-complexity features.

Result: The abstract doesn't present specific results, but the method aims to provide objective and scalable analysis of hyperkinetic movement disorders from routine clinical videos.

Conclusion: The proposed framework addresses the need for objective, scalable methods to distinguish overlapping hyperkinetic movement disorder phenotypes from standard clinical videos, potentially improving clinical recognition and longitudinal monitoring.

Abstract: Hyperkinetic movement disorders (HMDs) such as dystonia, tremor, chorea, myoclonus, and tics are disabling motor manifestations across childhood and adulthood. Their fluctuating, intermittent, and frequently co-occurring expressions hinder clinical recognition and longitudinal monitoring, which remain largely subjective and vulnerable to inter-rater variability. Objective and scalable methods to distinguish overlapping HMD phenotypes from routine clinical videos are still lacking. Here, we developed a pose-based machine-learning framework that converts standard outpatient videos into anatomically meaningful keypoint time series and computes kinematic descriptors spanning statistical, temporal, spectral, and higher-order irregularity-complexity features.

</details>


### [28] [YOLOE-26: Integrating YOLO26 with YOLOE for Real-Time Open-Vocabulary Instance Segmentation](https://arxiv.org/abs/2602.00168)
*Ranjan Sapkota,Manoj Karkee*

Main category: cs.CV

TL;DR: YOLOE-26 is a unified framework combining YOLOv26's deployment efficiency with open-vocabulary learning for real-time instance segmentation, supporting text, visual, and prompt-free inference.


<details>
  <summary>Details</summary>
Motivation: To extend YOLO's real-time efficiency beyond closed-set recognition to open-vocabulary instance segmentation, enabling flexible interaction with text descriptions, visual examples, or autonomous operation in dynamic real-world environments.

Method: Uses YOLOv26's NMS-free end-to-end design with convolutional backbone and PAN/FPN feature aggregation. Replaces fixed class logits with object embedding head for similarity matching against prompt embeddings. Incorporates RepRTA for zero-overhead text prompting, SAVPE for example-guided segmentation, and Lazy Region Prompt Contrast for prompt-free inference.

Result: Demonstrates consistent scaling behavior and favorable accuracy-efficiency trade-offs across model sizes in both prompted and prompt-free settings. Provides practical and scalable solution compatible with Ultralytics ecosystem.

Conclusion: YOLOE-26 successfully integrates deployment-optimized YOLO architecture with open-vocabulary learning, offering a unified framework for real-time instance segmentation that maintains YOLO's efficiency while enabling flexible interaction modalities for dynamic environments.

Abstract: This paper presents YOLOE-26, a unified framework that integrates the deployment-optimized YOLO26(or YOLOv26) architecture with the open-vocabulary learning paradigm of YOLOE for real-time open-vocabulary instance segmentation. Building on the NMS-free, end-to-end design of YOLOv26, the proposed approach preserves the hallmark efficiency and determinism of the YOLO family while extending its capabilities beyond closed-set recognition. YOLOE-26 employs a convolutional backbone with PAN/FPN-style multi-scale feature aggregation, followed by end-to-end regression and instance segmentation heads. A key architectural contribution is the replacement of fixed class logits with an object embedding head, which formulates classification as similarity matching against prompt embeddings derived from text descriptions, visual examples, or a built-in vocabulary. To enable efficient open-vocabulary reasoning, the framework incorporates Re-Parameterizable Region-Text Alignment (RepRTA) for zero-overhead text prompting, a Semantic-Activated Visual Prompt Encoder (SAVPE) for example-guided segmentation, and Lazy Region Prompt Contrast for prompt-free inference. All prompting modalities operate within a unified object embedding space, allowing seamless switching between text-prompted, visual-prompted, and fully autonomous segmentation. Extensive experiments demonstrate consistent scaling behavior and favorable accuracy-efficiency trade-offs across model sizes in both prompted and prompt-free settings. The training strategy leverages large-scale detection and grounding datasets with multi-task optimization and remains fully compatible with the Ultralytics ecosystem for training, validation, and deployment. Overall, YOLOE-26 provides a practical and scalable solution for real-time open-vocabulary instance segmentation in dynamic, real-world environments.

</details>


### [29] [Intra-Class Subdivision for Pixel Contrastive Learning: Application to Semi-supervised Cardiac Image Segmentation](https://arxiv.org/abs/2602.00174)
*Jiajun Zhao,Xuan Yang*

Main category: cs.CV

TL;DR: SPCL framework uses intra-class subdivision pixel contrastive learning with "unconcerned samples" to improve cardiac image segmentation by addressing boundary representation contamination.


<details>
  <summary>Details</summary>
Motivation: To address representation contamination at boundaries in cardiac image segmentation, where pixel representations at inner and boundary regions within the same class get confused, reducing segmentation quality and boundary precision.

Method: Proposes intra-class subdivision pixel contrastive learning (SPCL) with novel "unconcerned sample" concept to distinguish pixel representations at inner vs boundary regions within the same class, plus a novel boundary contrastive loss to enhance representation discrimination across boundaries.

Result: Experimental results on public cardiac datasets show SPCL significantly improves segmentation performance, outperforming existing methods in both segmentation quality and boundary precision.

Conclusion: SPCL effectively addresses boundary representation contamination in cardiac segmentation through intra-class subdivision and boundary contrastive learning, providing superior segmentation results with theoretical and empirical validation.

Abstract: We propose an intra-class subdivision pixel contrastive learning (SPCL) framework for cardiac image segmentation to address representation contamination at boundaries. The novel concept ``Unconcerned sample'' is proposed to distinguish pixel representations at the inner and boundary regions within the same class, facilitating a clearer characterization of intra-class variations. A novel boundary contrastive loss for boundary representations is proposed to enhance representation discrimination across boundaries. The advantages of the unconcerned sample and boundary contrastive loss are analyzed theoretically. Experimental results in public cardiac datasets demonstrate that SPCL significantly improves segmentation performance, outperforming existing methods with respect to segmentation quality and boundary precision. Our code is available at https://github.com/Jrstud203/SPCL.

</details>


### [30] [Stabilizing Diffusion Posterior Sampling by Noise--Frequency Continuation](https://arxiv.org/abs/2602.00176)
*Feng Tian,Yixuan Li,Weili Zeng,Weitian Zhang,Yichao Yan,Xiaokang Yang*

Main category: cs.CV

TL;DR: Proposes noise-frequency Continuation framework for diffusion posterior sampling that enforces measurement consistency only within noise-dependent frequency bands, improving detail recovery in inverse problems.


<details>
  <summary>Details</summary>
Motivation: Standard diffusion posterior sampling often fails to recover fine details because measurement-consistency guidance is weakly coupled to diffusion noise levels, causing early-step drift, spurious artifacts, and sensitivity to schedules/ill-conditioned operators.

Method: Noise-frequency Continuation framework creates intermediate posteriors with likelihood enforcing measurement consistency only within noise-dependent frequency bands. Combines diffusion predictor, band-limited likelihood guidance, and multi-resolution consistency strategy that commits reliable coarse corrections early while adopting high-frequency details only when identifiable.

Result: Achieves state-of-the-art performance across super-resolution, inpainting, and deblurring tasks, improving motion deblurring PSNR by up to 5 dB over strong baselines.

Conclusion: The proposed noise-frequency Continuation framework effectively addresses limitations of standard diffusion posterior sampling by better coupling measurement consistency with diffusion noise levels, leading to superior detail recovery and performance in inverse problems.

Abstract: Diffusion posterior sampling solves inverse problems by combining a pretrained diffusion prior with measurement-consistency guidance, but it often fails to recover fine details because measurement terms are applied in a manner that is weakly coupled to the diffusion noise level. At high noise, data-consistency gradients computed from inaccurate estimates can be geometrically incongruent with the posterior geometry, inducing early-step drift, spurious high-frequency artifacts, plus sensitivity to schedules and ill-conditioned operators. To address these concerns, we propose a noise--frequency Continuation framework that constructs a continuous family of intermediate posteriors whose likelihood enforces measurement consistency only within a noise-dependent frequency band. This principle is instantiated with a stabilized posterior sampler that combines a diffusion predictor, band-limited likelihood guidance, and a multi-resolution consistency strategy that aggressively commits reliable coarse corrections while conservatively adopting high-frequency details only when they become identifiable. Across super-resolution, inpainting, and deblurring, our method achieves state-of-the-art performance and improves motion deblurring PSNR by up to 5 dB over strong baselines.

</details>


### [31] [CamReasoner: Reinforcing Camera Movement Understanding via Structured Spatial Reasoning](https://arxiv.org/abs/2602.00181)
*Hang Wu,Yujun Cai,Zehao Li,Haonan Ge,Bowen Sun,Junsong Yuan,Yiwei Wang*

Main category: cs.CV

TL;DR: CamReasoner reformulates camera movement understanding as structured inference using Observation-Thinking-Answer paradigm with RL alignment, achieving SOTA performance by grounding motion in geometric cues rather than visual patterns.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal models treat camera dynamics as black-box classification, confusing physically distinct motions by relying on superficial visual patterns rather than geometric cues, creating a gap between perception and cinematic logic.

Method: Uses Observation-Thinking-Answer (O-T-A) paradigm to decode spatio-temporal cues like trajectories and view frustums. Constructs Large-scale Inference Trajectory Suite with 18k SFT reasoning chains and 38k RL feedback samples. First to employ RL for logical alignment in this domain.

Result: Achieves state-of-the-art performance across multiple benchmarks by effectively suppressing hallucinations and ensuring motion inferences are grounded in physical geometry rather than contextual guesswork.

Conclusion: CamReasoner successfully bridges the gap between perception and cinematic logic through structured inference, demonstrating that explicit reasoning blocks with RL alignment can significantly improve camera movement understanding in video spatial intelligence.

Abstract: Understanding camera dynamics is a fundamental pillar of video spatial intelligence. However, existing multimodal models predominantly treat this task as a black-box classification, often confusing physically distinct motions by relying on superficial visual patterns rather than geometric cues. We present CamReasoner, a framework that reformulates camera movement understanding as a structured inference process to bridge the gap between perception and cinematic logic. Our approach centers on the Observation-Thinking-Answer (O-T-A) paradigm, which compels the model to decode spatio-temporal cues such as trajectories and view frustums within an explicit reasoning block. To instill this capability, we construct a Large-scale Inference Trajectory Suite comprising 18k SFT reasoning chains and 38k RL feedback samples. Notably, we are the first to employ RL for logical alignment in this domain, ensuring motion inferences are grounded in physical geometry rather than contextual guesswork. By applying Reinforcement Learning to the Observation-Think-Answer (O-T-A) reasoning paradigm, CamReasoner effectively suppresses hallucinations and achieves state-of-the-art performance across multiple benchmarks.

</details>


### [32] [AI-Generated Image Detectors Overrely on Global Artifacts: Evidence from Inpainting Exchange](https://arxiv.org/abs/2602.00192)
*Elif Nebioglu,Emirhan Bilgiç,Adrian Popescu*

Main category: cs.CV

TL;DR: Current inpainting detectors rely on global artifacts rather than local synthesized content, making them vulnerable to attacks that restore original pixels outside edited regions.


<details>
  <summary>Details</summary>
Motivation: Modern deep learning inpainting creates realistic local manipulations, but current detectors focus on global artifacts from VAE-based reconstruction rather than the actual synthesized content, making them unreliable.

Method: Introduce Inpainting Exchange (INP-X) operation that restores original pixels outside edited regions while preserving synthesized content. Create 90K test dataset with real, inpainted, and exchanged images. Provide theoretical analysis linking detector failures to high-frequency attenuation from VAE information bottlenecks.

Result: Pretrained state-of-the-art detectors (including commercial ones) show dramatic accuracy drops (e.g., from 91% to 55%) when tested on exchanged images, often approaching chance level. Training on the INP-X dataset yields better generalization and localization than standard inpainting detection.

Conclusion: Current inpainting detectors are vulnerable because they rely on global spectral artifacts rather than local content. The INP-X approach exposes this weakness and enables more robust, content-aware detection through specialized training datasets.

Abstract: Modern deep learning-based inpainting enables realistic local image manipulation, raising critical challenges for reliable detection. However, we observe that current detectors primarily rely on global artifacts that appear as inpainting side effects, rather than on locally synthesized content. We show that this behavior occurs because VAE-based reconstruction induces a subtle but pervasive spectral shift across the entire image, including unedited regions. To isolate this effect, we introduce Inpainting Exchange (INP-X), an operation that restores original pixels outside the edited region while preserving all synthesized content. We create a 90K test dataset including real, inpainted, and exchanged images to evaluate this phenomenon. Under this intervention, pretrained state-of-the-art detectors, including commercial ones, exhibit a dramatic drop in accuracy (e.g., from 91\% to 55\%), frequently approaching chance level. We provide a theoretical analysis linking this behavior to high-frequency attenuation caused by VAE information bottlenecks. Our findings highlight the need for content-aware detection. Indeed, training on our dataset yields better generalization and localization than standard inpainting. Our dataset and code are publicly available at https://github.com/emirhanbilgic/INP-X.

</details>


### [33] [Vision-Language Model Purified Semi-Supervised Semantic Segmentation for Remote Sensing Images](https://arxiv.org/abs/2602.00202)
*Shanwen Wang,Xin Sun,Danfeng Hong,Fei Zhou*

Main category: cs.CV

TL;DR: SemiEarth introduces vision-language models to purify pseudo-labels in semi-supervised semantic segmentation for remote sensing, achieving state-of-the-art performance with improved interpretability.


<details>
  <summary>Details</summary>
Motivation: Traditional semi-supervised semantic segmentation architectures face challenges with low-quality pseudo-labels, especially in teacher-student frameworks, which is particularly problematic for remote sensing images with complex multi-class boundary regions.

Method: Proposes SemiEarth with a VLM pseudo-label purifying (VLM-PP) structure that uses vision-language models to purify teacher network's pseudo-labels. VLM-PP leverages VLMs' open-world capabilities to correct mispredicted categories in low-confidence pseudo-labels when discrepancies arise between VLM predictions and pseudo-labels.

Result: Extensive experiments on multiple remote sensing datasets demonstrate that SemiEarth achieves state-of-the-art performance. The model significantly improves pseudo-label quality, especially in multi-class boundary regions, and offers good interpretability unlike previous methods.

Conclusion: SemiEarth successfully addresses pseudo-label quality issues in semi-supervised semantic segmentation for remote sensing by integrating vision-language models, achieving superior performance while maintaining architectural independence and providing interpretability.

Abstract: The semi-supervised semantic segmentation (S4) can learn rich visual knowledge from low-cost unlabeled images. However, traditional S4 architectures all face the challenge of low-quality pseudo-labels, especially for the teacher-student framework.We propose a novel SemiEarth model that introduces vision-language models (VLMs) to address the S4 issues for the remote sensing (RS) domain. Specifically, we invent a VLM pseudo-label purifying (VLM-PP) structure to purify the teacher network's pseudo-labels, achieving substantial improvements. Especially in multi-class boundary regions of RS images, the VLM-PP module can significantly improve the quality of pseudo-labels generated by the teacher, thereby correctly guiding the student model's learning. Moreover, since VLM-PP equips VLMs with open-world capabilities and is independent of the S4 architecture, it can correct mispredicted categories in low-confidence pseudo-labels whenever a discrepancy arises between its prediction and the pseudo-label. We conducted extensive experiments on multiple RS datasets, which demonstrate that our SemiEarth achieves SOTA performance. More importantly, unlike previous SOTA RS S4 methods, our model not only achieves excellent performance but also offers good interpretability. The code is released at https://github.com/wangshanwen001/SemiEarth.

</details>


### [34] [Interpretable Unsupervised Deformable Image Registration via Confidence-bound Multi-Hop Visual Reasoning](https://arxiv.org/abs/2602.00211)
*Zafar Iqbal,Anwar Ul Haq,Srimannarayana Grandhi*

Main category: cs.CV

TL;DR: VCoR is a multi-hop visual reasoning framework for unsupervised medical image registration that provides interpretability through progressive refinement and uncertainty estimation.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning registration methods lack transparency and interpretability, leading to error drift and reduced clinical trust, despite achieving good accuracy.

Method: Multi-hop Visual Chain of Reasoning framework with Localized Spatial Refinement modules and Cross-Reference Attention mechanisms that iteratively refine registration through progressive reasoning steps.

Result: Achieves competitive registration accuracy on DIR-Lab 4D CT (lung) and IXI T1-weighted MRI (brain) datasets while providing interpretable intermediate predictions and confidence measures.

Conclusion: VCoR presents an interpretable, reliable, and clinically viable unsupervised medical image registration framework with built-in transparency and uncertainty estimation.

Abstract: Unsupervised deformable image registration requires aligning complex anatomical structures without reference labels, making interpretability and reliability critical. Existing deep learning methods achieve considerable accuracy but often lack transparency, leading to error drift and reduced clinical trust. We propose a novel Multi-Hop Visual Chain of Reasoning (VCoR) framework that reformulates registration as a progressive reasoning process. Inspired by the iterative nature of clinical decision-making, each visual reasoning hop integrates a Localized Spatial Refinement (LSR) module to enrich feature representations and a Cross-Reference Attention (CRA) mechanism that leads the iterative refinement process, preserving anatomical consistency. This multi-hop strategy enables robust handling of large deformations and produces a transparent sequence of intermediate predictions with a theoretical bound. Beyond accuracy, our framework offers built-in interpretability by estimating uncertainty via the stability and convergence of deformation fields across hops. Extensive evaluations on two challenging public datasets, DIR-Lab 4D CT (lung) and IXI T1-weighted MRI (brain), demonstrate that VCoR achieves competitive registration accuracy while offering rich intermediate visualizations and confidence measures. By embedding an implicit visual reasoning paradigm, we present an interpretable, reliable, and clinically viable unsupervised medical image registration.

</details>


### [35] [Deep Learning Based CNN Model for Automated Detection of Pneumonia from Chest XRay Images](https://arxiv.org/abs/2602.00212)
*Sathish Krishna Anumula,Vetrivelan Tamilmani,Aniruddha Arjun Singh,Dinesh Rajendran,Venkata Deepak Namburi*

Main category: cs.CV

TL;DR: A custom CNN model for pneumonia detection in chest X-rays using depthwise separable convolutions optimized for grayscale medical images, with CLAHE and geometric augmentation preprocessing.


<details>
  <summary>Details</summary>
Motivation: Pneumonia causes high morbidity/mortality, especially in pediatric/elderly populations in resource-limited areas. Traditional manual X-ray interpretation suffers from inter-observer variation, expert fatigue, and radiologist shortages, requiring automated solutions.

Method: Custom CNN architecture with depthwise separable convolutional design optimized for grayscale medical images. Uses CLAHE (Contrast Limited Adaptive Histogram Equalization) and geometric augmentation preprocessing to address class imbalance and improve generalization.

Result: Tested on dataset of 5,863 anterior-posterior chest X-rays, achieving high precision with minimal computational expense compared to generic transfer learning models with redundant parameters.

Conclusion: The proposed unified automated diagnostic model provides fast, precise pneumonia detection in chest X-rays, addressing limitations of traditional manual interpretation methods while being computationally efficient.

Abstract: Pneumonia has been one of the major causes of morbidities and mortality in the world and the prevalence of this disease is disproportionately high among the pediatric and elderly populations especially in resources trained areas Fast and precise diagnosis is a prerequisite for successful clinical intervention but due to inter observer variation fatigue among experts and a shortage of qualified radiologists traditional approaches that rely on manual interpretation of chest radiographs are frequently constrained To address these problems this paper introduces a unified automated diagnostic model using a custom Convolutional Neural Network CNN that can recognize pneumonia in chest Xray images with high precision and at minimal computational expense In contrast like other generic transfer learning based models which often possess redundant parameters the offered architecture uses a tailor made depth wise separable convolutional design which is optimized towards textural characteristics of grayscale medical images Contrast Limited Adaptive Histogram Equalization CLAHE and geometric augmentation are two significant preprocessing techniques used to ensure that the system does not experience class imbalance and is more likely to generalize The system is tested using a dataset of 5863 anterior posterior chest Xrays.

</details>


### [36] [A Geometric Multimodal Foundation Model Integrating Bp-MRI and Clinical Reports in Prostate Cancer Classification](https://arxiv.org/abs/2602.00214)
*Juan A. Olmos,Antoine Manzanera,Fabio Martínez*

Main category: cs.CV

TL;DR: MFM-Geom is a geometric multimodal foundation model that combines bi-parametric MRI and clinical reports using Riemannian deep learning for prostate cancer diagnosis, achieving strong performance with limited training data.


<details>
  <summary>Details</summary>
Motivation: Prostate cancer diagnosis using bi-parametric MRI and clinical variables is subjective and existing computer-aided methods focus only on imaging, ignoring clinical context and suffering from data scarcity, limiting robust representation learning.

Method: Proposed MFM-Geom, a geometric multimodal foundation model that learns representations from both bp-MRI and clinical reports. Uses symmetric positive definite matrices and Riemannian deep learning to integrate imaging-text representations from a biomedical multimodal foundation model.

Result: Using only 10% of training data, MFM-Geom outperformed baseline class token embedding-based classification by +8.3%, achieving AUC-PR of 90.67%. Generalization on external dataset confirmed robustness with AUC-PR of 90.6.

Conclusion: The geometric multimodal foundation model effectively integrates imaging and clinical data using Riemannian deep learning, demonstrating strong performance with limited training data and good generalization to external datasets for prostate cancer diagnosis.

Abstract: Prostate cancer (PCa) is one of the most common cancers in men worldwide. Bi-parametric MRI (bp-MRI) and clinical variables are crucial for PCa identification and improving treatment decisions. However, this process is subjective to expert interpretations. Furthermore, most existing computer-aided diagnosis methods focus on imaging-based models, overlooking the clinical context and suffering from data scarcity, limiting their ability to learn robust representations. We propose a geometric multimodal Foundation Model (FM), named MFM-Geom, that learns representations from bp-MRI and clinical reports, encoding visual findings and information from the context of clinical variables. In the representations classification head, the approach leverages symmetric positive definite (SPD) matrices and Riemannian deep learning to integrate imaging-text representations from a biomedical multimodal FM. Using 10% of the training data, MFM-Geom outperformed baseline class token embedding-based classification (+8.3%, AUC-PR of 90.67). Generalization on external dataset confirmed the robustness of fine-tuning biomedical FM, achieving an AUC-PR of 90.6.

</details>


### [37] [Development of a Cacao Disease Identification and Management App Using Deep Learning](https://arxiv.org/abs/2602.00216)
*Zaldy Pagaduan,Jason Occidental,Nathaniel Duro,Dexielito Badilles,Eleonor Palconit*

Main category: cs.CV

TL;DR: Mobile app with offline deep learning model helps Philippine cacao farmers identify diseases, achieving 96.93% accuracy for disease identification and 84.2% agreement with expert assessments.


<details>
  <summary>Details</summary>
Motivation: Smallholder cacao producers in the Philippines face challenges from pests/diseases, outdated farming techniques, and limited access to data/information compared to larger plantations with more resources.

Method: Developed a mobile application with offline functionality for remote areas, featuring a deep learning model trained to identify cacao diseases and detect infection levels.

Result: Disease identification model achieved 96.93% validation accuracy; black pod infection level detection achieved 79.49% accuracy; field testing showed 84.2% agreement with expert assessments.

Conclusion: The mobile app empowers smallholder farmers by providing accessible, technology-enabled tools to improve cacao crop health and productivity in remote areas.

Abstract: Smallholder cacao producers often rely on outdated farming techniques and face significant challenges from pests and diseases, unlike larger plantations with more resources and expertise. In the Philippines, cacao farmers have limited access to data, information, and good agricultural practices. This study addresses these issues by developing a mobile application for cacao disease identification and management that functions offline, enabling use in remote areas where farms are mostly located. The core of the system is a deep learning model trained to identify cacao diseases accurately. The trained model is integrated into the mobile app to support farmers in field diagnosis. The disease identification model achieved a validation accuracy of 96.93% while the model for detecting cacao black pod infection levels achieved 79.49% validation accuracy. Field testing of the application showed an agreement rate of 84.2% compared with expert cacao technician assessments. This approach empowers smallholder farmers by providing accessible, technology-enabled tools to improve cacao crop health and productivity.

</details>


### [38] [CAPA: Contribution-Aware Pruning and FFN Approximation for Efficient Large Vision-Language Models](https://arxiv.org/abs/2602.00247)
*Samyak Jha,Junho Kim*

Main category: cs.CV

TL;DR: CAPA improves efficiency in Large Vision-Language Models by pruning unimportant visual tokens using attention contribution scores and approximating redundant FFN computations with linear approximations.


<details>
  <summary>Details</summary>
Motivation: Current VLMs suffer from high computational costs due to processing thousands of visual tokens, but existing token importance measures (attention scores) are imperfect proxies for actual contribution, leading to inefficient inference.

Method: Introduces CAPA framework with two strategies: 1) prunes visual tokens using attention contribution (attention probabilities weighted by value vector magnitude) at critical functional transitions, and 2) reduces FFN computation through efficient linear approximations, especially for intermediate layers where image tokens exhibit linear behavior.

Result: CAPA achieves competent efficiency-performance trade-offs with improved robustness across various benchmarks and baselines, demonstrating that visual attention sinks consist of both prunable Probability Dumps and essential Structural Anchors.

Conclusion: Attention contribution provides a more accurate criterion for visual token selection than attention scores alone, and CAPA's dual-strategy approach effectively reduces computational costs while maintaining model performance through targeted pruning and FFN approximation.

Abstract: Efficient inference in Large Vision-Language Models is constrained by the high cost of processing thousands of visual tokens, yet it remains unclear which tokens and computations can be safely removed. While attention scores are commonly used to estimate visual token importance, they are an imperfect proxy for actual contribution. We show that Attention Contribution, which weights attention probabilities by value vector magnitude, provides a more accurate criterion for visual token selection. Our empirical analysis reveals that visual attention sinks are functionally heterogeneous, comprising Probability Dumps with low contribution that can be safely pruned, and Structural Anchors with high contribution essential for maintaining model performance. Further, we identify substantial redundancy in Feed-Forward Networks (FFNs) associated with visual tokens, particularly in intermediate layers where image tokens exhibit linear behavior. Based on our findings, we introduce CAPA (Contribution-Aware Pruning and FFN Approximation), a dual-strategy framework that prunes visual tokens using attention contribution at critical functional transitions and reduces FFN computation through efficient linear approximations. Experiments on various benchmarks across baselines show that CAPA achieves competent efficiency--performance trade-offs with improved robustness.

</details>


### [39] [SANEval: Open-Vocabulary Compositional Benchmarks with Failure-mode Diagnosis](https://arxiv.org/abs/2602.00249)
*Rishav Pramanik,Ian E. Nielsen,Jeff Smith,Saurav Pandit,Ravi P. Ramachandran,Zhaozheng Yin*

Main category: cs.CV

TL;DR: SANEval is a new benchmark for evaluating text-to-image models' compositional abilities, using LLMs for prompt understanding and open-vocabulary object detection to assess attribute binding, spatial relations, and numeracy.


<details>
  <summary>Details</summary>
Motivation: Current T2I models struggle with complex prompts involving multiple objects, attributes, and spatial relationships, and existing evaluation benchmarks are limited by closed vocabularies, lack fine-grained diagnostics, and don't provide interpretable feedback for diagnosing specific failures.

Method: SANEval establishes a scalable pipeline combining: 1) LLM for deep prompt understanding, and 2) LLM-enhanced open-vocabulary object detector to evaluate compositional adherence without vocabulary constraints.

Result: Experiments on six state-of-the-art T2I models show SANEval's automated evaluations provide a more faithful proxy for human assessment, with statistically different Spearman's rank correlation results than existing benchmarks across attribute binding, spatial relations, and numeracy tasks.

Conclusion: SANEval addresses critical limitations in T2I evaluation, offering a comprehensive, open-vocabulary benchmark with better human alignment, and will release dataset and open-source pipeline to facilitate future research in compositional T2I generation and evaluation.

Abstract: The rapid progress of text-to-image (T2I) models has unlocked unprecedented creative potential, yet their ability to faithfully render complex prompts involving multiple objects, attributes, and spatial relationships remains a significant bottleneck. Progress is hampered by a lack of adequate evaluation methods; current benchmarks are often restricted to closed-set vocabularies, lack fine-grained diagnostic capabilities, and fail to provide the interpretable feedback necessary to diagnose and remedy specific compositional failures. We solve these challenges by introducing SANEval (Spatial, Attribute, and Numeracy Evaluation), a comprehensive benchmark that establishes a scalable new pipeline for open-vocabulary compositional evaluation. SANEval combines a large language model (LLM) for deep prompt understanding with an LLM-enhanced, open-vocabulary object detector to robustly evaluate compositional adherence, unconstrained by a fixed vocabulary. Through extensive experiments on six state-of-the-art T2I models, we demonstrate that SANEval's automated evaluations provide a more faithful proxy for human assessment; our metric achieves a Spearman's rank correlation with statistically different results than those of existing benchmarks across tasks of attribute binding, spatial relations, and numeracy. To facilitate future research in compositional T2I generation and evaluation, we will release the SANEval dataset and our open-source evaluation pipeline.

</details>


### [40] [Subspace Clustering on Incomplete Data with Self-Supervised Contrastive Learning](https://arxiv.org/abs/2602.00262)
*Huanran Li,Daniel Pimentel-Alarcón*

Main category: cs.CV

TL;DR: Contrastive Subspace Clustering (CSC) uses self-supervised contrastive learning with masked views to cluster incomplete data, outperforming existing methods on six benchmarks.


<details>
  <summary>Details</summary>
Motivation: Most subspace clustering methods assume fully observed data, which limits their effectiveness in real-world scenarios where data often has missing entries. There's a need for robust clustering methods that can handle incomplete data.

Method: CSC generates masked views of partially observed inputs and trains a deep neural network using SimCLR-style contrastive loss to learn invariant embeddings, which are then clustered using sparse subspace clustering.

Result: Experiments on six benchmark datasets show CSC consistently outperforms both classical and deep learning baselines, demonstrating strong robustness to missing data and scalability to large datasets.

Conclusion: CSC provides an effective contrastive self-supervised framework for subspace clustering of incomplete data, addressing a key limitation of existing methods and showing practical advantages in real-world applications.

Abstract: Subspace clustering aims to group data points that lie in a union of low-dimensional subspaces and finds wide application in computer vision, hyperspectral imaging, and recommendation systems. However, most existing methods assume fully observed data, limiting their effectiveness in real-world scenarios with missing entries. In this paper, we propose a contrastive self-supervised framework, Contrastive Subspace Clustering (CSC), designed for clustering incomplete data. CSC generates masked views of partially observed inputs and trains a deep neural network using a SimCLR-style contrastive loss to learn invariant embeddings. These embeddings are then clustered using sparse subspace clustering. Experiments on six benchmark datasets show that CSC consistently outperforms both classical and deep learning baselines, demonstrating strong robustness to missing data and scalability to large datasets.

</details>


### [41] [World-Shaper: A Unified Framework for 360° Panoramic Editing](https://arxiv.org/abs/2602.00265)
*Dong Liang,Yuhao Liu,Jinyuan Jia,Youjun Zhao,Rynson W. H. Lau*

Main category: cs.CV

TL;DR: World-Shaper: A geometry-aware framework for panoramic image editing that operates directly in equirectangular projection to maintain global consistency, using a generate-then-edit paradigm with geometry-aware learning.


<details>
  <summary>Details</summary>
Motivation: Existing perspective-based editing methods fail to model panoramic spatial structure, and cube-map decompositions break global consistency due to mismatch with spherical geometry. There's a need for unified panoramic editing that maintains geometric consistency.

Method: Reformulates panoramic editing directly in ERP domain with generate-then-edit paradigm: controllable panoramic generation creates paired examples for supervised editing learning. Uses geometry-aware learning with explicit position-aware shape supervision and implicit panoramic priors through progressive training.

Result: Achieves superior geometric consistency, editing fidelity, and text controllability compared to SOTA methods on new benchmark PEBench, enabling coherent and flexible 360° visual world creation with unified editing control.

Conclusion: World-Shaper bridges panoramic generation and editing within a single editing-centric design, overcoming geometric distortion and data scarcity challenges to enable realistic 360° visual experiences with consistent geometry.

Abstract: Being able to edit panoramic images is crucial for creating realistic 360° visual experiences. However, existing perspective-based image editing methods fail to model the spatial structure of panoramas. Conventional cube-map decompositions attempt to overcome this problem but inevitably break global consistency due to their mismatch with spherical geometry. Motivated by this insight, we reformulate panoramic editing directly in the equirectangular projection (ERP) domain and present World-Shaper, a unified geometry-aware framework that bridges panoramic generation and editing within a single editing-centric design. To overcome the scarcity of paired data, we adopt a generate-then-edit paradigm, where controllable panoramic generation serves as an auxiliary stage to synthesize diverse paired examples for supervised editing learning. To address geometric distortion, we introduce a geometry-aware learning strategy that explicitly enforces position-aware shape supervision and implicitly internalizes panoramic priors through progressive training. Extensive experiments on our new benchmark, PEBench, demonstrate that our method achieves superior geometric consistency, editing fidelity, and text controllability compared to SOTA methods, enabling coherent and flexible 360° visual world creation with unified editing control. Code, model, and data will be released at our project page: https://world-shaper-project.github.io/

</details>


### [42] [PLACID: Identity-Preserving Multi-Object Compositing via Video Diffusion with Synthetic Trajectories](https://arxiv.org/abs/2602.00267)
*Gemma Canet Tarrés,Manel Baradad,Francesc Moreno-Noguer,Yumeng Li*

Main category: cs.CV

TL;DR: PLACID is a framework that uses pretrained image-to-video diffusion models to create studio-quality multi-object composites from collections of object images, preserving object identities and backgrounds while enabling layout control.


<details>
  <summary>Details</summary>
Motivation: Current generative AI models struggle with studio-level multi-object compositing, often altering object details, omitting/duplicating objects, and producing inconsistent layouts. There's a need for a solution that preserves object identity, background fidelity, layout control, and creates appealing displays.

Method: PLACID leverages pretrained image-to-video diffusion models with text control to exploit temporal priors for object consistency. It uses a novel data curation strategy generating synthetic sequences where randomly placed objects move to target positions, aligning with video model priors during training.

Result: Extensive evaluations show PLACID surpasses state-of-the-art methods in multi-object compositing, achieving superior identity, background, and color preservation with fewer omitted objects and more visually appealing results.

Conclusion: PLACID effectively bridges the gap in studio-level multi-object compositing by transforming object collections into appealing composites while preserving critical details through innovative use of video diffusion models and synthetic training data.

Abstract: Recent advances in generative AI have dramatically improved photorealistic image synthesis, yet they fall short for studio-level multi-object compositing. This task demands simultaneous (i) near-perfect preservation of each item's identity, (ii) precise background and color fidelity, (iii) layout and design elements control, and (iv) complete, appealing displays showcasing all objects. However, current state-of-the-art models often alter object details, omit or duplicate objects, and produce layouts with incorrect relative sizing or inconsistent item presentations. To bridge this gap, we introduce PLACID, a framework that transforms a collection of object images into an appealing multi-object composite. Our approach makes two main contributions. First, we leverage a pretrained image-to-video (I2V) diffusion model with text control to preserve objects consistency, identities, and background details by exploiting temporal priors from videos. Second, we propose a novel data curation strategy that generates synthetic sequences where randomly placed objects smoothly move to their target positions. This synthetic data aligns with the video model's temporal priors during training. At inference, objects initialized at random positions consistently converge into coherent layouts guided by text, with the final frame serving as the composite image. Extensive quantitative evaluations and user studies demonstrate that PLACID surpasses state-of-the-art methods in multi-object compositing, achieving superior identity, background, and color preservation, with less omitted objects and visually appealing results.

</details>


### [43] [TokenTrim: Inference-Time Token Pruning for Autoregressive Long Video Generation](https://arxiv.org/abs/2602.00268)
*Ariel Shaulov,Eitan Shaar,Amit Edenzon,Lior Wolf*

Main category: cs.CV

TL;DR: Proposes an inference-time method to mitigate temporal drift in auto-regressive video generation by identifying and removing unstable latent tokens before they're reused for conditioning.


<details>
  <summary>Details</summary>
Motivation: Auto-regressive video generation suffers from severe temporal drift where errors accumulate over long horizons, primarily due to inference-time error propagation from corrupted latent conditioning tokens being reused.

Method: Identifies unstable latent tokens (those deviating significantly from previous batch representations) and removes them from the auto-regressive context before they're reused for conditioning, preventing unreliable information from influencing future generation.

Result: Significantly improves long-horizon temporal consistency without modifying model architecture, training procedure, or leaving latent space.

Conclusion: Temporal drift in auto-regressive video generation can be effectively mitigated through inference-time correction by removing corrupted latent tokens, offering a simple yet effective solution to error accumulation problems.

Abstract: Auto-regressive video generation enables long video synthesis by iteratively conditioning each new batch of frames on previously generated content. However, recent work has shown that such pipelines suffer from severe temporal drift, where errors accumulate and amplify over long horizons. We hypothesize that this drift does not primarily stem from insufficient model capacity, but rather from inference-time error propagation. Specifically, we contend that drift arises from the uncontrolled reuse of corrupted latent conditioning tokens during auto-regressive inference. To correct this accumulation of errors, we propose a simple, inference-time method that mitigates temporal drift by identifying and removing unstable latent tokens before they are reused for conditioning. For this purpose, we define unstable tokens as latent tokens whose representations deviate significantly from those of the previously generated batch, indicating potential corruption or semantic drift. By explicitly removing corrupted latent tokens from the auto-regressive context, rather than modifying entire spatial regions or model parameters, our method prevents unreliable latent information from influencing future generation steps. As a result, it significantly improves long-horizon temporal consistency without modifying the model architecture, training procedure, or leaving latent space.

</details>


### [44] [TimeBlind: A Spatio-Temporal Compositionality Benchmark for Video LLMs](https://arxiv.org/abs/2602.00288)
*Baiqi Li,Kangyi Zhao,Ce Zhang,Chancharik Mitra,Jean de Dieu Nyandwi,Gedas Bertasius*

Main category: cs.CV

TL;DR: TimeBlind is a diagnostic benchmark that reveals MLLMs' poor compositional spatio-temporal understanding despite strong static visual capabilities, showing they rely on visual shortcuts rather than genuine temporal reasoning.


<details>
  <summary>Details</summary>
Motivation: Multimodal Large Language Models excel at static semantics but have brittle grasp of temporal dynamics, creating a need for diagnostic tools to assess genuine spatio-temporal understanding in video reasoning.

Method: TimeBlind uses a minimal-pairs paradigm where video pairs share identical static content but differ in temporal structure, with complementary questions to neutralize language priors. It categorizes temporal understanding into three cognitive levels and evaluates 20+ SOTA MLLMs on 600 instances (2400 video-question pairs).

Result: Best MLLM achieved only 48.2% Instance Accuracy (correctly distinguishing both videos in a pair), far below human performance of 98.2%, revealing models rely on static visual shortcuts rather than genuine temporal logic.

Conclusion: TimeBlind serves as a vital diagnostic tool exposing fundamental limitations in MLLMs' temporal reasoning capabilities, highlighting the need for improved video understanding models that go beyond static visual shortcuts.

Abstract: Fine-grained spatio-temporal understanding is essential for video reasoning and embodied AI. Yet, while Multimodal Large Language Models (MLLMs) master static semantics, their grasp of temporal dynamics remains brittle. We present TimeBlind, a diagnostic benchmark for compositional spatio-temporal understanding. Inspired by cognitive science, TimeBlind categorizes fine-grained temporal understanding into three levels: recognizing atomic events, characterizing event properties, and reasoning about event interdependencies. Unlike benchmarks that conflate recognition with temporal reasoning, TimeBlind leverages a minimal-pairs paradigm: video pairs share identical static visual content but differ solely in temporal structure, utilizing complementary questions to neutralize language priors. Evaluating over 20 state-of-the-art MLLMs (e.g., GPT-5, Gemini 3 Pro) on 600 curated instances (2400 video-question pairs), reveals that the Instance Accuracy (correctly distinguishing both videos in a pair) of the best performing MLLM is only 48.2%, far below the human performance (98.2%). These results demonstrate that even frontier models rely heavily on static visual shortcuts rather than genuine temporal logic, positioning TimeBlind as a vital diagnostic tool for next-generation video understanding. Dataset and code are available at https://baiqi-li.github.io/timeblind_project/ .

</details>


### [45] [Computer Vision and Its Relationship to Cognitive Science: A perspective from Bayes Decision Theory](https://arxiv.org/abs/2602.00289)
*Alan Yuille,Daniel Kersten*

Main category: cs.CV

TL;DR: This paper introduces computer vision through the lens of Bayes Decision Theory, comparing Bayesian and Deep Neural Network approaches and suggesting their integration.


<details>
  <summary>Details</summary>
Motivation: To provide a theoretical framework for understanding computer vision's relationship to Cognitive Science, capturing key concepts through Bayes Decision Theory while addressing the field's complexity and narrow scope.

Method: Uses Bayes Decision Theory as a unifying framework to analyze and compare two major approaches: Bayesian methods (conceptually attractive, resonates with Cognitive Science) and Deep Neural Networks (inspired by visual ventral stream hierarchy, practically successful).

Result: BDT successfully relates and captures the strengths and weaknesses of both Bayesian and DNN approaches, providing a theoretical lens that connects computer vision to Cognitive Science principles.

Conclusion: By discussing BDT's limitations, the paper points toward combining Bayesian and DNN approaches in a richer framework for future computer vision research.

Abstract: This document presents an introduction to computer vision, and its relationship to Cognitive Science, from the perspective of Bayes Decision Theory (Berger 1985). Computer vision is a vast and complex field, so this overview has a narrow scope and provides a theoretical lens which captures many key concepts. BDT is rich enough to include two different approaches: (i) the Bayesian viewpoint, which gives a conceptually attractive framework for vision with concepts that resonate with Cognitive Science (Griffiths et al., 2024), and (ii) the Deep Neural Network approach whose successes in the real world have made Computer Vision into a trillion-dollar industry and which is motivated by the hierarchical structure of the visual ventral stream. The BDT framework relates and captures the strengths and weakness of these two approaches and, by discussing the limitations of BDT, points the way to how they can be combined in a richer framework.

</details>


### [46] [LogicGaze: Benchmarking Causal Consistency in Visual Narratives via Counterfactual Verification](https://arxiv.org/abs/2602.00292)
*Rory Driscoll,Alexandros Christoforos,Chadbourne Davis*

Main category: cs.CV

TL;DR: LogicGaze is a benchmark framework that tests whether vision-language models can validate sequential causal reasoning chains against visual evidence, exposing hallucination vulnerabilities in current VLMs.


<details>
  <summary>Details</summary>
Motivation: While sequential reasoning enhances VLMs' ability to handle complex multimodal tasks, there's insufficient exploration of whether these reasoning chains are properly grounded in actual visual evidence, particularly regarding hallucination issues.

Method: LogicGaze integrates causal sequences with visually contradictory but linguistically plausible perturbations, curated from 40,000 video segments from ShareGPT4Video and Flickr30k imagery. It uses a tripartite evaluation protocol: Causal Validation, Grounded Narrative Synthesis, and Perturbation Rejection.

Result: The benchmark exposes significant vulnerabilities in state-of-the-art VLMs like Qwen2.5-VL-72B, revealing their inability to properly validate reasoning chains against visual evidence.

Conclusion: LogicGaze advocates for robust, trustworthy multimodal reasoning and provides publicly available resources to advance research in this area, highlighting the need for VLMs that can reliably ground sequential reasoning in visual evidence.

Abstract: While sequential reasoning enhances the capability of Vision-Language Models (VLMs) to execute complex multimodal tasks, their reliability in grounding these reasoning chains within actual visual evidence remains insufficiently explored. We introduce LogicGaze, a novel benchmark framework designed to rigorously interrogate whether VLMs can validate sequential causal chains against visual inputs, specifically targeting the pervasive issue of hallucination. Curated from 40,000 video segments from ShareGPT4Video and a subset of Flickr30k imagery, LogicGaze integrates causal sequences with visually contradictory yet linguistically plausible perturbations, compelling models to verify the authenticity of each reasoning step. Our tripartite evaluation protocol - Causal Validation, Grounded Narrative Synthesis, and Perturbation Rejection - exposes significant vulnerabilities in state-of-the-art VLMs such as Qwen2.5-VL-72B. LogicGaze advocates for robust, trustworthy multimodal reasoning, with all resources publicly available in an anonymized repository.

</details>


### [47] [Opportunistic Promptable Segmentation: Leveraging Routine Radiological Annotations to Guide 3D CT Lesion Segmentation](https://arxiv.org/abs/2602.00309)
*Samuel Church,Joshua D. Warner,Danyal Maqbool,Xin Tie,Junjie Hu,Meghan G. Lubner,Tyler J. Bradshaw*

Main category: cs.CV

TL;DR: SAM2CT: A promptable segmentation model that converts radiologists' sparse annotations (arrows/lines) in PACS into 3D CT segmentations, enabling large-scale dataset creation from existing clinical data.


<details>
  <summary>Details</summary>
Motivation: 3D CT segmentation datasets are costly to create due to extensive manual annotation requirements, while abundant sparse annotations (arrows, lines) exist in clinical PACS systems but remain underutilized for training ML models.

Method: SAM2CT extends SAM2 with: 1) prompt encoder supporting arrow and line inputs, and 2) Memory-Conditioned Memories (MCM) for 3D medical volume encoding. It converts sparse GSPS annotations into 3D segmentations via opportunistic promptable segmentation.

Result: Outperforms existing promptable segmentation models with Dice scores of 0.649 (arrow prompts) and 0.757 (line prompts). On clinical PACS data (N=60), produces clinically acceptable segmentations in 87% of cases. Shows strong zero-shot performance on Emergency Department findings.

Conclusion: Large-scale mining of historical GSPS annotations via SAM2CT represents a promising, scalable approach for generating 3D CT segmentation datasets, potentially revolutionizing medical imaging data collection.

Abstract: The development of machine learning models for CT imaging depends on the availability of large, high-quality, and diverse annotated datasets. Although large volumes of CT images and reports are readily available in clinical picture archiving and communication systems (PACS), 3D segmentations of critical findings are costly to obtain, typically requiring extensive manual annotation by radiologists. On the other hand, it is common for radiologists to provide limited annotations of findings during routine reads, such as line measurements and arrows, that are often stored in PACS as GSPS objects. We posit that these sparse annotations can be extracted along with CT volumes and converted into 3D segmentations using promptable segmentation models, a paradigm we term Opportunistic Promptable Segmentation. To enable this paradigm, we propose SAM2CT, the first promptable segmentation model designed to convert radiologist annotations into 3D segmentations in CT volumes. SAM2CT builds upon SAM2 by extending the prompt encoder to support arrow and line inputs and by introducing Memory-Conditioned Memories (MCM), a memory encoding strategy tailored to 3D medical volumes. On public lesion segmentation benchmarks, SAM2CT outperforms existing promptable segmentation models and similarly trained baselines, achieving Dice similarity coefficients of 0.649 for arrow prompts and 0.757 for line prompts. Applying the model to pre-existing GSPS annotations from a clinical PACS (N = 60), SAM2CT generates 3D segmentations that are clinically acceptable or require only minor adjustments in 87% of cases, as scored by radiologists. Additionally, SAM2CT demonstrates strong zero-shot performance on select Emergency Department findings. These results suggest that large-scale mining of historical GSPS annotations represents a promising and scalable approach for generating 3D CT segmentation datasets.

</details>


### [48] [On the Assessment of Sensitivity of Autonomous Vehicle Perception](https://arxiv.org/abs/2602.00314)
*Apostol Vassilev,Munawar Hasan,Edward Griffor,Honglan Jin,Pavel Piliptchak,Mahima Arora,Thoshitha Gamage*

Main category: cs.CV

TL;DR: Paper evaluates AV perception robustness using ensemble models under adverse conditions, finding lighting and distance to objects most impact performance.


<details>
  <summary>Details</summary>
Motivation: Automated driving requires reliable perception systems that must perform accurately under both ideal and challenging conditions (natural and adversarial factors), necessitating assessment of AV perception robustness.

Method: Uses predictive sensitivity quantification with ensemble of 5 state-of-the-art CV models (YOLO v8-v9, DETR50, DETR101, RT-DETR) to evaluate perception performance under adverse scenarios in simulated and real-world conditions, with assessment based on AV stopping distance at stop signs on varying road surfaces.

Result: Diminished lighting conditions (fog, low sun) have greatest impact on perception performance; adversarial road conditions (occlusions) increase perception sensitivity; performance drops further with combined adverse conditions; greater distance to objects reduces perception robustness.

Conclusion: Perception systems are vulnerable to specific adverse conditions, particularly lighting and object distance, highlighting the need for robust perception strategies in AV development and deployment.

Abstract: The viability of automated driving is heavily dependent on the performance of perception systems to provide real-time accurate and reliable information for robust decision-making and maneuvers. These systems must perform reliably not only under ideal conditions, but also when challenged by natural and adversarial driving factors. Both of these types of interference can lead to perception errors and delays in detection and classification. Hence, it is essential to assess the robustness of the perception systems of automated vehicles (AVs) and explore strategies for making perception more reliable. We approach this problem by evaluating perception performance using predictive sensitivity quantification based on an ensemble of models, capturing model disagreement and inference variability across multiple models, under adverse driving scenarios in both simulated environments and real-world conditions. A notional architecture for assessing perception performance is proposed. A perception assessment criterion is developed based on an AV's stopping distance at a stop sign on varying road surfaces, such as dry and wet asphalt, and vehicle speed. Five state-of-the-art computer vision models are used, including YOLO (v8-v9), DEtection TRansformer (DETR50, DETR101), Real-Time DEtection TRansformer (RT-DETR)in our experiments. Diminished lighting conditions, e.g., resulting from the presence of fog and low sun altitude, have the greatest impact on the performance of the perception models. Additionally, adversarial road conditions such as occlusions of roadway objects increase perception sensitivity and model performance drops when faced with a combination of adversarial road conditions and inclement weather conditions. Also, it is demonstrated that the greater the distance to a roadway object, the greater the impact on perception performance, hence diminished perception robustness.

</details>


### [49] [Bridging the Semantic Chasm: Synergistic Conceptual Anchoring for Generalized Few-Shot and Zero-Shot OOD Perception](https://arxiv.org/abs/2602.00340)
*Alexandros Christoforos,Sarah Jenkins,Michael Brown,Tuan Pham,David Chen*

Main category: cs.CV

TL;DR: SynerNet framework uses specialized neural agents to prevent cross-modal alignment degradation in VLMs for OOD concepts, improving few-shot and zero-shot performance by 1.2-5.4% on VISTA-Beyond benchmark.


<details>
  <summary>Details</summary>
Motivation: Address the problem of cross-modal alignment degeneration in Vision-Language Models when encountering Out-of-Distribution concepts, which limits their generalization capabilities.

Method: Synergistic Neural Agents Network with four specialized units (visual perception, linguistic context, nominal embedding, global coordination) using structured message-propagation protocol, multi-agent latent space nomenclature acquisition, semantic context-interchange algorithm, and adaptive dynamic equilibrium mechanism.

Result: Substantial performance improvements on VISTA-Beyond benchmark: 1.2% to 5.4% precision gains across diverse domains in both few-shot and zero-shot scenarios.

Conclusion: SynerNet effectively mitigates cross-modal alignment degeneration for OOD concepts through synergistic neural agent collaboration, enhancing VLM generalization capabilities.

Abstract: This manuscript presents a pioneering Synergistic Neural Agents Network (SynerNet) framework designed to mitigate the phenomenon of cross-modal alignment degeneration in Vision-Language Models (VLMs) when encountering Out-of-Distribution (OOD) concepts. Specifically, four specialized computational units - visual perception, linguistic context, nominal embedding, and global coordination - collaboratively rectify modality disparities via a structured message-propagation protocol. The principal contributions encompass a multi-agent latent space nomenclature acquisition framework, a semantic context-interchange algorithm for enhanced few-shot adaptation, and an adaptive dynamic equilibrium mechanism. Empirical evaluations conducted on the VISTA-Beyond benchmark demonstrate that SynerNet yields substantial performance augmentations in both few-shot and zero-shot scenarios, exhibiting precision improvements ranging from 1.2% to 5.4% across a diverse array of domains.

</details>


### [50] [When RAG Hurts: Diagnosing and Mitigating Attention Distraction in Retrieval-Augmented LVLMs](https://arxiv.org/abs/2602.00344)
*Beidi Zhao,Wenlong Deng,Xinting Liao,Yushu Li,Nazim Shaikh,Yao Nie,Xiaoxiao Li*

Main category: cs.CV

TL;DR: MAD-RAG addresses Attention Distraction in RAG for LVLMs by decoupling visual grounding from context integration using dual-question formulation and attention mixing, achieving significant performance gains with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: The paper identifies a previously overlooked failure mode in RAG for vision-language models called Attention Distraction, where retrieved context suppresses visual attention even when sufficient, causing models to fail on questions they could originally answer correctly without retrieval.

Method: Proposes MAD-RAG, a training-free intervention that uses dual-question formulation to decouple visual grounding from context integration, combined with attention mixing to preserve image-conditioned evidence.

Result: Extensive experiments on OK-VQA, E-VQA, and InfoSeek show MAD-RAG consistently outperforms existing baselines across different model families, with absolute gains up to 4.76%, 9.20%, and 6.18% over vanilla RAG, rectifying up to 74.68% of failure cases with negligible computational overhead.

Conclusion: MAD-RAG effectively mitigates Attention Distraction in RAG for LVLMs through a simple yet effective training-free approach that preserves visual grounding while integrating retrieved context, offering substantial improvements across multiple benchmarks.

Abstract: While Retrieval-Augmented Generation (RAG) is one of the dominant paradigms for enhancing Large Vision-Language Models (LVLMs) on knowledge-based VQA tasks, recent work attributes RAG failures to insufficient attention towards the retrieved context, proposing to reduce the attention allocated to image tokens. In this work, we identify a distinct failure mode that previous study overlooked: Attention Distraction (AD). When the retrieved context is sufficient (highly relevant or including the correct answer), the retrieved text suppresses the visual attention globally, and the attention on image tokens shifts away from question-relevant regions. This leads to failures on questions the model could originally answer correctly without the retrieved text. To mitigate this issue, we propose MAD-RAG, a training-free intervention that decouples visual grounding from context integration through a dual-question formulation, combined with attention mixing to preserve image-conditioned evidence. Extensive experiments on OK-VQA, E-VQA, and InfoSeek demonstrate that MAD-RAG consistently outperforms existing baselines across different model families, yielding absolute gains of up to 4.76%, 9.20%, and 6.18% over the vanilla RAG baseline. Notably, MAD-RAG rectifies up to 74.68% of failure cases with negligible computational overhead.

</details>


### [51] [AdaFuse: Adaptive Multimodal Fusion for Lung Cancer Risk Prediction via Reinforcement Learning](https://arxiv.org/abs/2602.00347)
*Chongyu Qu,Zhengyi Lu,Yuxiang Lai,Thomas Z. Li,Junchao Zhu,Junlin Guo,Juming Xiong,Yanfan Zhu,Yuechen Yang,Allen J. Luna,Kim L. Sandler,Bennett A. Landman,Yuankai Huo*

Main category: cs.CV

TL;DR: AdaFuse uses reinforcement learning to adaptively select which medical modalities to use for each patient's lung cancer risk prediction, achieving better accuracy with less computation than fixed fusion methods.


<details>
  <summary>Details</summary>
Motivation: Current multimodal fusion methods process all available modalities equally or with learned weights, but don't address whether certain modalities should be used at all for individual patients. There's a need for personalized modality selection rather than uniform fusion strategies.

Method: AdaFuse formulates multimodal fusion as a sequential decision process using reinforcement learning. A policy network iteratively decides whether to incorporate additional modalities or proceed to prediction based on already acquired information, enabling patient-specific modality selection and early termination.

Result: On the NLST dataset, AdaFuse achieved highest AUC (0.762) compared to single-modality baseline (0.732), best fixed fusion (0.759), and adaptive baselines DynMM (0.754) and MoE (0.742), while using fewer FLOPs than triple-modality methods.

Conclusion: Reinforcement learning enables personalized multimodal fusion in medical imaging, shifting from uniform fusion toward adaptive diagnostic pipelines that learn when to consult additional modalities versus when existing information suffices for accurate prediction.

Abstract: Multimodal fusion has emerged as a promising paradigm for disease diagnosis and prognosis, integrating complementary information from heterogeneous data sources such as medical images, clinical records, and radiology reports. However, existing fusion methods process all available modalities through the network, either treating them equally or learning to assign different contribution weights, leaving a fundamental question unaddressed: for a given patient, should certain modalities be used at all? We present AdaFuse, an adaptive multimodal fusion framework that leverages reinforcement learning (RL) to learn patient-specific modality selection and fusion strategies for lung cancer risk prediction. AdaFuse formulates multimodal fusion as a sequential decision process, where the policy network iteratively decides whether to incorporate an additional modality or proceed to prediction based on the information already acquired. This sequential formulation enables the model to condition each selection on previously observed modalities and terminate early when sufficient information is available, rather than committing to a fixed subset upfront. We evaluate AdaFuse on the National Lung Screening Trial (NLST) dataset. Experimental results demonstrate that AdaFuse achieves the highest AUC (0.762) compared to the best single-modality baseline (0.732), the best fixed fusion strategy (0.759), and adaptive baselines including DynMM (0.754) and MoE (0.742), while using fewer FLOPs than all triple-modality methods. Our work demonstrates the potential of reinforcement learning for personalized multimodal fusion in medical imaging, representing a shift from uniform fusion strategies toward adaptive diagnostic pipelines that learn when to consult additional modalities and when existing information suffices for accurate prediction.

</details>


### [52] [MASC: Metal-Aware Sampling and Correction via Reinforcement Learning for Accelerated MRI](https://arxiv.org/abs/2602.00348)
*Zhengyi Lu,Ming Lu,Chongyu Qu,Junchao Zhu,Junlin Guo,Marilyn Lionts,Yanfan Zhu,Yuechen Yang,Tianyuan Yao,Jayasai Rajagopal,Bennett Allan Landman,Xiao Wang,Xinqiang Yan,Yuankai Huo*

Main category: cs.CV

TL;DR: MASC is a reinforcement learning framework that jointly optimizes metal-aware k-space sampling and artifact correction for accelerated MRI with metal implants.


<details>
  <summary>Details</summary>
Motivation: Metal implants in MRI cause severe artifacts that degrade image quality, and traditional approaches treat metal artifact reduction and accelerated acquisition as separate problems rather than jointly optimizing them.

Method: Uses reinforcement learning with a PPO agent to learn k-space sampling policies, combined with a U-Net-based MAR network for artifact correction. Creates paired dataset via physics-based simulation for supervised training, and implements end-to-end training where acquisition policy and MAR network jointly adapt.

Result: MASC's learned policies outperform conventional sampling strategies, and end-to-end training improves performance compared to using frozen pre-trained MAR networks. Cross-dataset experiments on FastMRI confirm generalization to realistic clinical data.

Conclusion: The joint optimization of metal-aware sampling and artifact correction through reinforcement learning provides superior performance for accelerated MRI with metal implants, demonstrating the benefit of unified treatment of these traditionally separate problems.

Abstract: Metal implants in MRI cause severe artifacts that degrade image quality and hinder clinical diagnosis. Traditional approaches address metal artifact reduction (MAR) and accelerated MRI acquisition as separate problems. We propose MASC, a unified reinforcement learning framework that jointly optimizes metal-aware k-space sampling and artifact correction for accelerated MRI. To enable supervised training, we construct a paired MRI dataset using physics-based simulation, generating k-space data and reconstructions for phantoms with and without metal implants. This paired dataset provides simulated 3D MRI scans with and without metal implants, where each metal-corrupted sample has an exactly matched clean reference, enabling direct supervision for both artifact reduction and acquisition policy learning. We formulate active MRI acquisition as a sequential decision-making problem, where an artifact-aware Proximal Policy Optimization (PPO) agent learns to select k-space phase-encoding lines under a limited acquisition budget. The agent operates on undersampled reconstructions processed through a U-Net-based MAR network, learning patterns that maximize reconstruction quality. We further propose an end-to-end training scheme where the acquisition policy learns to select k-space lines that best support artifact removal while the MAR network simultaneously adapts to the resulting undersampling patterns. Experiments demonstrate that MASC's learned policies outperform conventional sampling strategies, and end-to-end training improves performance compared to using a frozen pre-trained MAR network, validating the benefit of joint optimization. Cross-dataset experiments on FastMRI with physics-based artifact simulation further confirm generalization to realistic clinical MRI data. The code and models of MASC have been made publicly available: https://github.com/hrlblab/masc

</details>


### [53] [ReLAPSe: Reinforcement-Learning-trained Adversarial Prompt Search for Erased concepts in unlearned diffusion models](https://arxiv.org/abs/2602.00350)
*Ignacy Kolton,Kacper Marzol,Paweł Batorski,Marcin Mazur,Paul Swoboda,Przemysław Spurek*

Main category: cs.CV

TL;DR: ReLAPSe is a reinforcement learning framework that efficiently restores unlearned concepts in diffusion models by learning transferable restoration strategies instead of per-instance optimization.


<details>
  <summary>Details</summary>
Motivation: Existing adversarial approaches for exploiting leakage in machine unlearning have limitations: optimization-based methods are computationally expensive (per-instance iterative search), while reasoning-based/heuristic techniques lack direct feedback from the model's latent visual representations.

Method: ReLAPSe reformulates concept restoration as a reinforcement learning problem using Reinforcement Learning with Verifiable Rewards (RLVR). It trains an agent using the diffusion model's noise prediction loss as model-intrinsic, verifiable feedback, creating a closed-loop design that aligns textual prompt manipulation with latent visual residuals.

Result: ReLAPSe achieves efficient, near-real-time recovery of fine-grained identities and styles across multiple state-of-the-art unlearning methods, providing a scalable tool for rigorous red-teaming of unlearned diffusion models.

Conclusion: By pioneering the shift from per-instance optimization to global policy learning, ReLAPSe enables efficient restoration of unlearned concepts and serves as a valuable tool for evaluating the robustness of machine unlearning defenses in diffusion models.

Abstract: Machine unlearning is a key defense mechanism for removing unauthorized concepts from text-to-image diffusion models, yet recent evidence shows that latent visual information often persists after unlearning. Existing adversarial approaches for exploiting this leakage are constrained by fundamental limitations: optimization-based methods are computationally expensive due to per-instance iterative search. At the same time, reasoning-based and heuristic techniques lack direct feedback from the target model's latent visual representations. To address these challenges, we introduce ReLAPSe, a policy-based adversarial framework that reformulates concept restoration as a reinforcement learning problem. ReLAPSe trains an agent using Reinforcement Learning with Verifiable Rewards (RLVR), leveraging the diffusion model's noise prediction loss as a model-intrinsic and verifiable feedback signal. This closed-loop design directly aligns textual prompt manipulation with latent visual residuals, enabling the agent to learn transferable restoration strategies rather than optimizing isolated prompts. By pioneering the shift from per-instance optimization to global policy learning, ReLAPSe achieves efficient, near-real-time recovery of fine-grained identities and styles across multiple state-of-the-art unlearning methods, providing a scalable tool for rigorous red-teaming of unlearned diffusion models. Some experimental evaluations involve sensitive visual concepts, such as nudity. Code is available at https://github.com/gmum/ReLaPSe

</details>


### [54] [Modeling Image-Caption Rating from Comparative Judgments](https://arxiv.org/abs/2602.00381)
*Kezia Minni,Qiang Zhang,Monoshiz Mahbub Khan,Zhe Yu*

Main category: cs.CV

TL;DR: Comparative learning framework models human pairwise caption comparisons instead of direct ratings, achieving near-regression performance while reducing annotation costs.


<details>
  <summary>Details</summary>
Motivation: Direct rating of image caption accuracy is time-consuming and subjective for humans, while pairwise comparison is easier and faster.

Method: Proposed comparative learning framework that models human comparative judgments; used VICR dataset with ResNet-50 for visual features and MiniLM for text features; trained both regression and comparative learning models.

Result: Regression model performed better (Pearson's ρ: 0.7609, Spearman's r_s: 0.7089), but comparative learning model steadily improved with more data and approached regression baseline; human evaluation showed comparative annotation is faster with greater inter-annotator agreement.

Conclusion: Comparative learning can effectively model human preferences while significantly reducing annotation costs, making it a practical alternative to direct rating approaches.

Abstract: Rating the accuracy of captions in describing images is time-consuming and subjective for humans. In contrast, it is often easier for people to compare two captions and decide which one better matches a given image. In this work, we propose a machine learning framework that models such comparative judgments instead of direct ratings. The model can then be applied to rank unseen image-caption pairs in the same way as a regression model trained on direct ratings. Using the VICR dataset, we extract visual features with ResNet-50 and text features with MiniLM, then train both a regression model and a comparative learning model. While the regression model achieves better performance (Pearson's $ρ$: 0.7609 and Spearman's $r_s$: 0.7089), the comparative learning model steadily improves with more data and approaches the regression baseline. In addition, a small-scale human evaluation study comparing absolute rating, pairwise comparison, and same-image comparison shows that comparative annotation yields faster results and has greater agreement among human annotators. These results suggest that comparative learning can effectively model human preferences while significantly reducing the cost of human annotations.

</details>


### [55] [Deep Learning-Based Object Detection for Autonomous Vehicles: A Comparative Study of One-Stage and Two-Stage Detectors on Basic Traffic Objects](https://arxiv.org/abs/2602.00385)
*Bsher Karbouj,Adam Michael Altenbuchner,Joerg Krueger*

Main category: cs.CV

TL;DR: Comparative analysis shows YOLOv5 outperforms Faster R-CNN in mAP, recall, and training efficiency for autonomous driving object detection, while Faster R-CNN excels at detecting small distant objects and handling challenging lighting.


<details>
  <summary>Details</summary>
Motivation: Object detection is critical for autonomous vehicle perception, but guidance on selecting appropriate deep learning methods (YOLO, SSD, Faster R-CNN) for specific autonomous driving applications is limited, despite significant impacts on system performance, robustness, and efficiency.

Method: Comprehensive experimental comparison of two prominent object detection models: YOLOv5 (one-stage detector) and Faster R-CNN (two-stage detector), evaluated on diverse dataset combining real and synthetic images using metrics including mAP, recall, and inference speed.

Result: YOLOv5 demonstrates superior performance in mAP, recall, and training efficiency, especially with larger datasets and higher image resolutions. Faster R-CNN shows advantages in detecting small distant objects and performs better in challenging lighting conditions.

Conclusion: The study provides practical insights for selecting object detection models in autonomous driving systems, suggesting YOLOv5 for overall performance and efficiency, while Faster R-CNN may be preferable for specific scenarios requiring detection of small objects or operation in difficult lighting conditions.

Abstract: Object detection is a crucial component in autonomous vehicle systems. It enables the vehicle to perceive and understand its environment by identifying and locating various objects around it. By utilizing advanced imaging and deep learning techniques, autonomous vehicle systems can rapidly and accurately identify objects based on their features. Different deep learning methods vary in their ability to accurately detect and classify objects in autonomous vehicle systems. Selecting the appropriate method significantly impacts system performance, robustness, and efficiency in real-world driving scenarios. While several generic deep learning architectures like YOLO, SSD, and Faster R-CNN have been proposed, guidance on their suitability for specific autonomous driving applications is often limited. The choice of method affects detection accuracy, processing speed, environmental robustness, sensor integration, scalability, and edge case handling. This study provides a comprehensive experimental analysis comparing two prominent object detection models: YOLOv5 (a one-stage detector) and Faster R-CNN (a two-stage detector). Their performance is evaluated on a diverse dataset combining real and synthetic images, considering various metrics including mean Average Precision (mAP), recall, and inference speed. The findings reveal that YOLOv5 demonstrates superior performance in terms of mAP, recall, and training efficiency, particularly as dataset size and image resolution increase. However, Faster R-CNN shows advantages in detecting small, distant objects and performs well in challenging lighting conditions. The models' behavior is also analyzed under different confidence thresholds and in various real-world scenarios, providing insights into their applicability for autonomous driving systems.

</details>


### [56] [Robust automatic brain vessel segmentation in 3D CTA scans using dynamic 4D-CTA data](https://arxiv.org/abs/2602.00391)
*Alberto Mario Ceballos-Arroyo,Shrikanth M. Yadav,Chu-Hsuan Lin,Jisoo Kim,Geoffrey S. Young,Huaizu Jiang,Lei Qin*

Main category: cs.CV

TL;DR: Novel dynamic 4D-CTA annotation method enhances brain vessel segmentation using temporal information, achieving state-of-the-art results with improved accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Manual annotation of brain vasculature is labor-intensive. The study aims to reduce annotation effort while improving segmentation quality by leveraging dynamic 4D-CTA scans with multiple time points.

Method: Developed methodology using dynamic 4D-CTA head scans with multiple time points to subtract bone/soft tissue and enhance vessel visualization. Trained deep learning models (nnUNet) on ground truth annotations, using same segmentation across multiple phases to effectively enlarge dataset 4-5x and induce robustness to contrast phases.

Result: Significantly better segmentation across all vascular regions compared to similar-sized datasets: mDC of 0.846 for arteries and 0.957 for veins in TopBrain dataset. Low error margins (aDHD 0.304mm arteries, 0.078mm veins) and high sensitivity (tSens 0.877 arteries, 0.974 veins). Dataset includes 110 training images from 25 patients and 165 test images from 14 patients.

Conclusion: The proposed dynamic 4D-CTA annotation methodology effectively reduces manual annotation effort while producing superior brain vessel segmentation results with excellent accuracy in capturing vessel morphology. Code and model weights are publicly available.

Abstract: In this study, we develop a novel methodology for annotating the brain vasculature using dynamic 4D-CTA head scans. By using multiple time points from dynamic CTA acquisitions, we subtract bone and soft tissue to enhance the visualization of arteries and veins, reducing the effort required to obtain manual annotations of brain vessels. We then train deep learning models on our ground truth annotations by using the same segmentation for multiple phases from the dynamic 4D-CTA collection, effectively enlarging our dataset by 4 to 5 times and inducing robustness to contrast phases. In total, our dataset comprises 110 training images from 25 patients and 165 test images from 14 patients. In comparison with two similarly-sized datasets for CTA-based brain vessel segmentation, a nnUNet model trained on our dataset can achieve significantly better segmentations across all vascular regions, with an average mDC of 0.846 for arteries and 0.957 for veins in the TopBrain dataset. Furthermore, metrics such as average directed Hausdorff distance (adHD) and topology sensitivity (tSens) reflected similar trends: using our dataset resulted in low error margins (aDHD of 0.304 mm for arteries and 0.078 for veins) and high sensitivity (tSens of 0.877 for arteries and 0.974 for veins), indicating excellent accuracy in capturing vessel morphology. Our code and model weights are available online: https://github.com/alceballosa/robust-vessel-segmentation

</details>


### [57] [Brazilian Portuguese Image Captioning with Transformers: A Study on Cross-Native-Translated Dataset](https://arxiv.org/abs/2602.00393)
*Gabriel Bromonschenkel,Alessandro L. Koerich,Thiago M. Paixão,Hilário Tomaz Alves de Oliveira*

Main category: cs.CV

TL;DR: Cross-native-translated evaluation of Transformer models for Brazilian Portuguese image captioning shows Swin-DistilBERTimbau performs best, ViTucano beats larger multilingual models, and GPT-4 excels at image-text alignment.


<details>
  <summary>Details</summary>
Motivation: Address the gap in Brazilian Portuguese image captioning research by evaluating Transformer-based vision-language models using both native and translated datasets, since most research focuses on English and low-resource languages lack specialized resources.

Method: Cross-context evaluation using Flickr30K with native Brazilian Portuguese captions vs. automatically translated captions; testing models trained on one dataset on the other; using attention maps for interpretation and CLIP-Score for image-description alignment assessment.

Result: Swin-DistilBERTimbau consistently outperformed other models with strong generalization; ViTucano (Brazilian Portuguese pre-trained VLM) surpassed larger multilingual models (GPT-4o, LLaMa 3.2 Vision) in text metrics; GPT-4 achieved highest CLIP-Score; attention analysis revealed systematic biases including gender misclassification and spatial inconsistencies.

Conclusion: The study demonstrates the importance of native datasets for Brazilian Portuguese IC, shows that specialized regional models can outperform larger multilingual ones, and reveals systematic biases in current models that need addressing for better performance.

Abstract: Image captioning (IC) refers to the automatic generation of natural language descriptions for images, with applications ranging from social media content generation to assisting individuals with visual impairments. While most research has been focused on English-based models, low-resource languages such as Brazilian Portuguese face significant challenges due to the lack of specialized datasets and models. Several studies create datasets by automatically translating existing ones to mitigate resource scarcity. This work addresses this gap by proposing a cross-native-translated evaluation of Transformer-based vision and language models for Brazilian Portuguese IC. We use a version of Flickr30K comprised of captions manually created by native Brazilian Portuguese speakers and compare it to a version with captions automatically translated from English to Portuguese. The experiments include a cross-context approach, where models trained on one dataset are tested on the other to assess the translation impact. Additionally, we incorporate attention maps for model inference interpretation and use the CLIP-Score metric to evaluate the image-description alignment. Our findings show that Swin-DistilBERTimbau consistently outperforms other models, demonstrating strong generalization across datasets. ViTucano, a Brazilian Portuguese pre-trained VLM, surpasses larger multilingual models (GPT-4o, LLaMa 3.2 Vision) in traditional text-based evaluation metrics, while GPT-4 models achieve the highest CLIP-Score, highlighting improved image-text alignment. Attention analysis reveals systematic biases, including gender misclassification, object enumeration errors, and spatial inconsistencies. The datasets and the models generated and analyzed during the current study are available in: https://github.com/laicsiifes/transformer-caption-ptbr.

</details>


### [58] [Modeling Art Evaluations from Comparative Judgments: A Deep Learning Approach to Predicting Aesthetic Preferences](https://arxiv.org/abs/2602.00394)
*Manoj Reddy Bethi,Sai Rupa Jhade,Pravallika Yaganti,Monoshiz Mahbub Khan,Zhe Yu*

Main category: cs.CV

TL;DR: Deep learning models using pairwise comparisons outperform traditional methods for predicting aesthetic judgments in art, with 60% faster annotation and 328% R² improvement.


<details>
  <summary>Details</summary>
Motivation: Human aesthetic judgment modeling is challenging due to individual preference variability and high labeling costs. The paper aims to reduce annotation costs while maintaining prediction accuracy by leveraging comparative judgments instead of direct ratings.

Method: Proposed comparative learning framework using pairwise preference assessments based on the Law of Comparative Judgment. Extracted ResNet-50 features from painting images, developed deep neural network regression and dual-branch pairwise comparison models. Explored four research questions comparing models, prediction methods, individual preference analysis, and annotation cost trade-offs.

Result: Deep regression model outperformed baseline by 328% in R². Comparative model approached regression performance without direct rating access. Individual preference prediction remained challenging. Comparative judgments required 60% less annotation time per item, demonstrating superior efficiency.

Conclusion: Pairwise comparative learning offers a practical, cost-effective alternative to direct rating collection for aesthetic preference modeling, with comparable performance and significantly reduced annotation burden, though individual preference prediction remains an open challenge.

Abstract: Modeling human aesthetic judgments in visual art presents significant challenges due to individual preference variability and the high cost of obtaining labeled data. To reduce cost of acquiring such labels, we propose to apply a comparative learning framework based on pairwise preference assessments rather than direct ratings. This approach leverages the Law of Comparative Judgment, which posits that relative choices exhibit less cognitive burden and greater cognitive consistency than direct scoring. We extract deep convolutional features from painting images using ResNet-50 and develop both a deep neural network regression model and a dual-branch pairwise comparison model. We explored four research questions: (RQ1) How does the proposed deep neural network regression model with CNN features compare to the baseline linear regression model using hand-crafted features? (RQ2) How does pairwise comparative learning compare to regression-based prediction when lacking access to direct rating values? (RQ3) Can we predict individual rater preferences through within-rater and cross-rater analysis? (RQ4) What is the annotation cost trade-off between direct ratings and comparative judgments in terms of human time and effort? Our results show that the deep regression model substantially outperforms the baseline, achieving up to $328\%$ improvement in $R^2$. The comparative model approaches regression performance despite having no access to direct rating values, validating the practical utility of pairwise comparisons. However, predicting individual preferences remains challenging, with both within-rater and cross-rater performance significantly lower than average rating prediction. Human subject experiments reveal that comparative judgments require $60\%$ less annotation time per item, demonstrating superior annotation efficiency for large-scale preference modeling.

</details>


### [59] [3DGS$^2$-TR: Scalable Second-Order Trust-Region Method for 3D Gaussian Splatting](https://arxiv.org/abs/2602.00395)
*Roger Hsiao,Yuchen Fang,Xiangru Huang,Ruilong Li,Hesam Rabeti,Zan Gojcic,Javad Lavaei,James Demmel,Sophia Shao*

Main category: cs.CV

TL;DR: 3DGS²-TR is a second-order optimizer for 3D Gaussian Splatting that uses diagonal Hessian approximation via Hutchinson's method and parameter-wise trust-region regularization, achieving 50% faster convergence with minimal memory overhead.


<details>
  <summary>Details</summary>
Motivation: Existing second-order optimizers for 3DGS (like 3DGS-LM and 3DGS2) rely on explicit or dense curvature representations that are computationally expensive and memory-intensive, limiting scalability to large scenes and distributed training.

Method: The method approximates curvature using only the diagonal of the Hessian matrix via Hutchinson's method, making it fully matrix-free with O(n) complexity. It introduces a parameter-wise trust-region technique based on squared Hellinger distance to regularize updates to Gaussian parameters, ensuring stable optimization despite strong nonlinearity in the 3DGS rasterization process.

Result: 3DGS²-TR achieves better reconstruction quality on standard datasets using 50% fewer training iterations compared to ADAM, with less than 1GB of peak GPU memory overhead (17% more than ADAM and 85% less than 3DGS-LM), enabling scalability to very large scenes.

Conclusion: The proposed second-order optimizer provides an efficient, memory-friendly alternative to existing methods, balancing convergence speed and computational overhead while maintaining reconstruction quality, making it suitable for large-scale 3DGS applications.

Abstract: We propose 3DGS$^2$-TR,a second-order optimizer for accelerating the scene training problem in 3D Gaussian Splatting (3DGS). Unlike existing second-order approaches that rely on explicit or dense curvature representations, such as 3DGS-LM (Höllein et al., 2025) or 3DGS2 (Lan et al., 2025), our method approximates curvature using only the diagonal of the Hessian matrix, efficiently via Hutchinson's method. Our approach is fully matrix-free and has the same complexity as ADAM (Kingma, 2024), $O(n)$ in both computation and memory costs. To ensure stable optimization in the presence of strong nonlinearity in the 3DGS rasterization process, we introduce a parameter-wise trust-region technique based on the squared Hellinger distance, regularizing updates to Gaussian parameters. Under identical parameter initialization and without densification, 3DGS$^2$-TR is able to achieve better reconstruction quality on standard datasets, using 50% fewer training iterations compared to ADAM, while incurring less than 1GB of peak GPU memory overhead (17% more than ADAM and 85% less than 3DGS-LM), enabling scalability to very large scenes and potentially to distributed training settings.

</details>


### [60] [Toward Autonomous Laboratory Safety Monitoring with Vision Language Models: Learning to See Hazards Through Scene Structure](https://arxiv.org/abs/2602.00414)
*Trishna Chakraborty,Udita Ghosh,Aldair Ernesto Gongora,Ruben Glatt,Yue Dong,Jiachen Li,Amit K. Roy-Chowdhury,Chengyu Song*

Main category: cs.CV

TL;DR: VLMs struggle with visual-only lab safety monitoring but perform well with structured scene graphs; proposed scene-graph-guided alignment bridges this gap by converting visual inputs to structured representations.


<details>
  <summary>Details</summary>
Motivation: Laboratories face safety risks from minor unsafe actions, but continuous monitoring is limited by human availability. While VLMs could enable autonomous safety monitoring, their effectiveness in realistic settings is unclear due to lack of visual evaluation data since most safety incidents are documented as unstructured text.

Method: 1) Created structured data generation pipeline converting textual lab scenarios into aligned triples (image, scene graph, ground truth) using LLMs as scene graph architects and image generation models as renderers. 2) Evaluated 7 open/closed-source models on 1,207 synthetic samples across 362 unique scenarios. 3) Proposed scene-graph-guided alignment post-training approach to bridge VLM perceptual gaps by translating visual inputs into structured scene graphs aligned with VLM reasoning.

Result: VLMs perform effectively given textual scene graphs but degrade substantially in visual-only settings, indicating difficulty extracting structured object relationships directly from pixels. The proposed scene-graph-guided alignment improves hazard detection performance in visual-only settings.

Conclusion: VLMs show promise for lab safety monitoring but need structured scene understanding; the proposed alignment method bridges the gap between visual perception and structured reasoning, enabling more effective autonomous safety monitoring systems.

Abstract: Laboratories are prone to severe injuries from minor unsafe actions, yet continuous safety monitoring -- beyond mandatory pre-lab safety training -- is limited by human availability. Vision language models (VLMs) offer promise for autonomous laboratory safety monitoring, but their effectiveness in realistic settings is unclear due to the lack of visual evaluation data, as most safety incidents are documented primarily as unstructured text. To address this gap, we first introduce a structured data generation pipeline that converts textual laboratory scenarios into aligned triples of (image, scene graph, ground truth), using large language models as scene graph architects and image generation models as renderers. Our experiments on the synthetic dataset of 1,207 samples across 362 unique scenarios and seven open- and closed-source models show that VLMs perform effectively given textual scene graph, but degrade substantially in visual-only settings indicating difficulty in extracting structured object relationships directly from pixels. To overcome this, we propose a post-training context-engineering approach, scene-graph-guided alignment, to bridge perceptual gaps in VLMs by translating visual inputs into structured scene graphs better aligned with VLM reasoning, improving hazard detection performance in visual only settings.

</details>


### [61] [Text is All You Need for Vision-Language Model Jailbreaking](https://arxiv.org/abs/2602.00420)
*Yihang Chen,Zhao Xu,Youyuan Jiang,Tianle Zheng,Cho-Jui Hsieh*

Main category: cs.CV

TL;DR: Text-DJ is a novel jailbreak attack that bypasses LVLM safety safeguards by decomposing harmful queries into benign sub-queries, adding irrelevant distraction queries, and presenting them as a grid of images to exploit OCR vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: Current LVLM safety defenses focus on explicit textual inputs or relevant visual scenes, but fail to address vulnerabilities in OCR capabilities when processing fragmented multimodal inputs.

Method: Three-stage approach: 1) Decompose harmful query into semantically related but benign sub-queries, 2) Select maximally irrelevant distraction queries, 3) Present all queries as a grid of images with sub-queries positioned in the middle.

Result: Successfully circumvents safety alignment of state-of-the-art LVLMs by bypassing text-based filters and overwhelming safety protocols with scattered sub-queries among irrelevant distractions.

Conclusion: Exposes critical vulnerability in LVLMs' OCR capabilities that are not robust to dispersed, multi-image adversarial inputs, highlighting need for defenses against fragmented multimodal attacks.

Abstract: Large Vision-Language Models (LVLMs) are increasingly equipped with robust safety safeguards to prevent responses to harmful or disallowed prompts. However, these defenses often focus on analyzing explicit textual inputs or relevant visual scenes. In this work, we introduce Text-DJ, a novel jailbreak attack that bypasses these safeguards by exploiting the model's Optical Character Recognition (OCR) capability. Our methodology consists of three stages. First, we decompose a single harmful query into multiple and semantically related but more benign sub-queries. Second, we pick a set of distraction queries that are maximally irrelevant to the harmful query. Third, we present all decomposed sub-queries and distraction queries to the LVLM simultaneously as a grid of images, with the position of the sub-queries being middle within the grid. We demonstrate that this method successfully circumvents the safety alignment of state-of-the-art LVLMs. We argue this attack succeeds by (1) converting text-based prompts into images, bypassing standard text-based filters, and (2) inducing distractions, where the model's safety protocols fail to link the scattered sub-queries within a high number of irrelevant queries. Overall, our findings expose a critical vulnerability in LVLMs' OCR capabilities that are not robust to dispersed, multi-image adversarial inputs, highlighting the need for defenses for fragmented multimodal inputs.

</details>


### [62] [DISK: Dynamic Inference SKipping for World Models](https://arxiv.org/abs/2602.00440)
*Anugunj Naman,Gaibo Zhang,Ayushman Singh,Yaguang Zhang*

Main category: cs.CV

TL;DR: DISK is a training-free adaptive inference method that speeds up autoregressive world models for driving by coordinating video and trajectory diffusion with skip decisions, achieving 2x speedup without quality loss.


<details>
  <summary>Details</summary>
Motivation: Autoregressive world models for driving scenarios (video and trajectory prediction) are computationally expensive for long-horizon rollouts. There's a need for efficient inference methods that maintain quality while reducing computational cost.

Method: DISK coordinates two coupled diffusion transformers for video and ego-trajectory using dual-branch controllers with cross-modal skip decisions. It extends higher-order latent-difference skip testing to autoregressive chains and propagates controller statistics through rollout loops for stability.

Result: On 1500 NuPlan and NuScenes samples, DISK achieves 2x speedup on trajectory diffusion and 1.6x speedup on video diffusion while maintaining L2 planning error, visual quality (FID/FVD), and NAVSIM PDMS scores.

Conclusion: DISK enables practical long-horizon video-and-trajectory prediction at substantially reduced computational cost without retraining, demonstrating effective training-free adaptive inference for autoregressive world models.

Abstract: We present DISK, a training-free adaptive inference method for autoregressive world models. DISK coordinates two coupled diffusion transformers for video and ego-trajectory via dual-branch controllers with cross-modal skip decisions, preserving motion-appearance consistency without retraining. We extend higher-order latent-difference skip testing to the autoregressive chain-of-forward regime and propagate controller statistics through rollout loops for long-horizon stability. When integrated into closed-loop driving rollouts on 1500 NuPlan and NuScenes samples using an NVIDIA L40S GPU, DISK achieves 2x speedup on trajectory diffusion and 1.6x speedup on video diffusion while maintaining L2 planning error, visual quality (FID/FVD), and NAVSIM PDMS scores, demonstrating practical long-horizon video-and-trajectory prediction at substantially reduced cost.

</details>


### [63] [Model Optimization for Multi-Camera 3D Detection and Tracking](https://arxiv.org/abs/2602.00450)
*Ethan Anderson,Justin Silva,Kyle Zheng,Sameer Pusegaonkar,Yizhou Wang,Zheng Tang,Sujit Biswas*

Main category: cs.CV

TL;DR: Sparse4D, a query-based multi-camera 3D tracking system, maintains performance with moderate FPS reductions but fails below 2 FPS. Selective quantization works best for speed-accuracy trade-offs, while attention modules are precision-sensitive. Low-FPS pretraining boosts WILDTRACK performance, and mixed precision improves latency but can harm identity stability.


<details>
  <summary>Details</summary>
Motivation: To improve multi-camera perception in indoor environments where static camera networks need to handle occlusion, heterogeneous viewpoints, and maintain multi-target tracking accuracy under practical constraints like reduced frame rates and quantization for deployment.

Method: Evaluates Sparse4D framework with multi-view feature fusion in world frame, sparse object query propagation via instance memory. Tests reduced input FPS, post-training quantization (INT8/FP8), transfer to WILDTRACK benchmark, and Transformer Engine mixed-precision fine-tuning. Introduces Average Track Duration metric for identity stability.

Result: Sparse4D stable with moderate FPS reductions but identity association collapses below 2 FPS. Selective quantization of backbone/neck offers best speed-accuracy trade-off. Attention modules sensitive to low precision. Low-FPS pretraining yields large zero-shot gains on WILDTRACK. Mixed precision reduces latency but can destabilize identity propagation.

Conclusion: Sparse4D shows practical deployment potential with careful quantization and FPS considerations, but identity stability remains challenging at very low frame rates and with mixed precision optimization, requiring stability-aware validation for real-world applications.

Abstract: Outside-in multi-camera perception is increasingly important in indoor environments, where networks of static cameras must support multi-target tracking under occlusion and heterogeneous viewpoints. We evaluate Sparse4D, a query-based spatiotemporal 3D detection and tracking framework that fuses multi-view features in a shared world frame and propagates sparse object queries via instance memory. We study reduced input frame rates, post-training quantization (INT8 and FP8), transfer to the WILDTRACK benchmark, and Transformer Engine mixed-precision fine-tuning. To better capture identity stability, we report Average Track Duration (AvgTrackDur), which measures identity persistence in seconds. Sparse4D remains stable under moderate FPS reductions, but below 2 FPS, identity association collapses even when detections are stable. Selective quantization of the backbone and neck offers the best speed-accuracy trade-off, while attention-related modules are consistently sensitive to low precision. On WILDTRACK, low-FPS pretraining yields large zero-shot gains over the base checkpoint, while small-scale fine-tuning provides limited additional benefit. Transformer Engine mixed precision reduces latency and improves camera scalability, but can destabilize identity propagation, motivating stability-aware validation.

</details>


### [64] [LatentLens: Revealing Highly Interpretable Visual Tokens in LLMs](https://arxiv.org/abs/2602.00462)
*Benno Krojer,Shravan Nayak,Oscar Mañas,Vaibhav Adlakha,Desmond Elliott,Siva Reddy,Marius Mosbach*

Main category: cs.CV

TL;DR: LatentLens is a new interpretability method that maps visual token representations in VLMs to natural language descriptions by comparing them to contextualized textual embeddings, revealing that visual tokens are highly interpretable across all layers.


<details>
  <summary>Details</summary>
Motivation: To understand why LLMs can easily process visual tokens when transformed into VLMs, and to develop better interpretability methods that reveal what visual tokens encode at every layer of processing.

Method: LatentLens encodes a large text corpus, stores contextualized token representations, then compares visual token representations to these textual representations. The top-k nearest neighbors provide natural language descriptions of the visual tokens.

Result: LatentLens shows that commonly used methods like LogitLens substantially underestimate visual token interpretability. With LatentLens, the majority of visual tokens are interpretable across all 10 studied VLMs and all layers, with semantically meaningful descriptions.

Conclusion: The method provides new evidence for alignment between vision and language representations, offers more fine-grained human interpretations than individual tokens, and opens new directions for analyzing latent representations in VLMs.

Abstract: Transforming a large language model (LLM) into a Vision-Language Model (VLM) can be achieved by mapping the visual tokens from a vision encoder into the embedding space of an LLM. Intriguingly, this mapping can be as simple as a shallow MLP transformation. To understand why LLMs can so readily process visual tokens, we need interpretability methods that reveal what is encoded in the visual token representations at every layer of LLM processing. In this work, we introduce LatentLens, a novel approach for mapping latent representations to descriptions in natural language. LatentLens works by encoding a large text corpus and storing contextualized token representations for each token in that corpus. Visual token representations are then compared to their contextualized textual representations, with the top-k nearest neighbor representations providing descriptions of the visual token. We evaluate this method on 10 different VLMs, showing that commonly used methods, such as LogitLens, substantially underestimate the interpretability of visual tokens. With LatentLens instead, the majority of visual tokens are interpretable across all studied models and all layers. Qualitatively, we show that the descriptions produced by LatentLens are semantically meaningful and provide more fine-grained interpretations for humans compared to individual tokens. More broadly, our findings contribute new evidence on the alignment between vision and language representations, opening up new directions for analyzing latent representations.

</details>


### [65] [PSGS: Text-driven Panorama Sliding Scene Generation via Gaussian Splatting](https://arxiv.org/abs/2602.00463)
*Xin Zhang,Shen Chen,Jiale Zhou,Lei Li*

Main category: cs.CV

TL;DR: PSGS is a two-stage framework for generating high-fidelity 3D scenes from text using panoramic image generation and 3D Gaussian Splatting, addressing limitations in existing text-to-3D methods.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-3D scene generation methods suffer from limited 3D-text training data and inconsistent multi-view stitching, resulting in overly simplistic scenes that don't meet the needs of immersive applications like VR, AR, and gaming.

Method: Two-stage framework: 1) Two-layer optimization for panorama generation (layout reasoning for spatial relationships + self-optimization with MLLM feedback), 2) Panorama sliding mechanism for 3D Gaussian Splatting initialization with depth and semantic coherence losses.

Result: PSGS outperforms existing methods in panorama generation quality and produces more appealing 3D scenes with improved detail fidelity and global consistency.

Conclusion: PSGS offers a robust solution for scalable immersive content creation by addressing key limitations in text-to-3D scene generation through innovative panorama-based approaches and optimization techniques.

Abstract: Generating realistic 3D scenes from text is crucial for immersive applications like VR, AR, and gaming. While text-driven approaches promise efficiency, existing methods suffer from limited 3D-text data and inconsistent multi-view stitching, resulting in overly simplistic scenes. To address this, we propose PSGS, a two-stage framework for high-fidelity panoramic scene generation. First, a novel two-layer optimization architecture generates semantically coherent panoramas: a layout reasoning layer parses text into structured spatial relationships, while a self-optimization layer refines visual details via iterative MLLM feedback. Second, our panorama sliding mechanism initializes globally consistent 3D Gaussian Splatting point clouds by strategically sampling overlapping perspectives. By incorporating depth and semantic coherence losses during training, we greatly improve the quality and detail fidelity of rendered scenes. Our experiments demonstrate that PSGS outperforms existing methods in panorama generation and produces more appealing 3D scenes, offering a robust solution for scalable immersive content creation.

</details>


### [66] [ZS-TreeSeg: A Zero-Shot Framework for Tree Crown Instance Segmentation](https://arxiv.org/abs/2602.00470)
*Pengyu Chen,Fangzheng Lyu,Sicheng Wang,Cuizhen Wang*

Main category: cs.CV

TL;DR: ZS-TreeSeg: A zero-shot framework for tree crown segmentation that adapts from canopy semantic segmentation and cells instance segmentation, using topological flow fields to separate overlapping crowns without training.


<details>
  <summary>Details</summary>
Motivation: Current tree crown segmentation methods face limitations: supervised deep learning requires expensive annotations and has poor generalization, while foundation models like SAM lack domain knowledge and under-segment dense canopies.

Method: Proposes ZS-TreeSeg that models tree crowns as star-convex objects within topological flow fields using Cellpose-SAM, forcing mathematical separation of touching crowns based on vector convergence, adapting from canopy semantic segmentation and cells instance segmentation.

Result: Experiments on NEON and BAMFOREST datasets show robust generalization across diverse sensor types and canopy densities, providing a training-free solution for tree crown instance segmentation and label generation.

Conclusion: ZS-TreeSeg offers an effective zero-shot approach that bridges the gap between annotation-heavy supervised methods and domain-agnostic foundation models for accurate tree crown segmentation in dense canopies.

Abstract: Individual tree crown segmentation is an important task in remote sensing for forest biomass estimation and ecological monitoring. However, accurate delineation in dense, overlapping canopies remains a bottleneck. While supervised deep learning methods suffer from high annotation costs and limited generalization, emerging foundation models (e.g., Segment Anything Model) often lack domain knowledge, leading to under-segmentation in dense clusters. To bridge this gap, we propose ZS-TreeSeg, a Zero-Shot framework that adapts from two mature tasks: 1) Canopy Semantic segmentation; and 2) Cells instance segmentation. By modeling tree crowns as star-convex objects within a topological flow field using Cellpose-SAM, the ZS-TreeSeg framework forces the mathematical separation of touching tree crown instances based on vector convergence. Experiments on the NEON and BAMFOREST datasets and visual inspection demonstrate that our framework generalizes robustly across diverse sensor types and canopy densities, which can offer a training-free solution for tree crown instance segmentation and labels generation.

</details>


### [67] [GTATrack: Winner Solution to SoccerTrack 2025 with Deep-EIoU and Global Tracklet Association](https://arxiv.org/abs/2602.00484)
*Rong-Lin Jian,Ming-Chi Luo,Chen-Wei Huang,Chia-Ming Lee,Yu-Fan Lin,Chih-Chung Hsu*

Main category: cs.CV

TL;DR: GTATrack wins SoccerTrack Challenge 2025 with hierarchical tracking framework using Deep-EIoU for motion-agnostic association and GTA for trajectory refinement, achieving HOTA 0.60.


<details>
  <summary>Details</summary>
Motivation: Multi-object tracking in sports faces challenges from irregular motion, uniform appearances, frequent occlusions, and fisheye camera distortions with extreme scale variation.

Method: Two-stage hierarchical framework: Deep Expansion IoU for motion-agnostic online association + Global Tracklet Association for trajectory refinement, plus pseudo-labeling to boost detector recall.

Result: Achieved winning HOTA score of 0.60 in SoccerTrack Challenge 2025, significantly reduced false positives to 982, demonstrating state-of-the-art fisheye soccer tracking accuracy.

Conclusion: GTATrack effectively addresses identity switches, occlusions, and fragmentation through synergy of local association and global reasoning, setting new benchmark for fisheye-based sports tracking.

Abstract: Multi-object tracking (MOT) in sports is highly challenging due to irregular player motion, uniform appearances, and frequent occlusions. These difficulties are further exacerbated by the geometric distortion and extreme scale variation introduced by static fisheye cameras. In this work, we present GTATrack, a hierarchical tracking framework that win first place in the SoccerTrack Challenge 2025. GTATrack integrates two core components: Deep Expansion IoU (Deep-EIoU) for motion-agnostic online association and Global Tracklet Association (GTA) for trajectory-level refinement. This two-stage design enables both robust short-term matching and long-term identity consistency. Additionally, a pseudo-labeling strategy is used to boost detector recall on small and distorted targets. The synergy between local association and global reasoning effectively addresses identity switches, occlusions, and tracking fragmentation. Our method achieved a winning HOTA score of 0.60 and significantly reduced false positives to 982, demonstrating state-of-the-art accuracy in fisheye-based soccer tracking. Our code is available at https://github.com/ron941/GTATrack-STC2025.

</details>


### [68] [Refining Strokes by Learning Offset Attributes between Strokes for Flexible Sketch Edit at Stroke-Level](https://arxiv.org/abs/2602.00489)
*Sicong Zang,Tao Sun,Cairong Yan*

Main category: cs.CV

TL;DR: SketchMod refines source strokes through transformation (scale, orientation, position) to align with target sketch patterns for precise stroke-level sketch editing.


<details>
  <summary>Details</summary>
Motivation: Existing sketch editing methods merely reposition source strokes without adjusting for size/orientation variations, leading to implausible results when source strokes differ significantly from target patterns.

Method: Proposes SketchMod that learns three offset attributes (scale, orientation, position) from source to target, then aligns strokes through: 1) resizing to match spatial proportions, 2) rotating to align with local geometry, and 3) displacing to meet semantic layout.

Result: Experimental results show SketchMod achieves precise and flexible performance on stroke-level sketch editing with precise control over stroke profiles.

Conclusion: SketchMod enables flexible and precise stroke-level sketch editing by transforming source strokes to align with target sketch patterns through learned scale, orientation, and position adjustments.

Abstract: Sketch edit at stroke-level aims to transplant source strokes onto a target sketch via stroke expansion or replacement, while preserving semantic consistency and visual fidelity with the target sketch. Recent studies addressed it by relocating source strokes at appropriate canvas positions. However, as source strokes could exhibit significant variations in both size and orientation, we may fail to produce plausible sketch editing results by merely repositioning them without further adjustments. For example, anchoring an oversized source stroke onto the target without proper scaling would fail to produce a semantically coherent outcome. In this paper, we propose SketchMod to refine the source stroke through transformation so as to align it with the target sketch's patterns, further realize flexible sketch edit at stroke-level. As the source stroke refinement is governed by the patterns of the target sketch, we learn three key offset attributes (scale, orientation and position) from the source stroke to another, and align it with the target by: 1) resizing to match spatial proportions by scale, 2) rotating to align with local geometry by orientation, and 3) displacing to meet with semantic layout by position. Besides, a stroke's profiles can be precisely controlled during sketch edit via the exposed captured stroke attributes. Experimental results indicate that SketchMod achieves precise and flexible performances on stroke-level sketch edit.

</details>


### [69] [HSSDCT: Factorized Spatial-Spectral Correlation for Hyperspectral Image Fusion](https://arxiv.org/abs/2602.00490)
*Chia-Ming Lee,Yu-Hao Ho,Yu-Fan Lin,Jen-Wei Lee,Li-Wei Kang,Chih-Chung Hsu*

Main category: cs.CV

TL;DR: HSSDCT: A hierarchical spatial-spectral dense correlation network for hyperspectral image fusion that achieves SOTA performance with linear complexity attention.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning methods for HSI fusion suffer from limited receptive fields, spectral redundancy, and quadratic complexity of self-attention, which restrict efficiency and robustness.

Method: Proposes HSSDCT with two key modules: 1) Hierarchical Dense-Residue Transformer Block (HDRTB) for multi-scale feature aggregation via progressive window enlargement and dense-residue connections, and 2) Spatial-Spectral Correlation Layer (SSCL) that factorizes spatial and spectral dependencies to reduce self-attention to linear complexity while mitigating spectral redundancy.

Result: Extensive experiments on benchmark datasets show HSSDCT delivers superior reconstruction quality with significantly lower computational costs, achieving new state-of-the-art performance in HSI fusion.

Conclusion: HSSDCT effectively addresses the limitations of existing methods by combining hierarchical multi-scale feature aggregation with efficient spatial-spectral factorization, providing a computationally efficient and robust solution for hyperspectral image fusion.

Abstract: Hyperspectral image (HSI) fusion aims to reconstruct a high-resolution HSI (HR-HSI) by combining the rich spectral information of a low-resolution HSI (LR-HSI) with the fine spatial details of a high-resolution multispectral image (HR-MSI). Although recent deep learning methods have achieved notable progress, they still suffer from limited receptive fields, redundant spectral bands, and the quadratic complexity of self-attention, which restrict both efficiency and robustness. To overcome these challenges, we propose the Hierarchical Spatial-Spectral Dense Correlation Network (HSSDCT). The framework introduces two key modules: (i) a Hierarchical Dense-Residue Transformer Block (HDRTB) that progressively enlarges windows and employs dense-residue connections for multi-scale feature aggregation, and (ii) a Spatial-Spectral Correlation Layer (SSCL) that explicitly factorizes spatial and spectral dependencies, reducing self-attention to linear complexity while mitigating spectral redundancy. Extensive experiments on benchmark datasets demonstrate that HSSDCT delivers superior reconstruction quality with significantly lower computational costs, achieving new state-of-the-art performance in HSI fusion. Our code is available at https://github.com/jemmyleee/HSSDCT.

</details>


### [70] [RGBX-R1: Visual Modality Chain-of-Thought Guided Reinforcement Learning for Multimodal Grounding](https://arxiv.org/abs/2602.00504)
*Jiahe Wu,Bing Cao,Qilong Wang,Qinghua Hu,Dongdong Li,Pengfei Zhu*

Main category: cs.CV

TL;DR: RGBX-R1 enhances MLLMs' perception across diverse visual modalities (infrared, depth, event) using UAV prompting and two-stage training, achieving 22.71% improvement on RGBX grounding tasks.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs are limited to RGB modality pre-training, hindering performance on other crucial visual modalities (infrared, depth, event) needed for complex scenarios.

Method: Proposes RGBX-R1 framework with UAV prompting to create Visual Modality Chain-of-Thought (VM-CoT), and two-stage training: Cold-Start SFT for fundamental modality cognition and Spatio-Temporal RFT with MuST reward for enhanced reasoning.

Result: Outperforms baselines by 22.71% on three RGBX grounding tasks, with extensive experiments verifying superiority in multimodal understanding and spatial perception.

Conclusion: RGBX-R1 successfully enhances MLLMs' perception and reasoning across diverse visual modalities, establishing the first RGBX-Grounding benchmark and demonstrating significant performance improvements.

Abstract: Multimodal Large Language Models (MLLM) are primarily pre-trained on the RGB modality, thereby limiting their performance on other modalities, such as infrared, depth, and event data, which are crucial for complex scenarios. To address this, we propose RGBX-R1, a framework to enhance MLLM's perception and reasoning capacities across various X visual modalities. Specifically, we employ an Understand-Associate-Validate (UAV) prompting strategy to construct the Visual Modality Chain-of-Thought (VM-CoT), which aims to expand the MLLMs' RGB understanding capability into X modalities. To progressively enhance reasoning capabilities, we introduce a two-stage training paradigm: Cold-Start Supervised Fine-Tuning (CS-SFT) and Spatio-Temporal Reinforcement Fine-Tuning (ST-RFT). CS-SFT supervises the reasoning process with the guidance of VM-CoT, equipping the MLLM with fundamental modality cognition. Building upon GRPO, ST-RFT employs a Modality-understanding Spatio-Temporal (MuST) reward to reinforce modality reasoning. Notably, we construct the first RGBX-Grounding benchmark, and extensive experiments verify our superiority in multimodal understanding and spatial perception, outperforming baselines by 22.71% on three RGBX grounding tasks.

</details>


### [71] [Sparse Shortcuts: Facilitating Efficient Fusion in Multimodal Large Language Models](https://arxiv.org/abs/2602.00505)
*Jingrui Zhang,Feng Liang,Yong Zhang,Wei Wang,Runhao Zeng,Xiping Hu*

Main category: cs.CV

TL;DR: SparseCut introduces sparse shortcut connections between cross-modal encoder and LLM for hierarchical visual feature integration in MLLMs, improving multimodal understanding without computational overhead.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs focus on scaling language models or better training data, but neglect effective cross-modal knowledge integration. Vision-language models using only high-level visual features discard rich semantic information from mid- and low-level features, limiting cross-modality understanding.

Method: Proposes SparseCut with sparse shortcut connections between cross-modal encoder and LLM for hierarchical visual feature integration. Includes efficient multi-grained feature fusion module that fuses visual features before routing through shortcuts, preserving language context without increasing input length.

Result: SparseCut significantly enhances MLLM performance across various multimodal benchmarks, demonstrating generality and scalability for different base LLMs.

Conclusion: SparseCut provides an effective architecture for hierarchical cross-modal feature integration in MLLMs, improving multimodal understanding while maintaining computational efficiency.

Abstract: With the remarkable success of large language models (LLMs) in natural language understanding and generation, multimodal large language models (MLLMs) have rapidly advanced in their ability to process data across multiple modalities. While most existing efforts focus on scaling up language models or constructing higher-quality training data, limited attention has been paid to effectively integrating cross-modal knowledge into the language space. In vision-language models, for instance, aligning modalities using only high-level visual features often discards the rich semantic information present in mid- and low-level features, limiting the model's ability of cross-modality understanding. To address this issue, we propose SparseCut, a general cross-modal fusion architecture for MLLMs, introducing sparse shortcut connections between the cross-modal encoder and the LLM. These shortcut connections enable the efficient and hierarchical integration of visual features at multiple levels, facilitating richer semantic fusion without increasing computational overhead. We further introduce an efficient multi-grained feature fusion module, which performs the fusion of visual features before routing them through the shortcuts. This preserves the original language context and does not increase the overall input length, thereby avoiding an increase in computational complexity for the LLM. Experiments demonstrate that SparseCut significantly enhances the performance of MLLMs across various multimodal benchmarks with generality and scalability for different base LLMs.

</details>


### [72] [DuoGen: Towards General Purpose Interleaved Multimodal Generation](https://arxiv.org/abs/2602.00508)
*Min Shi,Xiaohui Zeng,Jiannan Huang,Yin Cui,Francesco Ferroni,Jialuo Li,Shubham Pachori,Zhaoshuo Li,Yogesh Balaji,Haoxiang Wang,Tsung-Yi Lin,Xiao Fu,Yue Zhao,Chieh-Yun Chen,Ming-Yu Liu,Humphrey Shi*

Main category: cs.CV

TL;DR: DuoGen is a general-purpose interleaved multimodal generation framework that combines MLLM visual understanding with DiT visual generation, achieving SOTA performance through systematic data curation and two-stage training.


<details>
  <summary>Details</summary>
Motivation: Existing interleaved generation models have limited quality due to insufficient training data and base model capacity, despite their potential for applications like instructional guides, visual planning, and reasoning drafts.

Method: Systematic framework with: 1) Large-scale instruction-tuning dataset combining rewritten multimodal conversations and synthetic examples; 2) Architecture leveraging pretrained MLLM for visual understanding and DiT for visual generation; 3) Two-stage decoupled training: first instruction-tune MLLM, then align DiT with curated interleaved sequences.

Result: Outperforms prior open-source models in text quality, image fidelity, and image-context alignment on public and new benchmarks; achieves SOTA performance on text-to-image and image editing among unified generation models.

Conclusion: DuoGen demonstrates that systematic data curation and architectural design can enable high-quality interleaved multimodal generation without costly unimodal pretraining, providing a flexible framework for general-purpose multimodal generation.

Abstract: Interleaved multimodal generation enables capabilities beyond unimodal generation models, such as step-by-step instructional guides, visual planning, and generating visual drafts for reasoning. However, the quality of existing interleaved generation models under general instructions remains limited by insufficient training data and base model capacity. We present DuoGen, a general-purpose interleaved generation framework that systematically addresses data curation, architecture design, and evaluation. On the data side, we build a large-scale, high-quality instruction-tuning dataset by combining multimodal conversations rewritten from curated raw websites, and diverse synthetic examples covering everyday scenarios. Architecturally, DuoGen leverages the strong visual understanding of a pretrained multimodal LLM and the visual generation capabilities of a diffusion transformer (DiT) pretrained on video generation, avoiding costly unimodal pretraining and enabling flexible base model selection. A two-stage decoupled strategy first instruction-tunes the MLLM, then aligns DiT with it using curated interleaved image-text sequences. Across public and newly proposed benchmarks, DuoGen outperforms prior open-source models in text quality, image fidelity, and image-context alignment, and also achieves state-of-the-art performance on text-to-image and image editing among unified generation models. Data and code will be released at https://research.nvidia.com/labs/dir/duetgen/.

</details>


### [73] [SPARK: Stochastic Propagation via Affinity-guided Random walK for training-free unsupervised segmentation](https://arxiv.org/abs/2602.00516)
*Kunal Mahatha,Jose Dolz,Christian Desrosiers*

Main category: cs.CV

TL;DR: Training-free segmentation reformulated from spectral graph partitioning to stochastic flow equilibrium, achieving SOTA zero-shot performance with sharper boundaries.


<details>
  <summary>Details</summary>
Motivation: Existing training-free segmentation methods rely on spectral graph partitioning over diffusion-derived affinities, which suffers from drawbacks like requiring pre-selected cluster numbers, boundary oversmoothing, sensitivity to noisy affinities, and neglect of local neighborhood structure.

Method: Reformulate segmentation as stochastic flow equilibrium over diffusion-induced affinity graphs, using Markov propagation with random-walk-based label diffusion and adaptive pruning strategy that suppresses unreliable transitions while reinforcing confident affinity paths.

Result: Achieves state-of-the-art zero-shot performance across seven semantic segmentation benchmarks, producing sharper boundaries, more coherent regions, and significantly more stable masks compared to prior spectral-clustering-based approaches.

Conclusion: The stochastic flow equilibrium formulation with Markov propagation and adaptive pruning overcomes limitations of spectral graph partitioning, enabling more robust and accurate training-free segmentation that better preserves fine-grained contours.

Abstract: We argue that existing training-free segmentation methods rely on an implicit and limiting assumption, that segmentation is a spectral graph partitioning problem over diffusion-derived affinities. Such approaches, based on global graph partitioning and eigenvector-based formulations of affinity matrices, suffer from several fundamental drawbacks, they require pre-selecting the number of clusters, induce boundary oversmoothing due to spectral relaxation, and remain highly sensitive to noisy or multi-modal affinity distributions. Moreover, many prior works neglect the importance of local neighborhood structure, which plays a crucial role in stabilizing affinity propagation and preserving fine-grained contours. To address these limitations, we reformulate training-free segmentation as a stochastic flow equilibrium problem over diffusion-induced affinity graphs, where segmentation emerges from a stochastic propagation process that integrates global diffusion attention with local neighborhoods extracted from stable diffusion, yielding a sparse yet expressive affinity structure. Building on this formulation, we introduce a Markov propagation scheme that performs random-walk-based label diffusion with an adaptive pruning strategy that suppresses unreliable transitions while reinforcing confident affinity paths. Experiments across seven widely used semantic segmentation benchmarks demonstrate that our method achieves state-of-the-art zero-shot performance, producing sharper boundaries, more coherent regions, and significantly more stable masks compared to prior spectral-clustering-based approaches.

</details>


### [74] [MRAD: Zero-Shot Anomaly Detection with Memory-Driven Retrieval](https://arxiv.org/abs/2602.00522)
*Chaoran Xu,Chengkan Lv,Qiyu Chen,Feng Zhang,Zhengtao Zhang*

Main category: cs.CV

TL;DR: MRAD is a unified anomaly detection framework that replaces parametric fitting with direct memory retrieval, using a two-level memory bank from auxiliary data for train-free or lightweight training variants.


<details>
  <summary>Details</summary>
Motivation: Existing zero-shot anomaly detection methods often use prompt learning or complex modeling that leads to high training/inference costs and limited cross-domain stability. The authors aim to address these limitations by moving away from parametric fitting approaches.

Method: Proposes MRAD with a train-free base model (MRAD-TF) that freezes CLIP image encoder and constructs image-level and pixel-level memory banks from auxiliary data. Features are stored as keys with labels as values. Two lightweight variants: MRAD-FT adds linear layers for metric fine-tuning, and MRAD-CLIP injects region priors as dynamic biases into CLIP text prompts.

Result: Demonstrates superior performance across 16 industrial and medical datasets for both anomaly classification and segmentation, under both train-free and training-based settings.

Conclusion: Fully leveraging the empirical distribution of raw data through memory retrieval, rather than relying only on model fitting, can achieve stronger anomaly detection performance with better efficiency and cross-domain stability.

Abstract: Zero-shot anomaly detection (ZSAD) often leverages pretrained vision or vision-language models, but many existing methods use prompt learning or complex modeling to fit the data distribution, resulting in high training or inference cost and limited cross-domain stability. To address these limitations, we propose Memory-Retrieval Anomaly Detection method (MRAD), a unified framework that replaces parametric fitting with a direct memory retrieval. The train-free base model, MRAD-TF, freezes the CLIP image encoder and constructs a two-level memory bank (image-level and pixel-level) from auxiliary data, where feature-label pairs are explicitly stored as keys and values. During inference, anomaly scores are obtained directly by similarity retrieval over the memory bank. Based on the MRAD-TF, we further propose two lightweight variants as enhancements: (i) MRAD-FT fine-tunes the retrieval metric with two linear layers to enhance the discriminability between normal and anomaly; (ii) MRAD-CLIP injects the normal and anomalous region priors from the MRAD-FT as dynamic biases into CLIP's learnable text prompts, strengthening generalization to unseen categories. Across 16 industrial and medical datasets, the MRAD framework consistently demonstrates superior performance in anomaly classification and segmentation, under both train-free and training-based settings. Our work shows that fully leveraging the empirical distribution of raw data, rather than relying only on model fitting, can achieve stronger anomaly detection performance. The code will be publicly released at https://github.com/CROVO1026/MRAD.

</details>


### [75] [SAGE: Accelerating Vision-Language Models via Entropy-Guided Adaptive Speculative Decoding](https://arxiv.org/abs/2602.00523)
*Yujia Tong,Tian Zhang,Yunyang Wan,Kaiwei Lin,Jingling Yuan,Chuang Hu*

Main category: cs.CV

TL;DR: SAGE dynamically adjusts speculation tree structure based on real-time prediction uncertainty to accelerate vision-language model inference, achieving up to 3.36x speedup without quality loss.


<details>
  <summary>Details</summary>
Motivation: Existing speculative decoding methods for VLMs use static tree structures that don't adapt to varying prediction difficulty across generation steps, leading to suboptimal acceptance lengths and limited speedup.

Method: SAGE dynamically adjusts speculation tree structure based on real-time prediction uncertainty, using output entropy as a confidence indicator. It constructs deeper-narrower trees for high-confidence predictions and shallower-wider trees for uncertain predictions.

Result: SAGE improves acceptance lengths and achieves faster acceleration compared to static tree baselines, delivering up to 3.36x decoding speedup for LLaVA-OneVision-72B and 3.18x for Qwen2.5-VL-72B without output quality loss.

Conclusion: Dynamic adaptation of speculation tree structure based on prediction uncertainty significantly improves speculative decoding efficiency for vision-language models, enabling substantial inference acceleration while maintaining output quality.

Abstract: Speculative decoding has emerged as a promising approach to accelerate inference in vision-language models (VLMs) by enabling parallel verification of multiple draft tokens. However, existing methods rely on static tree structures that remain fixed throughout the decoding process, failing to adapt to the varying prediction difficulty across generation steps. This leads to suboptimal acceptance lengths and limited speedup. In this paper, we propose SAGE, a novel framework that dynamically adjusts the speculation tree structure based on real-time prediction uncertainty. Our key insight is that output entropy serves as a natural confidence indicator with strong temporal correlation across decoding steps. SAGE constructs deeper-narrower trees for high-confidence predictions to maximize speculation depth, and shallower-wider trees for uncertain predictions to diversify exploration. SAGE improves acceptance lengths and achieves faster acceleration compared to static tree baselines. Experiments on multiple benchmarks demonstrate the effectiveness of SAGE: without any loss in output quality, it delivers up to $3.36\times$ decoding speedup for LLaVA-OneVision-72B and $3.18\times$ for Qwen2.5-VL-72B.

</details>


### [76] [Enhancing Open-Vocabulary Object Detection through Multi-Level Fine-Grained Visual-Language Alignment](https://arxiv.org/abs/2602.00531)
*Tianyi Zhang,Antoine Simoulin,Kai Li,Sana Lakdawala,Shiqing Yu,Arpit Mittal,Hongyu Fu,Yu Lin*

Main category: cs.CV

TL;DR: VLDet is a novel open-vocabulary object detection framework that improves visual-language alignment through feature pyramid revamping and sigmoid-based contrastive alignment, achieving state-of-the-art performance on novel classes.


<details>
  <summary>Details</summary>
Motivation: Traditional object detection is limited to predefined categories, while open-vocabulary detection (OVD) enables identifying novel classes. Existing approaches struggle with adapting CLIP's single-scale backbone to detection frameworks and achieving robust visual-language alignment.

Method: VLDet introduces two key components: 1) VL-PUB module that revamps feature pyramid for fine-grained visual-language alignment and adapts CLIP backbone for detection, and 2) SigRPN block with sigmoid-based anchor-text contrastive alignment loss to improve novel category detection.

Result: Achieves 58.7 AP for novel classes on COCO2017 (27.6% improvement) and 24.8 AP on LVIS (6.9% improvement), surpassing all state-of-the-art methods. Also demonstrates superior zero-shot performance on closed-set object detection.

Conclusion: VLDet effectively addresses the visual-language alignment challenge in open-vocabulary detection by revamping feature pyramid and introducing novel alignment mechanisms, leading to significant performance improvements on novel object categories.

Abstract: Traditional object detection systems are typically constrained to predefined categories, limiting their applicability in dynamic environments. In contrast, open-vocabulary object detection (OVD) enables the identification of objects from novel classes not present in the training set. Recent advances in visual-language modeling have led to significant progress of OVD. However, prior works face challenges in either adapting the single-scale image backbone from CLIP to the detection framework or ensuring robust visual-language alignment. We propose Visual-Language Detection (VLDet), a novel framework that revamps feature pyramid for fine-grained visual-language alignment, leading to improved OVD performance. With the VL-PUB module, VLDet effectively exploits the visual-language knowledge from CLIP and adapts the backbone for object detection through feature pyramid. In addition, we introduce the SigRPN block, which incorporates a sigmoid-based anchor-text contrastive alignment loss to improve detection of novel categories. Through extensive experiments, our approach achieves 58.7 AP for novel classes on COCO2017 and 24.8 AP on LVIS, surpassing all state-of-the-art methods and achieving significant improvements of 27.6% and 6.9%, respectively. Furthermore, VLDet also demonstrates superior zero-shot performance on closed-set object detection.

</details>


### [77] [SADER: Structure-Aware Diffusion Framework with DEterministic Resampling for Multi-Temporal Remote Sensing Cloud Removal](https://arxiv.org/abs/2602.00536)
*Yifan Zhang,Qian Chen,Yi Liu,Wengen Li,Jihong Guan*

Main category: cs.CV

TL;DR: SADER is a structure-aware diffusion framework for multi-temporal remote sensing cloud removal that improves sampling efficiency and leverages structural/temporal priors through temporal fusion, hybrid attention, cloud-aware loss, and deterministic resampling.


<details>
  <summary>Details</summary>
Motivation: Cloud contamination severely degrades remote sensing imagery usability and poses fundamental challenges for downstream Earth observation tasks. Existing diffusion-based approaches suffer from limited sampling efficiency and insufficient exploitation of structural and temporal priors in multi-temporal scenarios.

Method: Proposes SADER framework with: 1) Scalable Multi-Temporal Conditional Diffusion Network (MTCDN) using temporal fusion and hybrid attention to capture multi-temporal/multimodal correlations; 2) Cloud-aware attention loss emphasizing cloud-dominated regions based on cloud thickness and brightness; 3) Deterministic resampling strategy for continuous diffusion models to iteratively refine samples by replacing outliers through guided correction.

Result: Extensive experiments on multiple multi-temporal datasets demonstrate that SADER consistently outperforms state-of-the-art cloud removal methods across all evaluation metrics.

Conclusion: SADER effectively addresses limitations of existing diffusion-based cloud removal methods by better exploiting structural and temporal priors while improving sampling efficiency, making it a superior solution for multi-temporal remote sensing cloud removal.

Abstract: Cloud contamination severely degrades the usability of remote sensing imagery and poses a fundamental challenge for downstream Earth observation tasks. Recently, diffusion-based models have emerged as a dominant paradigm for remote sensing cloud removal due to their strong generative capability and stable optimization. However, existing diffusion-based approaches often suffer from limited sampling efficiency and insufficient exploitation of structural and temporal priors in multi-temporal remote sensing scenarios. In this work, we propose SADER, a structure-aware diffusion framework for multi-temporal remote sensing cloud removal. SADER first develops a scalable Multi-Temporal Conditional Diffusion Network (MTCDN) to fully capture multi-temporal and multimodal correlations via temporal fusion and hybrid attention. Then, a cloud-aware attention loss is introduced to emphasize cloud-dominated regions by accounting for cloud thickness and brightness discrepancies. In addition, a deterministic resampling strategy is designed for continuous diffusion models to iteratively refine samples under fixed sampling steps by replacing outliers through guided correction. Extensive experiments on multiple multi-temporal datasets demonstrate that SADER consistently outperforms state-of-the-art cloud removal methods across all evaluation metrics. The code of SADER is publicly available at https://github.com/zyfzs0/SADER.

</details>


### [78] [NPNet: A Non-Parametric Network with Adaptive Gaussian-Fourier Positional Encoding for 3D Classification and Segmentation](https://arxiv.org/abs/2602.00542)
*Mohammad Saeid,Amir Salarpour,Pedram MohajerAnsari,Mert D. Pesé*

Main category: cs.CV

TL;DR: NPNet is a fully non-parametric 3D point-cloud method for classification and segmentation that uses deterministic operators and adaptive positional encoding without learned weights.


<details>
  <summary>Details</summary>
Motivation: To create a 3D point-cloud analysis method that doesn't rely on learned parameters, remains stable across different scales and sampling densities, and performs well in few-shot settings.

Method: Uses deterministic operators (farthest point sampling, k-nearest neighbors, pooling) with adaptive Gaussian-Fourier positional encoding whose parameters are chosen from input geometry. For segmentation, adds fixed-frequency Fourier features for global context.

Result: Achieves strong performance on ModelNet40/ModelNet-R, ScanObjectNN, and ShapeNetPart among non-parametric baselines, particularly effective in few-shot settings on ModelNet40, with favorable memory use and inference time.

Conclusion: NPNet demonstrates that effective 3D point-cloud analysis can be achieved without learned weights through adaptive geometric encoding and deterministic operators, offering advantages in few-shot learning and computational efficiency.

Abstract: We present NPNet, a fully non-parametric approach for 3D point-cloud classification and part segmentation. NPNet contains no learned weights; instead, it builds point features using deterministic operators such as farthest point sampling, k-nearest neighbors, and pooling. Our key idea is an adaptive Gaussian-Fourier positional encoding whose bandwidth and Gaussian-cosine mixing are chosen from the input geometry, helping the method remain stable across different scales and sampling densities. For segmentation, we additionally incorporate fixed-frequency Fourier features to provide global context alongside the adaptive encoding. Across ModelNet40/ModelNet-R, ScanObjectNN, and ShapeNetPart, NPNet achieves strong performance among non-parametric baselines, and it is particularly effective in few-shot settings on ModelNet40. NPNet also offers favorable memory use and inference time compared to prior non-parametric methods

</details>


### [79] [Learning to Decode Against Compositional Hallucination in Video Multimodal Large Language Models](https://arxiv.org/abs/2602.00559)
*Wenbin Xing,Quanxing Zha,Lizheng Zu,Mengran Li,Ming Li,Junchi Yan*

Main category: cs.CV

TL;DR: OmniVCHall benchmark evaluates isolated & compositional hallucinations in video VLLMs; TriCD framework improves accuracy by 10% via contrastive decoding with triple-pathway calibration.


<details>
  <summary>Details</summary>
Motivation: Current research focuses on isolated error types in video hallucination, leaving compositional hallucinations (incorrect reasoning over multiple spatial-temporal factors) underexplored.

Method: Introduces OmniVCHall benchmark with diverse video domains, camera-based hallucination type, fine-grained taxonomy, and adversarial answer options. Proposes TriCD framework with triple-pathway calibration: adaptive perturbation controller for negative video variants and saliency-guided enhancement for visual evidence reinforcement, optimized via reinforcement learning.

Result: Evaluation of 39 VLLMs shows even advanced models (Qwen3-VL, GPT-5) exhibit substantial performance degradation. TriCD consistently improves performance across two backbones, achieving average accuracy improvement over 10%.

Conclusion: OmniVCHall systematically addresses compositional hallucinations in video VLLMs, and TriCD framework effectively mitigates these issues through contrastive decoding with adaptive calibration mechanisms.

Abstract: Current research on video hallucination mitigation primarily focuses on isolated error types, leaving compositional hallucinations, arising from incorrect reasoning over multiple interacting spatial and temporal factors largely underexplored. We introduce OmniVCHall, a benchmark designed to systematically evaluate both isolated and compositional hallucinations in video multimodal large language models (VLLMs). OmniVCHall spans diverse video domains, introduces a novel camera-based hallucination type, and defines a fine-grained taxonomy, together with adversarial answer options (e.g., "All are correct" and "None of the above") to prevent shortcut reasoning. The evaluations of 39 representative VLLMs reveal that even advanced models (e.g., Qwen3-VL and GPT-5) exhibit substantial performance degradation. We propose TriCD, a contrastive decoding framework with a triple-pathway calibration mechanism. An adaptive perturbation controller dynamically selects distracting operations to construct negative video variants, while a saliency-guided enhancement module adaptively reinforces grounded token-wise visual evidences. These components are optimized via reinforcement learning to encourage precise decision-making under compositional hallucination settings. Experimental results show that TriCD consistently improves performance across two representative backbones, achieving an average accuracy improvement of over 10%. The data and code can be find at https://github.com/BMRETURN/OmniVCHall.

</details>


### [80] [GLAD: Generative Language-Assisted Visual Tracking for Low-Semantic Templates](https://arxiv.org/abs/2602.00570)
*Xingyu Luo,Yidong Cai,Jie Liu,Jie Tang,Gangshan Wu,Limin Wang*

Main category: cs.CV

TL;DR: GLAD introduces a generative language-assisted tracking model using diffusion models to fuse text descriptions and template images, improving compatibility between language and image features for better vision-language tracking performance.


<details>
  <summary>Details</summary>
Motivation: Current vision-language trackers struggle with low-semantic images (blurry, low-resolution) that degrade cross-modal understanding. Direct concatenation/fusion of textual and visual features has limited effectiveness due to the gap between modalities.

Method: Proposes GLAD, a generative language-assisted tracking model that uses diffusion models for generative multi-modal fusion of text descriptions and template images to enhance compatibility between language and image features and improve template semantic information.

Result: Establishes new state-of-the-art on multiple benchmarks with impressive inference speed. Blurry and semantically ambiguous template images can be restored to improve multi-modal features in the generative fusion paradigm.

Conclusion: GLAD demonstrates significant improvements over existing fusion paradigms by using generative diffusion models to better integrate language and visual information for vision-language tracking, particularly effective for low-semantic images.

Abstract: Vision-language tracking has gained increasing attention in many scenarios. This task simultaneously deals with visual and linguistic information to localize objects in videos. Despite its growing utility, the development of vision-language tracking methods remains in its early stage. Current vision-language trackers usually employ Transformer architectures for interactive integration of template, search, and text features. However, persistent challenges about low-semantic images including prevalent image blurriness, low resolution and so on, may compromise model performance through degraded cross-modal understanding. To solve this problem, language assistance is usually used to deal with the obstacles posed by low-semantic images. However, due to the existing gap between current textual and visual features, direct concatenation and fusion of these features may have limited effectiveness. To address these challenges, we introduce a pioneering Generative Language-AssisteD tracking model, GLAD, which utilizes diffusion models for the generative multi-modal fusion of text description and template image to bolster compatibility between language and image and enhance template image semantic information. Our approach demonstrates notable improvements over the existing fusion paradigms. Blurry and semantically ambiguous template images can be restored to improve multi-modal features in the generative fusion paradigm. Experiments show that our method establishes a new state-of-the-art on multiple benchmarks and achieves an impressive inference speed. The code and models will be released at: https://github.com/Confetti-lxy/GLAD

</details>


### [81] [Bridging Degradation Discrimination and Generation for Universal Image Restoration](https://arxiv.org/abs/2602.00579)
*JiaKui Hu,Zhengjian Yao,Lujia Jin,Yanye Lu*

Main category: cs.CV

TL;DR: BDG is a universal image restoration method that combines degradation discrimination via MAS-GLCM with a three-stage diffusion training process to handle multiple degradations while preserving texture details.


<details>
  <summary>Details</summary>
Motivation: Universal image restoration faces challenges in sampling high-quality image distributions and adapting to different degradation types/levels. Current methods struggle with fine-grained degradation discrimination while maintaining restoration quality across multi-task scenarios.

Method: Proposes BDG with two key components: 1) MAS-GLCM for fine-grained degradation type/level discrimination, and 2) Three-stage diffusion training (generation, bridging, restoration) that integrates discriminative information while preserving texture restoration capabilities.

Result: Achieves significant performance gains in all-in-one restoration and real-world super-resolution tasks, with substantial fidelity improvements without compromising perceptual quality, without changing model architecture.

Conclusion: BDG effectively bridges degradation discrimination and generation, enhancing universal image restoration by combining fine-grained degradation analysis with diffusion-based restoration in a unified framework.

Abstract: Universal image restoration is a critical task in low-level vision, requiring the model to remove various degradations from low-quality images to produce clean images with rich detail. The challenges lie in sampling the distribution of high-quality images and adjusting the outputs on the basis of the degradation. This paper presents a novel approach, Bridging Degradation discrimination and Generation (BDG), which aims to address these challenges concurrently. First, we propose the Multi-Angle and multi-Scale Gray Level Co-occurrence Matrix (MAS-GLCM) and demonstrate its effectiveness in performing fine-grained discrimination of degradation types and levels. Subsequently, we divide the diffusion training process into three distinct stages: generation, bridging, and restoration. The objective is to preserve the diffusion model's capability of restoring rich textures while simultaneously integrating the discriminative information from the MAS-GLCM into the restoration process. This enhances its proficiency in addressing multi-task and multi-degraded scenarios. Without changing the architecture, BDG achieves significant performance gains in all-in-one restoration and real-world super-resolution tasks, primarily evidenced by substantial improvements in fidelity without compromising perceptual quality. The code and pretrained models are provided in https://github.com/MILab-PKU/BDG.

</details>


### [82] [MAUGen: A Unified Diffusion Approach for Multi-Identity Facial Expression and AU Label Generation](https://arxiv.org/abs/2602.00583)
*Xiangdong Li,Ye Lou,Ao Gao,Wei Zhang,Siyang Song*

Main category: cs.CV

TL;DR: MAUGen is a diffusion-based framework that generates photorealistic facial expressions with precise Action Unit annotations from text prompts, creating a large synthetic dataset (MIFA) to address data scarcity in AU recognition.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the fundamental bottleneck in developing generalizable Action Unit recognition systems: the lack of large-scale, demographically diverse face images with precise AU occurrence and intensity annotations.

Method: MAUGen uses a diffusion-based multi-modal framework with two key modules: (1) Multi-modal Representation Learning (MRL) that captures relationships among text, facial identity, expression images, and AU activations in a unified latent space; and (2) Diffusion-based Image label Generator (DIG) that decodes joint representations into aligned facial image-label pairs across diverse identities.

Result: The framework creates MIFA (Multi-Identity Facial Action), a large-scale multimodal synthetic dataset with comprehensive AU annotations and identity variations. MAUGen outperforms existing methods in synthesizing photorealistic, demographically diverse facial images with semantically aligned AU labels.

Conclusion: MAUGen provides an effective solution to the data scarcity problem in AU recognition by generating high-quality synthetic data with precise annotations, enabling the development of more generalizable facial expression analysis systems.

Abstract: The lack of large-scale, demographically diverse face images with precise Action Unit (AU) occurrence and intensity annotations has long been recognized as a fundamental bottleneck in developing generalizable AU recognition systems. In this paper, we propose MAUGen, a diffusion-based multi-modal framework that jointly generates a large collection of photorealistic facial expressions and anatomically consistent AU labels, including both occurrence and intensity, conditioned on a single descriptive text prompt. Our MAUGen involves two key modules: (1) a Multi-modal Representation Learning (MRL) module that captures the relationships among the paired textual description, facial identity, expression image, and AU activations within a unified latent space; and (2) a Diffusion-based Image label Generator (DIG) that decodes the joint representation into aligned facial image-label pairs across diverse identities. Under this framework, we introduce Multi-Identity Facial Action (MIFA), a large-scale multimodal synthetic dataset featuring comprehensive AU annotations and identity variations. Extensive experiments demonstrate that MAUGen outperforms existing methods in synthesizing photorealistic, demographically diverse facial images along with semantically aligned AU labels.

</details>


### [83] [From Pixels to Facts (Pix2Fact): Benchmarking Multi-Hop Reasoning for Fine-Grained Visual Fact Checking](https://arxiv.org/abs/2602.00593)
*Yifan Jiang,Cong Zhang,Bofei Zhang,Yifan Yang,Bingzhang Wang,Yew-Soon Ong*

Main category: cs.CV

TL;DR: Pix2Fact is a new visual QA benchmark requiring expert-level perception and knowledge-intensive multi-hop reasoning, where state-of-the-art VLMs achieve only 24% accuracy vs human 56%.


<details>
  <summary>Details</summary>
Motivation: Current VLMs struggle with tasks requiring both detailed visual grounding and deliberate knowledge-based reasoning, but existing benchmarks evaluate these skills separately rather than their synergy.

Method: Created Pix2Fact benchmark with 1,000 high-resolution (4K+) images across 8 daily-life scenarios, with questions/answers crafted by PhD annotators from top universities in partnership with professional data annotation firm.

Result: Evaluation of 9 state-of-the-art VLMs (including Gemini-3-Pro and GPT-5) shows only 24.0% average accuracy, significantly lower than human performance of 56%.

Conclusion: Pix2Fact reveals substantial limitations in current VLMs' visual comprehension and will serve as a critical benchmark to drive development of next-generation multimodal agents combining fine-grained perception with robust knowledge-based reasoning.

Abstract: Despite progress on general tasks, VLMs struggle with challenges demanding both detailed visual grounding and deliberate knowledge-based reasoning, a synergy not captured by existing benchmarks that evaluate these skills separately. To close this gap, we introduce Pix2Fact, a new visual question-answering benchmark designed to evaluate expert-level perception and knowledge-intensive multi-hop reasoning. Pix2Fact contains 1,000 high-resolution (4K+) images spanning 8 daily-life scenarios and situations, with questions and answers meticulously crafted by annotators holding PhDs from top global universities working in partnership with a professional data annotation firm. Each question requires detailed visual grounding, multi-hop reasoning, and the integration of external knowledge to answer. Our evaluation of 9 state-of-the-art VLMs, including proprietary models like Gemini-3-Pro and GPT-5, reveals the substantial challenge posed by Pix2Fact: the most advanced model achieves only 24.0% average accuracy, in stark contrast to human performance of 56%. This significant gap underscores the limitations of current models in replicating human-level visual comprehension. We believe Pix2Fact will serve as a critical benchmark to drive the development of next-generation multimodal agents that combine fine-grained perception with robust, knowledge-based reasoning.

</details>


### [84] [Tune-Your-Style: Intensity-tunable 3D Style Transfer with Gaussian Splatting](https://arxiv.org/abs/2602.00618)
*Yian Zhao,Rushi Ye,Ruochong Zheng,Zesen Cheng,Chaoran Feng,Jiashu Yang,Pengchong Qiao,Chang Liu,Jie Chen*

Main category: cs.CV

TL;DR: Tune-Your-Style enables intensity-tunable 3D style transfer where users can flexibly adjust style intensity to balance content and artistic style preferences.


<details>
  <summary>Details</summary>
Motivation: Existing 3DGS-based style transfer methods use fixed-output paradigms that cannot adapt to diverse user preferences for content-style balance, limiting customizability.

Method: Introduces Gaussian neurons to model style intensity and a learnable style tuner for intensity control. Uses tunable stylization guidance with cross-view style alignment from diffusion models and two-stage optimization with modulated guidance balance.

Result: Method delivers visually appealing results with flexible customizability for 3D style transfer, allowing users to adjust style intensity to match desired content-style balance.

Conclusion: Proposes a novel intensity-tunable 3D style transfer paradigm that enhances customizability by allowing flexible adjustment of style intensity, addressing limitations of fixed-output approaches.

Abstract: 3D style transfer refers to the artistic stylization of 3D assets based on reference style images. Recently, 3DGS-based stylization methods have drawn considerable attention, primarily due to their markedly enhanced training and rendering speeds. However, a vital challenge for 3D style transfer is to strike a balance between the content and the patterns and colors of the style. Although the existing methods strive to achieve relatively balanced outcomes, the fixed-output paradigm struggles to adapt to the diverse content-style balance requirements from different users. In this work, we introduce a creative intensity-tunable 3D style transfer paradigm, dubbed \textbf{Tune-Your-Style}, which allows users to flexibly adjust the style intensity injected into the scene to match their desired content-style balance, thus enhancing the customizability of 3D style transfer. To achieve this goal, we first introduce Gaussian neurons to explicitly model the style intensity and parameterize a learnable style tuner to achieve intensity-tunable style injection. To facilitate the learning of tunable stylization, we further propose the tunable stylization guidance, which obtains multi-view consistent stylized views from diffusion models through cross-view style alignment, and then employs a two-stage optimization strategy to provide stable and efficient guidance by modulating the balance between full-style guidance from the stylized views and zero-style guidance from the initial rendering. Extensive experiments demonstrate that our method not only delivers visually appealing results, but also exhibits flexible customizability for 3D style transfer. Project page is available at https://zhao-yian.github.io/TuneStyle.

</details>


### [85] [Towards Interpretable Hallucination Analysis and Mitigation in LVLMs via Contrastive Neuron Steering](https://arxiv.org/abs/2602.00621)
*Guangtao Lyu,Xinyi Cheng,Qi Liu,Chenghao Xu,Jiexi Yan,Muli Yang,Fen Fang,Cheng Deng*

Main category: cs.CV

TL;DR: The paper introduces Contrastive Neuron Steering (CNS), a method that reduces hallucinations in LVLMs by identifying and controlling image-specific neurons through contrastive analysis, operating at the prefilling stage to complement existing decoding methods.


<details>
  <summary>Details</summary>
Motivation: Current LVLM hallucination mitigation focuses on output-level adjustments without exploring internal mechanisms. The paper aims to understand and address hallucinations at the representation level by analyzing how visual embeddings decompose into interpretable neurons.

Method: Uses sparse autoencoders to decompose dense visual embeddings into sparse neurons, identifies image-specific vs. always-on neurons, and proposes Contrastive Neuron Steering (CNS) that applies contrastive analysis between clean/noisy inputs to selectively amplify informative neurons while suppressing perturbation-induced activations.

Result: CNS consistently reduces hallucinations while preserving overall multimodal understanding across hallucination-focused and general multimodal benchmarks. It's fully compatible with existing decoding-stage methods and operates at the prefilling stage.

Conclusion: Hallucinations in LVLMs stem from disruptions of image-specific neurons, and controllable intervention through neuron steering (CNS) effectively mitigates hallucinations while maintaining visual grounding, offering a representation-level approach complementary to existing methods.

Abstract: LVLMs achieve remarkable multimodal understanding and generation but remain susceptible to hallucinations. Existing mitigation methods predominantly focus on output-level adjustments, leaving the internal mechanisms that give rise to these hallucinations largely unexplored. To gain a deeper understanding, we adopt a representation-level perspective by introducing sparse autoencoders (SAEs) to decompose dense visual embeddings into sparse, interpretable neurons. Through neuron-level analysis, we identify distinct neuron types, including always-on neurons and image-specific neurons. Our findings reveal that hallucinations often result from disruptions or spurious activations of image-specific neurons, while always-on neurons remain largely stable. Moreover, selectively enhancing or suppressing image-specific neurons enables controllable intervention in LVLM outputs, improving visual grounding and reducing hallucinations. Building on these insights, we propose Contrastive Neuron Steering (CNS), which identifies image-specific neurons via contrastive analysis between clean and noisy inputs. CNS selectively amplifies informative neurons while suppressing perturbation-induced activations, producing more robust and semantically grounded visual representations. This not only enhances visual understanding but also effectively mitigates hallucinations. By operating at the prefilling stage, CNS is fully compatible with existing decoding-stage methods. Extensive experiments on both hallucination-focused and general multimodal benchmarks demonstrate that CNS consistently reduces hallucinations while preserving overall multimodal understanding.

</details>


### [86] [FaceSnap: Enhanced ID-fidelity Network for Tuning-free Portrait Customization](https://arxiv.org/abs/2602.00627)
*Benxiang Zhai,Yifang Xu,Guofeng Zhang,Yang Li,Sidan Du*

Main category: cs.CV

TL;DR: FaceSnap: A plug-and-play Stable Diffusion method for personalized portrait generation using single reference image with high facial detail fidelity and pose diversity.


<details>
  <summary>Details</summary>
Motivation: Existing personalized portrait generation methods either require time-consuming fine-tuning and lack generalizability, or fail to achieve high fidelity in facial details. There's a need for a method that can generate highly consistent, detailed portraits from just a single reference image without extensive training.

Method: FaceSnap uses Stable Diffusion with three key components: 1) Facial Attribute Mixer that extracts comprehensive fused information from both low-level specific features and high-level abstract features, 2) Landmark Predictor that maintains reference identity across different poses for spatial control, and 3) ID-preserving module to inject these features into UNet. The method is plug-and-play and works with a single reference image in one inference stage.

Result: Experimental results show FaceSnap performs remarkably in personalized and customized portrait generation, surpassing other state-of-the-art methods in this domain. It produces extremely consistent results with high facial detail fidelity.

Conclusion: FaceSnap addresses key limitations in personalized portrait generation by providing a plug-and-play solution that requires only a single reference image, achieves high facial detail fidelity, maintains identity consistency across poses, and outperforms existing methods.

Abstract: Benefiting from the significant advancements in text-to-image diffusion models, research in personalized image generation, particularly customized portrait generation, has also made great strides recently. However, existing methods either require time-consuming fine-tuning and lack generalizability or fail to achieve high fidelity in facial details. To address these issues, we propose FaceSnap, a novel method based on Stable Diffusion (SD) that requires only a single reference image and produces extremely consistent results in a single inference stage. This method is plug-and-play and can be easily extended to different SD models. Specifically, we design a new Facial Attribute Mixer that can extract comprehensive fused information from both low-level specific features and high-level abstract features, providing better guidance for image generation. We also introduce a Landmark Predictor that maintains reference identity across landmarks with different poses, providing diverse yet detailed spatial control conditions for image generation. Then we use an ID-preserving module to inject these into the UNet. Experimental results demonstrate that our approach performs remarkably in personalized and customized portrait generation, surpassing other state-of-the-art methods in this domain.

</details>


### [87] [S$^3$POT: Contrast-Driven Face Occlusion Segmentation via Self-Supervised Prompt Learning](https://arxiv.org/abs/2602.00635)
*Lingsong Wang,Mancheng Meng,Ziyan Wu,Terrence Chen,Fan Yang,Dinggang Shen*

Main category: cs.CV

TL;DR: S³POT: A self-supervised framework for face occlusion segmentation using face generation and spatial prompting without occlusion ground truth masks.


<details>
  <summary>Details</summary>
Motivation: Existing face parsing methods struggle with occlusions because occlusions are high-level concepts with infinite possible object categories, making comprehensive dataset collection and accurate mask annotation impractical.

Method: Three-module framework: 1) Reference Generation creates occlusion-free face using structural guidance, 2) Feature Enhancement contrasts tokens between raw/reference images for initial prompts, 3) Prompt Selection constructs positive/negative prompts screened by self-attention network. Uses three novel objective functions without occlusion ground truth.

Result: Extensive experiments on dedicated dataset demonstrate superior performance and effectiveness of each module.

Conclusion: S³POT successfully addresses face occlusion segmentation through synergistic combination of face generation and self-supervised spatial prompting, eliminating need for occlusion ground truth masks.

Abstract: Existing face parsing methods usually misclassify occlusions as facial components. This is because occlusion is a high-level concept, it does not refer to a concrete category of object. Thus, constructing a real-world face dataset covering all categories of occlusion object is almost impossible and accurate mask annotation is labor-intensive. To deal with the problems, we present S$^3$POT, a contrast-driven framework synergizing face generation with self-supervised spatial prompting, to achieve occlusion segmentation. The framework is inspired by the insights: 1) Modern face generators' ability to realistically reconstruct occluded regions, creating an image that preserve facial geometry while eliminating occlusion, and 2) Foundation segmentation models' (e.g., SAM) capacity to extract precise mask when provided with appropriate prompts. In particular, S$^3$POT consists of three modules: Reference Generation (RF), Feature enhancement (FE), and Prompt Selection (PS). First, a reference image is produced by RF using structural guidance from parsed mask. Second, FE performs contrast of tokens between raw and reference images to obtain an initial prompt, then modifies image features with the prompt by cross-attention. Third, based on the enhanced features, PS constructs a set of positive and negative prompts and screens them with a self-attention network for a mask decoder. The network is learned under the guidance of three novel and complementary objective functions without occlusion ground truth mask involved. Extensive experiments on a dedicatedly collected dataset demonstrate S$^3$POT's superior performance and the effectiveness of each module.

</details>


### [88] [Any3D-VLA: Enhancing VLA Robustness via Diverse Point Clouds](https://arxiv.org/abs/2602.00807)
*Xianzhe Fan,Shengliang Deng,Xiaoyang Wu,Yuxiang Lu,Zhuoling Li,Mi Yan,Yujia Zhang,Zhizheng Zhang,He Wang,Hengshuang Zhao*

Main category: cs.CV

TL;DR: Any3D-VLA enhances Vision-Language-Action models by incorporating 3D point cloud representations alongside 2D images to improve spatial understanding in complex scenes.


<details>
  <summary>Details</summary>
Motivation: Existing VLA models rely on 2D images, which limits their spatial understanding in complex 3D environments. The authors aim to incorporate 3D information to enhance VLA capabilities.

Method: Propose Any3D-VLA, which unifies simulator, sensor, and model-estimated point clouds in a training pipeline, constructs diverse inputs, and learns domain-agnostic 3D representations that are fused with corresponding 2D representations.

Result: Simulation and real-world experiments demonstrate Any3D-VLA's advantages in improving performance and mitigating domain gaps caused by cross-environment differences and depth-scale biases.

Conclusion: Explicitly lifting visual input into point clouds yields representations that better complement 2D representations, and the proposed Any3D-VLA framework effectively addresses 3D data scarcity and domain gap challenges.

Abstract: Existing Vision-Language-Action (VLA) models typically take 2D images as visual input, which limits their spatial understanding in complex scenes. How can we incorporate 3D information to enhance VLA capabilities? We conduct a pilot study across different observation spaces and visual representations. The results show that explicitly lifting visual input into point clouds yields representations that better complement their corresponding 2D representations. To address the challenges of (1) scarce 3D data and (2) the domain gap induced by cross-environment differences and depth-scale biases, we propose Any3D-VLA. It unifies the simulator, sensor, and model-estimated point clouds within a training pipeline, constructs diverse inputs, and learns domain-agnostic 3D representations that are fused with the corresponding 2D representations. Simulation and real-world experiments demonstrate Any3D-VLA's advantages in improving performance and mitigating the domain gap. Our project homepage is available at https://xianzhefan.github.io/Any3D-VLA.github.io.

</details>


### [89] [VIZOR: Viewpoint-Invariant Zero-Shot Scene Graph Generation for 3D Scene Reasoning](https://arxiv.org/abs/2602.00637)
*Vivek Madhavaram,Vartika Sengar,Arkadipta De,Charu Sharma*

Main category: cs.CV

TL;DR: VIZOR is a training-free, end-to-end framework for viewpoint-invariant 3D scene graph generation that constructs dense scene graphs directly from raw 3D scenes without requiring annotated training data.


<details>
  <summary>Details</summary>
Motivation: Existing scene graph generation methods struggle with generalization and produce inaccurate spatial relationships (like "left/right") that become inconsistent across different viewpoints, as they rely on multiple inputs from specific reference views.

Method: VIZOR constructs dense, viewpoint-invariant 3D scene graphs directly from raw 3D scenes. It defines spatial relationships relative to each object's front-facing direction, making them consistent regardless of reference view, and infers open-vocabulary relationships without requiring annotated training data.

Result: VIZOR outperforms state-of-the-art methods in scene graph generation and achieves 22% and 4.81% gains in zero-shot grounding accuracy on Replica and Nr3D datasets respectively.

Conclusion: VIZOR provides an effective training-free solution for generating unambiguous, viewpoint-invariant 3D scene graphs that enable better scene understanding and reasoning across different viewpoints.

Abstract: Scene understanding and reasoning has been a fundamental problem in 3D computer vision, requiring models to identify objects, their properties, and spatial or comparative relationships among the objects. Existing approaches enable this by creating scene graphs using multiple inputs such as 2D images, depth maps, object labels, and annotated relationships from specific reference view. However, these methods often struggle with generalization and produce inaccurate spatial relationships like "left/right", which become inconsistent across different viewpoints. To address these limitations, we propose Viewpoint-Invariant Zero-shot scene graph generation for 3D scene Reasoning (VIZOR). VIZOR is a training-free, end-to-end framework that constructs dense, viewpoint-invariant 3D scene graphs directly from raw 3D scenes. The generated scene graph is unambiguous, as spatial relationships are defined relative to each object's front-facing direction, making them consistent regardless of the reference view. Furthermore, it infers open-vocabulary relationships that describe spatial and proximity relationships among scene objects without requiring annotated training data. We conduct extensive quantitative and qualitative evaluations to assess the effectiveness of VIZOR in scene graph generation and downstream tasks, such as query-based object grounding. VIZOR outperforms state-of-the-art methods, showing clear improvements in scene graph generation and achieving 22% and 4.81% gains in zero-shot grounding accuracy on the Replica and Nr3D datasets, respectively.

</details>


### [90] [VVLoc: Prior-free 3-DoF Vehicle Visual Localization](https://arxiv.org/abs/2602.00810)
*Ze Huang,Zhongyang Xiao,Mingliang Song,Longan Yang,Hongyuan Yuan,Li Sun*

Main category: cs.CV

TL;DR: VVLoc is a unified neural network pipeline that simultaneously performs topological and metric vehicle localization using multi-camera systems, with confidence estimation and efficient training requiring only visual data and ground-truth poses.


<details>
  <summary>Details</summary>
Motivation: Current localization methods have several limitations: they handle topological and metric localization separately, use single-camera setups, require additional 3D semantic or pose priors, and lack confidence quantification mechanisms, making them impractical for real industrial applications.

Method: VVLoc uses a single neural network with multi-camera inputs to first evaluate geo-proximity between visual observations (topological localization), then estimate relative metric poses through a matching strategy, while also providing confidence measures. Training is efficient, requiring only pairs of visual data and corresponding ground-truth poses.

Result: VVLoc achieves state-of-the-art localization accuracy across a wide range of localization tasks, demonstrated on both publicly available datasets and a more challenging self-collected dataset.

Conclusion: VVLoc provides a practical, unified solution for vehicle localization that addresses the limitations of conventional methods by combining topological and metric localization with confidence estimation, using only visual data and ground-truth poses for efficient training.

Abstract: Localization is a critical technology in autonomous driving, encompassing both topological localization, which identifies the most similar map keyframe to the current observation, and metric localization, which provides precise spatial coordinates. Conventional methods typically address these tasks independently, rely on single-camera setups, and often require additional 3D semantic or pose priors, while lacking mechanisms to quantify the confidence of localization results, making them less feasible for real industrial applications. In this paper, we propose VVLoc, a unified pipeline that employs a single neural network to concurrently achieve topological and metric vehicle localization using multi-camera system. VVLoc first evaluates the geo-proximity between visual observations, then estimates their relative metric poses using a matching strategy, while also providing a confidence measure. Additionally, the training process for VVLoc is highly efficient, requiring only pairs of visual data and corresponding ground-truth poses, eliminating the need for complex supplementary data. We evaluate VVLoc not only on the publicly available datasets, but also on a more challenging self-collected dataset, demonstrating its ability to deliver state-of-the-art localization accuracy across a wide range of localization tasks.

</details>


### [91] [Diff-PC: Identity-preserving and 3D-aware Controllable Diffusion for Zero-shot Portrait Customization](https://arxiv.org/abs/2602.00639)
*Yifang Xu,Benxiang Zhai,Chenyu Zhang,Ming Li,Yang Li,Sidan Du*

Main category: cs.CV

TL;DR: Diff-PC: A diffusion-based framework for zero-shot portrait customization with high identity fidelity, facial attribute control, and diverse background generation.


<details>
  <summary>Details</summary>
Motivation: Existing portrait customization methods lack precise identity preservation and facial control, limiting their practical applications.

Method: Uses 3D face reconstruction for facial priors, ID-Encoder for local/global feature fusion, ID-Ctrl for feature alignment, ID-Injector for enhanced fidelity, and trains on ID-centric dataset.

Result: Surpasses state-of-the-art methods in identity preservation, facial control, and text-to-image consistency; compatible with multi-style foundation models.

Conclusion: Diff-PC provides an effective solution for zero-shot portrait customization with superior identity fidelity and facial controllability.

Abstract: Portrait customization (PC) has recently garnered significant attention due to its potential applications. However, existing PC methods lack precise identity (ID) preservation and face control. To address these tissues, we propose Diff-PC, a diffusion-based framework for zero-shot PC, which generates realistic portraits with high ID fidelity, specified facial attributes, and diverse backgrounds. Specifically, our approach employs the 3D face predictor to reconstruct the 3D-aware facial priors encompassing the reference ID, target expressions, and poses. To capture fine-grained face details, we design ID-Encoder that fuses local and global facial features. Subsequently, we devise ID-Ctrl using the 3D face to guide the alignment of ID features. We further introduce ID-Injector to enhance ID fidelity and facial controllability. Finally, training on our collected ID-centric dataset improves face similarity and text-to-image (T2I) alignment. Extensive experiments demonstrate that Diff-PC surpasses state-of-the-art methods in ID preservation, facial control, and T2I consistency. Furthermore, our method is compatible with multi-style foundation models.

</details>


### [92] [Navigating Simply, Aligning Deeply: Winning Solutions for Mouse vs. AI 2025](https://arxiv.org/abs/2602.00982)
*Phu-Hoa Pham,Chi-Nguyen Tran,Dao Sy Duy Minh,Nguyen Lam Phu Quy,Huynh Trung Kiet*

Main category: cs.CV

TL;DR: The paper presents winning approaches for both tracks of NeurIPS 2025 Mouse vs. AI competition, showing that simple CNNs excel at visual robustness while deeper ResNet-like models achieve better neural alignment.


<details>
  <summary>Details</summary>
Motivation: To address critical challenges in visual robustness and neural alignment for developing artificial agents that can match biological vision systems, particularly through the NeurIPS 2025 Mouse vs. AI competition.

Method: For Track 1 (Visual Robustness): lightweight two-layer CNN with Gated Linear Units and observation normalization. For Track 2 (Neural Alignment): deep ResNet-like architecture with 16 convolutional layers and GLU-based gating. Systematic analysis of ten model checkpoints trained between 60K to 1.14M steps with comprehensive ablation studies.

Result: Track 1 achieved 95.4% final score with simple architecture; Track 2 achieved top-1 neural prediction performance with 17.8M parameters. Training duration showed non-monotonic relationship with performance, with optimal results around 200K steps.

Conclusion: Simpler architectures excel at visual robustness while deeper models with increased capacity achieve better neural alignment, challenging conventional assumptions about model complexity in visuomotor learning and offering practical guidance for biologically-inspired visual agents.

Abstract: Visual robustness and neural alignment remain critical challenges in developing artificial agents that can match biological vision systems. We present the winning approaches from Team HCMUS_TheFangs for both tracks of the NeurIPS 2025 Mouse vs. AI: Robust Visual Foraging Competition. For Track 1 (Visual Robustness), we demonstrate that architectural simplicity combined with targeted components yields superior generalization, achieving 95.4% final score with a lightweight two-layer CNN enhanced by Gated Linear Units and observation normalization. For Track 2 (Neural Alignment), we develop a deep ResNet-like architecture with 16 convolutional layers and GLU-based gating that achieves top-1 neural prediction performance with 17.8 million parameters. Our systematic analysis of ten model checkpoints trained between 60K to 1.14M steps reveals that training duration exhibits a non-monotonic relationship with performance, with optimal results achieved around 200K steps. Through comprehensive ablation studies and failure case analysis, we provide insights into why simpler architectures excel at visual robustness while deeper models with increased capacity achieve better neural alignment. Our results challenge conventional assumptions about model complexity in visuomotor learning and offer practical guidance for developing robust, biologically-inspired visual agents.

</details>


### [93] [A Hybrid Mamba-SAM Architecture for Efficient 3D Medical Image Segmentation](https://arxiv.org/abs/2602.00650)
*Mohammadreza Gholipour Shahraki,Mehdi Rezaeian,Mohammad Ghasemzadeh*

Main category: cs.CV

TL;DR: Mamba-SAM combines frozen SAM encoder with Mamba SSMs for efficient 3D medical image segmentation, using dual-branch or adapter approaches with Multi-Frequency Gated Convolution to address domain shift and computational cost issues.


<details>
  <summary>Details</summary>
Motivation: Foundation models like SAM struggle with medical imaging due to domain shift, 2D design limitations, and high computational costs of fine-tuning for 3D segmentation tasks.

Method: Two parameter-efficient adaptation strategies: 1) Dual-branch architecture fusing frozen SAM features with trainable VMamba encoder via cross-attention, 2) Adapter-based approach injecting lightweight 3D-aware Tri-Plane Mamba modules into frozen SAM ViT encoder, enhanced with Multi-Frequency Gated Convolution for spatial-frequency analysis.

Result: Dual-branch Mamba-SAM-Base achieves mean Dice score of 0.906 on ACDC cardiac MRI dataset, comparable to UNet++ (0.907), with superior Myocardium (0.910) and Left Ventricle (0.971) segmentation. TP MFGC variant offers 4.77 FPS inference speed with 0.880 Dice accuracy.

Conclusion: Hybridizing foundation models with efficient SSM-based architectures provides practical and effective solution for 3D medical image segmentation, balancing accuracy and computational efficiency.

Abstract: Accurate segmentation of 3D medical images such as MRI and CT is essential for clinical diagnosis and treatment planning. Foundation models like the Segment Anything Model (SAM) provide powerful general-purpose representations but struggle in medical imaging due to domain shift, their inherently 2D design, and the high computational cost of fine-tuning. To address these challenges, we propose Mamba-SAM, a novel and efficient hybrid architecture that combines a frozen SAM encoder with the linear-time efficiency and long-range modeling capabilities of Mamba-based State Space Models (SSMs). We investigate two parameter-efficient adaptation strategies. The first is a dual-branch architecture that explicitly fuses general features from a frozen SAM encoder with domain-specific representations learned by a trainable VMamba encoder using cross-attention. The second is an adapter-based approach that injects lightweight, 3D-aware Tri-Plane Mamba (TPMamba) modules into the frozen SAM ViT encoder to implicitly model volumetric context. Within this framework, we introduce Multi-Frequency Gated Convolution (MFGC), which enhances feature representation by jointly analyzing spatial and frequency-domain information via 3D discrete cosine transforms and adaptive gating. Extensive experiments on the ACDC cardiac MRI dataset demonstrate the effectiveness of the proposed methods. The dual-branch Mamba-SAM-Base model achieves a mean Dice score of 0.906, comparable to UNet++ (0.907), while outperforming all baselines on Myocardium (0.910) and Left Ventricle (0.971) segmentation. The adapter-based TP MFGC variant offers superior inference speed (4.77 FPS) with strong accuracy (0.880 Dice). These results show that hybridizing foundation models with efficient SSM-based architectures provides a practical and effective solution for 3D medical image segmentation.

</details>


### [94] [Improving Robustness of Vision-Language-Action Models by Restoring Corrupted Visual Inputs](https://arxiv.org/abs/2602.01158)
*Daniel Yezid Guarnizo Orjuela,Leonardo Scappatura,Veronica Di Gennaro,Riccardo Andrea Izzo,Gianluca Bardaro,Matteo Matteucci*

Main category: cs.CV

TL;DR: VLA models for robotics are fragile to image corruptions (noise, dead pixels, contaminants), dropping from 90% to 2% success. CRT (Corruption Restoration Transformer) is introduced as a plug-and-play vision transformer that restores clean observations from corrupted inputs without fine-tuning, recovering near-baseline performance.


<details>
  <summary>Details</summary>
Motivation: VLA models are promising for robotic manipulation but vulnerable to real-world visual disturbances like image corruptions (sensor artifacts, noise, contaminants). While physical occlusions are well-studied, sensor-level corruptions remain unexplored but critically degrade performance from 90% to 2% success rates, hindering reliable deployment.

Method: Introduces CRT (Corruption Restoration Transformer), a plug-and-play, model-agnostic vision transformer. Uses adversarial training to restore clean observations from corrupted inputs without computationally expensive fine-tuning of the underlying VLA model. Works as a preprocessing module that immunizes VLAs against sensor disturbances.

Result: Extensive experiments on LIBERO and Meta-World benchmarks show CRT effectively recovers lost performance. Enables VLAs (π₀.₅ and SmolVLA) to maintain near-baseline success rates even under severe visual corruption, addressing the catastrophic degradation from 90% to 2% success.

Conclusion: Image corruptions are a critical vulnerability for VLA models in real-world deployment. CRT provides an effective, practical solution as a plug-and-play restoration module that makes VLAs robust to sensor disturbances without requiring model retraining, enabling more reliable robotic manipulation.

Abstract: Vision-Language-Action (VLA) models have emerged as a dominant paradigm for generalist robotic manipulation, unifying perception and control within a single end-to-end architecture. However, despite their success in controlled environments, reliable real-world deployment is severely hindered by their fragility to visual disturbances. While existing literature extensively addresses physical occlusions caused by scene geometry, a critical mode remains largely unexplored: image corruptions. These sensor-level artifacts, ranging from electronic noise and dead pixels to lens contaminants, directly compromise the integrity of the visual signal prior to interpretation. In this work, we quantify this vulnerability, demonstrating that state-of-the-art VLAs such as $π_{0.5}$ and SmolVLA, suffer catastrophic performance degradation, dropping from 90\% success rates to as low as 2\%, under common signal artifacts. To mitigate this, we introduce the Corruption Restoration Transformer (CRT), a plug-and-play and model-agnostic vision transformer designed to immunize VLA models against sensor disturbances. Leveraging an adversarial training objective, CRT restores clean observations from corrupted inputs without requiring computationally expensive fine-tuning of the underlying model. Extensive experiments across the LIBERO and Meta-World benchmarks demonstrate that CRT effectively recovers lost performance, enabling VLAs to maintain near-baseline success rates, even under severe visual corruption.

</details>


### [95] [Non-Contrastive Vision-Language Learning with Predictive Embedding Alignment](https://arxiv.org/abs/2602.00653)
*Lukas Kuhn,Giuseppe Serra,Florian Buettner*

Main category: cs.CV

TL;DR: NOVA is a non-contrastive vision-language alignment framework that predicts text embeddings from augmented images with distributional regularization, eliminating complex contrastive training requirements.


<details>
  <summary>Details</summary>
Motivation: Contrastive vision-language approaches like CLIP require large batch sizes, careful negative sampling, and extensive hyperparameter tuning, making training complex and unstable.

Method: NOVA aligns visual representations to a frozen text encoder by predicting text embeddings from augmented image views, using Sketched Isotropic Gaussian Regularization (SIGReg) to enforce isotropic Gaussian structure without negative sampling, momentum encoders, or stop-gradients.

Result: NOVA outperforms multiple standard baselines on zero-shot chest X-ray classification across three benchmark datasets using ClinicalBERT and Vision Transformers, while showing substantially more consistent training runs.

Conclusion: Non-contrastive vision-language pretraining offers a simpler, more stable, and more effective alternative to contrastive methods for medical imaging applications.

Abstract: Vision-language models have transformed multimodal representation learning, yet dominant contrastive approaches like CLIP require large batch sizes, careful negative sampling, and extensive hyperparameter tuning. We introduce NOVA, a NOn-contrastive Vision-language Alignment framework based on joint embedding prediction with distributional regularization. NOVA aligns visual representations to a frozen, domain-specific text encoder by predicting text embeddings from augmented image views, while enforcing an isotropic Gaussian structure via Sketched Isotropic Gaussian Regularization (SIGReg). This eliminates the need for negative sampling, momentum encoders, or stop-gradients, reducing the training objective to a single hyperparameter. We evaluate NOVA on zeroshot chest X-ray classification using ClinicalBERT as the text encoder and Vision Transformers trained from scratch on MIMIC-CXR. On zero-shot classification across three benchmark datasets, NOVA outperforms multiple standard baselines while exhibiting substantially more consistent training runs. Our results demonstrate that non-contrastive vision-language pretraining offers a simpler, more stable, and more effective alternative to contrastive methods.

</details>


### [96] [OASIS-DC: Generalizable Depth Completion via Output-level Alignment of Sparse-Integrated Monocular Pseudo Depth](https://arxiv.org/abs/2602.01268)
*Jaehyeon Cho,Jhonghyun An*

Main category: cs.CV

TL;DR: Zero-shot monocular depth models produce relative depth; this work converts them to metric depth using sparse range measurements and a refinement network, achieving accurate metric predictions with few labeled samples.


<details>
  <summary>Details</summary>
Motivation: Monocular foundation models provide excellent zero-shot depth estimation but only produce relative depth outputs, which cannot be directly used in robotics and autonomous driving applications that require metric depth measurements.

Method: 1) Calibrate relative depth from foundation models with sparse range measurements to create pseudo metric depth prior. 2) Design a refinement network that follows the prior where reliable and deviates where necessary. 3) Use very few labeled samples for training.

Result: The system achieves accurate metric depth predictions from very few labeled samples, maintains stable scale and sharp edges across few-shot regimes, and works effectively even without curated validation data.

Conclusion: Coupling foundation model priors with sparse anchors provides a practical approach for robust, deployment-ready depth completion under real-world label scarcity conditions.

Abstract: Recent monocular foundation models excel at zero-shot depth estimation, yet their outputs are inherently relative rather than metric, limiting direct use in robotics and autonomous driving. We leverage the fact that relative depth preserves global layout and boundaries: by calibrating it with sparse range measurements, we transform it into a pseudo metric depth prior. Building on this prior, we design a refinement network that follows the prior where reliable and deviates where necessary, enabling accurate metric predictions from very few labeled samples. The resulting system is particularly effective when curated validation data are unavailable, sustaining stable scale and sharp edges across few-shot regimes. These findings suggest that coupling foundation priors with sparse anchors is a practical route to robust, deployment-ready depth completion under real-world label scarcity.

</details>


### [97] [Schrödinger-Inspired Time-Evolution for 4D Deformation Forecasting](https://arxiv.org/abs/2602.00661)
*Ahsan Raza Siyal,Markus Haltmeier,Ruth Steiger,Elke Ruth Gizewski,Astrid Ellen Grams*

Main category: cs.CV

TL;DR: A physics-guided neural architecture for 4D spatiotemporal forecasting that embeds a Schrödinger-type evolution operator within a deep convolutional framework to predict future volumetric states with stability and interpretability.


<details>
  <summary>Details</summary>
Motivation: Spatiotemporal forecasting of complex 3D+time phenomena is crucial for medical imaging, fluid dynamics, and geophysics, but existing neural forecasting models lack physical constraints, leading to instability and poor interpretability in long-horizon predictions.

Method: Proposes a Schrödinger-inspired neural architecture that learns voxelwise amplitude, phase, and potential fields from observed volumetric sequences. These define a complex-valued wavefunction ψ = A e^{iφ}, which is evolved forward in time using a differentiable, unrolled Schrödinger time stepper within a deep convolutional framework.

Result: Demonstrates accurate and stable prediction of future 4D states (volumetric intensities and deformation fields) on synthetic benchmarks emulating realistic shape deformations and topological changes, with improved temporal stability and interpretable latent representations.

Conclusion: The approach successfully integrates physical priors into deep learning, combining neural network expressivity with physics-based robustness and interpretability, offering the first end-to-end 4D neural forecasting framework with Schrödinger-type evolution for anatomically consistent spatiotemporal prediction.

Abstract: Spatiotemporal forecasting of complex three-dimensional phenomena (4D: 3D + time) is fundamental to applications in medical imaging, fluid and material dynamics, and geophysics. In contrast to unconstrained neural forecasting models, we propose a Schrödinger-inspired, physics-guided neural architecture that embeds an explicit time-evolution operator within a deep convolutional framework for 4D prediction. From observed volumetric sequences, the model learns voxelwise amplitude, phase, and potential fields that define a complex-valued wavefunction $ψ= A e^{iφ}$, which is evolved forward in time using a differentiable, unrolled Schrödinger time stepper. This physics-guided formulation yields several key advantages: (i) temporal stability arising from the structured evolution operator, which mitigates drift and error accumulation in long-horizon forecasting; (ii) an interpretable latent representation, where phase encodes transport dynamics, amplitude captures structural intensity, and the learned potential governs spatiotemporal interactions; and (iii) natural compatibility with deformation-based synthesis, which is critical for preserving anatomical fidelity in medical imaging applications. By integrating physical priors directly into the learning process, the proposed approach combines the expressivity of deep networks with the robustness and interpretability of physics-based modeling. We demonstrate accurate and stable prediction of future 4D states, including volumetric intensities and deformation fields, on synthetic benchmarks that emulate realistic shape deformations and topological changes. To our knowledge, this is the first end-to-end 4D neural forecasting framework to incorporate a Schrödinger-type evolution operator, offering a principled pathway toward interpretable, stable, and anatomically consistent spatiotemporal prediction.

</details>


### [98] [Real-Time Loop Closure Detection in Visual SLAM via NetVLAD and Faiss](https://arxiv.org/abs/2602.01673)
*Enguang Fan*

Main category: cs.CV

TL;DR: NetVLAD outperforms DBoW for loop closure detection in SLAM with real-time performance using Faiss acceleration.


<details>
  <summary>Details</summary>
Motivation: Traditional bag-of-words methods like DBoW degrade under appearance change and perceptual aliasing, while deep learning VPR descriptors offer better robustness but are considered too computationally expensive for real-time SLAM.

Method: Empirically evaluate NetVLAD as LCD module vs DBoW on KITTI dataset, introduce Fine-Grained Top-K precision-recall curve for LCD evaluation, and use Faiss-accelerated nearest neighbor search for real-time performance.

Result: NetVLAD achieves real-time query speed with Faiss acceleration while improving accuracy and robustness over DBoW, making it a practical drop-in alternative for LCD in SLAM.

Conclusion: NetVLAD with Faiss acceleration provides a viable, high-performance alternative to traditional bag-of-words methods for loop closure detection in SLAM systems.

Abstract: Loop closure detection (LCD) is a core component of simultaneous localization and mapping (SLAM): it identifies revisited places and enables pose-graph constraints that correct accumulated drift. Classic bag-of-words approaches such as DBoW are efficient but often degrade under appearance change and perceptual aliasing. In parallel, deep learning-based visual place recognition (VPR) descriptors (e.g., NetVLAD and Transformer-based models) offer stronger robustness, but their computational cost is often viewed as a barrier to real-time SLAM. In this paper, we empirically evaluate NetVLAD as an LCD module and compare it against DBoW on the KITTI dataset. We introduce a Fine-Grained Top-K precision-recall curve that better reflects LCD settings where a query may have zero or multiple valid matches. With Faiss-accelerated nearestneighbor search, NetVLAD achieves real-time query speed while improving accuracy and robustness over DBoW, making it a practical drop-in alternative for LCD in SLAM.

</details>


### [99] [Improving Neuropathological Reconstruction Fidelity via AI Slice Imputation](https://arxiv.org/abs/2602.00669)
*Marina Crespo Aguirre,Jonathan Williams-Ramirez,Dina Zemlyanker,Xiaoling Hu,Lucas J. Deden-Binder,Rogeny Herisse,Mark Montine,Theresa R. Connors,Christopher Mount,Christine L. MacDonald,C. Dirk Keene,Caitlin S. Latimer,Derek H. Oakley,Bradley T. Hyman,Ana Lawry Aguila,Juan Eugenio Iglesias*

Main category: cs.CV

TL;DR: A super-resolution method enhances 3D brain reconstructions from 2D dissection photos by imputing slices to create isotropic volumes, improving anatomical accuracy and segmentation performance.


<details>
  <summary>Details</summary>
Motivation: Existing 3D reconstructions from 2D dissection photographs often produce coarse, overly smooth results, especially with thick slabs (high anisotropy), limiting anatomical precision and morphometric accuracy in neuropathological analysis.

Method: Introduces a computationally efficient super-resolution step that imputes slices to generate anatomically consistent isotropic volumes from anisotropic 3D reconstructions. Trained on domain-randomized synthetic data to ensure generalization across dissection protocols and robustness to large slab thicknesses.

Result: The imputed volumes yield improved automated segmentations with higher Dice scores, particularly in cortical and white matter regions. Validation shows more accurate cortical surfaces and MRI registration compared to previous methods.

Conclusion: The approach enhances resolution and anatomical fidelity of photograph-based reconstructions, strengthening the bridge between neuropathology and neuroimaging. The method is publicly available for research use.

Abstract: Neuropathological analyses benefit from spatially precise volumetric reconstructions that enhance anatomical delineation and improve morphometric accuracy. Our prior work has shown the feasibility of reconstructing 3D brain volumes from 2D dissection photographs. However these outputs sometimes exhibit coarse, overly smooth reconstructions of structures, especially under high anisotropy (i.e., reconstructions from thick slabs). Here, we introduce a computationally efficient super-resolution step that imputes slices to generate anatomically consistent isotropic volumes from anisotropic 3D reconstructions of dissection photographs. By training on domain-randomized synthetic data, we ensure that our method generalizes across dissection protocols and remains robust to large slab thicknesses. The imputed volumes yield improved automated segmentations, achieving higher Dice scores, particularly in cortical and white matter regions. Validation on surface reconstruction and atlas registration tasks demonstrates more accurate cortical surfaces and MRI registration. By enhancing the resolution and anatomical fidelity of photograph-based reconstructions, our approach strengthens the bridge between neuropathology and neuroimaging. Our method is publicly available at https://surfer.nmr.mgh.harvard.edu/fswiki/mri_3d_photo_recon

</details>


### [100] [DDP-WM: Disentangled Dynamics Prediction for Efficient World Models](https://arxiv.org/abs/2602.01780)
*Shicheng Yin,Kaixuan Yin,Weixing Chen,Yang Liu,Guanbin Li,Liang Lin*

Main category: cs.CV

TL;DR: DDP-WM is an efficient world model using disentangled dynamics prediction that separates primary physical interactions from background updates, achieving 9x speedup and improved planning performance.


<details>
  <summary>Details</summary>
Motivation: Existing dense Transformer-based world models have high computational overhead that hinders real-time deployment in robotics, creating an efficiency-performance bottleneck.

Method: DDP-WM uses Disentangled Dynamics Prediction to decompose latent state evolution into sparse primary dynamics (physical interactions) and secondary background updates, with efficient historical processing and dynamic localization for primary dynamics isolation, plus cross-attention for background updates.

Result: Achieves significant efficiency and performance gains across diverse tasks including navigation, tabletop manipulation, and complex interactions. On Push-T task: ~9x inference speedup and MPC success rate improvement from 90% to 98% compared to SOTA dense models.

Conclusion: DDP-WM establishes a promising path for developing efficient, high-fidelity world models for real-time robotic planning applications.

Abstract: World models are essential for autonomous robotic planning. However, the substantial computational overhead of existing dense Transformerbased models significantly hinders real-time deployment. To address this efficiency-performance bottleneck, we introduce DDP-WM, a novel world model centered on the principle of Disentangled Dynamics Prediction (DDP). We hypothesize that latent state evolution in observed scenes is heterogeneous and can be decomposed into sparse primary dynamics driven by physical interactions and secondary context-driven background updates. DDP-WM realizes this decomposition through an architecture that integrates efficient historical processing with dynamic localization to isolate primary dynamics. By employing a crossattention mechanism for background updates, the framework optimizes resource allocation and provides a smooth optimization landscape for planners. Extensive experiments demonstrate that DDP-WM achieves significant efficiency and performance across diverse tasks, including navigation, precise tabletop manipulation, and complex deformable or multi-body interactions. Specifically, on the challenging Push-T task, DDP-WM achieves an approximately 9 times inference speedup and improves the MPC success rate from 90% to98% compared to state-of-the-art dense models. The results establish a promising path for developing efficient, high-fidelity world models. Codes will be available at https://github.com/HCPLabSYSU/DDP-WM.

</details>


### [101] [HPC: Hierarchical Point-based Latent Representation for Streaming Dynamic Gaussian Splatting Compression](https://arxiv.org/abs/2602.00671)
*Yangzhi Ma,Bojun Liu,Wenting Liao,Dong Liu,Zhu Li,Li Li*

Main category: cs.CV

TL;DR: HPC is a streaming compression framework for dynamic Gaussian Splatting that uses hierarchical point-based latent representation and neural network compression to reduce storage by 67% while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: Current streaming dynamic Gaussian Splatting compression methods have limitations: grid-based approaches waste parameters on unoccupied space, while point-based methods lack compactness due to poor local correlation exploitation. There's a need for efficient compression with small memory footprint for streaming transmission.

Method: HPC uses hierarchical point-based latent representation operating per-Gaussian to avoid unoccupied space redundancy. It employs tailored aggregation for compactness, and compresses neural networks by mining inter-frame parameter correlations, forming an end-to-end compression framework.

Result: HPC achieves 67% storage reduction compared to baseline while maintaining high reconstruction fidelity, substantially outperforming state-of-the-art methods in comprehensive experimental evaluations.

Conclusion: HPC effectively addresses the limitations of existing dynamic Gaussian Splatting compression methods by combining hierarchical point-based representation with neural network compression, enabling efficient streaming with minimal storage requirements.

Abstract: While dynamic Gaussian Splatting has driven significant advances in free-viewpoint video, maintaining its rendering quality with a small memory footprint for efficient streaming transmission still presents an ongoing challenge. Existing streaming dynamic Gaussian Splatting compression methods typically leverage a latent representation to drive the neural network for predicting Gaussian residuals between frames. Their core latent representations can be categorized into structured grid-based and unstructured point-based paradigms. However, the former incurs significant parameter redundancy by inevitably modeling unoccupied space, while the latter suffers from limited compactness as it fails to exploit local correlations. To relieve these limitations, we propose HPC, a novel streaming dynamic Gaussian Splatting compression framework. It employs a hierarchical point-based latent representation that operates on a per-Gaussian basis to avoid parameter redundancy in unoccupied space. Guided by a tailored aggregation scheme, these latent points achieve high compactness with low spatial redundancy. To improve compression efficiency, we further undertake the first investigation to compress neural networks for streaming dynamic Gaussian Splatting through mining and exploiting the inter-frame correlation of parameters. Combined with latent compression, this forms a fully end-to-end compression framework. Comprehensive experimental evaluations demonstrate that HPC substantially outperforms state-of-the-art methods. It achieves a storage reduction of 67% against its baseline while maintaining high reconstruction fidelity.

</details>


### [102] [LangMap: A Hierarchical Benchmark for Open-Vocabulary Goal Navigation](https://arxiv.org/abs/2602.02220)
*Bo Miao,Weijia Liu,Jun Luo,Lachlan Shinnick,Jian Liu,Thomas Hamilton-Smith,Yuhe Yang,Zijie Wu,Vanja Videnovic,Feras Dayoub,Anton van den Hengel*

Main category: cs.CV

TL;DR: HieraNav introduces multi-granularity open-vocabulary goal navigation with four semantic levels, supported by LangMap benchmark built on real 3D scans with comprehensive annotations.


<details>
  <summary>Details</summary>
Motivation: To advance language-driven embodied intelligence by establishing rigorous evaluation for agents that interpret natural language instructions at multiple semantic levels (scene, room, region, instance).

Method: Created LangMap benchmark using real-world 3D indoor scans with human-verified annotations including region labels, discriminative region/instance descriptions covering 414 object categories, and over 18K navigation tasks with both concise and detailed descriptions.

Result: LangMap achieves 23.8% higher discriminative accuracy than GOAT-Bench using 4x fewer words. Evaluations show richer context and memory improve success, but challenges remain with long-tailed, small, context-dependent, and distant goals, plus multi-goal completion.

Conclusion: HieraNav and LangMap establish a rigorous testbed for advancing language-driven embodied navigation, providing comprehensive evaluation across multiple semantic granularities.

Abstract: The relationships between objects and language are fundamental to meaningful communication between humans and AI, and to practically useful embodied intelligence. We introduce HieraNav, a multi-granularity, open-vocabulary goal navigation task where agents interpret natural language instructions to reach targets at four semantic levels: scene, room, region, and instance. To this end, we present Language as a Map (LangMap), a large-scale benchmark built on real-world 3D indoor scans with comprehensive human-verified annotations and tasks spanning these levels. LangMap provides region labels, discriminative region descriptions, discriminative instance descriptions covering 414 object categories, and over 18K navigation tasks. Each target features both concise and detailed descriptions, enabling evaluation across different instruction styles. LangMap achieves superior annotation quality, outperforming GOAT-Bench by 23.8% in discriminative accuracy using four times fewer words. Comprehensive evaluations of zero-shot and supervised models on LangMap reveal that richer context and memory improve success, while long-tailed, small, context-dependent, and distant goals, as well as multi-goal completion, remain challenging. HieraNav and LangMap establish a rigorous testbed for advancing language-driven embodied navigation. Project: https://bo-miao.github.io/LangMap

</details>


### [103] [Video Understanding: Through A Temporal Lens](https://arxiv.org/abs/2602.00683)
*Thong Thanh Nguyen*

Main category: cs.CV

TL;DR: This thesis introduces five contributions for better video understanding through explicit temporal modeling: automatic annotation with noise-robust contrastive learning, recurrent adapters for temporal dynamics, State Space Layers for long-form videos, motion-moment contrastive learning, and a temporal-oriented recipe for LVLMs.


<details>
  <summary>Details</summary>
Motivation: Existing video understanding methods have limitations in leveraging temporal relations among video elements. The thesis aims to address these limitations by developing approaches that better capture and utilize temporal dynamics in video content.

Method: Five key contributions: (1) automatic annotation framework using large vision-language models with noise-robust contrastive learning and subtractive angular margin; (2) parameter-efficient fine-tuning with "recurrent adapters" for temporal dynamics; (3) State Space Layers for efficient long-form video modeling with new benchmarks; (4) contrastive learning framework for fine-grained motion-video moment relations; (5) empirical study on LVLMs leading to temporal-oriented recipe.

Result: The contributions collectively demonstrate that explicit temporal modeling significantly enhances a model's ability to represent and reason about video content. The work provides practical frameworks, benchmarks, and recipes for improved video understanding.

Conclusion: Explicit temporal modeling is crucial for advancing video understanding, and the thesis provides comprehensive solutions across annotation, fine-tuning, long-form modeling, motion-moment relations, and LVLM optimization to achieve this goal.

Abstract: This thesis explores the central question of how to leverage temporal relations among video elements to advance video understanding. Addressing the limitations of existing methods, the work presents a five-fold contribution: (1) an automatic annotation framework that utilizes large vision-language models and a noise-robust contrastive learning objective with a subtractive angular margin; (2) a parameter-efficient fine-tuning strategy using "recurrent adapters" to capture temporal dynamics in low-data regimes; (3) the integration of State Space Layers (SSL) for efficient long-form video modeling, supported by the introduction of two new long-term benchmarks for egocentric and feature-length content; (4) a novel contrastive learning framework designed to explicitly model fine-grained relations between motions and video moments; and (5) a comprehensive empirical study on Large Vision-Language Models (LVLMs) that identifies the visual-language interface as a bottleneck for temporal reasoning, leading to a new "temporal-oriented recipe" for upscaled video understanding. Collectively, these contributions demonstrate that explicit temporal modeling significantly enhances a model's ability to represent and reason about the fluid nature of video content.

</details>


### [104] [V2X-DSC: Multi-Agent Collaborative Perception with Distributed Source Coding Guided Communication](https://arxiv.org/abs/2602.00687)
*Yuankun Zeng,Shaohui Li,Zhi Li,Shulan Ruan,Yu Liu,You He*

Main category: cs.CV

TL;DR: V2X-DSC: A distributed source coding framework for bandwidth-efficient collaborative perception that compresses BEV features by transmitting only complementary information beyond local context.


<details>
  <summary>Details</summary>
Motivation: Intermediate-feature sharing in collaborative perception faces strict bandwidth constraints as dense BEV features saturate V2X links, while collaborators' features are strongly correlated since they view the same physical world.

Method: Proposes V2X-DSC with a Conditional Codec (DCC) where sender compresses BEV features into compact codes, and receiver performs conditional reconstruction using local features as side information, allocating bits only to complementary cues rather than redundant content.

Result: Achieves state-of-the-art accuracy-bandwidth trade-offs under KB-level communication on DAIR-V2X, OPV2V, and V2X-Real datasets, and generalizes as a plug-and-play communication layer across multiple fusion backbones.

Conclusion: The conditional structure regularizes learning, encourages incremental representation, yields lower-noise features, and provides an effective solution for bandwidth-constrained collaborative perception through distributed source coding principles.

Abstract: Collaborative perception improves 3D understanding by fusing multi-agent observations, yet intermediate-feature sharing faces strict bandwidth constraints as dense BEV features saturate V2X links. We observe that collaborators view the same physical world, making their features strongly correlated; thus receivers only need innovation beyond their local context. Revisiting this from a distributed source coding perspective, we propose V2X-DSC, a framework with a Conditional Codec (DCC) for bandwidth-constrained fusion. The sender compresses BEV features into compact codes, while the receiver performs conditional reconstruction using its local features as side information, allocating bits to complementary cues rather than redundant content. This conditional structure regularizes learning, encouraging incremental representation and yielding lower-noise features. Experiments on DAIR-V2X, OPV2V, and V2X-Real demonstrate state-of-the-art accuracy-bandwidth trade-offs under KB-level communication, and generalizes as a plug-and-play communication layer across multiple fusion backbones.

</details>


### [105] [JoyAvatar: Unlocking Highly Expressive Avatars via Harmonized Text-Audio Conditioning](https://arxiv.org/abs/2602.00702)
*Ruikui Wang,Jinheng Feng,Lang Tian,Huaishao Luo,Chaochao Li,Liangbo Zhou,Huan Zhang,Youzheng Wu,Xiaodong He*

Main category: cs.CV

TL;DR: JoyAvatar is a framework for generating long-duration avatar videos with enhanced text controllability for complex scenarios like full-body movement, camera trajectories, and human-object interactions.


<details>
  <summary>Details</summary>
Motivation: Existing video avatar models have limited alignment with text instructions, especially for complex prompts involving full-body movement, dynamic camera trajectories, background transitions, or human-object interactions.

Method: Two key innovations: 1) Twin-teacher enhanced training algorithm to transfer text-controllability from foundation models while learning audio-visual synchronization; 2) Dynamic modulation of multi-modal condition strengths based on denoising timesteps to mitigate conflicts between heterogeneous conditioning signals.

Result: Outperforms state-of-the-art models like Omnihuman-1.5 and KlingAvatar 2.0 in GSB evaluation, enables complex applications including multi-person dialogues and non-human subjects role-playing.

Conclusion: JoyAvatar substantially expands avatar models' capacity to generate natural, temporally coherent full-body motions and dynamic camera movements while preserving basic avatar capabilities like accurate lip-sync and identity consistency.

Abstract: Existing video avatar models have demonstrated impressive capabilities in scenarios such as talking, public speaking, and singing. However, the majority of these methods exhibit limited alignment with respect to text instructions, particularly when the prompts involve complex elements including large full-body movement, dynamic camera trajectory, background transitions, or human-object interactions. To break out this limitation, we present JoyAvatar, a framework capable of generating long duration avatar videos, featuring two key technical innovations. Firstly, we introduce a twin-teacher enhanced training algorithm that enables the model to transfer inherent text-controllability from the foundation model while simultaneously learning audio-visual synchronization. Secondly, during training, we dynamically modulate the strength of multi-modal conditions (e.g., audio and text) based on the distinct denoising timestep, aiming to mitigate conflicts between the heterogeneous conditioning signals. These two key designs serve to substantially expand the avatar model's capacity to generate natural, temporally coherent full-body motions and dynamic camera movements as well as preserve the basic avatar capabilities, such as accurate lip-sync and identity consistency. GSB evaluation results demonstrate that our JoyAvatar model outperforms the state-of-the-art models such as Omnihuman-1.5 and KlingAvatar 2.0. Moreover, our approach enables complex applications including multi-person dialogues and non-human subjects role-playing. Some video samples are provided on https://joyavatar.github.io/.

</details>


### [106] [StomataSeg: Semi-Supervised Instance Segmentation for Sorghum Stomatal Components](https://arxiv.org/abs/2602.00703)
*Zhongtian Huang,Zhi Chen,Zi Huang,Xin Yu,Daniel Smith,Chaitanya Purushothama,Erik Van Oosterom,Alex Wu,William Salter,Yan Li,Scott Chapman*

Main category: cs.CV

TL;DR: Semi-supervised instance segmentation framework improves sorghum stomatal analysis using patch-based preprocessing and pseudo-labeling, achieving significant performance gains for high-throughput phenotyping.


<details>
  <summary>Details</summary>
Motivation: Sorghum is a drought-tolerant cereal crucial for climate-resilient agriculture, but automated stomatal analysis is challenging due to small size (often <40μm) and shape variations across genotypes. Current methods struggle with nested small structures and annotation bottlenecks.

Method: Proposed semi-supervised instance segmentation framework with: 1) collection of 11,060 human-annotated patches covering three stomatal components (pore, guard cell, complex area); 2) splitting high-resolution images into overlapping small patches for tiny structure detection; 3) pseudo-labeling strategy generating 56,428 additional patches from unannotated images.

Result: Significant performance improvements: semantic segmentation mIoU increased from 65.93% to 70.35%; instance segmentation AP rose from 28.30% to 46.10%. Demonstrates patch-based preprocessing with semi-supervised learning effectively improves fine stomatal structure segmentation.

Conclusion: The framework enables scalable extraction of stomatal traits and facilitates broader adoption of AI-driven phenotyping in crop science, supporting high-throughput analysis of sorghum stomatal components for improved water-use efficiency.

Abstract: Sorghum is a globally important cereal grown widely in water-limited and stress-prone regions. Its strong drought tolerance makes it a priority crop for climate-resilient agriculture. Improving water-use efficiency in sorghum requires precise characterisation of stomatal traits, as stomata control of gas exchange, transpiration and photosynthesis have a major influence on crop performance. Automated analysis of sorghum stomata is difficult because the stomata are small (often less than 40 $μ$m in length in grasses such as sorghum) and vary in shape across genotypes and leaf surfaces. Automated segmentation contributes to high-throughput stomatal phenotyping, yet current methods still face challenges related to nested small structures and annotation bottlenecks. In this paper, we propose a semi-supervised instance segmentation framework tailored for analysis of sorghum stomatal components. We collect and annotate a sorghum leaf imagery dataset containing 11,060 human-annotated patches, covering the three stomatal components (pore, guard cell and complex area) across multiple genotypes and leaf surfaces. To improve the detection of tiny structures, we split high-resolution microscopy images into overlapping small patches. We then apply a pseudo-labelling strategy to unannotated images, producing an additional 56,428 pseudo-labelled patches. Benchmarking across semantic and instance segmentation models shows substantial performance gains: for semantic models the top mIoU increases from 65.93% to 70.35%, whereas for instance models the top AP rises from 28.30% to 46.10%. These results demonstrate that combining patch-based preprocessing with semi-supervised learning significantly improves the segmentation of fine stomatal structures. The proposed framework supports scalable extraction of stomatal traits and facilitates broader adoption of AI-driven phenotyping in crop science.

</details>


### [107] [Supervised makeup transfer with a curated dataset: Decoupling identity and makeup features for enhanced transformation](https://arxiv.org/abs/2602.00729)
*Qihe Pan,Yiming Wu,Xing Zhao,Liang Xie,Guodao Sun,Ronghua Liang*

Main category: cs.CV

TL;DR: A diffusion-based makeup transfer framework with curated dataset, disentangled identity/makeup features, and text-guided region-specific control.


<details>
  <summary>Details</summary>
Motivation: Existing makeup transfer methods suffer from limited datasets, poor disentanglement between identity and makeup features, and weak controllability.

Method: Three contributions: 1) Curated high-quality dataset using train-generate-filter-retrain strategy, 2) Diffusion-based framework disentangling identity and makeup features, 3) Text-guided mechanism for fine-grained region-specific control with natural language prompts.

Result: Experiments on benchmarks and real-world scenarios demonstrate improvements in fidelity, identity preservation, and flexibility.

Conclusion: The proposed approach addresses key limitations in makeup transfer by combining dataset curation, feature disentanglement, and text-guided control in a diffusion framework.

Abstract: Diffusion models have recently shown strong progress in generative tasks, offering a more stable alternative to GAN-based approaches for makeup transfer. Existing methods often suffer from limited datasets, poor disentanglement between identity and makeup features, and weak controllability. To address these issues, we make three contributions. First, we construct a curated high-quality dataset using a train-generate-filter-retrain strategy that combines synthetic, realistic, and filtered samples to improve diversity and fidelity. Second, we design a diffusion-based framework that disentangles identity and makeup features, ensuring facial structure and skin tone are preserved while applying accurate and diverse cosmetic styles. Third, we propose a text-guided mechanism that allows fine-grained and region-specific control, enabling users to modify eyes, lips, or face makeup with natural language prompts. Experiments on benchmarks and real-world scenarios demonstrate improvements in fidelity, identity preservation, and flexibility. Examples of our dataset can be found at: https://makeup-adapter.github.io.

</details>


### [108] [Diffusion-Driven Inter-Outer Surface Separation for Point Clouds with Open Boundaries](https://arxiv.org/abs/2602.00739)
*Zhengyan Qin,Liyuan Qiu*

Main category: cs.CV

TL;DR: Diffusion-based algorithm separates inter/outer surfaces from double-layered point clouds with open boundaries, addressing TSDF truncation artifacts in 10 seconds for 40K points.


<details>
  <summary>Details</summary>
Motivation: Double-layered point clouds with "double surface artifact" from TSDF fusion cause overlapping surfaces and disordered normals, particularly problematic in indoor/medical 3D reconstruction where accurate surface representation is crucial.

Method: Diffusion-based algorithm specifically designed for point clouds with open boundaries (topological openings/holes), not missing surface regions. Lightweight post-hoc module after TSDF fusion that separates inter and outer layer surfaces.

Result: Achieves extraction of inter layer from 20,000 inter and 20,000 outer points in approximately 10 seconds. Works for both watertight and open-boundary models, robustly processing double-layered point clouds.

Conclusion: Effective solution for applications requiring accurate surface representations (indoor scene modeling, medical imaging) where double-layered point clouds are prevalent. Post-hoc separation module complements rather than replaces full reconstruction pipelines.

Abstract: We propose a diffusion-based algorithm for separating the inter and outer layer surfaces from double-layered point clouds, particularly those exhibiting the "double surface artifact" caused by truncation in Truncated Signed Distance Function (TSDF) fusion during indoor or medical 3D reconstruction. This artifact arises from asymmetric truncation thresholds, leading to erroneous inter and outer shells in the fused volume, which our method addresses by extracting the true inter layer to mitigate challenges like overlapping surfaces and disordered normals. We focus on point clouds with \emph{open boundaries} (i.e., sampled surfaces with topological openings/holes through which particles may escape), rather than point clouds with \emph{missing surface regions} where no samples exist. Our approach enables robust processing of both watertight and open-boundary models, achieving extraction of the inter layer from 20,000 inter and 20,000 outer points in approximately 10 seconds. This solution is particularly effective for applications requiring accurate surface representations, such as indoor scene modeling and medical imaging, where double-layered point clouds are prevalent, and it accommodates both closed (watertight) and open-boundary surface geometries. Our goal is \emph{post-hoc} inter/outer shell separation as a lightweight module after TSDF fusion; we do not aim to replace full variational or learning-based reconstruction pipelines.

</details>


### [109] [HSI-VAR: Rethinking Hyperspectral Restoration through Spatial-Spectral Visual Autoregression](https://arxiv.org/abs/2602.00749)
*Xiangming Wang,Benteng Sun,Yungeng Liu,Haijin Zeng,Yongyong Chen,Jingyong Su,Jie Liu*

Main category: cs.CV

TL;DR: HSI-VAR introduces an autoregressive approach for hyperspectral image restoration that handles mixed degradations (noise, blur, missing bands) with 95.5× faster inference than diffusion models while achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Real-world hyperspectral images suffer from composite degradations, but existing methods have limitations: diffusion models are computationally impractical (hundreds of iterative steps), while regression models produce oversmoothed results that fail to preserve structural details.

Method: HSI-VAR rethinks HSI restoration as an autoregressive generation problem, progressively modeling spectral and spatial dependencies. It features three innovations: (1) Latent-condition alignment for semantic consistency, (2) Degradation-aware guidance encoding mixed degradations as linear combinations for automatic control and 50% computational reduction, (3) Spatial-spectral adaptation module refining details across both domains during decoding.

Result: Achieves state-of-the-art performance on nine all-in-one HSI restoration benchmarks, with 3.77 dB PSNR improvement on ICVL dataset, superior structure preservation, and 95.5× inference speed-up compared to diffusion-based methods.

Conclusion: HSI-VAR provides a highly practical solution for real-world HSI restoration by breaking the computational-performance trade-off, offering both superior restoration quality and dramatically faster inference through its autoregressive approach with degradation-aware guidance.

Abstract: Hyperspectral images (HSIs) capture richer spatial-spectral information beyond RGB, yet real-world HSIs often suffer from a composite mix of degradations, such as noise, blur, and missing bands. Existing generative approaches for HSI restoration like diffusion models require hundreds of iterative steps, making them computationally impractical for high-dimensional HSIs. While regression models tend to produce oversmoothed results, failing to preserve critical structural details. We break this impasse by introducing HSI-VAR, rethinking HSI restoration as an autoregressive generation problem, where spectral and spatial dependencies can be progressively modeled rather than globally reconstructed. HSI-VAR incorporates three key innovations: (1) Latent-condition alignment, which couples semantic consistency between latent priors and conditional embeddings for precise reconstruction; (2) Degradation-aware guidance, which uniquely encodes mixed degradations as linear combinations in the embedding space for automatic control, remarkably achieving a nearly $50\%$ reduction in computational cost at inference; (3) A spatial-spectral adaptation module that refines details across both domains in the decoding phase. Extensive experiments on nine all-in-one HSI restoration benchmarks confirm HSI-VAR's state-of-the-art performance, achieving a 3.77 dB PSNR improvement on \textbf{\textit{ICVL}} and offering superior structure preservation with an inference speed-up of up to $95.5 \times$ compared with diffusion-based methods, making it a highly practical solution for real-world HSI restoration.

</details>


### [110] [Evaluating Deep Learning-Based Nerve Segmentation in Brachial Plexus Ultrasound Under Realistic Data Constraints](https://arxiv.org/abs/2602.00763)
*Dylan Yves,Khush Agarwal,Jonathan Hoyin Chan,Patcharapit Promoppatum,Aroonkamon Pattanasiricharoen*

Main category: cs.CV

TL;DR: Deep learning U-Net for ultrasound nerve segmentation shows multi-machine training regularizes lower-performing sources but doesn't beat single-source domain matching, multi-class supervision hurts nerve segmentation accuracy, and smaller nerves are harder to segment.


<details>
  <summary>Details</summary>
Motivation: Manual nerve identification in ultrasound-guided anesthesia is challenging due to low contrast, speckle noise, and anatomical variability, necessitating automated segmentation solutions.

Method: U-Net architecture for ultrasound nerve segmentation, evaluating dataset composition (multi-machine vs single-machine) and annotation strategy (binary nerve vs multi-class: artery, vein, nerve, muscle).

Result: Multi-machine training regularizes lower-performing sources but doesn't surpass single-source domain matching; multi-class supervision reduces nerve Dice scores by 9-61%; nerve size positively correlates with segmentation accuracy (r=0.587).

Conclusion: Smaller nerves remain challenging, multi-class supervision hurts performance likely due to class imbalance, and realistic clinical data constraints require careful dataset composition strategies for robust nerve segmentation.

Abstract: Accurate nerve localization is critical for the success of ultrasound-guided regional anesthesia, yet manual identification remains challenging due to low image contrast, speckle noise, and inter-patient anatomical variability. This study evaluates deep learning-based nerve segmentation in ultrasound images of the brachial plexus using a U-Net architecture, with a focus on how dataset composition and annotation strategy influence segmentation performance. We find that training on combined data from multiple ultrasound machines (SIEMENS ACUSON NX3 Elite and Philips EPIQ5) provides regularization benefits for lower-performing acquisition sources, though it does not surpass single-source training when matched to the target domain. Extending the task from binary nerve segmentation to multi-class supervision (artery, vein, nerve, muscle) results in decreased nerve-specific Dice scores, with performance drops ranging from 9% to 61% depending on dataset, likely due to class imbalance and boundary ambiguity. Additionally, we observe a moderate positive correlation between nerve size and segmentation accuracy (Pearson r=0.587, p<0.001), indicating that smaller nerves remain a primary challenge. These findings provide methodological guidance for developing robust ultrasound nerve segmentation systems under realistic clinical data constraints.

</details>


### [111] [DVLA-RL: Dual-Level Vision-Language Alignment with Reinforcement Learning Gating for Few-Shot Learning](https://arxiv.org/abs/2602.00795)
*Wenhao Li,Xianjing Meng,Qiangchang Wang,Zhongyi Han,Zhibin Wu,Yilong Yin*

Main category: cs.CV

TL;DR: DVLA-RL: A novel few-shot learning method that uses dual-level vision-language alignment with reinforcement learning gating to improve cross-modal semantic integration from low-level attributes to high-level descriptions.


<details>
  <summary>Details</summary>
Motivation: Existing FSL methods that incorporate LLMs overlook progressive and adaptive alignment between vision and language from low-level to high-level semantics, resulting in limited semantic gains. There's a need for more effective cross-modal integration that can handle both fine-grained grounding and holistic class understanding with few samples.

Method: Proposes DVLA-RL with two main components: 1) Dual-level Semantic Construction (DSC) - conditions LLMs on class names and support samples to generate discriminative attributes, progressively selects relevant ones, and synthesizes them into coherent class descriptions; 2) RL-gated Attention (RLA) - formulates cross-modal fusion as sequential decision process where a lightweight policy trained with episodic REINFORCE adaptively adjusts contributions of self-attention and cross-attention to integrate textual and visual tokens.

Result: Achieves new state-of-the-art performance across nine benchmarks in three diverse FSL scenarios, demonstrating superior generalization with few support samples.

Conclusion: DVLA-RL enables progressive vision-language alignment from low-level attributes to high-level semantics through dual-level semantic construction and adaptive RL-gated attention, achieving both class-specific discrimination and generalized representations with minimal samples.

Abstract: Few-shot learning (FSL) aims to generalize to novel categories with only a few samples. Recent approaches incorporate large language models (LLMs) to enrich visual representations with semantic embeddings derived from class names. However, they overlook progressive and adaptive alignment between vision and language from low-level to high-level semantics, resulting in limited semantic gains. To address these challenges, we propose Dual-level Vision-Language Alignment with Reinforcement Learning gating (DVLA-RL), which consists of Dual-level Semantic Construction (DSC) and RL-gated Attention (RLA). Specifically, DSC conditions LLMs on both class names and support samples to generate discriminative attributes, progressively selects the most relevant ones, and then synthesizes them into coherent class descriptions. This process provides complementary low-level attributes and high-level descriptions, enabling both fine-grained grounding and holistic class understanding. To dynamically integrate dual-level semantics along with the visual network layers, RLA formulates cross-modal fusion as a sequential decision process. A lightweight policy trained with episodic REINFORCE adaptively adjusts the contributions of self-attention and cross-attention to integrate textual and visual tokens. As a result, shallow layers refine local attributes and deep layers emphasize global semantics, enabling more precise cross-modal alignment. This achieves class-specific discrimination and generalized representations with merely a few support samples. DVLA-RL achieves new state-of-the-art performance across nine benchmarks in three diverse FSL scenarios.

</details>


### [112] [Generating a Paracosm for Training-Free Zero-Shot Composed Image Retrieval](https://arxiv.org/abs/2602.00813)
*Tong Wang,Yunhan Zhao,Shu Kong*

Main category: cs.CV

TL;DR: Paracosm is a training-free zero-shot CIR method that generates a "mental image" from multimodal queries using LMMs, then matches it to synthetic counterparts of database images for improved retrieval accuracy.


<details>
  <summary>Details</summary>
Motivation: Current zero-shot CIR methods use LMMs to generate textual descriptions from multimodal queries, then match text to images. The fundamental challenge is that the "mental image" is not physically available, leading to suboptimal matching.

Method: 1) Prompt LMM to generate a "mental image" from multimodal query (reference image + modification text). 2) Generate synthetic counterparts for all real database images. 3) Match the generated "mental image" to synthetic database images in a "paracosm" domain. 4) Use this matching to retrieve target real images.

Result: Significantly outperforms existing zero-shot methods on four challenging benchmarks, achieving state-of-the-art performance for zero-shot CIR.

Conclusion: Directly generating the "mental image" for matching, rather than converting to text, provides more accurate CIR. The paracosm approach bridges synthetic-to-real domain gaps and enables training-free zero-shot CIR with superior performance.

Abstract: Composed Image Retrieval (CIR) is the task of retrieving a target image from a database using a multimodal query, which consists of a reference image and a modification text. The text specifies how to alter the reference image to form a ``mental image'', based on which CIR should find the target image in the database. The fundamental challenge of CIR is that this ``mental image'' is not physically available and is only implicitly defined by the query. The contemporary literature pursues zero-shot methods and uses a Large Multimodal Model (LMM) to generate a textual description for a given multimodal query, and then employs a Vision-Language Model (VLM) for textual-visual matching to search the target image. In contrast, we address CIR from first principles by directly generating the ``mental image'' for more accurate matching. Particularly, we prompt an LMM to generate a ``mental image'' for a given multimodal query and propose to use this ``mental image'' to search for the target image. As the ``mental image'' has a synthetic-to-real domain gap with real images, we also generate a synthetic counterpart for each real image in the database to facilitate matching. In this sense, our method uses LMM to construct a ``paracosm'', where it matches the multimodal query and database images. Hence, we call this method Paracosm. Notably, Paracosm is a training-free zero-shot CIR method. It significantly outperforms existing zero-shot methods on four challenging benchmarks, achieving state-of-the-art performance for zero-shot CIR.

</details>


### [113] [Edge-Native Generative De-identification: Inversion-Free Flow for Privacy-Preserving Federated Skin Image Analysis](https://arxiv.org/abs/2602.00821)
*Konstantinos Moutselos,Ilias Maglogiannis*

Main category: cs.CV

TL;DR: A federated learning framework for clinical dermatology that preserves diagnostic features while protecting patient privacy through identity-agnostic pathology preservation using inversion-free Rectified Flow Transformers.


<details>
  <summary>Details</summary>
Motivation: Traditional de-identification methods degrade pathological fidelity needed for diagnosis, while standard generative editing techniques are computationally intensive and unsuitable for resource-constrained edge devices in clinical settings.

Method: Uses inversion-free Rectified Flow Transformers (FlowEdit) for high-fidelity identity transformation in near real-time (<20s). Introduces "Segment-by-Synthesis" mechanism to generate counterfactual healthy and pathological twin pairs locally, enabling extraction of differential erythema masks decoupled from biometric markers.

Result: Pilot validation shows Intersection over Union (IoU) stability >0.67 across synthetic identities. The framework enables privacy-compliant synthetic surrogate generation at the edge, mitigating gradient leakage risks.

Conclusion: Provides a secure pathway for high-precision skin image analysis in federated environments by generating privacy-preserving synthetic data locally on clinical nodes, balancing privacy protection with diagnostic feature preservation.

Abstract: The deployment of Federated Learning (FL) for clinical dermatology is hindered by the competing requirements of protecting patient privacy and preserving diagnostic features. Traditional de-identification methods often degrade pathological fidelity, while standard generative editing techniques rely on computationally intensive inversion processes unsuitable for resource-constrained edge devices. We propose a framework for identity-agnostic pathology preservation that serves as a client-side privacy-preserving utility. By leveraging inversion-free Rectified Flow Transformers (FlowEdit), the system performs high-fidelity identity transformation in near real-time (less than 20s), facilitating local deployment on clinical nodes. We introduce a "Segment-by-Synthesis" mechanism that generates counterfactual healthy and pathological twin pairs locally. This enables the extraction of differential erythema masks that are decoupled from biometric markers and semantic artifacts (e.g. jewelry). Pilot validation on high-resolution clinical samples demonstrates an Intersection over Union (IoU) stability greater than 0.67 across synthetic identities. By generating privacy-compliant synthetic surrogates at the edge, this framework mitigates the risk of gradient leakage at the source, providing a secure pathway for high-precision skin image analysis in federated environments.

</details>


### [114] [TransNormal: Dense Visual Semantics for Diffusion-based Transparent Object Normal Estimation](https://arxiv.org/abs/2602.00839)
*Mingwei Li,Hehe Fan,Yi Yang*

Main category: cs.CV

TL;DR: TransNormal: A diffusion-based monocular normal estimation framework for transparent objects that integrates DINOv3 semantics and wavelet regularization, achieving SOTA performance on transparent object benchmarks.


<details>
  <summary>Details</summary>
Motivation: Monocular normal estimation for transparent objects is challenging due to complex light refraction and reflection, which cause failures in conventional sensors and hinder embodied AI deployment in scientific/lab environments.

Method: Adapts pre-trained diffusion priors for single-step normal regression, integrates dense visual semantics from DINOv3 via cross-attention, uses multi-task learning objective and wavelet-based regularization to preserve fine structural details.

Result: Significantly outperforms SOTA methods: on ClearGrasp benchmark, reduces mean error by 24.4% and improves 11.25° accuracy by 22.8%; on ClearPose, achieves 15.2% reduction in mean error.

Conclusion: TransNormal effectively addresses transparent object normal estimation challenges through diffusion priors and semantic integration, with code and new TransNormal-Synthetic dataset to be publicly released.

Abstract: Monocular normal estimation for transparent objects is critical for laboratory automation, yet it remains challenging due to complex light refraction and reflection. These optical properties often lead to catastrophic failures in conventional depth and normal sensors, hindering the deployment of embodied AI in scientific environments. We propose TransNormal, a novel framework that adapts pre-trained diffusion priors for single-step normal regression. To handle the lack of texture in transparent surfaces, TransNormal integrates dense visual semantics from DINOv3 via a cross-attention mechanism, providing strong geometric cues. Furthermore, we employ a multi-task learning objective and wavelet-based regularization to ensure the preservation of fine-grained structural details. To support this task, we introduce TransNormal-Synthetic, a physics-based dataset with high-fidelity normal maps for transparent labware. Extensive experiments demonstrate that TransNormal significantly outperforms state-of-the-art methods: on the ClearGrasp benchmark, it reduces mean error by 24.4% and improves 11.25° accuracy by 22.8%; on ClearPose, it achieves a 15.2% reduction in mean error. The code and dataset will be made publicly available at https://longxiang-ai.github.io/TransNormal.

</details>


### [115] [Invariance on Manifolds: Understanding Robust Visual Representations for Place Recognition](https://arxiv.org/abs/2602.00841)
*Jintao Cheng,Weibin Li,Zhijian He,Jin Wu,Chi Man Vong,Wei Zhang*

Main category: cs.CV

TL;DR: Training-free VPR framework using second-order geometric statistics on SPD manifolds for robust zero-shot place recognition


<details>
  <summary>Details</summary>
Motivation: Current VPR methods need lots of supervised data or use simple statistics, missing structural correlations needed for robustness to environmental/viewpoint changes

Method: Represent scenes as covariance descriptors on SPD manifold, model perturbations as congruence transformations, use Riemannian mappings to linearize into Euclidean space, all training-free using fixed pre-trained backbones

Result: Achieves highly competitive performance vs SOTA baselines, especially excels in challenging zero-shot scenarios without parameter updates

Conclusion: Second-order geometric statistics on SPD manifolds provide effective training-free framework for robust VPR with strong zero-shot generalization

Abstract: Visual Place Recognition (VPR) demands representations robust to drastic environmental and viewpoint shifts. Current aggregation paradigms, however, either rely on data-hungry supervision or simplistic first-order statistics, often neglecting intrinsic structural correlations. In this work, we propose a Second-Order Geometric Statistics framework that inherently captures geometric stability without training. We conceptualize scenes as covariance descriptors on the Symmetric Positive Definite (SPD) manifold, where perturbations manifest as tractable congruence transformations. By leveraging geometry-aware Riemannian mappings, we project these descriptors into a linearized Euclidean embedding, effectively decoupling signal structure from noise. Our approach introduces a training-free framework built upon fixed, pre-trained backbones, achieving strong zero-shot generalization without parameter updates. Extensive experiments confirm that our method achieves highly competitive performance against state-of-the-art baselines, particularly excelling in challenging zero-shot scenarios.

</details>


### [116] [Distill3R: A Pipeline for Democratizing 3D Foundation Models on Commodity Hardware](https://arxiv.org/abs/2602.00865)
*Brandon Leblanc,Charalambos Poullis*

Main category: cs.CV

TL;DR: Distill3R distills large 3D foundation models into compact students trainable on single workstations, enabling accessible 3D vision research without massive compute clusters.


<details>
  <summary>Details</summary>
Motivation: Large-scale 3D foundation models require massive computational clusters for training, creating a significant barrier to entry for academic laboratories without access to such resources.

Method: Two key innovations: (1) offline caching pipeline that decouples heavy teacher inference from training via compressed supervision signals, and (2) confidence-aware distillation loss leveraging teacher uncertainty for training on commodity hardware.

Result: 72M-parameter student achieves 9x parameter reduction and 5x inference speedup compared to 650M-parameter teacher. Student trains in under 3 days on single workstation vs. teacher requiring week-long training on GPU clusters.

Conclusion: Distill3R provides reproducible, single-workstation training recipe for democratized 3D vision research, serving as accessible baseline for labs to train domain-specific models without large-scale compute.

Abstract: While multi-view 3D reconstruction has shifted toward large-scale foundation models capable of inferring globally consistent geometry, their reliance on massive computational clusters for training has created a significant barrier to entry for most academic laboratories. To bridge this compute divide, we introduce Distill3R, a framework designed to distill the geometric reasoning of 3D foundation models into compact students fully trainable on a single workstation. Our methodology centers on two primary innovations: (1) an offline caching pipeline that decouples heavy teacher inference from the training loop through compressed supervision signals, and (2) a confidence-aware distillation loss that leverages teacher uncertainty to enable training on commodity hardware. We propose a 72M-parameter student model which achieves a 9x reduction in parameters and a 5x inference speedup compared to its 650M-parameter teacher. The student is fully trainable in under 3 days on a single workstation, whereas its teacher requires massive GPU clusters for up to a week. We demonstrate that the student preserves the structural consistency and qualitative geometric understanding required for functional 3D awareness. By providing a reproducible, single-workstation training recipe, Distill3R serves as an exploratory entry point for democratized 3D vision research and efficient edge deployment. This work is not intended to compete with state-of-the-art foundation models, but to provide an accessible research baseline for laboratories without access to large-scale compute to train and specialize models on their own domain-specific data at minimal cost.

</details>


### [117] [DIAMOND: Directed Inference for Artifact Mitigation in Flow Matching Models](https://arxiv.org/abs/2602.00883)
*Alicja Polowczyk,Agnieszka Polowczyk,Piotr Borycki,Joanna Waczyńska,Jacek Tabor,Przemysław Spurek*

Main category: cs.CV

TL;DR: DIAMOND is a training-free method that corrects generative trajectories during inference to reduce artifacts in text-to-image models without modifying weights or requiring additional training.


<details>
  <summary>Details</summary>
Motivation: Current text-to-image models like FLUX still produce visual and anatomical artifacts that hinder practical use. Existing artifact reduction methods are post-hoc, invasive to model weights, or computationally expensive for regional refinement.

Method: DIAMOND applies trajectory correction during inference by reconstructing clean sample estimates at each generative step, actively steering the process away from latent states that lead to artifacts.

Result: DIAMOND provides a robust, zero-shot path to high-fidelity, artifact-free image synthesis without additional training or weight modifications, and extends to standard Diffusion Models.

Conclusion: DIAMOND offers an effective training-free solution for artifact reduction in modern generative architectures by intervening during the core image formation process rather than post-hoc.

Abstract: Despite impressive results from recent text-to-image models like FLUX, visual and anatomical artifacts remain a significant hurdle for practical and professional use. Existing methods for artifact reduction, typically work in a post-hoc manner, consequently failing to intervene effectively during the core image formation process. Notably, current techniques require problematic and invasive modifications to the model weights, or depend on a computationally expensive and time-consuming process of regional refinement. To address these limitations, we propose DIAMOND, a training-free method that applies trajectory correction to mitigate artifacts during inference. By reconstructing an estimate of the clean sample at every step of the generative trajectory, DIAMOND actively steers the generation process away from latent states that lead to artifacts. Furthermore, we extend the proposed method to standard Diffusion Models, demonstrating that DIAMOND provides a robust, zero-shot path to high-fidelity, artifact-free image synthesis without the need for additional training or weight modifications in modern generative architectures. Code is available at https://gmum.github.io/DIAMOND/

</details>


### [118] [OCTOPUS: Enhancing the Spatial-Awareness of Vision SSMs with Multi-Dimensional Scans and Traversal Selection](https://arxiv.org/abs/2602.00904)
*Kunal Mahatha,Ali Bahri,Pierre Marza,Sahar Dastani,Maria Vakalopoulou,Stergios Christodoulidis,Jose Dolz,Christian Desrosiers*

Main category: cs.CV

TL;DR: OCTOPUS is a novel vision architecture using multi-directional state space models that preserves both global context and local spatial structure while maintaining linear complexity, addressing limitations of standard SSMs in vision tasks.


<details>
  <summary>Details</summary>
Motivation: Standard state space models (SSMs) have limited success in vision tasks because their causal formulation, designed for sequential text, breaks spatial relationships in images. They fail to capture local spatial coherence, often linking non-adjacent patches while ignoring neighboring visually correlated ones.

Method: OCTOPUS performs discrete reoccurrence along eight principal orientations (forward/backward in horizontal, vertical, and diagonal directions), enabling effective information exchange across all spatially connected regions while maintaining independence among unrelated patches. This multi-directional recurrence captures both global context and local spatial structure with SSM-level efficiency.

Result: In classification and segmentation benchmarks, OCTOPUS demonstrates notable improvements in boundary preservation and region consistency in segmentation results, while maintaining relatively better classification accuracy compared to existing V-SSM based models.

Conclusion: OCTOPUS emerges as a foundation method for multi-directional recurrence, providing a scalable and effective mechanism for building spatially aware and computationally efficient vision architectures that preserve both global context and local spatial structure.

Abstract: State space models (SSMs) have recently emerged as an alternative to transformers due to their unique ability of modeling global relationships in text with linear complexity. However, their success in vision tasks has been limited due to their causal formulation, which is suitable for sequential text but detrimental in the spatial domain where causality breaks the inherent spatial relationships among pixels or patches. As a result, standard SSMs fail to capture local spatial coherence, often linking non-adjacent patches while ignoring neighboring ones that are visually correlated. To address these limitations, we introduce OCTOPUS , a novel architecture that preserves both global context and local spatial structure within images, while maintaining the linear complexity of SSMs. OCTOPUS performs discrete reoccurrence along eight principal orientations, going forward or backward in the horizontal, vertical, and diagonal directions, allowing effective information exchange across all spatially connected regions while maintaining independence among unrelated patches. This design enables multi-directional recurrence, capturing both global context and local spatial structure with SSM-level efficiency. In our classification and segmentation benchmarks, OCTOPUS demonstrates notable improvements in boundary preservation and region consistency, as evident from the segmentation results, while maintaining relatively better classification accuracy compared to existing V-SSM based models. These results suggest that OCTOPUS appears as a foundation method for multi-directional recurrence as a scalable and effective mechanism for building spatially aware and computationally efficient vision architectures.

</details>


### [119] [ConsensusDrop: Fusing Visual and Cross-Modal Saliency for Efficient Vision Language Models](https://arxiv.org/abs/2602.00946)
*Dhruv Parikh,Haoyang Fan,Rajgopal Kannan,Viktor Prasanna*

Main category: cs.CV

TL;DR: ConsensusDrop fuses vision encoder saliency with LLM cross-attention to reduce redundant visual tokens in VLMs, achieving better accuracy-efficiency trade-offs than prior methods.


<details>
  <summary>Details</summary>
Motivation: Vision-Language Models are expensive due to processing hundreds of redundant visual tokens. Existing token reduction methods use either vision-encoder saliency (query-agnostic) or LLM cross-attention (query-aware but costly), but neither alone is sufficient for optimal performance.

Method: Proposes ConsensusDrop, a training-free framework that derives consensus ranking by reconciling vision encoder saliency with query-aware cross-attention. It retains the most informative tokens while compressing the remainder via encoder-guided token merging.

Result: ConsensusDrop consistently outperforms prior pruning methods under identical token budgets across LLaVA-1.5/NeXT, Video-LLaVA, and other open-source VLMs. It delivers stronger accuracy-efficiency Pareto frontier, preserving near-baseline accuracy even at aggressive token reductions while reducing TTFT and KV cache footprint.

Conclusion: Fusing vision encoder saliency with LLM cross-attention is essential for effective visual token reduction in VLMs. ConsensusDrop provides a practical, training-free solution that achieves superior efficiency-accuracy trade-offs compared to unimodal approaches.

Abstract: Vision-Language Models (VLMs) are expensive because the LLM processes hundreds of largely redundant visual tokens. Existing token reduction methods typically exploit \textit{either} vision-encoder saliency (broad but query-agnostic) \textit{or} LLM cross-attention (query-aware but sparse and costly). We show that neither signal alone is sufficient: fusing them consistently improves performance compared to unimodal visual token selection (ranking). However, making such fusion practical is non-trivial: cross-modal saliency is usually only available \emph{inside} the LLM (too late for efficient pre-LLM pruning), and the two signals are inherently asymmetric, so naive fusion underutilizes their complementary strengths. We propose \textbf{ConsensusDrop}, a training-free framework that derives a \emph{consensus} ranking by reconciling vision encoder saliency with query-aware cross-attention, retaining the most informative tokens while compressing the remainder via encoder-guided token merging. Across LLaVA-1.5/NeXT, Video-LLaVA, and other open-source VLMs, ConsensusDrop consistently outperforms prior pruning methods under identical token budgets and delivers a stronger accuracy-efficiency Pareto frontier -- preserving near-baseline accuracy even at aggressive token reductions while reducing TTFT and KV cache footprint. Our code will be open-sourced.

</details>


### [120] [Data Augmentation for High-Fidelity Generation of CAR-T/NK Immunological Synapse Images](https://arxiv.org/abs/2602.00949)
*Xiang Zhang,Boxuan Zhang,Alireza Naghizadeh,Mohab Mohamed,Dongfang Liu,Ruixiang Tang,Dimitris Metaxas,Dongfang Liu*

Main category: cs.CV

TL;DR: The paper presents two complementary data augmentation frameworks (IAAA and SAAA) to generate synthetic CAR-T/NK immunological synapse images and segmentation masks, addressing limited annotated microscopy datasets and improving IS detection/segmentation for immunotherapy biomarker development.


<details>
  <summary>Details</summary>
Motivation: Limited size of annotated microscopy datasets restricts artificial neural networks' ability to generalize for accurate CAR-T/NK immunological synapse detection and segmentation, which is crucial for developing reliable imaging-based biomarkers to predict therapeutic efficacy in cancer immunotherapy.

Method: Two complementary data augmentation frameworks: 1) Instance Aware Automatic Augmentation (IAAA) - automated, instance-preserving augmentation method applying optimized policies to original IS data; 2) Semantic-Aware AI Augmentation (SAAA) - combines diffusion-based mask generator with Pix2Pix conditional image synthesizer to create diverse, anatomically realistic masks and corresponding high-fidelity images.

Result: The augmentation strategies generate synthetic images with visual and structural properties closely matching real IS data, significantly improving CAR-T/NK IS detection and segmentation performance, enhancing robustness and accuracy of IS quantification.

Conclusion: By addressing the data limitation challenge through complementary augmentation frameworks, this work supports development of more reliable imaging-based biomarkers for predicting patient response to CAR-T/NK immunotherapy, potentially improving cancer treatment outcomes.

Abstract: Chimeric antigen receptor (CAR)-T and NK cell immunotherapies have transformed cancer treatment, and recent studies suggest that the quality of the CAR-T/NK cell immunological synapse (IS) may serve as a functional biomarker for predicting therapeutic efficacy. Accurate detection and segmentation of CAR-T/NK IS structures using artificial neural networks (ANNs) can greatly increase the speed and reliability of IS quantification. However, a persistent challenge is the limited size of annotated microscopy datasets, which restricts the ability of ANNs to generalize. To address this challenge, we integrate two complementary data-augmentation frameworks. First, we employ Instance Aware Automatic Augmentation (IAAA), an automated, instance-preserving augmentation method that generates synthetic CAR-T/NK IS images and corresponding segmentation masks by applying optimized augmentation policies to original IS data. IAAA supports multiple imaging modalities (e.g., fluorescence and brightfield) and can be applied directly to CAR-T/NK IS images derived from patient samples. In parallel, we introduce a Semantic-Aware AI Augmentation (SAAA) pipeline that combines a diffusion-based mask generator with a Pix2Pix conditional image synthesizer. This second method enables the creation of diverse, anatomically realistic segmentation masks and produces high-fidelity CAR-T/NK IS images aligned with those masks, further expanding the training corpus beyond what IAAA alone can provide. Together, these augmentation strategies generate synthetic images whose visual and structural properties closely match real IS data, significantly improving CAR-T/NK IS detection and segmentation performance. By enhancing the robustness and accuracy of IS quantification, this work supports the development of more reliable imaging-based biomarkers for predicting patient response to CAR-T/NK immunotherapy.

</details>


### [121] [Hybrid Topological and Deep Feature Fusion for Accurate MRI-Based Alzheimer's Disease Severity Classification](https://arxiv.org/abs/2602.00956)
*Faisal Ahmed*

Main category: cs.CV

TL;DR: Novel hybrid deep learning framework combining Topological Data Analysis (TDA) with DenseNet121 achieves 99.93% accuracy for four-class Alzheimer's disease classification on OASIS MRI data.


<details>
  <summary>Details</summary>
Motivation: Early and accurate diagnosis of Alzheimer's disease remains challenging in neuroimaging-based clinical systems. Conventional neural networks may overlook important topological characteristics of brain structures that could improve classification performance.

Method: Proposed hybrid framework integrates TDA with DenseNet121 backbone for four-class AD classification using structural MRI data. TDA captures complementary topological features of brain structures, while DenseNet121 learns hierarchical spatial features from MRI slices. Features are fused to enhance class separability across AD stages.

Result: Model achieves 99.93% accuracy and 100% AUC on OASIS-1 Kaggle MRI dataset, significantly outperforming existing state-of-the-art approaches including CNN-based, transfer learning, ensemble, and multi-scale architectures.

Conclusion: Incorporating topological insights into deep learning pipelines is effective for AD diagnosis. The proposed TDA+DenseNet121 framework shows potential as a robust and highly accurate tool for automated Alzheimer's disease diagnosis.

Abstract: Early and accurate diagnosis of Alzheimer's disease (AD) remains a critical challenge in neuroimaging-based clinical decision support systems. In this work, we propose a novel hybrid deep learning framework that integrates Topological Data Analysis (TDA) with a DenseNet121 backbone for four-class Alzheimer's disease classification using structural MRI data from the OASIS dataset. TDA is employed to capture complementary topological characteristics of brain structures that are often overlooked by conventional neural networks, while DenseNet121 efficiently learns hierarchical spatial features from MRI slices. The extracted deep and topological features are fused to enhance class separability across the four AD stages.
  Extensive experiments conducted on the OASIS-1 Kaggle MRI dataset demonstrate that the proposed TDA+DenseNet121 model significantly outperforms existing state-of-the-art approaches. The model achieves an accuracy of 99.93% and an AUC of 100%, surpassing recently published CNN-based, transfer learning, ensemble, and multi-scale architectures. These results confirm the effectiveness of incorporating topological insights into deep learning pipelines and highlight the potential of the proposed framework as a robust and highly accurate tool for automated Alzheimer's disease diagnosis.

</details>


### [122] [Unveiling the Cognitive Compass: Theory-of-Mind-Guided Multimodal Emotion Reasoning](https://arxiv.org/abs/2602.00971)
*Meng Luo,Bobo Li,Shanqing Xu,Shize Zhang,Qiuchan Chen,Menglu Han,Wenhao Chen,Yanxiang Huang,Hao Fei,Mong-Li Lee,Wynne Hsu*

Main category: cs.CV

TL;DR: HitEmotion introduces a Theory of Mind-grounded benchmark and reasoning methods to enhance multimodal LLMs' emotional understanding by modeling cognitive depth and mental states.


<details>
  <summary>Details</summary>
Motivation: Current multimodal large language models lack deep emotional understanding capabilities, which requires explicit modeling of Theory of Mind as the cognitive foundation from which emotions arise.

Method: Three main contributions: 1) HitEmotion benchmark with hierarchical cognitive depth levels, 2) ToM-guided reasoning chain tracking mental states and calibrating cross-modal evidence, 3) TMPO reinforcement learning using intermediate mental states as process-level supervision.

Result: HitEmotion exposes deep emotional reasoning deficits in state-of-the-art models, especially on cognitively demanding tasks. The proposed methods improve end-task accuracy and yield more faithful, coherent rationales.

Conclusion: Provides practical toolkit for evaluating and enhancing cognition-based emotional understanding in MLLMs, with dataset and code publicly available.

Abstract: Despite rapid progress in multimodal large language models (MLLMs), their capability for deep emotional understanding remains limited. We argue that genuine affective intelligence requires explicit modeling of Theory of Mind (ToM), the cognitive substrate from which emotions arise. To this end, we introduce HitEmotion, a ToM-grounded hierarchical benchmark that diagnoses capability breakpoints across increasing levels of cognitive depth. Second, we propose a ToM-guided reasoning chain that tracks mental states and calibrates cross-modal evidence to achieve faithful emotional reasoning. We further introduce TMPO, a reinforcement learning method that uses intermediate mental states as process-level supervision to guide and strengthen model reasoning. Extensive experiments show that HitEmotion exposes deep emotional reasoning deficits in state-of-the-art models, especially on cognitively demanding tasks. In evaluation, the ToM-guided reasoning chain and TMPO improve end-task accuracy and yield more faithful, more coherent rationales. In conclusion, our work provides the research community with a practical toolkit for evaluating and enhancing the cognition-based emotional understanding capabilities of MLLMs. Our dataset and code are available at: https://HitEmotion.github.io/.

</details>


### [123] [VAMOS-OCTA: Vessel-Aware Multi-Axis Orthogonal Supervision for Inpainting Motion-Corrupted OCT Angiography Volumes](https://arxiv.org/abs/2602.00995)
*Nick DiSanto,Ehsan Khodapanah Aghdam,Han Liu,Jacob Watson,Yuankai K. Tao,Hao Li,Ipek Oguz*

Main category: cs.CV

TL;DR: VAMOS-OCTA is a deep learning framework that uses vessel-aware multi-axis supervision to inpaint motion-corrupted B-scans in handheld OCTA imaging, improving both cross-sectional and volumetric image quality.


<details>
  <summary>Details</summary>
Motivation: Handheld OCTA enables retinal imaging in uncooperative subjects but suffers from severe motion artifacts that create unsampled regions (blank bands) in volumetric data, degrading both B-scan quality and en face projections.

Method: A 2.5D U-Net architecture takes neighboring B-scans as input to reconstruct corrupted center B-scans, guided by a novel Vessel-Aware Multi-Axis Orthogonal Supervision (VAMOS) loss that combines vessel-weighted intensity reconstruction with axial and lateral projection consistency.

Result: VAMOS-OCTA outperforms prior methods, producing reconstructions with sharp capillaries, restored vessel continuity, and clean en face projections, as demonstrated through both perceptual quality and pixel-wise accuracy metrics on synthetic and real-world corrupted volumes.

Conclusion: Multi-axis supervision provides a powerful constraint for restoring motion-degraded 3D OCTA data, enabling improved retinal imaging in challenging clinical scenarios where motion artifacts are prevalent.

Abstract: Handheld Optical Coherence Tomography Angiography (OCTA) enables noninvasive retinal imaging in uncooperative or pediatric subjects, but is highly susceptible to motion artifacts that severely degrade volumetric image quality. Sudden motion during 3D acquisition can lead to unsampled retinal regions across entire B-scans (cross-sectional slices), resulting in blank bands in en face projections. We propose VAMOS-OCTA, a deep learning framework for inpainting motion-corrupted B-scans using vessel-aware multi-axis supervision. We employ a 2.5D U-Net architecture that takes a stack of neighboring B-scans as input to reconstruct a corrupted center B-scan, guided by a novel Vessel-Aware Multi-Axis Orthogonal Supervision (VAMOS) loss. This loss combines vessel-weighted intensity reconstruction with axial and lateral projection consistency, encouraging vascular continuity in native B-scans and across orthogonal planes. Unlike prior work that focuses primarily on restoring the en face MIP, VAMOS-OCTA jointly enhances both cross-sectional B-scan sharpness and volumetric projection accuracy, even under severe motion corruptions. We trained our model on both synthetic and real-world corrupted volumes and evaluated its performance using both perceptual quality and pixel-wise accuracy metrics. VAMOS-OCTA consistently outperforms prior methods, producing reconstructions with sharp capillaries, restored vessel continuity, and clean en face projections. These results demonstrate that multi-axis supervision offers a powerful constraint for restoring motion-degraded 3D OCTA data. Our source code is available at https://github.com/MedICL-VU/VAMOS-OCTA.

</details>


### [124] [CortiNet: A Physics-Perception Hybrid Cortical-Inspired Dual-Stream Network for Gallbladder Disease Diagnosis from Ultrasound](https://arxiv.org/abs/2602.01000)
*Vagish Kumar,Souvik Chakraborty*

Main category: cs.CV

TL;DR: CortiNet: A lightweight, cortical-inspired dual-stream neural network for gallbladder disease diagnosis from ultrasound images that separates structural and textural features for efficient, interpretable analysis.


<details>
  <summary>Details</summary>
Motivation: Ultrasound imaging is primary for gallbladder disease diagnosis but suffers from low resolution and speckle noise. Existing deep learning approaches use large CNNs that are difficult to deploy clinically. Need for lightweight, interpretable models that can handle ultrasound-specific challenges.

Method: CortiNet uses a cortical-inspired dual-stream architecture that separates low-frequency structural information from high-frequency perceptual details through specialized encoding streams. It employs multi-scale signal decomposition and late-stage cortical-style fusion. Also includes a structure-aware explainability framework using gradient-weighted class activation mapping only on the structural branch.

Result: Achieves 98.74% diagnostic accuracy on 10,692 expert-annotated images spanning nine gallbladder disease categories, with significantly fewer parameters than conventional deep convolutional models.

Conclusion: CortiNet provides an efficient, lightweight solution for gallbladder disease diagnosis that combines physics-based inductive bias with perception-driven learning, offering high accuracy with clinical deployability and interpretability.

Abstract: Ultrasound imaging is the primary diagnostic modality for detecting Gallbladder diseases due to its non-invasive nature, affordability, and wide accessibility. However, the low resolution and speckle noise inherent to ultrasound images hinder diagnostic reliability, prompting the use of large convolutional neural networks that are difficult to deploy in routine clinical settings. In this work, we propose CortiNet, a lightweight, cortical-inspired dual-stream neural architecture for gallbladder disease diagnosis that integrates physically interpretable multi-scale signal decomposition with perception-driven feature learning. Inspired by parallel processing pathways in the human visual cortex, CortiNet explicitly separates low-frequency structural information from high-frequency perceptual details and processes them through specialized encoding streams. By operating directly on structured, frequency-selective representations rather than raw pixel intensities, the architecture embeds strong physics-based inductive bias, enabling efficient feature learning with a significantly reduced parameter footprint. A late-stage cortical-style fusion mechanism integrates complementary structural and textural cues while preserving computational efficiency. Additionally, we propose a structure-aware explainability framework wherein gradient-weighted class activation mapping is only applied to the structural branch of the proposed CortiNet architecture. This choice allows the model to only focus on the structural features, making it robust against speckle noise. We evaluate CortiNet on 10,692 expert-annotated images spanning nine clinically relevant gallbladder disease categories. Experimental results demonstrate that CortiNet achieves high diagnostic accuracy (98.74%) with only a fraction of the parameters required by conventional deep convolutional models.

</details>


### [125] [SRVAU-R1: Enhancing Video Anomaly Understanding via Reflection-Aware Learning](https://arxiv.org/abs/2602.01004)
*Zihao Zhao,Shengting Cao,Muchao Ye*

Main category: cs.CV

TL;DR: SRVAU-R1 introduces a reflection-aware learning framework that enhances MLLM reasoning for video anomaly understanding through self-reflection and self-correction mechanisms.


<details>
  <summary>Details</summary>
Motivation: Existing MLLM-based approaches for video anomaly understanding focus only on surface-level descriptions and lack deep reasoning capabilities like self-reflection and self-correction.

Method: Proposes SRVAU-R1 framework with: 1) First reflection-oriented Chain-of-Thought dataset for VAU with structured supervision (initial reasoning, self-reflection, revised reasoning), 2) Reflection-aware learning paradigm combining supervised fine-tuning and reinforcement fine-tuning.

Result: Extensive experiments on multiple video anomaly benchmarks show SRVAU-R1 consistently outperforms existing methods with significant improvements in both temporal anomaly localization accuracy and reasoning quality.

Conclusion: The reflection-aware learning framework effectively enhances multi-modal reasoning for video anomaly understanding, demonstrating the importance of incorporating self-reflection mechanisms in MLLM-based approaches.

Abstract: Multi-modal large language models (MLLMs) have demonstrated significant progress in reasoning capabilities and shown promising effectiveness in video anomaly understanding (VAU) tasks. However, existing MLLM-based approaches remain largely focused on surface-level descriptions of anomalies, lacking deep reasoning over abnormal behaviors like explicit self-reflection and self-correction. To address that, we propose Self-Reflection-Enhanced Reasoning for Video Anomaly Understanding (SRVAU-R1), a reflection-aware learning framework that incorporates reflection in MLLM reasoning. Specifically, SRVAU-R1 introduces the first reflection-oriented Chain-of-Thought dataset tailored for VAU, providing structured supervision with initial reasoning, self-reflection, and revised reasoning. Based on that, it includes a novel reflection-aware learning paradigm with supervised fine-tuning and reinforcement fine-tuning to enhance multi-modal reasoning for VAU. Extensive experiments on multiple video anomaly benchmarks demonstrate that SRVAU-R1 consistently outperforms existing methods, achieving significant improvements in both temporal anomaly localization accuracy and reasoning quality.

</details>


### [126] [LocalScore: Local Density-Aware Similarity Scoring for Biometrics](https://arxiv.org/abs/2602.01012)
*Yiyang Su,Minchul Kim,Jie Zhu,Christopher Perry,Feng Liu,Anil Jain,Xiaoming Liu*

Main category: cs.CV

TL;DR: LocalScore improves open-set biometrics by using k-nearest neighbors to incorporate local gallery density, achieving significant performance gains with minimal computational overhead.


<details>
  <summary>Details</summary>
Motivation: Traditional biometric systems struggle with open-set scenarios where probe subjects may not be enrolled in the gallery. Most existing methods collapse intra-subject variability into single global representations, leading to poor decision boundaries and suboptimal open-set robustness, especially problematic with multi-sample galleries common in real-world deployments.

Method: Proposes LocalScore, a simple scoring algorithm that explicitly incorporates the local density of gallery feature distribution using k-th nearest neighbors. The method is architecture-agnostic, loss-independent, and adds negligible computational overhead, making it a plug-and-play solution for existing biometric systems.

Result: Extensive experiments across multiple modalities show LocalScore consistently achieves substantial gains: open-set retrieval FNIR@FPIR reduced from 53% to 40%, and verification TAR@FAR improved from 51% to 74%. Theoretical analysis and empirical validation explain when and why the method achieves most significant gains based on dataset characteristics.

Conclusion: LocalScore effectively addresses open-set biometric challenges by leveraging local gallery density, providing a practical, efficient solution that significantly improves performance across various biometric modalities without requiring architectural changes to existing systems.

Abstract: Open-set biometrics faces challenges with probe subjects who may not be enrolled in the gallery, as traditional biometric systems struggle to detect these non-mated probes. Despite the growing prevalence of multi-sample galleries in real-world deployments, most existing methods collapse intra-subject variability into a single global representation, leading to suboptimal decision boundaries and poor open-set robustness. To address this issue, we propose LocalScore, a simple yet effective scoring algorithm that explicitly incorporates the local density of the gallery feature distribution using the k-th nearest neighbors. LocalScore is architecture-agnostic, loss-independent, and incurs negligible computational overhead, making it a plug-and-play solution for existing biometric systems. Extensive experiments across multiple modalities demonstrate that LocalScore consistently achieves substantial gains in open-set retrieval (FNIR@FPIR reduced from 53% to 40%) and verification (TAR@FAR improved from 51% to 74%). We further provide theoretical analysis and empirical validation explaining when and why the method achieves the most significant gains based on dataset characteristics.

</details>


### [127] [Effectiveness of Automatically Curated Dataset in Thyroid Nodules Classification Algorithms Using Deep Learning](https://arxiv.org/abs/2602.01020)
*Jichen Yang,Jikai Zhang,Benjamin Wildman-Tobriner,Maciej A. Mazurowski*

Main category: cs.CV

TL;DR: Automatically-curated thyroid nodule datasets improve deep learning performance over manually annotated data, with full datasets outperforming accurate subsets.


<details>
  <summary>Details</summary>
Motivation: Limited availability of manually curated thyroid nodule ultrasound datasets for training deep learning models, despite previous development of automatic curation methods with unknown utility for model training.

Method: Trained deep learning models on three datasets: manually annotated, automatically-curated full dataset, and accurate subset of automatically-curated data; compared performance using AUC metrics.

Result: Automatically-curated dataset (AUC: 0.694) significantly outperformed manually annotated dataset (AUC: 0.643). Accurate subset (AUC: 0.689) performed insignificantly worse than full automatically-curated dataset.

Conclusion: Automatically-curated datasets substantially improve deep learning algorithm performance for thyroid nodule classification, and using all automatically-curated data is preferable to using only accurate subsets.

Abstract: The diagnosis of thyroid nodule cancers commonly utilizes ultrasound images. Several studies showed that deep learning algorithms designed to classify benign and malignant thyroid nodules could match radiologists' performance. However, data availability for training deep learning models is often limited due to the significant effort required to curate such datasets. The previous study proposed a method to curate thyroid nodule datasets automatically. It was tested to have a 63% yield rate and 83% accuracy. However, the usefulness of the generated data for training deep learning models remains unknown. In this study, we conducted experiments to determine whether using a automatically-curated dataset improves deep learning algorithms' performance. We trained deep learning models on the manually annotated and automatically-curated datasets. We also trained with a smaller subset of the automatically-curated dataset that has higher accuracy to explore the optimum usage of such dataset. As a result, the deep learning model trained on the manually selected dataset has an AUC of 0.643 (95% confidence interval [CI]: 0.62, 0.66). It is significantly lower than the AUC of the 6automatically-curated dataset trained deep learning model, 0.694 (95% confidence interval [CI]: 0.67, 0.73, P < .001). The AUC of the accurate subset trained deep learning model is 0.689 (95% confidence interval [CI]: 0.66, 0.72, P > .43), which is insignificantly worse than the AUC of the full automatically-curated dataset. In conclusion, we showed that using a automatically-curated dataset can substantially increase the performance of deep learning algorithms, and it is suggested to use all the data rather than only using the accurate subset.

</details>


### [128] [GMAC: Global Multi-View Constraint for Automatic Multi-Camera Extrinsic Calibration](https://arxiv.org/abs/2602.01033)
*Chentian Sun*

Main category: cs.CV

TL;DR: GMAC is a multi-camera extrinsic calibration framework that uses implicit geometric representations from multi-view reconstruction networks to estimate camera poses without explicit 3D reconstruction or manual calibration.


<details>
  <summary>Details</summary>
Motivation: Existing multi-camera calibration methods rely on calibration targets, explicit geometric modeling, or task-specific neural networks, which lack robustness and applicability in complex dynamic environments or online scenarios, making practical deployment difficult.

Method: GMAC models extrinsics as global variables constrained by latent multi-view geometric structure, prunes and reconfigures existing networks to use their latent features for extrinsic prediction via lightweight regression head, and jointly optimizes cross-view reprojection consistency and multi-view cycle consistency.

Result: Experiments on synthetic and real-world multi-camera datasets show GMAC achieves accurate and stable extrinsic estimation without explicit 3D reconstruction or manual calibration.

Conclusion: GMAC provides a new solution for efficient deployment and online calibration of multi-camera systems by leveraging implicit geometric representations from existing reconstruction networks.

Abstract: Automatic calibration of multi-camera systems, namely the accurate estimation of spatial extrinsic parameters, is fundamental for 3D reconstruction, panoramic perception, and multi-view data fusion. Existing methods typically rely on calibration targets, explicit geometric modeling, or task-specific neural networks. Such approaches often exhibit limited robustness and applicability in complex dynamic environments or online scenarios, making them difficult to deploy in practical applications. To address this, this paper proposes GMAC, a multi-camera extrinsic estimation framework based on the implicit geometric representations learned by multi-view reconstruction networks. GMAC models extrinsics as global variables constrained by the latent multi-view geometric structure and prunes and structurally reconfigures existing networks so that their latent features can directly support extrinsic prediction through a lightweight regression head, without requiring a completely new network design. Furthermore, GMAC jointly optimizes cross-view reprojection consistency and multi-view cycle consistency, ensuring geometric coherence across cameras while improving prediction accuracy and optimization stability. Experiments on both synthetic and real-world multi-camera datasets demonstrate that GMAC achieves accurate and stable extrinsic estimation without explicit 3D reconstruction or manual calibration, providing a new solution for efficient deployment and online calibration of multi-camera systems.

</details>


### [129] [FUSE-Flow: Scalable Real-Time Multi-View Point Cloud Reconstruction Using Confidence](https://arxiv.org/abs/2602.01035)
*Chentian Sun*

Main category: cs.CV

TL;DR: FUSE-Flow: A real-time, stateless point cloud reconstruction framework using adaptive spatial hashing and weighted fusion for multi-camera systems


<details>
  <summary>Details</summary>
Motivation: Existing multi-view point cloud reconstruction methods struggle with real-time performance, computational complexity, and scalability while maintaining quality. Current voxel-based, temporal accumulation, and global optimization approaches fail to balance speed, quality, and extensibility for large-scale multi-camera systems.

Method: Frame-wise stateless framework where each frame independently generates point cloud fragments. Uses two weights (measurement confidence and 3D distance consistency) for noise suppression. Introduces adaptive spatial hashing-based weighted aggregation: 3D space is partitioned by local density, representative points are selected per cell, and weighted fusion handles sparse/dense regions. GPU parallelized for linear complexity.

Result: Achieves high-throughput, low-latency point cloud generation with linear complexity. Improves reconstruction stability and geometric fidelity in overlapping, depth-discontinuous, and dynamic scenes. Maintains real-time frame rates on modern GPUs, demonstrating effectiveness, robustness, and scalability.

Conclusion: FUSE-Flow successfully addresses the real-time multi-view point cloud reconstruction challenge by providing a linearly scalable, stateless framework that balances reconstruction quality with computational efficiency, enabling practical applications in VR/AR, robotics, and digital twins.

Abstract: Real-time multi-view point cloud reconstruction is a core problem in 3D vision and immersive perception, with wide applications in VR, AR, robotic navigation, digital twins, and computer interaction. Despite advances in multi-camera systems and high-resolution depth sensors, fusing large-scale multi-view depth observations into high-quality point clouds under strict real-time constraints remains challenging. Existing methods relying on voxel-based fusion, temporal accumulation, or global optimization suffer from high computational complexity, excessive memory usage, and limited scalability, failing to simultaneously achieve real-time performance, reconstruction quality, and multi-camera extensibility. We propose FUSE-Flow, a frame-wise, stateless, and linearly scalable point cloud streaming reconstruction framework. Each frame independently generates point cloud fragments, fused via two weights, measurement confidence and 3D distance consistency to suppress noise while preserving geometric details. For large-scale multi-camera efficiency, we introduce an adaptive spatial hashing-based weighted aggregation method: 3D space is adaptively partitioned by local point cloud density, representative points are selected per cell, and weighted fusion is performed to handle both sparse and dense regions. With GPU parallelization, FUSE-Flow achieves high-throughput, low-latency point cloud generation and fusion with linear complexity. Experiments demonstrate that the framework improves reconstruction stability and geometric fidelity in overlapping, depth-discontinuous, and dynamic scenes, while maintaining real-time frame rates on modern GPUs, verifying its effectiveness, robustness, and scalability.

</details>


### [130] [VEQ: Modality-Adaptive Quantization for MoE Vision-Language Models](https://arxiv.org/abs/2602.01037)
*Guangshuo Qin,Zhiteng Li,Zheng Chen,Weihang Zhang,Linghe Kong,Yulun Zhang*

Main category: cs.CV

TL;DR: VEQ is a dual-aware quantization framework for Mixture-of-Experts VLMs that addresses cross-modal differences and expert heterogeneity through modality-expert-aware and modality-affinity-aware quantization.


<details>
  <summary>Details</summary>
Motivation: MoE VLMs offer great performance but have prohibitive memory/computational costs. Existing quantization methods fail to address two critical heterogeneities: discrepancy between vision/language tokens and non-uniform contributions of different experts.

Method: VEQ uses two key techniques: 1) Modality-expert-aware quantization that prioritizes error minimization for pivotal experts using expert activation frequency, and 2) Modality-affinity-aware quantization that constructs enhanced Hessian matrix by integrating token-expert affinity with modality information to guide calibration.

Result: Under W3A16 configuration, VEQ achieves significant average accuracy gains of 2.04% on Kimi-VL and 3.09% on Qwen3-VL compared to previous SOTA quantization methods, demonstrating superior robustness across various multimodal tasks.

Conclusion: VEQ effectively addresses the dual heterogeneity in MoE VLMs through its dual-aware quantization framework, consistently outperforming state-of-the-art baselines while maintaining computational efficiency.

Abstract: Mixture-of-Experts(MoE) Vision-Language Models (VLMs) offer remarkable performance but incur prohibitive memory and computational costs, making compression essential. Post-Training Quantization (PTQ) is an effective training-free technique to address the massive memory and computation overhead. Existing quantization paradigms fall short as they are oblivious to two critical forms of heterogeneity: the inherent discrepancy between vision and language tokens, and the non-uniform contribution of different experts. To bridge this gap, we propose Visual Expert Quantization (VEQ), a dual-aware quantization framework designed to simultaneously accommodate cross-modal differences and heterogeneity between experts. Specifically, VEQ incorporates 1)Modality-expert-aware Quantization, which utilizes expert activation frequency to prioritize error minimization for pivotal experts, and 2)Modality-affinity-aware Quantization, which constructs an enhanced Hessian matrix by integrating token-expert affinity with modality information to guide the calibration process. Extensive experiments across diverse benchmarks verify that VEQ consistently outperforms state-of-the-art baselines. Specifically, under the W3A16 configuration, our method achieves significant average accuracy gains of 2.04\% on Kimi-VL and 3.09\% on Qwen3-VL compared to the previous SOTA quantization methods, demonstrating superior robustness across various multimodal tasks. Our code will be available at https://github.com/guangshuoqin/VEQ.

</details>


### [131] [From Videos to Conversations: Egocentric Instructions for Task Assistance](https://arxiv.org/abs/2602.01038)
*Lavisha Aggarwal,Vikas Bahirwani,Andrea Colaco*

Main category: cs.CV

TL;DR: Automated framework converts instructional videos into expert-novice conversations, creating HowToDIV dataset for multimodal task guidance AI.


<details>
  <summary>Details</summary>
Motivation: AI agents for AR task assistance need large-scale multimodal conversational data, but manual collection is costly and complex. Need scalable alternative.

Method: Automatic pipeline using LLMs to transform single-person instructional videos into two-person expert-novice conversations with multimodal content.

Result: Created HowToDIV dataset: 507 conversations, 6,636 QA pairs, 24 hours of video across multiple domains. Baseline results with Gemma 3 and Qwen 2.5.

Conclusion: Framework enables scalable multimodal conversational data generation for task guidance AI, with HowToDIV providing benchmark for future research.

Abstract: Many everyday tasks, ranging from appliance repair and cooking to car maintenance, require expert knowledge, particularly for complex, multi-step procedures. Despite growing interest in AI agents for augmented reality (AR) assistance, progress remains limited by the scarcity of large-scale multimodal conversational datasets grounded in real-world task execution, in part due to the cost and logistical complexity of human-assisted data collection. In this paper, we present a framework to automatically transform single person instructional videos into two-person multimodal task-guidance conversations. Our fully automatic pipeline, based on large language models, provides a scalable and cost efficient alternative to traditional data collection approaches. Using this framework, we introduce HowToDIV, a multimodal dataset comprising 507 conversations, 6,636 question answer pairs, and 24 hours of video spanning multiple domains. Each session consists of a multi-turn expert-novice interaction. Finally, we report baseline results using Gemma 3 and Qwen 2.5 on HowToDIV, providing an initial benchmark for multimodal procedural task assistance.

</details>


### [132] [ReLayout: Versatile and Structure-Preserving Design Layout Editing via Relation-Aware Design Reconstruction](https://arxiv.org/abs/2602.01046)
*Jiawei Lin,Shizhao Sun,Danqing Huang,Ting Liu,Ji Li,Jiang Bian*

Main category: cs.CV

TL;DR: ReLayout is a novel framework for automated design layout editing that preserves layout structure without requiring triplet training data, using relation graphs and self-supervised learning.


<details>
  <summary>Details</summary>
Motivation: Automated redesign without manual adjustments is a key advancement in design workflows. Current challenges include ambiguity in natural language user intents, the need to preserve layout structure of unedited elements, and scarcity of training data (original design, editing operation, edited design triplets).

Method: ReLayout introduces relation graphs to capture position/size relationships among unedited elements as constraints. Uses relation-aware design reconstruction (RADR) with multi-modal LLM backbone to learn editing in self-supervised manner by reconstructing designs from elements, relation graphs, and synthesized editing operations.

Result: Qualitative, quantitative results and user studies show ReLayout significantly outperforms baseline models in editing quality, accuracy, and layout structure preservation.

Conclusion: ReLayout provides a versatile, structure-preserving framework for design layout editing that operates without triplet data, representing an important step toward fully automated redesign workflows.

Abstract: Automated redesign without manual adjustments marks a key step forward in the design workflow. In this work, we focus on a foundational redesign task termed design layout editing, which seeks to autonomously modify the geometric composition of a design based on user intents. To overcome the ambiguity of user needs expressed in natural language, we introduce four basic and important editing actions and standardize the format of editing operations. The underexplored task presents a unique challenge: satisfying specified editing operations while simultaneously preserving the layout structure of unedited elements. Besides, the scarcity of triplet (original design, editing operation, edited design) samples poses another formidable challenge. To this end, we present ReLayout, a novel framework for versatile and structure-preserving design layout editing that operates without triplet data. Specifically, ReLayout first introduces the relation graph, which contains the position and size relationships among unedited elements, as the constraint for layout structure preservation. Then, relation-aware design reconstruction (RADR) is proposed to bypass the data challenge. By learning to reconstruct a design from its elements, a relation graph, and a synthesized editing operation, RADR effectively emulates the editing process in a self-supervised manner. A multi-modal large language model serves as the backbone for RADR, unifying multiple editing actions within a single model and thus achieving versatile editing after fine-tuning. Qualitative, quantitative results and user studies show that ReLayout significantly outperforms the baseline models in terms of editing quality, accuracy, and layout structure preservation.

</details>


### [133] [Residual Decoding: Mitigating Hallucinations in Large Vision-Language Models via History-Aware Residual Guidance](https://arxiv.org/abs/2602.01047)
*Xinrong Chen,Xu Chu,Yingmin Qiu,Hengyuan Zhang,Jing Xiong,Shiyu Tang,Shuai Liu,Shaokang Yang,Cheng Yang,Hayden Kwok-Hay So,Ngai Wong*

Main category: cs.CV

TL;DR: ResDec is a training-free method that uses historical decoding information to suppress hallucinations in Large Vision-Language Models by correcting language priors and improving visual grounding.


<details>
  <summary>Details</summary>
Motivation: Large Vision-Language Models suffer from language priors and hallucinations - generating content that appears coherent but doesn't match actual visual inputs, limiting their reliability in multimodal tasks.

Method: Residual Decoding (ResDec) leverages historical information during decoding, using the model's internal implicit reasoning mechanism and token logits evolution to correct biases without requiring additional training.

Result: ResDec effectively suppresses hallucinations induced by language priors, significantly improves visual grounding, reduces object hallucinations, and performs exceptionally well on comprehensive LVLM benchmarks.

Conclusion: ResDec is a broadly applicable, training-free solution that addresses hallucination problems in LVLMs while maintaining strong performance across various multimodal tasks.

Abstract: Large Vision-Language Models (LVLMs) can reason effectively from image-text inputs and perform well in various multimodal tasks. Despite this success, they are affected by language priors and often produce hallucinations. Hallucinations denote generated content that is grammatically and syntactically coherent, yet bears no match or direct relevance to actual visual input. To address this problem, we propose Residual Decoding (ResDec). It is a novel training-free method that uses historical information to aid decoding. The method relies on the internal implicit reasoning mechanism and token logits evolution mechanism of LVLMs to correct biases. Extensive experiments demonstrate that ResDec effectively suppresses hallucinations induced by language priors, significantly improves visual grounding, and reduces object hallucinations. In addition to mitigating hallucinations, ResDec also performs exceptionally well on comprehensive LVLM benchmarks, highlighting its broad applicability.

</details>


### [134] [Baseline Method of the Foundation Model Challenge for Ultrasound Image Analysis](https://arxiv.org/abs/2602.01055)
*Bo Deng,Yitong Tang,Jiake Li,Yuxin Huang,Li Wang,Yu Zhang,Yufei Zhan,Hua Lu,Xiaoshen Zhang,Jieyun Bai*

Main category: cs.CV

TL;DR: A unified multi-task learning framework serves as the official baseline for the FM_UIA 2026 challenge, addressing ultrasound imaging heterogeneity through a single shared network supporting 27 diverse subtasks.


<details>
  <summary>Details</summary>
Motivation: Ultrasound imaging exhibits substantial heterogeneity across anatomical structures and acquisition protocols, posing challenges for developing generalizable analysis models. Most existing methods are task-specific, limiting their suitability as clinically deployable foundation models.

Method: A unified Multi-Head Multi-Task Learning (MH-MTL) framework with ImageNet-pretrained EfficientNet-B4 backbone and Feature Pyramid Network (FPN) for multi-scale feature extraction. Task-specific routing enables global tasks to use high-level semantic features while dense prediction tasks exploit spatially detailed FPN representations. Training uses composite loss with task-adaptive learning rate scaling and cosine annealing.

Result: Validation results demonstrate the feasibility and robustness of this unified design, establishing a strong and extensible baseline for ultrasound foundation model research.

Conclusion: The proposed framework provides an effective baseline for the FM_UIA 2026 challenge, showing promise for developing clinically deployable ultrasound foundation models that can handle diverse tasks within a single shared network.

Abstract: Ultrasound (US) imaging exhibits substantial heterogeneity across anatomical structures and acquisition protocols, posing significant challenges to the development of generalizable analysis models. Most existing methods are task-specific, limiting their suitability as clinically deployable foundation models. To address this limitation, the Foundation Model Challenge for Ultrasound Image Analysis (FM\_UIA~2026) introduces a large-scale multi-task benchmark comprising 27 subtasks across segmentation, classification, detection, and regression. In this paper, we present the official baseline for FM\_UIA~2026 based on a unified Multi-Head Multi-Task Learning (MH-MTL) framework that supports all tasks within a single shared network. The model employs an ImageNet-pretrained EfficientNet--B4 backbone for robust feature extraction, combined with a Feature Pyramid Network (FPN) to capture multi-scale contextual information. A task-specific routing strategy enables global tasks to leverage high-level semantic features, while dense prediction tasks exploit spatially detailed FPN representations. Training incorporates a composite loss with task-adaptive learning rate scaling and a cosine annealing schedule. Validation results demonstrate the feasibility and robustness of this unified design, establishing a strong and extensible baseline for ultrasound foundation model research. The code and dataset are publicly available at \href{https://github.com/lijiake2408/Foundation-Model-Challenge-for-Ultrasound-Image-Analysis}{GitHub}.

</details>


### [135] [Radioactive 3D Gaussian Ray Tracing for Tomographic Reconstruction](https://arxiv.org/abs/2602.01057)
*Ling Chen,Bao Yang*

Main category: cs.CV

TL;DR: 3D Gaussian ray tracing for tomographic reconstruction improves accuracy over splatting-based methods by enabling analytic line integrals and precise geometric corrections.


<details>
  <summary>Details</summary>
Motivation: While 3D Gaussian Splatting (3DGS) and its tomographic extension R2-Gaussian show promise, they suffer from limitations: the local affine approximation degrades quantitative accuracy and complicates nonlinear geometric corrections needed for realistic tomography systems.

Method: Proposes a tomographic reconstruction framework based on 3D Gaussian ray tracing instead of splatting. Computes line integrals through 3D Gaussian primitives analytically, avoiding local affine collapse. Ray-tracing formulation provides explicit control over ray origins/directions for precise application of nonlinear geometric corrections.

Result: The approach yields a more physically consistent forward projection model and extends applicability to a wider range of realistic tomography systems while improving projection accuracy.

Conclusion: 3D Gaussian ray tracing overcomes limitations of splatting-based methods by enabling analytic integration and precise geometric corrections, making Gaussian-based reconstruction more accurate and applicable to diverse tomography systems.

Abstract: 3D Gaussian Splatting (3DGS) has recently emerged in computer vision as a promising rendering technique. By adapting the principles of Elliptical Weighted Average (EWA) splatting to a modern differentiable pipeline, 3DGS enables real-time, high-quality novel view synthesis. Building upon this, R2-Gaussian extended the 3DGS paradigm to tomographic reconstruction by rectifying integration bias, achieving state-of-the-art performance in computed tomography (CT). To enable differentiability, R2-Gaussian adopts a local affine approximation: each 3D Gaussian is locally mapped to a 2D Gaussian on the detector and composed via alpha blending to form projections. However, the affine approximation can degrade reconstruction quantitative accuracy and complicate the incorporation of nonlinear geometric corrections. To address these limitations, we propose a tomographic reconstruction framework based on 3D Gaussian ray tracing. Our approach provides two key advantages over splatting-based models: (i) it computes the line integral through 3D Gaussian primitives analytically, avoiding the local affine collapse and thus yielding a more physically consistent forward projection model; and (ii) the ray-tracing formulation gives explicit control over ray origins and directions, which facilitates the precise application of nonlinear geometric corrections, e.g., arc-correction used in positron emission tomography (PET). These properties extend the applicability of Gaussian-based reconstruction to a wider range of realistic tomography systems while improving projection accuracy.

</details>


### [136] [DRFormer: A Dual-Regularized Bidirectional Transformer for Person Re-identification](https://arxiv.org/abs/2602.01059)
*Ying Shu,Pujian Zhan,Huiqi Yang,Hehe Fan,Youfang Lin,Kai Lv*

Main category: cs.CV

TL;DR: DRFormer is a dual-regularized bidirectional transformer framework that synergizes DINO's local texture mining with CLIP's global semantic features for improved person re-identification under occlusion and pose variations.


<details>
  <summary>Details</summary>
Motivation: Person re-identification faces challenges like occlusion and pose variations. While DINO excels at local texture mining and CLIP captures global semantic differences, existing methods use only one paradigm, missing the benefits of integrating both complementary architectures.

Method: Proposes DRFormer (Dual-Regularized Bidirectional Transformer) framework with dual-regularization mechanism to ensure diverse feature extraction and balance contributions from both DINO and CLIP models.

Result: Extensive experiments on five benchmarks show the method effectively harmonizes local and global representations, achieving competitive performance against state-of-the-art methods.

Conclusion: The integration of DINO's local texture mining and CLIP's global semantic features through a dual-regularized bidirectional transformer framework successfully addresses person re-identification challenges and achieves state-of-the-art performance.

Abstract: Both fine-grained discriminative details and global semantic features can contribute to solving person re-identification challenges, such as occlusion and pose variations. Vision foundation models (\textit{e.g.}, DINO) excel at mining local textures, and vision-language models (\textit{e.g.}, CLIP) capture strong global semantic difference. Existing methods predominantly rely on a single paradigm, neglecting the potential benefits of their integration. In this paper, we analyze the complementary roles of these two architectures and propose a framework to synergize their strengths by a \textbf{D}ual-\textbf{R}egularized Bidirectional \textbf{Transformer} (\textbf{DRFormer}). The dual-regularization mechanism ensures diverse feature extraction and achieves a better balance in the contributions of the two models. Extensive experiments on five benchmarks show that our method effectively harmonizes local and global representations, achieving competitive performance against state-of-the-art methods.

</details>


### [137] [PDE-Constrained Optimization for Neural Image Segmentation with Physics Priors](https://arxiv.org/abs/2602.01069)
*Seema K. Poudel,Sunny K. Khadka*

Main category: cs.CV

TL;DR: PDE-constrained optimization improves microscopy image segmentation by integrating physical priors into deep learning, enhancing accuracy, boundary fidelity, and generalization compared to unconstrained models.


<details>
  <summary>Details</summary>
Motivation: Microscopy image segmentation is challenging due to noise, weak boundaries, and limited labeled data. Unconstrained deep learning often leads to unstable solutions and poor generalization, necessitating structured regularization.

Method: Formulates segmentation as PDE-constrained optimization with variational regularization. Combines data fidelity with penalty terms from reaction-diffusion equations and phase-field interface energies, implemented as differentiable residual losses. Uses UNet as baseline on LIVECell dataset.

Result: PDE-regularized models show consistent improvements in segmentation accuracy and boundary fidelity over unconstrained baselines. Enhanced stability and better generalization in low-sample regimes, especially when evaluated on unseen cell types.

Conclusion: PDE-constrained optimization provides a principled bridge between variational methods and deep learning, strengthening data-driven frameworks with structured physical priors for improved scientific machine learning.

Abstract: Segmentation of microscopy images constitutes an ill-posed inverse problem due to measurement noise, weak object boundaries, and limited labeled data. Although deep neural networks provide flexible nonparametric estimators, unconstrained empirical risk minimization often leads to unstable solutions and poor generalization. In this work, image segmentation is formulated as a PDE-constrained optimization problem that integrates physically motivated priors into deep learning models through variational regularization. The proposed framework minimizes a composite objective function consisting of a data fidelity term and penalty terms derived from reaction-diffusion equations and phase-field interface energies, all implemented as differentiable residual losses. Experiments are conducted on the LIVECell dataset, a high-quality, manually annotated collection of phase-contrast microscopy images. Training is performed on two cell types, while evaluation is carried out on a distinct, unseen cell type to assess generalization. A UNet architecture is used as the unconstrained baseline model. Experimental results demonstrate consistent improvements in segmentation accuracy and boundary fidelity compared to unconstrained deep learning baselines. Moreover, the PDE-regularized models exhibit enhanced stability and improved generalization in low-sample regimes, highlighting the advantages of incorporating structured priors. The proposed approach illustrates how PDE-constrained optimization can strengthen data-driven learning frameworks, providing a principled bridge between variational methods, statistical learning, and scientific machine learning.

</details>


### [138] [PISA: Piecewise Sparse Attention Is Wiser for Efficient Diffusion Transformers](https://arxiv.org/abs/2602.01077)
*Haopeng Li,Shitong Shao,Wenliang Zhong,Zikai Zhou,Lichen Bai,Hui Xiong,Zeke Xie*

Main category: cs.CV

TL;DR: PISA introduces a training-free piecewise sparse attention method that approximates non-critical blocks via Taylor expansion instead of dropping them, achieving significant speedups while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: Diffusion Transformers suffer from quadratic attention complexity, and existing block sparse attention methods degrade quality at high sparsity by discarding context from non-critical blocks.

Method: PISA uses an exact-or-approximate strategy: maintains exact computation for critical blocks while approximating non-critical blocks through block-wise Taylor expansion, leveraging distributional stability of attention scores.

Result: Achieves 1.91× speedup on Wan2.1-14B, 2.57× on Hunyuan-Video, and 1.2× on FLUX image generation without quality degradation, outperforming other sparse attention methods.

Conclusion: PISA effectively bridges the speed-quality gap in sparse attention by approximating rather than discarding non-critical blocks, serving as a faithful proxy to full attention with sub-quadratic complexity.

Abstract: Diffusion Transformers are fundamental for video and image generation, but their efficiency is bottlenecked by the quadratic complexity of attention. While block sparse attention accelerates computation by attending only critical key-value blocks, it suffers from degradation at high sparsity by discarding context. In this work, we discover that attention scores of non-critical blocks exhibit distributional stability, allowing them to be approximated accurately and efficiently rather than discarded, which is essentially important for sparse attention design. Motivated by this key insight, we propose PISA, a training-free Piecewise Sparse Attention that covers the full attention span with sub-quadratic complexity. Unlike the conventional keep-or-drop paradigm that directly drop the non-critical block information, PISA introduces a novel exact-or-approximate strategy: it maintains exact computation for critical blocks while efficiently approximating the remainder through block-wise Taylor expansion. This design allows PISA to serve as a faithful proxy to full attention, effectively bridging the gap between speed and quality. Experimental results demonstrate that PISA achieves 1.91 times and 2.57 times speedups on Wan2.1-14B and Hunyuan-Video, respectively, while consistently maintaining the highest quality among sparse attention methods. Notably, even for image generation on FLUX, PISA achieves a 1.2 times acceleration without compromising visual quality. Code is available at: https://github.com/xie-lab-ml/piecewise-sparse-attention.

</details>


### [139] [MedAD-R1: Eliciting Consistent Reasoning in Interpretible Medical Anomaly Detection via Consistency-Reinforced Policy Optimization](https://arxiv.org/abs/2602.01081)
*Haitao Zhang,Yingying Wang,Jiaxiang Wang,Haote Xu,Hongyang Zhang,Yirong Chen,Yue Huang,Xinghao Ding*

Main category: cs.CV

TL;DR: MedAD-R1 achieves SOTA on MedAD-38K benchmark with 10%+ improvement using two-stage training: Cognitive Injection SFT + Con-GRPO for consistent reasoning.


<details>
  <summary>Details</summary>
Motivation: Current MedAD models rely on simplistic SFT datasets, lacking plausible reasoning and robust multimodal generalization needed for trustworthy clinical decision support.

Method: Two-stage framework: 1) Cognitive Injection SFT to instill medical knowledge and structured think-then-answer paradigm; 2) Consistency Group Relative Policy Optimization (Con-GRPO) with consistency reward to ensure reasoning aligns with final diagnosis.

Result: MedAD-R1 achieves state-of-the-art performance on MedAD-38K benchmark, outperforming strong baselines by more than 10%.

Conclusion: The approach generates transparent, logically consistent reasoning pathways, enhancing trustworthiness and interpretability of AI for clinical decision support.

Abstract: Medical Anomaly Detection (MedAD) presents a significant opportunity to enhance diagnostic accuracy using Large Multimodal Models (LMMs) to interpret and answer questions based on medical images. However, the reliance on Supervised Fine-Tuning (SFT) on simplistic and fragmented datasets has hindered the development of models capable of plausible reasoning and robust multimodal generalization. To overcome this, we introduce MedAD-38K, the first large-scale, multi-modal, and multi-center benchmark for MedAD featuring diagnostic Chain-of-Thought (CoT) annotations alongside structured Visual Question-Answering (VQA) pairs. On this foundation, we propose a two-stage training framework. The first stage, Cognitive Injection, uses SFT to instill foundational medical knowledge and align the model with a structured think-then-answer paradigm. Given that standard policy optimization can produce reasoning that is disconnected from the final answer, the second stage incorporates Consistency Group Relative Policy Optimization (Con-GRPO). This novel algorithm incorporates a crucial consistency reward to ensure the generated reasoning process is relevant and logically coherent with the final diagnosis. Our proposed model, MedAD-R1, achieves state-of-the-art (SOTA) performance on the MedAD-38K benchmark, outperforming strong baselines by more than 10\%. This superior performance stems from its ability to generate transparent and logically consistent reasoning pathways, offering a promising approach to enhancing the trustworthiness and interpretability of AI for clinical decision support.

</details>


### [140] [Differential Vector Erasure: Unified Training-Free Concept Erasure for Flow Matching Models](https://arxiv.org/abs/2602.01089)
*Zhiqi Zhang,Xinhao Zhong,Yi Sun,Shuoyang Sun,Bin Chen,Shu-Tao Xia,Xuan Wang*

Main category: cs.CV

TL;DR: DVE is a training-free method for erasing unwanted concepts from flow matching diffusion models by projecting velocity fields away from concept-specific directions.


<details>
  <summary>Details</summary>
Motivation: Text-to-image diffusion models can generate undesirable content (NSFW, copyrighted styles, objects), but existing erasure methods focus on DDPM models and require costly fine-tuning. Flow matching models present a different paradigm where prior methods don't apply.

Method: Differential Vector Erasure (DVE) constructs a differential vector field that captures directional differences between target and anchor concepts in the velocity field. During inference, it projects the velocity field onto this differential direction to remove concept-specific components without affecting other semantics.

Result: Extensive experiments on FLUX show DVE consistently outperforms existing baselines across NSFW suppression, artistic style removal, and object erasure tasks while preserving image quality and diversity.

Conclusion: DVE provides an effective training-free solution for concept erasure in flow matching models by leveraging the directional structure of velocity fields, enabling precise concept suppression without compromising other aspects of image generation.

Abstract: Text-to-image diffusion models have demonstrated remarkable capabilities in generating high-quality images, yet their tendency to reproduce undesirable concepts, such as NSFW content, copyrighted styles, or specific objects, poses growing concerns for safe and controllable deployment. While existing concept erasure approaches primarily focus on DDPM-based diffusion models and rely on costly fine-tuning, the recent emergence of flow matching models introduces a fundamentally different generative paradigm for which prior methods are not directly applicable. In this paper, we propose Differential Vector Erasure (DVE), a training-free concept erasure method specifically designed for flow matching models. Our key insight is that semantic concepts are implicitly encoded in the directional structure of the velocity field governing the generative flow. Leveraging this observation, we construct a differential vector field that characterizes the directional discrepancy between a target concept and a carefully chosen anchor concept. During inference, DVE selectively removes concept-specific components by projecting the velocity field onto the differential direction, enabling precise concept suppression without affecting irrelevant semantics. Extensive experiments on FLUX demonstrate that DVE consistently outperforms existing baselines on a wide range of concept erasure tasks, including NSFW suppression, artistic style removal, and object erasure, while preserving image quality and diversity.

</details>


### [141] [PandaPose: 3D Human Pose Lifting from a Single Image via Propagating 2D Pose Prior to 3D Anchor Space](https://arxiv.org/abs/2602.01095)
*Jinghong Zheng,Changlong Jiang,Yang Xiao,Jiaqi Li,Haohong Kuang,Hang Xu,Ran Wang,Zhiguo Cao,Min Du,Joey Tianyi Zhou*

Main category: cs.CV

TL;DR: PandaPose: A 3D human pose lifting method that uses 3D anchor space as intermediate representation to address error propagation and self-occlusion issues in single-image 3D pose estimation.


<details>
  <summary>Details</summary>
Motivation: Existing 3D pose lifting methods suffer from two fundamental limitations: (1) inevitable error propagation from input 2D pose predictions to 3D outputs, and (2) inherent difficulties in handling self-occlusion cases where joints are hidden from view.

Method: Proposes PandaPose with three key components: (1) Joint-wise 3D anchors in canonical coordinate system for robust priors, (2) Depth-aware joint-wise feature lifting to resolve occlusion ambiguities, (3) Anchor-feature interaction decoder that combines 3D anchors with lifted features to generate unified anchor queries containing joint information, visual cues, and geometric depth.

Result: Achieves state-of-the-art performance on Human3.6M, MPI-INF-3DHP, and 3DPW benchmarks, with 14.7% error reduction compared to previous SOTA methods on challenging Human3.6M conditions. Qualitative comparisons demonstrate effectiveness and robustness.

Conclusion: PandaPose effectively addresses error propagation and self-occlusion problems in 3D human pose lifting by introducing a unified 3D anchor space representation, leading to significant performance improvements over existing methods.

Abstract: 3D human pose lifting from a single RGB image is a challenging task in 3D vision. Existing methods typically establish a direct joint-to-joint mapping from 2D to 3D poses based on 2D features. This formulation suffers from two fundamental limitations: inevitable error propagation from input predicted 2D pose to 3D predictions and inherent difficulties in handling self-occlusion cases. In this paper, we propose PandaPose, a 3D human pose lifting approach via propagating 2D pose prior to 3D anchor space as the unified intermediate representation. Specifically, our 3D anchor space comprises: (1) Joint-wise 3D anchors in the canonical coordinate system, providing accurate and robust priors to mitigate 2D pose estimation inaccuracies. (2) Depth-aware joint-wise feature lifting that hierarchically integrates depth information to resolve self-occlusion ambiguities. (3) The anchor-feature interaction decoder that incorporates 3D anchors with lifted features to generate unified anchor queries encapsulating joint-wise 3D anchor set, visual cues and geometric depth information. The anchor queries are further employed to facilitate anchor-to-joint ensemble prediction. Experiments on three well-established benchmarks (i.e., Human3.6M, MPI-INF-3DHP and 3DPW) demonstrate the superiority of our proposition. The substantial reduction in error by $14.7\%$ compared to SOTA methods on the challenging conditions of Human3.6M and qualitative comparisons further showcase the effectiveness and robustness of our approach.

</details>


### [142] [Robust Harmful Meme Detection under Missing Modalities via Shared Representation Learning](https://arxiv.org/abs/2602.01101)
*Felix Breiteneder,Mohammad Belal,Muhammad Saad Saeed,Shahed Masoudian,Usman Naseem,Kulshrestha Juhi,Markus Schedl,Shah Nawaz*

Main category: cs.CV

TL;DR: Proposes a new baseline method for harmful meme detection that handles modal-incomplete data (e.g., missing text) by learning shared multimodal representations, improving robustness in real-world scenarios.


<details>
  <summary>Details</summary>
Motivation: Internet memes can spread harmful content, but existing detection methods rely on complete multimodal data (text+images). Real-world scenarios often have missing modalities (e.g., poor OCR quality), causing performance deterioration. Need robust methods that handle modal-incomplete data.

Method: Proposes a baseline method that learns shared representations for multiple modalities by projecting them independently. These shared representations can be leveraged when data is modal-incomplete, allowing better integration of visual features and reducing dependence on text.

Result: Experimental results on two benchmark datasets show the method outperforms existing approaches when text is missing. Better visual feature integration reduces text dependence and improves robustness in scenarios with missing textual information.

Conclusion: First comprehensive investigation of harmful meme detection with modal-incomplete data. The method represents significant progress toward real-world application, particularly when modalities are absent. Enables more robust harmful meme detection in practical scenarios.

Abstract: Internet memes are powerful tools for communication, capable of spreading political, psychological, and sociocultural ideas. However, they can be harmful and can be used to disseminate hate toward targeted individuals or groups. Although previous studies have focused on designing new detection methods, these often rely on modal-complete data, such as text and images. In real-world settings, however, modalities like text may be missing due to issues like poor OCR quality, making existing methods sensitive to missing information and leading to performance deterioration. To address this gap, in this paper, we present the first-of-its-kind work to comprehensively investigate the behavior of harmful meme detection methods in the presence of modal-incomplete data. Specifically, we propose a new baseline method that learns a shared representation for multiple modalities by projecting them independently. These shared representations can then be leveraged when data is modal-incomplete. Experimental results on two benchmark datasets demonstrate that our method outperforms existing approaches when text is missing. Moreover, these results suggest that our method allows for better integration of visual features, reducing dependence on text and improving robustness in scenarios where textual information is missing. Our work represents a significant step forward in enabling the real-world application of harmful meme detection, particularly in situations where a modality is absent.

</details>


### [143] [LightCity: An Urban Dataset for Outdoor Inverse Rendering and Reconstruction under Multi-illumination Conditions](https://arxiv.org/abs/2602.01118)
*Jingjing Wang,Qirui Hu,Chong Bao,Yuke Zhu,Hujun Bao,Zhaopeng Cui,Guofeng Zhang*

Main category: cs.CV

TL;DR: LightCity is a new synthetic urban dataset with diverse illumination conditions for inverse rendering research in autonomous driving and digital twins.


<details>
  <summary>Details</summary>
Motivation: Inverse rendering in urban scenes faces challenges from complex illumination (multi-illumination, indirect light, shadows), but lacks appropriate datasets to study these effects on intrinsic decomposition and 3D reconstruction.

Method: Created LightCity - a high-quality synthetic urban dataset with over 300 sky maps, controllable illumination, street-level and aerial perspectives (50K+ images), and rich properties including depth, normal, material components, light and indirect light.

Result: Provides comprehensive dataset for benchmarking three fundamental tasks in urban environments, enabling analysis of illumination effects on inverse rendering.

Conclusion: LightCity lays a robust foundation for advancing inverse rendering research in urban scenes by addressing the dataset gap for studying complex illumination effects.

Abstract: Inverse rendering in urban scenes is pivotal for applications like autonomous driving and digital twins. Yet, it faces significant challenges due to complex illumination conditions, including multi-illumination and indirect light and shadow effects. However, the effects of these challenges on intrinsic decomposition and 3D reconstruction have not been explored due to the lack of appropriate datasets. In this paper, we present LightCity, a novel high-quality synthetic urban dataset featuring diverse illumination conditions with realistic indirect light and shadow effects. LightCity encompasses over 300 sky maps with highly controllable illumination, varying scales with street-level and aerial perspectives over 50K images, and rich properties such as depth, normal, material components, light and indirect light, etc. Besides, we leverage LightCity to benchmark three fundamental tasks in the urban environments and conduct a comprehensive analysis of these benchmarks, laying a robust foundation for advancing related research.

</details>


### [144] [Koo-Fu CLIP: Closed-Form Adaptation of Vision-Language Models via Fukunaga-Koontz Linear Discriminant Analysis](https://arxiv.org/abs/2602.01127)
*Matej Suchanek,Klara Janouskova,Ondrej Vasatko,Jiri Matas*

Main category: cs.CV

TL;DR: Koo-Fu CLIP improves CLIP embeddings for classification using Fukunaga-Koontz LDA to enhance class separability and reduce dimensionality, achieving accuracy gains and 10-12x compression.


<details>
  <summary>Details</summary>
Motivation: Raw CLIP embeddings have limited class separation and excessive dimensionality, making them suboptimal for supervised classification tasks despite their powerful general-purpose representations.

Method: Uses Fukunaga-Koontz Linear Discriminant Analysis in whitened embedding space to suppress within-class variation and enhance between-class discrimination through a closed-form linear projection.

Result: Improves ImageNet-1K top-1 accuracy from 75.1% to 79.1%, maintains gains with 14K and 21K classes, and enables 10-12x compression with minimal accuracy loss.

Conclusion: Koo-Fu CLIP provides an efficient, lightweight adaptation method that significantly improves CLIP's classification performance while enabling substantial dimensionality reduction for large-scale applications.

Abstract: Visual-language models such as CLIP provide powerful general-purpose representations, but their raw embeddings are not optimized for supervised classification, often exhibiting limited class separation and excessive dimensionality. We propose Koo-Fu CLIP, a supervised CLIP adaptation method based on Fukunaga-Koontz Linear Discriminant Analysis, which operates in a whitened embedding space to suppress within-class variation and enhance between-class discrimination. The resulting closed-form linear projection reshapes the geometry of CLIP embeddings, improving class separability while performing effective dimensionality reduction, and provides a lightweight and efficient adaptation of CLIP representations.
  Across large-scale ImageNet benchmarks, nearest visual prototype classification in the Koo-Fu CLIP space improves top-1 accuracy from 75.1% to 79.1% on ImageNet-1K, with consistent gains persisting as the label space expands to 14K and 21K classes. The method supports substantial compression by up to 10-12x with little or no loss in accuracy, enabling efficient large-scale classification and retrieval.

</details>


### [145] [Semantically Aware UAV Landing Site Assessment from Remote Sensing Imagery via Multimodal Large Language Models](https://arxiv.org/abs/2602.01163)
*Chunliang Hua,Zeyuan Yang,Lei Zhang,Jiayang Sun,Fengwen Chen,Chunlan Zeng,Xiao Hu*

Main category: cs.CV

TL;DR: A framework using remote sensing imagery and multimodal LLMs for UAV emergency landing site assessment that goes beyond geometric analysis to understand semantic risks like crowds and temporary structures.


<details>
  <summary>Details</summary>
Motivation: Traditional geometric sensors fail to identify complex semantic risks (crowds, temporary structures) crucial for safe UAV emergency landing, requiring a more comprehensive approach.

Method: Coarse-to-fine pipeline: 1) lightweight semantic segmentation pre-screens candidate areas, 2) vision-language reasoning agent fuses visual features with POI data to detect subtle hazards.

Result: Framework significantly outperforms geometric baselines in risk identification accuracy and generates human-like, interpretable justifications, enhancing trust in automated decision-making.

Conclusion: The proposed RS imagery + MLLM framework provides superior global context-aware landing site assessment, with benchmark dataset released to advance research in this area.

Abstract: Safe UAV emergency landing requires more than just identifying flat terrain; it demands understanding complex semantic risks (e.g., crowds, temporary structures) invisible to traditional geometric sensors. In this paper, we propose a novel framework leveraging Remote Sensing (RS) imagery and Multimodal Large Language Models (MLLMs) for global context-aware landing site assessment. Unlike local geometric methods, our approach employs a coarse-to-fine pipeline: first, a lightweight semantic segmentation module efficiently pre-screens candidate areas; second, a vision-language reasoning agent fuses visual features with Point-of-Interest (POI) data to detect subtle hazards. To validate this approach, we construct and release the Emergency Landing Site Selection (ELSS) benchmark. Experiments demonstrate that our framework significantly outperforms geometric baselines in risk identification accuracy. Furthermore, qualitative results confirm its ability to generate human-like, interpretable justifications, enhancing trust in automated decision-making. The benchmark dataset is publicly accessible at https://anonymous.4open.science/r/ELSS-dataset-43D7.

</details>


### [146] [EEmo-Logic: A Unified Dataset and Multi-Stage Framework for Comprehensive Image-Evoked Emotion Assessment](https://arxiv.org/abs/2602.01173)
*Lancheng Gao,Ziheng Jia,Zixuan Xing,Wei Sun,Huiyu Duan,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TL;DR: EEmoDB is the largest image-evoked emotion dataset with 5 analysis dimensions and 5 task categories, featuring 1.2M QA pairs and 36k fine-grained assessments. EEmo-Logic MLLM achieves robust performance via instruction fine-tuning and novel GRPO reward design.


<details>
  <summary>Details</summary>
Motivation: Existing models for image-evoked emotion understanding are limited to coarse-grained perception or deficient reasoning capabilities, hindering machine empathy and human-computer interaction applications.

Method: Created EEmoDB dataset with 5 analysis dimensions and 5 task categories, including 1.2M QA pairs from 125k images and 36k fine-grained assessments from 25k images. Developed EEmo-Logic MLLM using instruction fine-tuning and task-customized group relative preference optimization (GRPO) with novel reward design.

Result: EEmo-Logic demonstrates robust performance in both in-domain and cross-domain datasets, excelling in emotion QA and fine-grained assessment tasks.

Conclusion: The proposed EEmoDB dataset and EEmo-Logic model bridge the gap in comprehensive image-evoked emotion understanding, advancing machine empathy and enabling diverse human-computer interaction applications.

Abstract: Understanding the multi-dimensional attributes and intensity nuances of image-evoked emotions is pivotal for advancing machine empathy and empowering diverse human-computer interaction applications. However, existing models are still limited to coarse-grained emotion perception or deficient reasoning capabilities. To bridge this gap, we introduce EEmoDB, the largest image-evoked emotion understanding dataset to date. It features $5$ analysis dimensions spanning $5$ distinct task categories, facilitating comprehensive interpretation. Specifically, we compile $1.2M$ question-answering (QA) pairs (EEmoDB-QA) from $125k$ images via automated generation, alongside a $36k$ dataset (EEmoDB-Assess) curated from $25k$ images for fine-grained assessment. Furthermore, we propose EEmo-Logic, an all-in-one multimodal large language model (MLLM) developed via instruction fine-tuning and task-customized group relative preference optimization (GRPO) with novel reward design. Extensive experiments demonstrate that EEmo-Logic achieves robust performance in in-domain and cross-domain datasets, excelling in emotion QA and fine-grained assessment. The code is available at https://anonymous.4open.science/r/EEmoLogic.

</details>


### [147] [Refining Context-Entangled Content Segmentation via Curriculum Selection and Anti-Curriculum Promotion](https://arxiv.org/abs/2602.01183)
*Chunming He,Rihan Zhang,Fengyang Xiao,Dingming Zhang,Zhiwen Cao,Sina Farsiu*

Main category: cs.CV

TL;DR: CurriSeg is a dual-phase learning framework that combines curriculum and anti-curriculum principles to improve segmentation of context-entangled objects (like camouflaged objects) by dynamically selecting training samples and suppressing high-frequency features.


<details>
  <summary>Details</summary>
Motivation: Biological learning progresses from easy to difficult tasks, gradually building robustness. Current segmentation networks focus on architectural improvements but ignore learning dynamics, especially for challenging Context-Entangled Content Segmentation where objects share visual patterns with surroundings.

Method: CurriSeg has two phases: 1) Curriculum Selection dynamically selects training data based on temporal loss statistics to distinguish hard-but-informative samples from noisy ones; 2) Anti-Curriculum Promotion uses Spectral-Blindness Fine-Tuning to suppress high-frequency components, forcing reliance on low-frequency structural and contextual cues.

Result: Extensive experiments show CurriSeg achieves consistent improvements across diverse CECS benchmarks without adding parameters or increasing total training time.

Conclusion: CurriSeg offers a principled approach to how progression and challenge interplay can foster robust and context-aware segmentation, demonstrating that learning dynamics are crucial for handling context-entangled data distributions.

Abstract: Biological learning proceeds from easy to difficult tasks, gradually reinforcing perception and robustness. Inspired by this principle, we address Context-Entangled Content Segmentation (CECS), a challenging setting where objects share intrinsic visual patterns with their surroundings, as in camouflaged object detection. Conventional segmentation networks predominantly rely on architectural enhancements but often ignore the learning dynamics that govern robustness under entangled data distributions. We introduce CurriSeg, a dual-phase learning framework that unifies curriculum and anti-curriculum principles to improve representation reliability. In the Curriculum Selection phase, CurriSeg dynamically selects training data based on the temporal statistics of sample losses, distinguishing hard-but-informative samples from noisy or ambiguous ones, thus enabling stable capability enhancement. In the Anti-Curriculum Promotion phase, we design Spectral-Blindness Fine-Tuning, which suppresses high-frequency components to enforce dependence on low-frequency structural and contextual cues and thus strengthens generalization. Extensive experiments demonstrate that CurriSeg achieves consistent improvements across diverse CECS benchmarks without adding parameters or increasing total training time, offering a principled view of how progression and challenge interplay to foster robust and context-aware segmentation. Code will be released.

</details>


### [148] [EMFormer: Efficient Multi-Scale Transformer for Accumulative Context Weather Forecasting](https://arxiv.org/abs/2602.01194)
*Hao Chen,Tao Han,Jie Zhang,Song Guo,Fenghua Ling,Lei Bai*

Main category: cs.CV

TL;DR: EMFormer: Efficient Multi-scale Transformer with accumulative context finetuning and composite loss for improved long-term weather forecasting with reduced computational overhead.


<details>
  <summary>Details</summary>
Motivation: Address limitations of current weather forecasting approaches including catastrophic forgetting, error accumulation, and high training overhead in long-term prediction.

Method: Introduces EMFormer (Efficient Multi-scale Transformer) with single convolution for multi-scale feature extraction, accumulative context finetuning for temporal consistency, and composite loss with sinusoidal weighting for adaptive optimization.

Result: Achieves strong performance in weather forecasting and extreme event prediction, improves long-term forecast accuracy, demonstrates generalization on vision benchmarks (ImageNet-1K, ADE20K), and provides 5.69x speedup over conventional multi-scale modules.

Conclusion: The proposed pipeline effectively addresses key challenges in long-term weather forecasting while reducing computational costs and maintaining strong generalization capabilities across domains.

Abstract: Long-term weather forecasting is critical for socioeconomic planning and disaster preparedness. While recent approaches employ finetuning to extend prediction horizons, they remain constrained by the issues of catastrophic forgetting, error accumulation, and high training overhead. To address these limitations, we present a novel pipeline across pretraining, finetuning and forecasting to enhance long-context modeling while reducing computational overhead. First, we introduce an Efficient Multi-scale Transformer (EMFormer) to extract multi-scale features through a single convolution in both training and inference. Based on the new architecture, we further employ an accumulative context finetuning to improve temporal consistency without degrading short-term accuracy. Additionally, we propose a composite loss that dynamically balances different terms via a sinusoidal weighting, thereby adaptively guiding the optimization trajectory throughout pretraining and finetuning. Experiments show that our approach achieves strong performance in weather forecasting and extreme event prediction, substantially improving long-term forecast accuracy. Moreover, EMFormer demonstrates strong generalization on vision benchmarks (ImageNet-1K and ADE20K) while delivering a 5.69x speedup over conventional multi-scale modules.

</details>


### [149] [Med3D-R1: Incentivizing Clinical Reasoning in 3D Medical Vision-Language Models for Abnormality Diagnosis](https://arxiv.org/abs/2602.01200)
*Haoran Lai,Zihang Jiang,Kun Zhang,Qingsong Yao,Rongsheng Wang,Zhiyang He,Xiaodong Tao,Wei Wei,Shaohua Kevin Zhou*

Main category: cs.CV

TL;DR: Med3D-R1 is a reinforcement learning framework for 3D medical vision-language models that improves clinical reasoning through residual alignment, abnormality re-weighting, and redesigned consistency rewards, achieving state-of-the-art results on CT-RATE and RAD-ChestCT benchmarks.


<details>
  <summary>Details</summary>
Motivation: Developing robust 3D vision-language models for medical imaging is challenging due to volumetric complexity, model tendency to overfit superficial report patterns, and lack of interpretability-aware reward designs.

Method: Two-stage RL framework: 1) SFT stage with residual alignment to bridge 3D features and text embeddings, and abnormality re-weighting to emphasize clinical tokens; 2) RL stage with redesigned consistency reward to promote step-by-step diagnostic reasoning.

Result: Achieved state-of-the-art accuracies of 41.92% on CT-RATE and 44.99% on RAD-ChestCT benchmarks, outperforming prior methods and demonstrating improved abnormality diagnosis and clinical reasoning.

Conclusion: The approach enhances real-world diagnostic workflows by enabling more reliable and transparent 3D medical vision-language systems with better clinical reasoning capabilities.

Abstract: Developing 3D vision-language models with robust clinical reasoning remains a challenge due to the inherent complexity of volumetric medical imaging, the tendency of models to overfit superficial report patterns, and the lack of interpretability-aware reward designs. In this paper, we propose Med3D-R1, a reinforcement learning framework with a two-stage training process: Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). During SFT stage, we introduce a residual alignment mechanism to bridge the gap between high-dimensional 3D features and textual embeddings, and an abnormality re-weighting strategy to emphasize clinically informative tokens and reduce structural bias in reports. In RL stage, we redesign the consistency reward to explicitly promote coherent, step-by-step diagnostic reasoning. We evaluate our method on medical multiple-choice visual question answering using two 3D diagnostic benchmarks, CT-RATE and RAD-ChestCT, where our model attains state-of-the-art accuracies of 41.92\% on CT-RATE and 44.99\% on RAD-ChestCT. These results indicate improved abnormality diagnosis and clinical reasoning and outperform prior methods on both benchmarks. Overall, our approach holds promise for enhancing real-world diagnostic workflows by enabling more reliable and transparent 3D medical vision-language systems.

</details>


### [150] [Boosting Point-supervised Temporal Action Localization via Text Refinement and Alignment](https://arxiv.org/abs/2602.01257)
*Yunchuan Ma,Laiyun Qing,Guorong Li,Yuqing Liu,Yuankai Qi,Qingming Huang*

Main category: cs.CV

TL;DR: TRA framework improves point-supervised temporal action localization by incorporating textual features from video descriptions using PTR and PMA modules to refine text and align multimodal features.


<details>
  <summary>Details</summary>
Motivation: Current point-supervised temporal action localization methods only use visual features, ignoring helpful semantic information from text descriptions that could complement visual features.

Method: Proposes Text Refinement and Alignment (TRA) framework with two modules: Point-based Text Refinement (PTR) refines initial video descriptions using point annotations and pre-trained models; Point-based Multimodal Alignment (PMA) projects features into unified semantic space and uses point-level multimodal contrastive learning to align visual and textual features.

Result: Extensive experiments on five benchmarks show favorable performance compared to state-of-the-art methods, with framework running on single 24GB RTX 3090 GPU, demonstrating practicality and scalability.

Conclusion: The TRA framework effectively leverages textual features to enhance point-supervised temporal action localization, achieving better performance while maintaining practical computational requirements.

Abstract: Recently, point-supervised temporal action localization has gained significant attention for its effective balance between labeling costs and localization accuracy. However, current methods only consider features from visual inputs, neglecting helpful semantic information from the text side. To address this issue, we propose a Text Refinement and Alignment (TRA) framework that effectively utilizes textual features from visual descriptions to complement the visual features as they are semantically rich. This is achieved by designing two new modules for the original point-supervised framework: a Point-based Text Refinement module (PTR) and a Point-based Multimodal Alignment module (PMA). Specifically, we first generate descriptions for video frames using a pre-trained multimodal model. Next, PTR refines the initial descriptions by leveraging point annotations together with multiple pre-trained models. PMA then projects all features into a unified semantic space and leverages a point-level multimodal feature contrastive learning to reduce the gap between visual and linguistic modalities. Last, the enhanced multi-modal features are fed into the action detector for precise localization. Extensive experimental results on five widely used benchmarks demonstrate the favorable performance of our proposed framework compared to several state-of-the-art methods. Moreover, our computational overhead analysis shows that the framework can run on a single 24 GB RTX 3090 GPU, indicating its practicality and scalability.

</details>


### [151] [Q-DiT4SR: Exploration of Detail-Preserving Diffusion Transformer Quantization for Real-World Image Super-Resolution](https://arxiv.org/abs/2602.01273)
*Xun Zhang,Kaicheng Yang,Hongliang Lu,Haotong Qin,Yong Guo,Yulun Zhang*

Main category: cs.CV

TL;DR: Q-DiT4SR is the first post-training quantization framework specifically designed for Diffusion Transformer-based Real-World Image Super-Resolution, achieving state-of-the-art performance with significant model compression (5.8× size reduction) and computational savings (60× operations reduction).


<details>
  <summary>Details</summary>
Motivation: Diffusion Transformers (DiTs) show promise for Real-World Image Super-Resolution but suffer from heavy inference burden. Existing quantization methods are either designed for U-Net architectures or generic DiTs for text-to-image tasks, causing severe texture degradation when applied to DiT-based super-resolution models.

Method: Proposes Q-DiT4SR with two key components: 1) H-SVD (hierarchical SVD) that integrates global low-rank and local block-wise rank-1 branches under matched parameter budget, and 2) Variance-aware Spatio-Temporal Mixed Precision (VaSMP for cross-layer weight bit-width allocation via rate-distortion theory, and VaTMP for intra-layer activation precision scheduling across diffusion timesteps via dynamic programming).

Result: Achieves state-of-the-art performance on multiple real-world datasets under both W4A6 and W4A4 quantization settings. W4A4 configuration reduces model size by 5.8× and computational operations by over 60× while maintaining high-quality texture generation.

Conclusion: Q-DiT4SR successfully addresses the quantization challenges for DiT-based Real-ISR models, enabling practical deployment through significant acceleration and compression while preserving texture quality, with code and models made publicly available.

Abstract: Recently, Diffusion Transformers (DiTs) have emerged in Real-World Image Super-Resolution (Real-ISR) to generate high-quality textures, yet their heavy inference burden hinders real-world deployment. While Post-Training Quantization (PTQ) is a promising solution for acceleration, existing methods in super-resolution mostly focus on U-Net architectures, whereas generic DiT quantization is typically designed for text-to-image tasks. Directly applying these methods to DiT-based super-resolution models leads to severe degradation of local textures. Therefore, we propose Q-DiT4SR, the first PTQ framework specifically tailored for DiT-based Real-ISR. We propose H-SVD, a hierarchical SVD that integrates a global low-rank branch with a local block-wise rank-1 branch under a matched parameter budget. We further propose Variance-aware Spatio-Temporal Mixed Precision: VaSMP allocates cross-layer weight bit-widths in a data-free manner based on rate-distortion theory, while VaTMP schedules intra-layer activation precision across diffusion timesteps via dynamic programming (DP) with minimal calibration. Experiments on multiple real-world datasets demonstrate that our Q-DiT4SR achieves SOTA performance under both W4A6 and W4A4 settings. Notably, the W4A4 quantization configuration reduces model size by 5.8$\times$ and computational operations by over 60$\times$. Our code and models will be available at https://github.com/xunzhang1128/Q-DiT4SR.

</details>


### [152] [TF-Lane: Traffic Flow Module for Robust Lane Perception](https://arxiv.org/abs/2602.01277)
*Yihan Xie,Han Xia,Zhen Yang*

Main category: cs.CV

TL;DR: TFM integrates real-time traffic flow data with lane perception algorithms to improve detection in challenging scenarios like occlusions, achieving up to +4.1% mAP gain on Nuscenes.


<details>
  <summary>Details</summary>
Motivation: Vision-based lane detection struggles in occluded/lane-missing scenarios, while HD map solutions have cost and real-time limitations. Traffic flow offers free, real-time supplementary information to enhance lane perception.

Method: Proposes TrafficFlow-aware Lane perception Module (TFM) that extracts real-time traffic flow features and integrates them with existing lane perception algorithms.

Result: TFM consistently improves performance across four mainstream models and two public datasets (Nuscenes and OpenLaneV2), achieving up to +4.1% mAP gain on Nuscenes dataset.

Conclusion: Traffic flow is an effective, cost-free information source that enhances lane perception in autonomous driving, with TFM demonstrating significant performance improvements across various models and datasets.

Abstract: Autonomous driving systems require robust lane perception capabilities, yet existing vision-based detection methods suffer significant performance degradation when visual sensors provide insufficient cues, such as in occluded or lane-missing scenarios. While some approaches incorporate high-definition maps as supplementary information, these solutions face challenges of high subscription costs and limited real-time performance. To address these limitations, we explore an innovative information source: traffic flow, which offers real-time capabilities without additional costs. This paper proposes a TrafficFlow-aware Lane perception Module (TFM) that effectively extracts real-time traffic flow features and seamlessly integrates them with existing lane perception algorithms. This solution originated from real-world autonomous driving conditions and was subsequently validated on open-source algorithms and datasets. Extensive experiments on four mainstream models and two public datasets (Nuscenes and OpenLaneV2) using standard evaluation metrics show that TFM consistently improves performance, achieving up to +4.1% mAP gain on the Nuscenes dataset.

</details>


### [153] [DSFC-Net: A Dual-Encoder Spatial and Frequency Co-Awareness Network for Rural Road Extraction](https://arxiv.org/abs/2602.01278)
*Zhengbo Zhang,Yihe Tian,Wanke Xia,Lin Chen,Yue Sun,Kun Ding,Ying Wang,Bing Xu,Shiming Xiang*

Main category: cs.CV

TL;DR: DSFC-Net: A dual-encoder network fusing spatial and frequency-domain information for accurate rural road extraction from high-resolution remote sensing imagery, addressing challenges like vegetation occlusions and narrow roads.


<details>
  <summary>Details</summary>
Motivation: Rural road extraction from remote sensing imagery faces unique challenges including high intra-class variability, low inter-class separability, vegetation occlusions disrupting spatial continuity, and narrow road widths. Existing methods optimized for urban environments underperform in rural settings due to overlooking these distinctive characteristics.

Method: Proposes DSFC-Net with dual-encoder framework: CNN branch captures fine-grained local road boundaries and short-range continuity, while Spatial-Frequency Hybrid Transformer (SFT) models global topological dependencies against vegetation occlusions. SFT incorporates Cross-Frequency Interaction Attention (CFIA) module using Laplacian Pyramid strategy to decouple high- and low-frequency information. Channel Feature Fusion Module (CFFM) adaptively recalibrates channel-wise feature responses to integrate local textures with global semantics.

Result: Comprehensive experiments on WHU-RuR+, DeepGlobe, and Massachusetts datasets validate DSFC-Net's superiority over state-of-the-art approaches for rural road extraction.

Conclusion: DSFC-Net effectively addresses rural road extraction challenges by synergistically fusing spatial and frequency-domain information, preserving connectivity of narrow roads, and robustly handling vegetation occlusions through innovative attention mechanisms and feature fusion strategies.

Abstract: Accurate extraction of rural roads from high-resolution remote sensing imagery is essential for infrastructure planning and sustainable development. However, this task presents unique challenges in rural settings due to several factors. These include high intra-class variability and low inter-class separability from diverse surface materials, frequent vegetation occlusions that disrupt spatial continuity, and narrow road widths that exacerbate detection difficulties. Existing methods, primarily optimized for structured urban environments, often underperform in these scenarios as they overlook such distinctive characteristics. To address these challenges, we propose DSFC-Net, a dual-encoder framework that synergistically fuses spatial and frequency-domain information. Specifically, a CNN branch is employed to capture fine-grained local road boundaries and short-range continuity, while a novel Spatial-Frequency Hybrid Transformer (SFT) is introduced to robustly model global topological dependencies against vegetation occlusions. Distinct from standard attention mechanisms that suffer from frequency bias, the SFT incorporates a Cross-Frequency Interaction Attention (CFIA) module that explicitly decouples high- and low-frequency information via a Laplacian Pyramid strategy. This design enables the dynamic interaction between spatial details and frequency-aware global contexts, effectively preserving the connectivity of narrow roads. Furthermore, a Channel Feature Fusion Module (CFFM) is proposed to bridge the two branches by adaptively recalibrating channel-wise feature responses, seamlessly integrating local textures with global semantics for accurate segmentation. Comprehensive experiments on the WHU-RuR+, DeepGlobe, and Massachusetts datasets validate the superiority of DSFC-Net over state-of-the-art approaches.

</details>


### [154] [Who Transfers Safety? Identifying and Targeting Cross-Lingual Shared Safety Neurons](https://arxiv.org/abs/2602.01283)
*Xianhui Zhang,Chengyu Xie,Linxia Zhu,Yonghui Yang,Weixiang Zhao,Zifeng Cheng,Cong Wang,Fei Shen,Tat-Seng Chua*

Main category: cs.CV

TL;DR: LLMs contain cross-lingual shared safety neurons (SS-Neurons) that regulate safety behavior across languages; targeting these neurons improves multilingual safety alignment, especially for non-high-resource languages.


<details>
  <summary>Details</summary>
Motivation: Multilingual safety is imbalanced, with non-high-resource languages vulnerable compared to high-resource ones, and neural mechanisms behind safety alignment remain unclear despite observed cross-lingual transfer.

Method: Identify monolingual safety neurons (MS-Neurons) and validate their causal role, then find SS-Neurons as the shared subset between languages. Propose neuron-oriented training targeting SS-Neurons based on language resource distribution and model architecture.

Result: Suppressing SS-Neurons causes safety drops across non-high-resource languages, while reinforcing them improves cross-lingual defensive consistency. Fine-tuning this small neuronal subset outperforms state-of-the-art methods, enhancing non-high-resource safety while maintaining general capabilities.

Conclusion: SS-Neurons serve as a bridge for transferring safety capabilities from high-resource to non-high-resource languages, and targeted intervention on these neurons provides an effective approach for improving multilingual safety alignment.

Abstract: Multilingual safety remains significantly imbalanced, leaving non-high-resource (NHR) languages vulnerable compared to robust high-resource (HR) ones. Moreover, the neural mechanisms driving safety alignment remain unclear despite observed cross-lingual representation transfer.
  In this paper, we find that LLMs contain a set of cross-lingual shared safety neurons (SS-Neurons), a remarkably small yet critical neuronal subset that jointly regulates safety behavior across languages.
  We first identify monolingual safety neurons (MS-Neurons) and validate their causal role in safety refusal behavior through targeted activation and suppression.
  Our cross-lingual analyses then identify SS-Neurons as the subset of MS-Neurons shared between HR and NHR languages, serving as a bridge to transfer safety capabilities from HR to NHR domains.
  We observe that suppressing these neurons causes concurrent safety drops across NHR languages, whereas reinforcing them improves cross-lingual defensive consistency.
  Building on these insights, we propose a simple neuron-oriented training strategy that targets SS-Neurons based on language resource distribution and model architecture. Experiments demonstrate that fine-tuning this tiny neuronal subset outperforms state-of-the-art methods, significantly enhancing NHR safety while maintaining the model's general capabilities.
  The code and dataset will be available athttps://github.com/1518630367/SS-Neuron-Expansion.

</details>


### [155] [Interacted Planes Reveal 3D Line Mapping](https://arxiv.org/abs/2602.01296)
*Zeran Ke,Bin Tan,Gui-Song Xia,Yujun Shen,Nan Xue*

Main category: cs.CV

TL;DR: LiP-Map is a joint line-plane optimization framework that models 3D lines as edges of planar patches, achieving accurate 3D line mapping with strong efficiency (3-5 minutes per scene).


<details>
  <summary>Details</summary>
Motivation: 3D line mapping provides compact scene representation, but current methods lack explicit modeling of the physical relationship where 3D lines naturally emerge as edges of planar patches.

Method: Joint optimization framework with learnable line and planar primitives that explicitly constructs interactions between planes and lines, integrating planar topology into line mapping without pairwise coplanarity constraints.

Result: Outperforms state-of-the-art methods on accuracy and completeness across 100+ scenes from multiple datasets (ScanNetV2, ScanNet++, Hypersim, 7Scenes, Tanks&Temple), and significantly advances line-assisted visual localization on 7Scenes.

Conclusion: LiP-Map pioneers principled integration of planar topology into 3D line mapping, offering structured reconstruction for man-made environments with strong efficiency and improved performance.

Abstract: 3D line mapping from multi-view RGB images provides a compact and structured visual representation of scenes. We study the problem from a physical and topological perspective: a 3D line most naturally emerges as the edge of a finite 3D planar patch. We present LiP-Map, a line-plane joint optimization framework that explicitly models learnable line and planar primitives. This coupling enables accurate and detailed 3D line mapping while maintaining strong efficiency (typically completing a reconstruction in 3 to 5 minutes per scene). LiP-Map pioneers the integration of planar topology into 3D line mapping, not by imposing pairwise coplanarity constraints but by explicitly constructing interactions between plane and line primitives, thus offering a principled route toward structured reconstruction in man-made environments. On more than 100 scenes from ScanNetV2, ScanNet++, Hypersim, 7Scenes, and Tanks\&Temple, LiP-Map improves both accuracy and completeness over state-of-the-art methods. Beyond line mapping quality, LiP-Map significantly advances line-assisted visual localization, establishing strong performance on 7Scenes. Our code is released at https://github.com/calmke/LiPMAP for reproducible research.

</details>


### [156] [Interaction-Consistent Object Removal via MLLM-Based Reasoning](https://arxiv.org/abs/2602.01298)
*Ching-Kai Huang,Wen-Chieh Lin,Yan-Cen Lee*

Main category: cs.CV

TL;DR: REORM is a framework that uses multimodal LLMs to identify and remove not just target objects but also their interaction evidence (like shadows, connected objects, etc.) for semantically consistent image editing.


<details>
  <summary>Details</summary>
Motivation: Current object removal methods only erase the named target object, leaving behind interaction evidence (shadows, reflections, physically connected objects, etc.) that creates semantically inconsistent results. The paper formalizes this as the Interaction-Consistent Object Removal (ICOR) problem.

Method: Proposes REORM (Reasoning-Enhanced Object Removal with MLLM), a modular framework that uses multimodal LLMs to analyze images and infer which interaction elements must be jointly removed. It includes MLLM-driven analysis, mask-guided removal, and a self-correction mechanism, with a local-deployment variant for resource-limited settings.

Result: REORM outperforms state-of-the-art image editing systems on the proposed ICOREval benchmark, demonstrating effectiveness in producing interaction-consistent removal results.

Conclusion: The paper successfully addresses the ICOR problem by leveraging MLLM reasoning to identify and remove interaction evidence, resulting in more semantically consistent object removal compared to existing methods.

Abstract: Image-based object removal often erases only the named target, leaving behind interaction evidence that renders the result semantically inconsistent. We formalize this problem as Interaction-Consistent Object Removal (ICOR), which requires removing not only the target object but also associated interaction elements, such as lighting-dependent effects, physically connected objects, targetproduced elements, and contextually linked objects. To address this task, we propose Reasoning-Enhanced Object Removal with MLLM (REORM), a reasoningenhanced object removal framework that leverages multimodal large language models to infer which elements must be jointly removed. REORM features a modular design that integrates MLLM-driven analysis, mask-guided removal, and a self-correction mechanism, along with a local-deployment variant that supports accurate editing under limited resources. To support evaluation, we introduce ICOREval, a benchmark consisting of instruction-driven removals with rich interaction dependencies. On ICOREval, REORM outperforms state-of-the-art image editing systems, demonstrating its effectiveness in producing interactionconsistent results.

</details>


### [157] [ReDiStory: Region-Disentangled Diffusion for Consistent Visual Story Generation](https://arxiv.org/abs/2602.01303)
*Ayushman Sarkar,Zhenyu Yu,Chu Chen,Wei Tang,Kangning Cui,Mohd Yamani Idna Idris*

Main category: cs.CV

TL;DR: ReDiStory improves multi-frame story generation by reorganizing prompt embeddings to separate identity and frame-specific components, reducing cross-frame interference without training.


<details>
  <summary>Details</summary>
Motivation: Current training-free methods for visual story generation concatenate identity and frame prompts, causing inter-frame semantic interference that weakens subject identity preservation in complex stories.

Method: ReDiStory decomposes text embeddings into identity-related and frame-specific components, then decorrelates frame embeddings by suppressing shared directions across frames during inference without modifying diffusion parameters.

Result: Experiments on ConsiStory+ benchmark show consistent gains over 1Prompt1Story on multiple identity consistency metrics while maintaining prompt fidelity, using identical diffusion backbones and inference settings.

Conclusion: ReDiStory effectively improves identity consistency in multi-frame story generation through inference-time prompt embedding reorganization, offering a training-free solution to reduce cross-frame interference.

Abstract: Generating coherent visual stories requires maintaining subject identity across multiple images while preserving frame-specific semantics. Recent training-free methods concatenate identity and frame prompts into a unified representation, but this often introduces inter-frame semantic interference that weakens identity preservation in complex stories. We propose ReDiStory, a training-free framework that improves multi-frame story generation via inference-time prompt embedding reorganization. ReDiStory explicitly decomposes text embeddings into identity-related and frame-specific components, then decorrelates frame embeddings by suppressing shared directions across frames. This reduces cross-frame interference without modifying diffusion parameters or requiring additional supervision. Under identical diffusion backbones and inference settings, ReDiStory improves identity consistency while maintaining prompt fidelity. Experiments on the ConsiStory+ benchmark show consistent gains over 1Prompt1Story on multiple identity consistency metrics. Code is available at: https://github.com/YuZhenyuLindy/ReDiStory

</details>


### [158] [StoryState: Agent-Based State Control for Consistent and Editable Storybooks](https://arxiv.org/abs/2602.01305)
*Ayushman Sarkar,Zhenyu Yu,Wei Tang,Chu Chen,Kangning Cui,Mohd Yamani Idna Idris*

Main category: cs.CV

TL;DR: StoryState introduces an explicit, editable story state layer for multimodal story generation, enabling localized edits and better visual consistency through structured character sheets and scene constraints.


<details>
  <summary>Details</summary>
Motivation: Current one-click storybook generation lacks explicit story state representation, making edits coarse-grained and breaking visual consistency across pages.

Method: Agent-based orchestration layer with explicit story state (character sheet, global settings, per-page constraints) using LLM agents to maintain state and generate prompts for training-free text-to-image generation.

Result: Enables localized page edits, improves cross-page consistency, reduces unintended changes, interaction turns, and editing time compared to 1Prompt1Story, approaching Gemini Storybook's one-shot consistency.

Conclusion: StoryState provides a model-agnostic, prompt-based solution for maintaining visual consistency in story generation while enabling fine-grained editing through explicit story state representation.

Abstract: Large multimodal models have enabled one-click storybook generation, where users provide a short description and receive a multi-page illustrated story. However, the underlying story state, such as characters, world settings, and page-level objects, remains implicit, making edits coarse-grained and often breaking visual consistency. We present StoryState, an agent-based orchestration layer that introduces an explicit and editable story state on top of training-free text-to-image generation. StoryState represents each story as a structured object composed of a character sheet, global settings, and per-page scene constraints, and employs a small set of LLM agents to maintain this state and derive 1Prompt1Story-style prompts for generation and editing. Operating purely through prompts, StoryState is model-agnostic and compatible with diverse generation backends. System-level experiments on multi-page editing tasks show that StoryState enables localized page edits, improves cross-page consistency, and reduces unintended changes, interaction turns, and editing time compared to 1Prompt1Story, while approaching the one-shot consistency of Gemini Storybook. Code is available at https://github.com/YuZhenyuLindy/StoryState

</details>


### [159] [DeCorStory: Gram-Schmidt Prompt Embedding Decorrelation for Consistent Storytelling](https://arxiv.org/abs/2602.01306)
*Ayushman Sarkar,Zhenyu Yu,Mohd Yamani Idna Idris*

Main category: cs.CV

TL;DR: DeCorStory is a training-free inference-time framework that reduces inter-frame semantic interference in text-to-image storytelling by decorrelating prompt embeddings and strengthening identity preservation.


<details>
  <summary>Details</summary>
Motivation: Existing training-free methods for text-to-image storytelling suffer from color leakage, background blending, and identity drift due to strong embedding correlation when concatenating all prompts into a single sequence.

Method: Uses Gram-Schmidt prompt embedding decorrelation to orthogonalize frame-level semantics, singular value reweighting to strengthen prompt-specific information, and identity-preserving cross-attention to stabilize character identity during diffusion.

Result: Demonstrates consistent improvements in prompt-image alignment, identity consistency, and visual diversity, achieving state-of-the-art performance among training-free baselines.

Conclusion: DeCorStory provides an effective training-free solution for maintaining visual and semantic consistency in text-to-image storytelling without model modification or fine-tuning.

Abstract: Maintaining visual and semantic consistency across frames is a key challenge in text-to-image storytelling. Existing training-free methods, such as One-Prompt-One-Story, concatenate all prompts into a single sequence, which often induces strong embedding correlation and leads to color leakage, background blending, and identity drift. We propose DeCorStory, a training-free inference-time framework that explicitly reduces inter-frame semantic interference. DeCorStory applies Gram-Schmidt prompt embedding decorrelation to orthogonalize frame-level semantics, followed by singular value reweighting to strengthen prompt-specific information and identity-preserving cross-attention to stabilize character identity during diffusion. The method requires no model modification or fine-tuning and can be seamlessly integrated into existing diffusion pipelines. Experiments demonstrate consistent improvements in prompt-image alignment, identity consistency, and visual diversity, achieving state-of-the-art performance among training-free baselines. Code is available at: https://github.com/YuZhenyuLindy/DeCorStory

</details>


### [160] [FlowCast: Trajectory Forecasting for Scalable Zero-Cost Speculative Flow Matching](https://arxiv.org/abs/2602.01329)
*Divya Jyoti Bajpai,Shubham Agarwal,Apoorv Saxena,Kuldeep Kulkarni,Subrata Mitra,Manjesh Kumar Hanawal*

Main category: cs.CV

TL;DR: FlowCast: Training-free speculative generation framework that accelerates Flow Matching models by exploiting constant velocity training, achieving >2.5× speedup without quality loss.


<details>
  <summary>Details</summary>
Motivation: Flow Matching models produce high-quality visual generation but suffer from slow inference due to many denoising steps, limiting real-time applications. Existing acceleration methods degrade quality, require costly retraining, or lack generalization.

Method: FlowCast speculates future velocity by extrapolating current velocity without time cost, accepting it if within MSE threshold. This constant-velocity forecasting skips redundant steps in stable regions while maintaining precision in complex areas. It's plug-and-play, requires no auxiliary networks or retraining.

Result: Achieves >2.5× speedup in image generation, video generation, and editing tasks, outperforming existing baselines with no quality loss compared to standard full generation.

Conclusion: FlowCast provides an effective training-free acceleration framework for Flow Matching models that maintains quality while significantly improving inference speed, making FM models more practical for real-time applications.

Abstract: Flow Matching (FM) has recently emerged as a powerful approach for high-quality visual generation. However, their prohibitively slow inference due to a large number of denoising steps limits their potential use in real-time or interactive applications. Existing acceleration methods, like distillation, truncation, or consistency training, either degrade quality, incur costly retraining, or lack generalization. We propose FlowCast, a training-free speculative generation framework that accelerates inference by exploiting the fact that FM models are trained to preserve constant velocity. FlowCast speculates future velocity by extrapolating current velocity without incurring additional time cost, and accepts it if it is within a mean-squared error threshold. This constant-velocity forecasting allows redundant steps in stable regions to be aggressively skipped while retaining precision in complex ones. FlowCast is a plug-and-play framework that integrates seamlessly with any FM model and requires no auxiliary networks. We also present a theoretical analysis and bound the worst-case deviation between speculative and full FM trajectories. Empirical evaluations demonstrate that FlowCast achieves $>2.5\times$ speedup in image generation, video generation, and editing tasks, outperforming existing baselines with no quality loss as compared to standard full generation.

</details>


### [161] [What Does Vision Tool-Use Reinforcement Learning Really Learn? Disentangling Tool-Induced and Intrinsic Effects for Crop-and-Zoom](https://arxiv.org/abs/2602.01334)
*Yan Ma,Weiyu Zhang,Tianle Li,Linge Du,Xuyang Shen,Pengfei Liu*

Main category: cs.CV

TL;DR: Vision tool-use RL improves performance but mainly through intrinsic learning gains, not tool mastery - tools reduce harm rather than enable correction of failures.


<details>
  <summary>Details</summary>
Motivation: To disentangle whether performance gains in vision tool-use RL come from improved tool use or evolving intrinsic capabilities of vision-language models.

Method: MED framework: Measure-Explain-Diagnose with coarse-to-fine analysis that separates intrinsic capability changes from tool effects, decomposes tool-induced differences into gain/harm terms, and probes underlying mechanisms.

Result: Across two VLMs and six benchmarks, improvements are dominated by intrinsic learning; tool-use RL mainly reduces tool-induced harm (fewer call errors, weaker tool schema interference) with limited progress in tool-based correction of intrinsic failures.

Conclusion: Current vision tool-use RL learns to coexist safely with tools rather than master them, suggesting tools serve as safety mechanisms rather than capability enhancers.

Abstract: Vision tool-use reinforcement learning (RL) can equip vision-language models with visual operators such as crop-and-zoom and achieves strong performance gains, yet it remains unclear whether these gains are driven by improvements in tool use or evolving intrinsic capabilities.We introduce MED (Measure-Explain-Diagnose), a coarse-to-fine framework that disentangles intrinsic capability changes from tool-induced effects, decomposes the tool-induced performance difference into gain and harm terms, and probes the mechanisms driving their evolution. Across checkpoint-level analyses on two VLMs with different tool priors and six benchmarks, we find that improvements are dominated by intrinsic learning, while tool-use RL mainly reduces tool-induced harm (e.g., fewer call-induced errors and weaker tool schema interference) and yields limited progress in tool-based correction of intrinsic failures. Overall, current vision tool-use RL learns to coexist safely with tools rather than master them.

</details>


### [162] [Beyond Pixels: Visual Metaphor Transfer via Schema-Driven Agentic Reasoning](https://arxiv.org/abs/2602.01335)
*Yu Xu,Yuxin Zhang,Juan Cao,Lin Gao,Chunyu Wang,Oliver Deussen,Tong-Yee Lee,Fan Tang*

Main category: cs.CV

TL;DR: Proposes Visual Metaphor Transfer (VMT) task and a multi-agent framework based on Conceptual Blending Theory to generate visual metaphors by decoupling abstract creative logic from reference images and transferring it to target subjects.


<details>
  <summary>Details</summary>
Motivation: Current generative AI models focus on pixel-level alignment and surface appearance but fail to capture the abstract logic needed for genuine metaphorical creativity. There's a gap in models' ability to understand and transfer the underlying creative essence of visual metaphors.

Method: Introduces VMT task and proposes a cognitive-inspired multi-agent framework with Schema Grammar ("G") based on Conceptual Blending Theory. Uses four specialized agents: perception agent distills reference into schema, transfer agent maintains generic space invariance, generation agent synthesizes output, and hierarchical diagnostic agent performs closed-loop error correction.

Result: Extensive experiments and human evaluations show the method significantly outperforms state-of-the-art baselines in metaphor consistency, analogy appropriateness, and visual creativity.

Conclusion: The framework successfully bridges the gap in metaphorical generation, enabling automated high-impact creative applications in advertising and media by operationalizing cognitive theories of creativity.

Abstract: A visual metaphor constitutes a high-order form of human creativity, employing cross-domain semantic fusion to transform abstract concepts into impactful visual rhetoric. Despite the remarkable progress of generative AI, existing models remain largely confined to pixel-level instruction alignment and surface-level appearance preservation, failing to capture the underlying abstract logic necessary for genuine metaphorical generation. To bridge this gap, we introduce the task of Visual Metaphor Transfer (VMT), which challenges models to autonomously decouple the "creative essence" from a reference image and re-materialize that abstract logic onto a user-specified target subject. We propose a cognitive-inspired, multi-agent framework that operationalizes Conceptual Blending Theory (CBT) through a novel Schema Grammar ("G"). This structured representation decouples relational invariants from specific visual entities, providing a rigorous foundation for cross-domain logic re-instantiation. Our pipeline executes VMT through a collaborative system of specialized agents: a perception agent that distills the reference into a schema, a transfer agent that maintains generic space invariance to discover apt carriers, a generation agent for high-fidelity synthesis and a hierarchical diagnostic agent that mimics a professional critic, performing closed-loop backtracking to identify and rectify errors across abstract logic, component selection, and prompt encoding. Extensive experiments and human evaluations demonstrate that our method significantly outperforms SOTA baselines in metaphor consistency, analogy appropriateness, and visual creativity, paving the way for automated high-impact creative applications in advertising and media. Source code will be made publicly available.

</details>


### [163] [MTC-VAE: Multi-Level Temporal Compression with Content Awareness](https://arxiv.org/abs/2602.01340)
*Yubo Dong,Linchao Zhu*

Main category: cs.CV

TL;DR: A technique to convert fixed compression rate VAEs into models supporting multi-level temporal compression for video diffusion models, with minimal fine-tuning to maintain performance at higher compression rates.


<details>
  <summary>Details</summary>
Motivation: Latent Video Diffusion Models (LVDMs) use VAEs for video compression, but higher compression rates cause efficiency decline when extra sampling layers are added without expanding hidden channel dimensions. There's a need for flexible temporal compression without performance degradation.

Method: Propose a technique to convert fixed compression rate VAEs into models supporting multi-level temporal compression, providing a straightforward minimal fine-tuning approach to counteract performance decline at elevated compression rates.

Result: Examine how varying compression levels impact model performance across diverse video segments, provide empirical evidence of approach effectiveness, and investigate integration with diffusion-based generative models (DiT), showing successful concurrent training and framework compatibility.

Conclusion: The proposed multi-level temporal compression technique for VAEs demonstrates practical utility for video diffusion models, offering flexible compression while maintaining performance, with potential applications in diffusion-based generative frameworks.

Abstract: Latent Video Diffusion Models (LVDMs) rely on Variational Autoencoders (VAEs) to compress videos into compact latent representations. For continuous Variational Autoencoders (VAEs), achieving higher compression rates is desirable; yet, the efficiency notably declines when extra sampling layers are added without expanding the dimensions of hidden channels. In this paper, we present a technique to convert fixed compression rate VAEs into models that support multi-level temporal compression, providing a straightforward and minimal fine-tuning approach to counteract performance decline at elevated compression rates.Moreover, we examine how varying compression levels impact model performance over video segments with diverse characteristics, offering empirical evidence on the effectiveness of our proposed approach. We also investigate the integration of our multi-level temporal compression VAE with diffusion-based generative models, DiT, highlighting successful concurrent training and compatibility within these frameworks. This investigation illustrates the potential uses of multi-level temporal compression.

</details>


### [164] [Adaptive Visual Autoregressive Acceleration via Dual-Linkage Entropy Analysis](https://arxiv.org/abs/2602.01345)
*Yu Zhang,Jingyi Liu,Feng Liu,Duoqian Miao,Qi Zhang,Kexue Fu,Changwei Wang,Longbing Cao*

Main category: cs.CV

TL;DR: NOVA is a training-free token reduction framework for Visual AutoRegressive models that uses entropy analysis to dynamically prune low-entropy tokens and accelerate inference while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: Current VAR token reduction methods have three key limitations: heuristic stage partition, non-adaptive schedules, and limited acceleration scope, leaving significant acceleration potential untapped. Since entropy variation reflects predictive uncertainty evolution, it offers a principled way to capture modeling dynamics.

Method: NOVA adaptively determines acceleration activation scale during inference by online identification of the inflection point of scale entropy growth. It uses scale-linkage and layer-linkage ratio adjustment to dynamically compute distinct token reduction ratios for each scale and layer, pruning low-entropy tokens while reusing cache from prior scale residuals.

Result: Extensive experiments and analyses validate NOVA as a simple yet effective training-free acceleration framework for VAR models.

Conclusion: NOVA provides an effective training-free solution to accelerate VAR models through entropy-based adaptive token reduction, addressing limitations of existing methods while maintaining generation quality.

Abstract: Visual AutoRegressive modeling (VAR) suffers from substantial computational cost due to the massive token count involved. Failing to account for the continuous evolution of modeling dynamics, existing VAR token reduction methods face three key limitations: heuristic stage partition, non-adaptive schedules, and limited acceleration scope, thereby leaving significant acceleration potential untapped. Since entropy variation intrinsically reflects the transition of predictive uncertainty, it offers a principled measure to capture modeling dynamics evolution. Therefore, we propose NOVA, a training-free token reduction acceleration framework for VAR models via entropy analysis. NOVA adaptively determines the acceleration activation scale during inference by online identifying the inflection point of scale entropy growth. Through scale-linkage and layer-linkage ratio adjustment, NOVA dynamically computes distinct token reduction ratios for each scale and layer, pruning low-entropy tokens while reusing the cache derived from the residuals at the prior scale to accelerate inference and maintain generation quality. Extensive experiments and analyses validate NOVA as a simple yet effective training-free acceleration framework.

</details>


### [165] [T2M Mamba: Motion Periodicity-Saliency Coupling Approach for Stable Text-Driven Motion Generation](https://arxiv.org/abs/2602.01352)
*Xingzu Zhan,Chen Xie,Honghang Chen,Yixun Lin,Xiaochun Mai*

Main category: cs.CV

TL;DR: T2M Mamba improves text-to-motion generation by jointly modeling motion periodicity and keyframe saliency, and enhancing robustness to paraphrases through cross-modal alignment.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-motion models have two core limitations: (1) treating motion periodicity and keyframe saliency as independent factors, causing generation drift in long sequences, and (2) being fragile to semantically equivalent paraphrases where minor synonym substitutions produce unstable or erroneous motions.

Method: Proposes T2M Mamba with two key components: (1) Periodicity-Saliency Aware Mamba that uses enhanced Density Peaks Clustering for keyframe weight estimation and FFT-accelerated autocorrelation for motion periodicity estimation to capture coupled dynamics, and (2) Periodic Differential Cross-modal Alignment Module (PDCAM) to enhance robust alignment of textual and motion embeddings.

Result: Extensive experiments on HumanML3D and KIT-ML datasets show effectiveness, achieving an FID of 0.068 and consistent gains on all other metrics.

Conclusion: T2M Mamba successfully addresses the limitations of existing text-to-motion models by jointly modeling periodicity and saliency while improving robustness to paraphrases through better cross-modal alignment.

Abstract: Text-to-motion generation, which converts motion language descriptions into coherent 3D human motion sequences, has attracted increasing attention in fields, such as avatar animation and humanoid robotic interaction. Though existing models have achieved significant fidelity, they still suffer from two core limitations: (i) They treat motion periodicity and keyframe saliency as independent factors, overlooking their coupling and causing generation drift in long sequences. (ii) They are fragile to semantically equivalent paraphrases, where minor synonym substitutions distort textual embeddings, propagating through the decoder and producing unstable or erroneous motions. In this work, we propose T2M Mamba to address these limitations by (i) proposing Periodicity-Saliency Aware Mamba, which utilizes novel algorithms for keyframe weight estimation via enhanced Density Peaks Clustering and motion periodicity estimation via FFT-accelerated autocorrelation to capture coupled dynamics with minimal computational overhead, and (ii) constructing a Periodic Differential Cross-modal Alignment Module (PDCAM) to enhance robust alignment of textual and motion embeddings. Extensive experiments on HumanML3D and KIT-ML datasets have been conducted, confirming the effectiveness of our approach, achieving an FID of 0.068 and consistent gains on all other metrics.

</details>


### [166] [Exposing and Defending the Achilles' Heel of Video Mixture-of-Experts](https://arxiv.org/abs/2602.01369)
*Songping Wang,Qinglong Liu,Yueming Lyu,Ning Li,Ziwen He,Caifeng Shan*

Main category: cs.CV

TL;DR: TLGA framework exposes component-level vulnerabilities in video MoE models through router attacks and joint router-expert attacks, then uses J-TLAT for defense with 60%+ inference cost reduction.


<details>
  <summary>Details</summary>
Motivation: MoE models show strong video understanding performance but their adversarial robustness is underexplored, with existing attacks treating MoE as unified architecture rather than addressing independent component weaknesses.

Method: Propose Temporal Lipschitz-Guided Attacks (TLGA) to investigate component vulnerabilities: 1) router attacks reveal independent weaknesses, 2) Joint TLGA (J-TLGA) collaboratively perturbs routers and experts to expose collaborative weaknesses, 3) Joint Temporal Lipschitz Adversarial Training (J-TLAT) for defense.

Result: Framework is plug-and-play, reduces inference cost by >60% vs dense models, consistently enhances adversarial robustness across diverse datasets/architectures, effectively mitigates both independent and collaborative MoE weaknesses.

Conclusion: TLGA framework successfully exposes and mitigates component-level vulnerabilities in video MoE models through targeted attacks and joint adversarial training, achieving both robustness improvements and computational efficiency.

Abstract: Mixture-of-Experts (MoE) has demonstrated strong performance in video understanding tasks, yet its adversarial robustness remains underexplored. Existing attack methods often treat MoE as a unified architecture, overlooking the independent and collaborative weaknesses of key components such as routers and expert modules. To fill this gap, we propose Temporal Lipschitz-Guided Attacks (TLGA) to thoroughly investigate component-level vulnerabilities in video MoE models. We first design attacks on the router, revealing its independent weaknesses. Building on this, we introduce Joint Temporal Lipschitz-Guided Attacks (J-TLGA), which collaboratively perturb both routers and experts. This joint attack significantly amplifies adversarial effects and exposes the Achilles' Heel (collaborative weaknesses) of the MoE architecture. Based on these insights, we further propose Joint Temporal Lipschitz Adversarial Training (J-TLAT). J-TLAT performs joint training to further defend against collaborative weaknesses, enhancing component-wise robustness. Our framework is plug-and-play and reduces inference cost by more than 60% compared with dense models. It consistently enhances adversarial robustness across diverse datasets and architectures, effectively mitigating both the independent and collaborative weaknesses of MoE.

</details>


### [167] [PolyGen: Fully Synthetic Vision-Language Training via Multi-Generator Ensembles](https://arxiv.org/abs/2602.01370)
*Leonardo Brusini,Cristian Sbrolli,Eugenio Lomurno,Toshihiko Yamasaki,Matteo Matteucci*

Main category: cs.CV

TL;DR: PolyGen introduces a framework that uses multiple distinct generators for synthetic data creation to reduce model-specific biases and improve feature diversity, outperforming single-source methods on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current synthetic data methods rely on scaling up single generative backbones, which introduces generator-specific spectral biases and limits feature diversity. There's a need for more robust synthetic data construction that prioritizes manifold coverage over simple dataset size scaling.

Method: PolyGen employs a Polylithic approach that trains on the intersection of architecturally distinct generators to marginalize out model-specific artifacts. It also introduces a Programmatic Hard Negative curriculum to enforce fine-grained syntactic understanding, and reallocates data budget from unique captions to multi-source variations.

Result: PolyGen outperforms the leading single-source baseline (SynthCLIP) by +19.0% on aggregate multi-task benchmarks and achieves +9.1% improvement on the SugarCrepe++ compositionality benchmark, demonstrating superior performance with the same data budget.

Conclusion: Structural diversity in synthetic data generation is a more data-efficient scaling law than simply increasing the volume of single-source samples, and prioritizing manifold coverage and compositional rigor leads to more robust feature spaces.

Abstract: Synthetic data offers a scalable solution for vision-language pre-training, yet current state-of-the-art methods typically rely on scaling up a single generative backbone, which introduces generator-specific spectral biases and limits feature diversity. In this work, we introduce PolyGen, a framework that redefines synthetic data construction by prioritizing manifold coverage and compositional rigor over simple dataset size. PolyGen employs a Polylithic approach to train on the intersection of architecturally distinct generators, effectively marginalizing out model-specific artifacts. Additionally, we introduce a Programmatic Hard Negative curriculum that enforces fine-grained syntactic understanding. By structurally reallocating the same data budget from unique captions to multi-source variations, PolyGen achieves a more robust feature space, outperforming the leading single-source baseline (SynthCLIP) by +19.0% on aggregate multi-task benchmarks and on the SugarCrepe++ compositionality benchmark (+9.1%). These results demonstrate that structural diversity is a more data-efficient scaling law than simply increasing the volume of single-source samples.

</details>


### [168] [PromptRL: Prompt Matters in RL for Flow-Based Image Generation](https://arxiv.org/abs/2602.01382)
*Fu-Yun Wang,Han Zhang,Michael Gharbi,Hongsheng Li,Taesung Park*

Main category: cs.CV

TL;DR: PromptRL addresses sample inefficiency and prompt overfitting in RL for flow matching models by integrating language models as trainable prompt refinement agents within the optimization loop, achieving SOTA performance with 2× fewer rollouts.


<details>
  <summary>Details</summary>
Motivation: Current RL pipelines for flow matching models suffer from two key limitations: sample inefficiency due to insufficient generation diversity, and pronounced prompt overfitting where models memorize specific training formulations and fail on semantically equivalent but stylistically varied prompts.

Method: PromptRL incorporates language models as trainable prompt refinement agents directly within the flow-based RL optimization loop, enabling rapid development of sophisticated prompt rewriting capabilities and creating a synergistic training regime that reshapes optimization dynamics.

Result: Achieves state-of-the-art performance: 0.97 on GenEval, 0.98 on OCR accuracy, 24.05 on PickScore. Improves EditReward of FLUX.1-Kontext from 1.19 to 1.43 with only 0.06M rollouts, surpassing Gemini 2.5 Flash Image (1.37) and matching ReasonNet (1.44). Consistently achieves higher performance with over 2× fewer rollouts compared to naive flow-only RL.

Conclusion: PromptRL effectively addresses the limitations of current RL pipelines for flow matching models by integrating prompt refinement capabilities, demonstrating superior efficiency and performance across multiple benchmarks while requiring significantly fewer training rollouts.

Abstract: Flow matching models (FMs) have revolutionized text-to-image (T2I) generation, with reinforcement learning (RL) serving as a critical post-training strategy for alignment with reward objectives. In this research, we show that current RL pipelines for FMs suffer from two underappreciated yet important limitations: sample inefficiency due to insufficient generation diversity, and pronounced prompt overfitting, where models memorize specific training formulations and exhibit dramatic performance collapse when evaluated on semantically equivalent but stylistically varied prompts. We present PromptRL (Prompt Matters in RL for Flow-Based Image Generation), a framework that incorporates language models (LMs) as trainable prompt refinement agents directly within the flow-based RL optimization loop. This design yields two complementary benefits: rapid development of sophisticated prompt rewriting capabilities and, critically, a synergistic training regime that reshapes the optimization dynamics. PromptRL achieves state-of-the-art performance across multiple benchmarks, obtaining scores of 0.97 on GenEval, 0.98 on OCR accuracy, and 24.05 on PickScore.
  Furthermore, we validate the effectiveness of our RL approach on large-scale image editing models, improving the EditReward of FLUX.1-Kontext from 1.19 to 1.43 with only 0.06 million rollouts, surpassing Gemini 2.5 Flash Image (also known as Nano Banana), which scores 1.37, and achieving comparable performance with ReasonNet (1.44), which relied on fine-grained data annotations along with a complex multi-stage training. Our extensive experiments empirically demonstrate that PromptRL consistently achieves higher performance ceilings while requiring over 2$\times$ fewer rollouts compared to naive flow-only RL. Our code is available at https://github.com/G-U-N/UniRL.

</details>


### [169] [Stronger Semantic Encoders Can Harm Relighting Performance: Probing Visual Priors via Augmented Latent Intrinsics](https://arxiv.org/abs/2602.01391)
*Xiaoyan Xing,Xiao Zhang,Sezer Karaoglu,Theo Gevers,Anand Bhattad*

Main category: cs.CV

TL;DR: ALI introduces a method that fuses semantic and photometric features to improve image relighting, especially for challenging materials like metal and glass.


<details>
  <summary>Details</summary>
Motivation: Current image relighting methods using latent intrinsic representations struggle with challenging materials like metal and glass, and stronger semantic priors actually degrade performance, revealing a trade-off between semantic abstraction and photometric fidelity.

Method: Augmented Latent Intrinsics (ALI) balances semantic context and dense photometric structure by fusing features from a pixel-aligned visual encoder into a latent-intrinsic framework, with self-supervised refinement to address data scarcity.

Result: ALI achieves strong improvements in relighting quality, with the largest gains on complex, specular materials, trained only on unlabeled real-world image pairs.

Conclusion: The approach successfully addresses the semantic-photometric trade-off in image relighting and demonstrates superior performance on challenging materials.

Abstract: Image-to-image relighting requires representations that disentangle scene properties from illumination. Recent methods rely on latent intrinsic representations but remain under-constrained and often fail on challenging materials such as metal and glass. A natural hypothesis is that stronger pretrained visual priors should resolve these failures. We find the opposite: features from top-performing semantic encoders often degrade relighting quality, revealing a fundamental trade-off between semantic abstraction and photometric fidelity. We study this trade-off and introduce Augmented Latent Intrinsics (ALI), which balances semantic context and dense photometric structure by fusing features from a pixel-aligned visual encoder into a latent-intrinsic framework, together with a self-supervised refinement strategy to mitigate the scarcity of paired real-world data. Trained only on unlabeled real-world image pairs and paired with a dense, pixel-aligned visual prior, ALI achieves strong improvements in relighting, with the largest gains on complex, specular materials. Project page: https:\\augmented-latent-intrinsics.github.io

</details>


### [170] [Where to Attend: A Principled Vision-Centric Position Encoding with Parabolas](https://arxiv.org/abs/2602.01418)
*Christoffer Koo Øhrstrøm,Rafael I. Cabral Muchacho,Yifei Dong,Filippos Moumtzidellis,Ronja Güldenring,Florian T. Pokorny,Lazaros Nalpantidis*

Main category: cs.CV

TL;DR: PaPE is a parabola-based position encoding designed specifically for vision modalities that incorporates key vision characteristics like translation/rotation invariance and distance decay, achieving top performance on 7/8 datasets across 4 modalities.


<details>
  <summary>Details</summary>
Motivation: Existing position encodings for vision are often just extensions from 1D language models without fully accounting for vision-specific characteristics like translation invariance, rotation invariance, distance decay, directionality, and context awareness.

Method: Parabolic Position Encoding (PaPE) is designed from principles distilled from prior work: translation invariance, rotation invariance (PaPE-RI variant), distance decay, directionality, and context awareness. It uses a parabola-based approach to encode positions for vision tokens across different modalities.

Result: PaPE or its rotation-invariant variant PaPE-RI achieves top performance on 7 out of 8 datasets spanning 4 vision modalities (images, point clouds, videos, event camera streams). On ImageNet-1K extrapolation experiments, PaPE improves by up to 10.5% absolute over the next-best position encoding.

Conclusion: PaPE effectively addresses the gap in vision-specific position encoding by incorporating key vision characteristics, demonstrating superior performance across multiple modalities and strong extrapolation capabilities.

Abstract: We propose Parabolic Position Encoding (PaPE), a parabola-based position encoding for vision modalities in attention-based architectures. Given a set of vision tokens-such as images, point clouds, videos, or event camera streams-our objective is to encode their positions while accounting for the characteristics of vision modalities. Prior works have largely extended position encodings from 1D-sequences in language to nD-structures in vision, but only with partial account of vision characteristics. We address this gap by designing PaPE from principles distilled from prior work: translation invariance, rotation invariance (PaPE-RI), distance decay, directionality, and context awareness. We evaluate PaPE on 8 datasets that span 4 modalities. We find that either PaPE or PaPE-RI achieves the top performance on 7 out of 8 datasets. Extrapolation experiments on ImageNet-1K show that PaPE extrapolates remarkably well, improving in absolute terms by up to 10.5% over the next-best position encoding. Code is available at https://github.com/DTU-PAS/parabolic-position-encoding.

</details>


### [171] [BioTamperNet: Affinity-Guided State-Space Model Detecting Tampered Biomedical Images](https://arxiv.org/abs/2602.01435)
*Soumyaroop Nandi,Prem Natarajan*

Main category: cs.CV

TL;DR: BioTamperNet is a novel framework for detecting duplicated regions in tampered biomedical images using affinity-guided attention inspired by State Space Models.


<details>
  <summary>Details</summary>
Motivation: Existing forensic models trained on natural images underperform on biomedical data where subtle manipulations can compromise experimental validity, creating a need for specialized biomedical image tampering detection.

Method: Introduces affinity-guided self-attention to capture intra-image similarities and affinity-guided cross-attention to model cross-image correspondences, integrating lightweight SSM-inspired linear attention mechanisms for efficient fine-grained localization.

Result: Extensive experiments on benchmark bio-forensic datasets demonstrate significant improvements over competitive baselines in accurately detecting duplicated regions.

Conclusion: BioTamperNet effectively addresses the limitations of existing forensic models on biomedical data and provides an efficient framework for detecting duplicated regions in tampered biomedical images.

Abstract: We propose BioTamperNet, a novel framework for detecting duplicated regions in tampered biomedical images, leveraging affinity-guided attention inspired by State Space Model (SSM) approximations. Existing forensic models, primarily trained on natural images, often underperform on biomedical data where subtle manipulations can compromise experimental validity. To address this, BioTamperNet introduces an affinity-guided self-attention module to capture intra-image similarities and an affinity-guided cross-attention module to model cross-image correspondences. Our design integrates lightweight SSM-inspired linear attention mechanisms to enable efficient, fine-grained localization. Trained end-to-end, BioTamperNet simultaneously identifies tampered regions and their source counterparts. Extensive experiments on the benchmark bio-forensic datasets demonstrate significant improvements over competitive baselines in accurately detecting duplicated regions. Code - https://github.com/SoumyaroopNandi/BioTamperNet

</details>


### [172] [Cross-Paradigm Evaluation of Gaze-Based Semantic Object Identification for Intelligent Vehicles](https://arxiv.org/abs/2602.01452)
*Penghao Deng,Jidong J. Yang,Jiachen Bian*

Main category: cs.CV

TL;DR: Paper compares three vision approaches for identifying driver gaze targets in road scenes, finding YOLOv13 and large VLMs perform best, with VLMs excelling at detecting small safety-critical objects in adverse conditions.


<details>
  <summary>Details</summary>
Motivation: Understanding where drivers look during driving is crucial for developing better driver-assistance systems and improving road safety. The paper aims to identify what objects drivers are looking at from front-view camera footage.

Method: Three vision-based approaches: 1) Direct object detection (YOLOv13), 2) Segmentation-assisted classification (SAM2 with EfficientNetV2 vs YOLOv13), and 3) Query-based Vision-Language Models (Qwen2.5-VL-7b vs Qwen2.5-VL-32b). The task is framed as semantic identification of gaze targets from road scenes.

Result: YOLOv13 and Qwen2.5-VL-32b significantly outperformed other approaches with Macro F1-Scores over 0.84. Large VLMs showed superior robustness for identifying small safety-critical objects like traffic lights, especially in nighttime conditions. Segmentation-assisted approaches suffered from "part-versus-whole" semantic gaps leading to poor recall.

Conclusion: There's a fundamental trade-off between real-time efficiency of traditional detectors and richer contextual understanding/robustness of large VLMs. Large VLMs excel at detecting small safety-critical objects in adverse conditions, providing critical insights for designing human-aware driver monitoring systems.

Abstract: Understanding where drivers direct their visual attention during driving, as characterized by gaze behavior, is critical for developing next-generation advanced driver-assistance systems and improving road safety. This paper tackles this challenge as a semantic identification task from the road scenes captured by a vehicle's front-view camera. Specifically, the collocation of gaze points with object semantics is investigated using three distinct vision-based approaches: direct object detection (YOLOv13), segmentation-assisted classification (SAM2 paired with EfficientNetV2 versus YOLOv13), and query-based Vision-Language Models, VLMs (Qwen2.5-VL-7b versus Qwen2.5-VL-32b). The results demonstrate that the direct object detection (YOLOv13) and Qwen2.5-VL-32b significantly outperform other approaches, achieving Macro F1-Scores over 0.84. The large VLM (Qwen2.5-VL-32b), in particular, exhibited superior robustness and performance for identifying small, safety-critical objects such as traffic lights, especially in adverse nighttime conditions. Conversely, the segmentation-assisted paradigm suffers from a "part-versus-whole" semantic gap that led to large failure in recall. The results reveal a fundamental trade-off between the real-time efficiency of traditional detectors and the richer contextual understanding and robustness offered by large VLMs. These findings provide critical insights and practical guidance for the design of future human-aware intelligent driver monitoring systems.

</details>


### [173] [Understanding vision transformer robustness through the lens of out-of-distribution detection](https://arxiv.org/abs/2602.01459)
*Joey Kuang,Alexander Wong*

Main category: cs.CV

TL;DR: Vision transformers show quantization sensitivity, especially when pretrained on large datasets, with OOD detection revealing greater performance drops than in-distribution tasks.


<details>
  <summary>Details</summary>
Motivation: To understand how quantization affects vision transformers' performance, particularly in out-of-distribution scenarios, and investigate whether pretraining on large datasets helps or hinders quantization robustness.

Method: Analyzed quantized small-variant vision transformers (DeiT, DeiT3, ViT) on common OOD datasets, comparing ID vs OOD performance, and examining effects of pretraining scale (ImageNet-1k vs ImageNet-22k).

Result: 4-bit quantization causes significant performance drops, especially for models pretrained on larger datasets. DeiT3 (strongest FP32 model) dropped 17% with quantization. OOD detection revealed larger quantization deltas (15-19% for ImageNet-22k models vs 9-12% for ImageNet-1k models).

Conclusion: Pretraining on large-scale datasets may hinder low-bit quantization robustness in OOD detection, and data augmentation may be a more beneficial option for maintaining performance with quantization.

Abstract: Vision transformers have shown remarkable performance in vision tasks, but enabling them for accessible and real-time use is still challenging. Quantization reduces memory and inference costs at the risk of performance loss. Strides have been made to mitigate low precision issues mainly by understanding in-distribution (ID) task behaviour, but the attention mechanism may provide insight on quantization attributes by exploring out-of-distribution (OOD) situations. We investigate the behaviour of quantized small-variant popular vision transformers (DeiT, DeiT3, and ViT) on common OOD datasets. ID analyses show the initial instabilities of 4-bit models, particularly of those trained on the larger ImageNet-22k, as the strongest FP32 model, DeiT3, sharply drop 17% from quantization error to be one of the weakest 4-bit models. While ViT shows reasonable quantization robustness for ID calibration, OOD detection reveals more: ViT and DeiT3 pretrained on ImageNet-22k respectively experienced a 15.0% and 19.2% average quantization delta in AUPR-out between full precision to 4-bit while their ImageNet-1k-only counterparts experienced a 9.5% and 12.0% delta. Overall, our results suggest pretraining on large scale datasets may hinder low-bit quantization robustness in OOD detection and that data augmentation may be a more beneficial option.

</details>


### [174] [Preserving Localized Patch Semantics in VLMs](https://arxiv.org/abs/2602.01530)
*Parsa Esmaeilkhani,Longin Jan Latecki*

Main category: cs.CV

TL;DR: The paper introduces Logit Lens Loss (LLL) to fix Logit Lens visualization issues in VLMs by preserving visual token locality through semantic alignment with textual concepts, improving both explainability and vision task performance.


<details>
  <summary>Details</summary>
Motivation: Logit Lens visualization in Vision-Language Models suffers because visual content from image tokens diffuses to language tokens, destroying locality and making the visualization unusable for explainability purposes.

Method: Proposes Logit Lens Loss (LLL) - a complementary loss to next-token prediction that aligns visual token embeddings with textual concepts describing their image regions, preventing visual information loss without architectural changes or large-scale training.

Result: LLL makes Logit Lens practically relevant by producing meaningful object confidence maps in images and improves performance on vision-centric tasks like segmentation without adding special heads.

Conclusion: The Logit Lens Loss successfully preserves visual token locality in VLMs, enabling effective Logit Lens visualization for explainability while enhancing vision task performance through better semantic alignment between visual tokens and textual concepts.

Abstract: Logit Lens has been proposed for visualizing tokens that contribute most to LLM answers. Recently, Logit Lens was also shown to be applicable in autoregressive Vision-Language Models (VLMs), where it illustrates the conceptual content of image tokens in the form of heatmaps, e.g., which image tokens are likely to depict the concept of cat in a given image. However, the visual content of image tokens often gets diffused to language tokens, and consequently, the locality of visual information gets mostly destroyed, which renders Logit Lens visualization unusable for explainability. To address this issue, we introduce a complementary loss to next-token prediction (NTP) to prevent the visual tokens from losing the visual representation inherited from corresponding image patches. The proposed Logit Lens Loss (LLL) is designed to make visual token embeddings more semantically aligned with the textual concepts that describe their image regions (e.g., patches containing a cat with the word "cat"), without requiring any architectural modification or large-scale training. This way, LLL constrains the mixing of image and text tokens in the self-attention layers in order to prevent image tokens from losing their localized visual information. As our experiments show, LLL not only makes Logit Lens practically relevant by producing meaningful object confidence maps in images, but also improves performance on vision-centric tasks like segmentation without attaching any special heads.

</details>


### [175] [Rotation-free Online Handwritten Character Recognition Using Linear Recurrent Units](https://arxiv.org/abs/2602.01533)
*Zhe Ling,Sicheng Yu,Danyu Yang*

Main category: cs.CV

TL;DR: SW-PS+LRU framework achieves state-of-the-art rotation-invariant online handwritten character recognition with high accuracy across digits, English letters, and Chinese radicals.


<details>
  <summary>Details</summary>
Motivation: Online handwritten character recognition is sensitive to rotational deformations that disrupt stroke spatial layout, reducing accuracy. Existing methods struggle with rotation-invariant feature extraction.

Method: Combines Sliding Window Path Signature (SW-PS) for local structural feature extraction with Linear Recurrent Units (LRU) classifier that combines RNN's incremental processing with SSM's parallel training efficiency.

Result: Achieved 99.62% accuracy on digits, 96.67% on English uppercase letters, and 94.33% on Chinese radicals with random rotations up to ±180° on CASIA-OLHWDB1.1 dataset.

Conclusion: The SW-PS+LRU framework outperforms competing models in both convergence speed and test accuracy for rotation-invariant online handwritten character recognition.

Abstract: Online handwritten character recognition leverages stroke order and dynamic features, which generally provide higher accuracy and robustness compared with offline recognition. However, in practical applications, rotational deformations can disrupt the spatial layout of strokes, substantially reducing recognition accuracy. Extracting rotation-invariant features therefore remains a challenging open problem. In this work, we employ the Sliding Window Path Signature (SW-PS) to capture local structural features of characters, and introduce the lightweight Linear Recurrent Units (LRU) as the classifier. The LRU combine the fast incremental processing capability of recurrent neural networks (RNN) with the efficient parallel training of state space models (SSM), while reliably modelling dynamic stroke characteristics. We conducted recognition experiments with random rotation angle up to $\pm 180^{\circ}$ on three subsets of the CASIA-OLHWDB1.1 dataset: digits, English upper letters, and Chinese radicals. The accuracies achieved after ensemble learning were $99.62\%$, $96.67\%$, and $94.33\%$, respectively. Experimental results demonstrate that the proposed SW-PS+LRU framework consistently surpasses competing models in both convergence speed and test accuracy.

</details>


### [176] [Making Avatars Interact: Towards Text-Driven Human-Object Interaction for Controllable Talking Avatars](https://arxiv.org/abs/2602.01538)
*Youliang Zhang,Zhengguang Zhou,Zhentao Yu,Ziyao Huang,Teng Hu,Sen Liang,Guozhen Zhang,Ziqiao Peng,Shunkai Li,Yi Chen,Zixiang Zhou,Yuan Zhou,Qinglin Lu,Xiu Li*

Main category: cs.CV

TL;DR: InteractAvatar: A dual-stream framework for generating talking avatars that perform text-aligned grounded human-object interactions, addressing environmental perception and control-quality challenges through parallel motion and video generation.


<details>
  <summary>Details</summary>
Motivation: Existing talking avatar methods can generate full-body avatars with simple motion, but extending to grounded human-object interaction (GHOI) remains challenging due to environmental perception needs and control-quality trade-offs in GHOI generation.

Method: Proposes InteractAvatar with dual-stream framework: Perception and Interaction Module (PIM) for text-aligned interaction motions using detection-enhanced perception, and Audio-Interaction Aware Generation Module (AIM) for synthesizing vivid talking avatars. Uses motion-to-video aligner for parallel co-generation of motions and videos.

Result: Establishes GroundedInter benchmark for GHOI video generation evaluation. Extensive experiments demonstrate effectiveness in generating grounded human-object interactions for talking avatars.

Conclusion: InteractAvatar successfully addresses GHOI challenges by decoupling perception/planning from video synthesis, enabling parallel motion and video generation for realistic talking avatars performing object interactions.

Abstract: Generating talking avatars is a fundamental task in video generation. Although existing methods can generate full-body talking avatars with simple human motion, extending this task to grounded human-object interaction (GHOI) remains an open challenge, requiring the avatar to perform text-aligned interactions with surrounding objects. This challenge stems from the need for environmental perception and the control-quality dilemma in GHOI generation. To address this, we propose a novel dual-stream framework, InteractAvatar, which decouples perception and planning from video synthesis for grounded human-object interaction. Leveraging detection to enhance environmental perception, we introduce a Perception and Interaction Module (PIM) to generate text-aligned interaction motions. Additionally, an Audio-Interaction Aware Generation Module (AIM) is proposed to synthesize vivid talking avatars performing object interactions. With a specially designed motion-to-video aligner, PIM and AIM share a similar network structure and enable parallel co-generation of motions and plausible videos, effectively mitigating the control-quality dilemma. Finally, we establish a benchmark, GroundedInter, for evaluating GHOI video generation. Extensive experiments and comparisons demonstrate the effectiveness of our method in generating grounded human-object interactions for talking avatars. Project page: https://interactavatar.github.io

</details>


### [177] [FSCA-Net: Feature-Separated Cross-Attention Network for Robust Multi-Dataset Training](https://arxiv.org/abs/2602.01540)
*Yuehai Chen*

Main category: cs.CV

TL;DR: FSCA-Net: A unified framework for crowd counting that disentangles domain-invariant and domain-specific features using cross-attention fusion and mutual information optimization to prevent negative transfer and improve cross-dataset generalization.


<details>
  <summary>Details</summary>
Motivation: Current CNN- and Transformer-based crowd counting models suffer from performance degradation across diverse environments due to domain discrepancies. Joint training on multiple datasets causes negative transfer as shared and domain-specific representations become entangled, limiting generalization capabilities.

Method: Proposes FSCA-Net with: 1) Explicit disentanglement of features into domain-invariant and domain-specific components, 2) Cross-attention fusion module to model interactions between these components, 3) Mutual information optimization objective to maximize consistency among domain-invariant features and minimize redundancy among domain-specific ones.

Result: Extensive experiments on multiple crowd counting benchmarks demonstrate that FSCA-Net effectively mitigates negative transfer and achieves state-of-the-art cross-dataset generalization performance.

Conclusion: FSCA-Net provides a robust and scalable solution for real-world crowd analysis by enabling effective knowledge transfer while preserving dataset-specific discriminability through complementary shared-private representations.

Abstract: Crowd counting plays a vital role in public safety, traffic regulation, and smart city management. However, despite the impressive progress achieved by CNN- and Transformer-based models, their performance often deteriorates when applied across diverse environments due to severe domain discrepancies. Direct joint training on multiple datasets, which intuitively should enhance generalization, instead results in negative transfer, as shared and domain-specific representations become entangled. To address this challenge, we propose the Feature Separation and Cross-Attention Network FSCA-Net, a unified framework that explicitly disentangles feature representations into domain-invariant and domain-specific components. A novel cross-attention fusion module adaptively models interactions between these components, ensuring effective knowledge transfer while preserving dataset-specific discriminability. Furthermore, a mutual information optimization objective is introduced to maximize consistency among domain-invariant features and minimize redundancy among domain-specific ones, promoting complementary shared-private representations. Extensive experiments on multiple crowd counting benchmarks demonstrate that FSCA-Net effectively mitigates negative transfer and achieves state-of-the-art cross-dataset generalization, providing a robust and scalable solution for real-world crowd analysis.

</details>


### [178] [Toward Cognitive Supersensing in Multimodal Large Language Model](https://arxiv.org/abs/2602.01541)
*Boyi Li,Yifan Shen,Yuanzhe Liu,Yifan Xu,Jiateng Liu,Xinzhuo Li,Zhengyuan Li,Jingyuan Zhu,Yunhan Zhong,Fangzhou Lan,Jianguo Cao,James M. Rehg,Heng Ji,Ismini Lourentzou,Xu Cao*

Main category: cs.CV

TL;DR: Cognitive Supersensing trains MLLMs with visual imagery capabilities using a Latent Visual Imagery Prediction head and reinforcement learning, achieving state-of-the-art performance on cognitive VQA benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs excel at perceptual tasks but struggle with complex cognitive problems requiring visual memory and reasoning. They primarily use text-based Chain-of-Thought reasoning even when visual details are abstract, neglecting human-like visual reasoning mechanisms like the visuospatial sketchpad and visual imagery.

Method: Introduces Cognitive Supersensing training paradigm with: 1) Latent Visual Imagery Prediction (LVIP) head that learns sequences of visual cognitive latent embeddings aligned with answers, forming vision-based internal reasoning chains; 2) Reinforcement learning stage that optimizes text reasoning paths based on grounded visual latent representations.

Result: MLLMs trained with Cognitive Supersensing significantly outperform state-of-the-art baselines on CogSense-Bench (comprehensive VQA benchmark assessing five cognitive dimensions) and show superior generalization on out-of-domain mathematics and science VQA benchmarks.

Conclusion: Internal visual imagery is potentially key to bridging the gap between perceptual recognition and cognitive understanding in MLLMs. The approach demonstrates that human-like visual reasoning mechanisms can enhance complex problem-solving capabilities beyond current text-based reasoning approaches.

Abstract: Multimodal Large Language Models (MLLMs) have achieved remarkable success in open-vocabulary perceptual tasks, yet their ability to solve complex cognitive problems remains limited, especially when visual details are abstract and require visual memory. Current approaches primarily scale Chain-of-Thought (CoT) reasoning in the text space, even when language alone is insufficient for clear and structured reasoning, and largely neglect visual reasoning mechanisms analogous to the human visuospatial sketchpad and visual imagery. To mitigate this deficiency, we introduce Cognitive Supersensing, a novel training paradigm that endows MLLMs with human-like visual imagery capabilities by integrating a Latent Visual Imagery Prediction (LVIP) head that jointly learns sequences of visual cognitive latent embeddings and aligns them with the answer, thereby forming vision-based internal reasoning chains. We further introduce a reinforcement learning stage that optimizes text reasoning paths based on this grounded visual latent. To evaluate the cognitive capabilities of MLLMs, we present CogSense-Bench, a comprehensive visual question answering (VQA) benchmark assessing five cognitive dimensions. Extensive experiments demonstrate that MLLMs trained with Cognitive Supersensing significantly outperform state-of-the-art baselines on CogSense-Bench and exhibit superior generalization on out-of-domain mathematics and science VQA benchmarks, suggesting that internal visual imagery is potentially key to bridging the gap between perceptual recognition and cognitive understanding. We will open-source the CogSense-Bench and our model weights.

</details>


### [179] [Combined Flicker-banding and Moire Removal for Screen-Captured Images](https://arxiv.org/abs/2602.01559)
*Libo Zhu,Zihan Zhou,Zhiyi Zhou,Yiyang Qu,Weihang Zhang,Keyu Shi,Yifan Fu,Yulun Zhang*

Main category: cs.CV

TL;DR: CLEAR: A unified framework for joint removal of moiré patterns and flicker-banding in screen-captured images, featuring frequency-domain decomposition and trajectory alignment loss.


<details>
  <summary>Details</summary>
Motivation: Mobile-captured screen images suffer from severe degradations from coexisting moiré patterns and flicker-banding, but existing single-degradation methods fail to handle these compound artifacts effectively.

Method: Proposes CLEAR framework with: 1) Large-scale dataset of moiré and flicker-banding artifacts, 2) ISP-based flicker simulation pipeline for stable training, 3) Frequency-domain decomposition and re-composition module, 4) Trajectory alignment loss for compound artifact modeling.

Result: Extensive experiments show CLEAR consistently outperforms existing image restoration approaches across multiple evaluation metrics in complex real-world scenarios.

Conclusion: First systematic study on joint removal of moiré and flicker-banding, with CLEAR providing an effective unified solution for screen-captured image restoration.

Abstract: Capturing display screens with mobile devices has become increasingly common, yet the resulting images often suffer from severe degradations caused by the coexistence of moiré patterns and flicker-banding, leading to significant visual quality degradation. Due to the strong coupling of these two artifacts in real imaging processes, existing methods designed for single degradations fail to generalize to such compound scenarios. In this paper, we present the first systematic study on joint removal of moiré patterns and flicker-banding in screen-captured images, and propose a unified restoration framework, named CLEAR. To support this task, we construct a large-scale dataset containing both moiré patterns and flicker-banding, and introduce an ISP-based flicker simulation pipeline to stabilize model training and expand the degradation distribution. Furthermore, we design a frequency-domain decomposition and re-composition module together with a trajectory alignment loss to enhance the modeling of compound artifacts. Extensive experiments demonstrate that the proposed method consistently. outperforms existing image restoration approaches across multiple evaluation metrics, validating its effectiveness in complex real-world scenarios.

</details>


### [180] [Multimodal UNcommonsense: From Odd to Ordinary and Ordinary to Odd](https://arxiv.org/abs/2602.01561)
*Yejin Son,Saejin Kim,Dongjun Min,Younjae Yu*

Main category: cs.CV

TL;DR: MUN benchmark evaluates multimodal commonsense reasoning on surprising/unlikely scenarios, with R-ICL framework using retrieval-based in-context learning to improve performance by 8.3% over baselines.


<details>
  <summary>Details</summary>
Motivation: Commonsense reasoning in multimodal contexts remains challenging, especially for scenarios that deviate from typical visual or contextual expectations. Current models struggle with surprising outcomes and culturally diverse, non-prototypical scenarios.

Method: Proposes Multimodal UNcommonsense (MUN) benchmark with visual scenes paired with surprising outcomes. Introduces retrieval-based in-context learning (R-ICL) framework with Multimodal Ensemble Retriever (MER) to identify relevant exemplars even with discordant image-text pairs, transferring reasoning from larger to smaller models without training.

Result: R-ICL achieves average 8.3% improvement over baseline ICL methods, demonstrating effectiveness in low-frequency, atypical settings. The MER successfully identifies semantically relevant exemplars despite deliberate discordance.

Conclusion: MUN enables evaluation of visual-language models' robustness in real-world, culturally diverse, non-prototypical scenarios. R-ICL effectively transfers reasoning capabilities and opens new directions for improving model adaptability to unexpected situations.

Abstract: Commonsense reasoning in multimodal contexts remains a foundational challenge in artificial intelligence. We introduce Multimodal UNcommonsense(MUN), a benchmark designed to evaluate models' ability to handle scenarios that deviate from typical visual or contextual expectations. MUN pairs visual scenes with surprising or unlikely outcomes described in natural language, prompting models to either rationalize seemingly odd images using everyday logic or uncover unexpected interpretations in ordinary scenes. To support this task, we propose a retrieval-based in-context learning (R-ICL) framework that transfers reasoning capabilities from larger models to smaller ones without additional training. Leveraging a novel Multimodal Ensemble Retriever (MER), our method identifies semantically relevant exemplars even when image and text pairs are deliberately discordant. Experiments show an average improvement of 8.3% over baseline ICL methods, highlighting the effectiveness of R-ICL in low-frequency, atypical settings. MUN opens new directions for evaluating and improving visual-language models' robustness and adaptability in real-world, culturally diverse, and non-prototypical scenarios.

</details>


### [181] [One-Step Diffusion for Perceptual Image Compression](https://arxiv.org/abs/2602.01570)
*Yiwen Jia,Hao Wei,Yanhui Zhou,Chenyang Ge*

Main category: cs.CV

TL;DR: Single-step diffusion image compression method achieves 46× faster inference while maintaining comparable quality to multi-step approaches.


<details>
  <summary>Details</summary>
Motivation: Current diffusion-based image compression methods suffer from high inference latency and computational overhead due to requiring many denoising steps during decoding, hindering practical deployment.

Method: Proposes a diffusion-based compression method that requires only a single-step diffusion process. Introduces a discriminator operating on compact feature representations rather than raw pixels to enhance perceptual quality by better capturing high-level texture and structural details.

Result: Achieves comparable compression performance while offering 46× faster inference speed compared to recent diffusion-based approaches.

Conclusion: The proposed single-step diffusion compression method significantly reduces inference latency while maintaining perceptual quality, making diffusion-based compression more practical for real-world deployment.

Abstract: Diffusion-based image compression methods have achieved notable progress, delivering high perceptual quality at low bitrates. However, their practical deployment is hindered by significant inference latency and heavy computational overhead, primarily due to the large number of denoising steps required during decoding. To address this problem, we propose a diffusion-based image compression method that requires only a single-step diffusion process, significantly improving inference speed. To enhance the perceptual quality of reconstructed images, we introduce a discriminator that operates on compact feature representations instead of raw pixels, leveraging the fact that features better capture high-level texture and structural details. Experimental results show that our method delivers comparable compression performance while offering a 46$\times$ faster inference speed compared to recent diffusion-based approaches. The source code and models are available at https://github.com/cheesejiang/OSDiff.

</details>


### [182] [SGHA-Attack: Semantic-Guided Hierarchical Alignment for Transferable Targeted Attacks on Vision-Language Models](https://arxiv.org/abs/2602.01574)
*Haobo Wang,Weiqi Luo,Xiaojun Jia,Xiaochun Cao*

Main category: cs.CV

TL;DR: SGHA-Attack improves targeted adversarial transferability across VLMs using semantic-guided hierarchical alignment with multiple target references and intermediate-layer consistency.


<details>
  <summary>Details</summary>
Motivation: Existing targeted transfer attacks overfit to surrogate models by relying on single references and focusing only on final-layer alignment, which limits transferability across heterogeneous vision-language models and underutilizes intermediate semantic information.

Method: Proposes SGHA-Attack framework that: 1) Creates a visually grounded reference pool using a frozen text-to-image model conditioned on target prompts, 2) Selects Top-K most semantically relevant anchors under surrogate for weighted mixture guidance, 3) Aligns intermediate visual representations at global and spatial granularities across multiple depths, 4) Synchronizes intermediate visual and textual features in shared latent subspace for early cross-modal supervision.

Result: Extensive experiments on open-source and commercial black-box VLMs show SGHA-Attack achieves stronger targeted transferability than prior methods and remains robust under preprocessing and purification defenses.

Conclusion: SGHA-Attack effectively addresses limitations of prior targeted transfer attacks by leveraging multiple semantic references and hierarchical alignment, significantly improving adversarial transferability across diverse vision-language models while maintaining robustness against defenses.

Abstract: Large vision-language models (VLMs) are vulnerable to transfer-based adversarial perturbations, enabling attackers to optimize on surrogate models and manipulate black-box VLM outputs. Prior targeted transfer attacks often overfit surrogate-specific embedding space by relying on a single reference and emphasizing final-layer alignment, which underutilizes intermediate semantics and degrades transfer across heterogeneous VLMs. To address this, we propose SGHA-Attack, a Semantic-Guided Hierarchical Alignment framework that adopts multiple target references and enforces intermediate-layer consistency. Concretely, we generate a visually grounded reference pool by sampling a frozen text-to-image model conditioned on the target prompt, and then carefully select the Top-K most semantically relevant anchors under the surrogate to form a weighted mixture for stable optimization guidance. Building on these anchors, SGHA-Attack injects target semantics throughout the feature hierarchy by aligning intermediate visual representations at both global and spatial granularities across multiple depths, and by synchronizing intermediate visual and textual features in a shared latent subspace to provide early cross-modal supervision before the final projection. Extensive experiments on open-source and commercial black-box VLMs show that SGHA-Attack achieves stronger targeted transferability than prior methods and remains robust under preprocessing and purification defenses.

</details>


### [183] [HandMCM: Multi-modal Point Cloud-based Correspondence State Space Model for 3D Hand Pose Estimation](https://arxiv.org/abs/2602.01586)
*Wencan Cheng,Gim Hee Lee*

Main category: cs.CV

TL;DR: HandMCM: A novel 3D hand pose estimation method using state space models (Mamba) with local information injection/filtering and correspondence modeling to handle occlusions, achieving SOTA performance.


<details>
  <summary>Details</summary>
Motivation: 3D hand pose estimation is crucial for human-computer interaction applications like AR, but faces challenges from self-occlusion and object interactions that cause severe occlusions.

Method: Proposes HandMCM based on state space models (Mamba) with modules for local information injection/filtering and correspondence modeling to learn dynamic kinematic topology across occlusion scenarios, plus multi-modal image feature integration.

Result: Significantly outperforms current state-of-the-art methods on three benchmark datasets, especially in challenging scenarios with severe occlusions.

Conclusion: The approach advances accuracy and reliability of 3D hand pose estimation in practical applications, demonstrating the potential of state space models for handling occlusion challenges.

Abstract: 3D hand pose estimation that involves accurate estimation of 3D human hand keypoint locations is crucial for many human-computer interaction applications such as augmented reality. However, this task poses significant challenges due to self-occlusion of the hands and occlusions caused by interactions with objects. In this paper, we propose HandMCM to address these challenges. Our HandMCM is a novel method based on the powerful state space model (Mamba). By incorporating modules for local information injection/filtering and correspondence modeling, the proposed correspondence Mamba effectively learns the highly dynamic kinematic topology of keypoints across various occlusion scenarios. Moreover, by integrating multi-modal image features, we enhance the robustness and representational capacity of the input, leading to more accurate hand pose estimation. Empirical evaluations on three benchmark datasets demonstrate that our model significantly outperforms current state-of-the-art methods, particularly in challenging scenarios involving severe occlusions. These results highlight the potential of our approach to advance the accuracy and reliability of 3D hand pose estimation in practical applications.

</details>


### [184] [Know Your Step: Faster and Better Alignment for Flow Matching Models via Step-aware Advantages](https://arxiv.org/abs/2602.01591)
*Zhixiong Yue,Zixuan Ni,Feiyang Ye,Jinshan Zhang,Sheng Shen,Zhenpeng Mi*

Main category: cs.CV

TL;DR: TAFS-GRPO is a novel RL framework that improves human preference alignment in few-step text-to-image flow matching models through temperature annealing and group relative policy optimization.


<details>
  <summary>Details</summary>
Motivation: Existing RL approaches for flow matching models suffer from sparse/imprecise reward signals and require many denoising steps, leading to suboptimal human preference alignment in few-step text-to-image generation.

Method: Proposes TAFS-GRPO with two key components: 1) Temperature Annealed Few-step Sampling - iteratively injects adaptive temporal noise onto one-step samples to introduce stochasticity while preserving semantic integrity; 2) Step-aware advantage integration with Group Relative Policy Optimization (GRPO) to provide dense, step-specific rewards without requiring differentiable reward functions.

Result: Extensive experiments show TAFS-GRPO achieves strong performance in few-step text-to-image generation and significantly improves alignment of generated images with human preferences.

Conclusion: TAFS-GRPO effectively addresses limitations of existing RL approaches for flow matching models, enabling efficient few-step generation with better human preference alignment. Code and models will be released to facilitate further research.

Abstract: Recent advances in flow matching models, particularly with reinforcement learning (RL), have significantly enhanced human preference alignment in few step text to image generators. However, existing RL based approaches for flow matching models typically rely on numerous denoising steps, while suffering from sparse and imprecise reward signals that often lead to suboptimal alignment. To address these limitations, we propose Temperature Annealed Few step Sampling with Group Relative Policy Optimization (TAFS GRPO), a novel framework for training flow matching text to image models into efficient few step generators well aligned with human preferences. Our method iteratively injects adaptive temporal noise onto the results of one step samples. By repeatedly annealing the model's sampled outputs, it introduces stochasticity into the sampling process while preserving the semantic integrity of each generated image. Moreover, its step aware advantage integration mechanism combines the GRPO to avoid the need for the differentiable of reward function and provide dense and step specific rewards for stable policy optimization. Extensive experiments demonstrate that TAFS GRPO achieves strong performance in few step text to image generation and significantly improves the alignment of generated images with human preferences. The code and models of this work will be available to facilitate further research.

</details>


### [185] [Samba+: General and Accurate Salient Object Detection via A More Unified Mamba-based Framework](https://arxiv.org/abs/2602.01593)
*Wenzhuo Zhao,Keren Fu,Jiahao He,Xiaohong Liu,Qijun Zhao,Guangtao Zhai*

Main category: cs.CV

TL;DR: Samba is a pure Mamba-based architecture for salient object detection that outperforms existing methods across six SOD tasks with lower computational cost, while Samba+ is a unified multi-task version that achieves even better results.


<details>
  <summary>Details</summary>
Motivation: Existing SOD models are limited by CNNs' restricted receptive fields and Transformers' high computational complexity. Mamba offers a promising balance between global receptive fields and computational efficiency.

Method: Proposes Saliency Mamba (Samba) with saliency-guided Mamba block using spatial neighborhood scanning to preserve spatial continuity, and context-aware upsampling for hierarchical feature alignment. Samba+ adds hub-and-spoke graph attention for cross-modal fusion and modality-anchored continual learning to prevent catastrophic forgetting.

Result: Samba outperforms existing methods across six SOD tasks on 22 datasets with lower computational cost. Samba+ achieves even superior results using a single trained versatile model.

Conclusion: The proposed Samba framework demonstrates strong potential for salient object detection, offering computational efficiency, versatility across multiple SOD tasks, and superior performance compared to existing approaches.

Abstract: Existing salient object detection (SOD) models are generally constrained by the limited receptive fields of convolutional neural networks (CNNs) and quadratic computational complexity of Transformers. Recently, the emerging state-space model, namely Mamba, has shown great potential in balancing global receptive fields and computational efficiency. As a solution, we propose Saliency Mamba (Samba), a pure Mamba-based architecture that flexibly handles various distinct SOD tasks, including RGB/RGB-D/RGB-T SOD, video SOD (VSOD), RGB-D VSOD, and visible-depth-thermal SOD. Specifically, we rethink the scanning strategy of Mamba for SOD, and introduce a saliency-guided Mamba block (SGMB) that features a spatial neighborhood scanning (SNS) algorithm to preserve the spatial continuity of salient regions. A context-aware upsampling (CAU) method is also proposed to promote hierarchical feature alignment and aggregation by modeling contextual dependencies. As one step further, to avoid the "task-specific" problem as in previous SOD solutions, we develop Samba+, which is empowered by training Samba in a multi-task joint manner, leading to a more unified and versatile model. Two crucial components that collaboratively tackle challenges encountered in input of arbitrary modalities and continual adaptation are investigated. Specifically, a hub-and-spoke graph attention (HGA) module facilitates adaptive cross-modal interactive fusion, and a modality-anchored continual learning (MACL) strategy alleviates inter-modal conflicts together with catastrophic forgetting. Extensive experiments demonstrate that Samba individually outperforms existing methods across six SOD tasks on 22 datasets with lower computational cost, whereas Samba+ achieves even superior results on these tasks and datasets by using a single trained versatile model. Additional results further demonstrate the potential of our Samba framework.

</details>


### [186] [UV-M3TL: A Unified and Versatile Multimodal Multi-Task Learning Framework for Assistive Driving Perception](https://arxiv.org/abs/2602.01594)
*Wenzhuo Liu,Qiannan Guo,Zhen Wang,Wenshuo Wang,Lei Yang,Yicheng Qiao,Lening Wang,Zhiwei Li,Chen Lv,Shanghang Zhang,Junqiang Xi,Huaping Liu*

Main category: cs.CV

TL;DR: UV-M3TL framework for multimodal multi-task learning in ADAS that simultaneously handles driver behavior, emotion, vehicle behavior, and traffic context while mitigating negative transfer through dual-branch architecture and adaptive loss.


<details>
  <summary>Details</summary>
Motivation: ADAS need to understand human driver behavior while perceiving navigation context, but jointly learning heterogeneous tasks causes inter-task negative transfer that impairs system performance.

Method: Proposes UV-M3TL framework with two core components: 1) DB-SCME (dual-branch spatial channel multimodal embedding) to model task-shared and task-specific features, and 2) AFD-Loss (adaptive feature-decoupled multi-task loss) with adaptive weighting based on learning dynamics and feature decoupling constraints.

Result: Achieves state-of-the-art performance on AIDE dataset across all four tasks (driver behavior, emotion, vehicle behavior, traffic context). Also demonstrates strong performance on additional benchmarks (BDD100K, CityScapes, NYUD-v2, PASCAL-Context) with state-of-the-art results on most tasks.

Conclusion: UV-M3TL effectively mitigates inter-task negative transfer in multimodal multi-task learning for ADAS, achieving superior performance across diverse tasks and demonstrating versatility across multiple benchmarks.

Abstract: Advanced Driver Assistance Systems (ADAS) need to understand human driver behavior while perceiving their navigation context, but jointly learning these heterogeneous tasks would cause inter-task negative transfer and impair system performance. Here, we propose a Unified and Versatile Multimodal Multi-Task Learning (UV-M3TL) framework to simultaneously recognize driver behavior, driver emotion, vehicle behavior, and traffic context, while mitigating inter-task negative transfer. Our framework incorporates two core components: dual-branch spatial channel multimodal embedding (DB-SCME) and adaptive feature-decoupled multi-task loss (AFD-Loss). DB-SCME enhances cross-task knowledge transfer while mitigating task conflicts by employing a dual-branch structure to explicitly model salient task-shared and task-specific features. AFD-Loss improves the stability of joint optimization while guiding the model to learn diverse multi-task representations by introducing an adaptive weighting mechanism based on learning dynamics and feature decoupling constraints. We evaluate our method on the AIDE dataset, and the experimental results demonstrate that UV-M3TL achieves state-of-the-art performance across all four tasks. To further prove the versatility, we evaluate UV-M3TL on additional public multi-task perception benchmarks (BDD100K, CityScapes, NYUD-v2, and PASCAL-Context), where it consistently delivers strong performance across diverse task combinations, attaining state-of-the-art results on most tasks.

</details>


### [187] [Token Pruning for In-Context Generation in Diffusion Transformers](https://arxiv.org/abs/2602.01609)
*Junqing Lin,Xingyu Zheng,Pei Cheng,Bin Fu,Jingwei Sun,Guangzhong Sun*

Main category: cs.CV

TL;DR: ToPi is a training-free token pruning framework that speeds up in-context generation in Diffusion Transformers by selectively pruning reference context tokens while maintaining image quality.


<details>
  <summary>Details</summary>
Motivation: In-context generation in Diffusion Transformers creates computational bottlenecks due to increased sequence length from reference examples. Existing token reduction methods fail because they treat reference contexts and target latents uniformly, ignoring their different roles across spatial, temporal, and functional dimensions.

Method: ToPi uses offline calibration-driven sensitivity analysis to identify pivotal attention layers as a proxy for redundancy estimation. It then develops a novel influence metric to quantify each context token's contribution for selective pruning, combined with a temporal update strategy that adapts to the evolving diffusion trajectory.

Result: ToPi achieves over 30% speedup in inference while maintaining structural fidelity and visual consistency across complex image generation tasks.

Conclusion: The proposed ToPi framework effectively addresses the computational bottleneck in in-context generation for DiTs through targeted token pruning that respects the asymmetric roles of reference contexts and target latents.

Abstract: In-context generation significantly enhances Diffusion Transformers (DiTs) by enabling controllable image-to-image generation through reference examples. However, the resulting input concatenation drastically increases sequence length, creating a substantial computational bottleneck. Existing token reduction techniques, primarily tailored for text-to-image synthesis, fall short in this paradigm as they apply uniform reduction strategies, overlooking the inherent role asymmetry between reference contexts and target latents across spatial, temporal, and functional dimensions. To bridge this gap, we introduce ToPi, a training-free token pruning framework tailored for in-context generation in DiTs. Specifically, ToPi utilizes offline calibration-driven sensitivity analysis to identify pivotal attention layers, serving as a robust proxy for redundancy estimation. Leveraging these layers, we derive a novel influence metric to quantify the contribution of each context token for selective pruning, coupled with a temporal update strategy that adapts to the evolving diffusion trajectory. Empirical evaluations demonstrate that ToPi can achieve over 30\% speedup in inference while maintaining structural fidelity and visual consistency across complex image generation tasks.

</details>


### [188] [Omni-Judge: Can Omni-LLMs Serve as Human-Aligned Judges for Text-Conditioned Audio-Video Generation?](https://arxiv.org/abs/2602.01623)
*Susan Liang,Chao Huang,Filippos Bellos,Yolo Yunlong Tang,Qianxiang Shen,Jing Bi,Luchuan Song,Zeliang Zhang,Jason Corso,Chenliang Xu*

Main category: cs.CV

TL;DR: Omni-Judge uses omni-modal LLMs to evaluate text-to-video generation with audio, achieving human-aligned correlation on semantic tasks but limited on high-FPS perceptual metrics.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation methods for text-to-video generation with audio are inadequate - human evaluation is costly and traditional metrics focus on isolated modalities, struggle with complex prompts, and lack interpretability.

Method: Introduces Omni-Judge, which leverages omni-modal large language models that can process audio, video, and text simultaneously to evaluate tri-modal outputs across nine perceptual and alignment metrics.

Result: Omni-Judge achieves correlation comparable to traditional metrics and excels at semantically demanding tasks (audio-text alignment, video-text alignment, audio-video-text coherence), but underperforms on high-FPS perceptual metrics like video quality and audio-video synchronization due to limited temporal resolution.

Conclusion: Omni-LLMs show promise as unified evaluators for multi-modal generation with interpretable explanations, but have current limitations in temporal resolution for high-FPS perceptual tasks.

Abstract: State-of-the-art text-to-video generation models such as Sora 2 and Veo 3 can now produce high-fidelity videos with synchronized audio directly from a textual prompt, marking a new milestone in multi-modal generation. However, evaluating such tri-modal outputs remains an unsolved challenge. Human evaluation is reliable but costly and difficult to scale, while traditional automatic metrics, such as FVD, CLAP, and ViCLIP, focus on isolated modality pairs, struggle with complex prompts, and provide limited interpretability. Omni-modal large language models (omni-LLMs) present a promising alternative: they naturally process audio, video, and text, support rich reasoning, and offer interpretable chain-of-thought feedback. Driven by this, we introduce Omni-Judge, a study assessing whether omni-LLMs can serve as human-aligned judges for text-conditioned audio-video generation. Across nine perceptual and alignment metrics, Omni-Judge achieves correlation comparable to traditional metrics and excels on semantically demanding tasks such as audio-text alignment, video-text alignment, and audio-video-text coherence. It underperforms on high-FPS perceptual metrics, including video quality and audio-video synchronization, due to limited temporal resolution. Omni-Judge provides interpretable explanations that expose semantic or physical inconsistencies, enabling practical downstream uses such as feedback-based refinement. Our findings highlight both the potential and current limitations of omni-LLMs as unified evaluators for multi-modal generation.

</details>


### [189] [PISCES: Annotation-free Text-to-Video Post-Training via Optimal Transport-Aligned Rewards](https://arxiv.org/abs/2602.01624)
*Minh-Quan Le,Gaurav Mittal,Cheng Zhao,David Gu,Dimitris Samaras,Mei Chen*

Main category: cs.CV

TL;DR: PISCES introduces an annotation-free post-training method for text-to-video generation using Dual Optimal Transport-aligned Rewards to improve video quality and semantic alignment without human preference data.


<details>
  <summary>Details</summary>
Motivation: Current reward-based post-training methods for text-to-video generation either require large-scale human preference annotations or use misaligned embeddings from pre-trained vision-language models, leading to limited scalability and suboptimal supervision.

Method: PISCES uses a novel Dual Optimal Transport-aligned Rewards module with two components: (1) Distributional OT-aligned Quality Reward for visual quality and temporal coherence, and (2) Discrete Token-level OT-aligned Semantic Reward for spatio-temporal correspondence between text and video tokens.

Result: PISCES outperforms both annotation-based and annotation-free methods on VBench across Quality and Semantic scores for both short- and long-video generation, with human preference studies validating its effectiveness.

Conclusion: PISCES is the first annotation-free reward supervision method using Optimal Transport for generative post-training, offering compatibility with multiple optimization paradigms including direct backpropagation and reinforcement learning fine-tuning.

Abstract: Text-to-video (T2V) generation aims to synthesize videos with high visual quality and temporal consistency that are semantically aligned with input text. Reward-based post-training has emerged as a promising direction to improve the quality and semantic alignment of generated videos. However, recent methods either rely on large-scale human preference annotations or operate on misaligned embeddings from pre-trained vision-language models, leading to limited scalability or suboptimal supervision. We present $\texttt{PISCES}$, an annotation-free post-training algorithm that addresses these limitations via a novel Dual Optimal Transport (OT)-aligned Rewards module. To align reward signals with human judgment, $\texttt{PISCES}$ uses OT to bridge text and video embeddings at both distributional and discrete token levels, enabling reward supervision to fulfill two objectives: (i) a Distributional OT-aligned Quality Reward that captures overall visual quality and temporal coherence; and (ii) a Discrete Token-level OT-aligned Semantic Reward that enforces semantic, spatio-temporal correspondence between text and video tokens. To our knowledge, $\texttt{PISCES}$ is the first to improve annotation-free reward supervision in generative post-training through the lens of OT. Experiments on both short- and long-video generation show that $\texttt{PISCES}$ outperforms both annotation-based and annotation-free methods on VBench across Quality and Semantic scores, with human preference studies further validating its effectiveness. We show that the Dual OT-aligned Rewards module is compatible with multiple optimization paradigms, including direct backpropagation and reinforcement learning fine-tuning.

</details>


### [190] [Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks](https://arxiv.org/abs/2602.01630)
*Bohan Zeng,Kaixin Zhu,Daili Hua,Bozhou Li,Chengzhuo Tong,Yuran Wang,Xinyi Huang,Yifan Dai,Zixiang Zhang,Yifan Yang,Zhou Liu,Hao Liang,Xiaochen Ma,Ruichuan An,Tianyi Bai,Hongcheng Gao,Junbo Niu,Yang Shi,Xinlong Chen,Yue Ding,Minglei Shi,Kai Zeng,Yiwen Tang,Yuanxing Zhang,Pengfei Wan,Xintao Wang,Wentao Zhang*

Main category: cs.CV

TL;DR: Proposes a unified design specification for world models to address fragmented research approaches and establish systematic coherence for holistic world understanding.


<details>
  <summary>Details</summary>
Motivation: Current world model research is fragmented with task-specific approaches (visual prediction, 3D estimation, symbol grounding) that lack systematic coherence and unified frameworks, limiting holistic world understanding.

Method: Analyzes limitations of fragmented approaches and proposes a unified design specification that integrates interaction, perception, symbolic reasoning, and spatial representation as a normative framework.

Result: Provides a structured perspective and normative framework for world models that moves beyond loose collections of capabilities toward systematic coherence.

Conclusion: A robust world model should be a unified framework integrating multiple capabilities, providing guidance for future research toward more general, robust, and principled models of the world.

Abstract: World models have emerged as a critical frontier in AI research, aiming to enhance large models by infusing them with physical dynamics and world knowledge. The core objective is to enable agents to understand, predict, and interact with complex environments. However, current research landscape remains fragmented, with approaches predominantly focused on injecting world knowledge into isolated tasks, such as visual prediction, 3D estimation, or symbol grounding, rather than establishing a unified definition or framework. While these task-specific integrations yield performance gains, they often lack the systematic coherence required for holistic world understanding. In this paper, we analyze the limitations of such fragmented approaches and propose a unified design specification for world models. We suggest that a robust world model should not be a loose collection of capabilities but a normative framework that integrally incorporates interaction, perception, symbolic reasoning, and spatial representation. This work aims to provide a structured perspective to guide future research toward more general, robust, and principled models of the world.

</details>


### [191] [Federated Vision Transformer with Adaptive Focal Loss for Medical Image Classification](https://arxiv.org/abs/2602.01633)
*Xinyuan Zhao,Yihang Wu,Ahmad Chaddad,Tareef Daqqaq,Reem Kateb*

Main category: cs.CV

TL;DR: Proposes a federated learning framework with dynamic adaptive focal loss and client-aware aggregation to handle data heterogeneity and class imbalance in medical image classification.


<details>
  <summary>Details</summary>
Motivation: Deep learning models require large datasets but data privacy regulations restrict access to medical images. Federated learning enables global model aggregation without data exchange, but faces challenges with data heterogeneity and class imbalance across local clients.

Method: Uses dynamic adaptive focal loss (DAFL) with dynamic class imbalance coefficient that adjusts based on each client's sample distribution, and implements client-aware weighted aggregation strategy that adapts to data size and characteristics.

Result: Outperforms DenseNet121, ResNet50, ViT-S/16, ViT-L/32, FedCLIP, Swin Transformer, CoAtNet, and MixNet on three medical datasets (ISIC, Ocular Disease, RSNA-ICH) with accuracy improvements from 0.98% to 41.69%.

Conclusion: The proposed FL framework effectively addresses class imbalance and client heterogeneity in federated medical image classification, with ablation studies validating the effectiveness of both the loss function and aggregation strategy.

Abstract: While deep learning models like Vision Transformer (ViT) have achieved significant advances, they typically require large datasets. With data privacy regulations, access to many original datasets is restricted, especially medical images. Federated learning (FL) addresses this challenge by enabling global model aggregation without data exchange. However, the heterogeneity of the data and the class imbalance that exist in local clients pose challenges for the generalization of the model. This study proposes a FL framework leveraging a dynamic adaptive focal loss (DAFL) and a client-aware aggregation strategy for local training. Specifically, we design a dynamic class imbalance coefficient that adjusts based on each client's sample distribution and class data distribution, ensuring minority classes receive sufficient attention and preventing sparse data from being ignored. To address client heterogeneity, a weighted aggregation strategy is adopted, which adapts to data size and characteristics to better capture inter-client variations. The classification results on three public datasets (ISIC, Ocular Disease and RSNA-ICH) show that the proposed framework outperforms DenseNet121, ResNet50, ViT-S/16, ViT-L/32, FedCLIP, Swin Transformer, CoAtNet, and MixNet in most cases, with accuracy improvements ranging from 0.98\% to 41.69\%. Ablation studies on the imbalanced ISIC dataset validate the effectiveness of the proposed loss function and aggregation strategy compared to traditional loss functions and other FL approaches. The codes can be found at: https://github.com/AIPMLab/ViT-FLDAF.

</details>


### [192] [ReCALL: Recalibrating Capability Degradation for MLLM-based Composed Image Retrieval](https://arxiv.org/abs/2602.01639)
*Tianyu Yang,ChenWei He,Xiangzhao Hao,Tianyue Wang,Jiarui Guo,Haiyun Guo,Leigang Qu,Jinqiao Wang,Tat-Seng Chua*

Main category: cs.CV

TL;DR: ReCALL framework addresses capability degradation in adapting generative MLLMs to discriminative retrieval for composed image retrieval, using diagnose-generate-refine pipeline to recalibrate fine-grained reasoning.


<details>
  <summary>Details</summary>
Motivation: Adapting generative Multimodal Large Language Models (MLLMs) for composed image retrieval causes paradigm conflict leading to Capability Degradation - loss of native fine-grained reasoning after retrieval adaptation.

Method: Three-step pipeline: 1) Diagnose cognitive blind spots via self-guided informative instance mining, 2) Generate corrective instructions/triplets using CoT prompting and VQA-based consistency filtering, 3) Refine retriever through continual training with grouped contrastive scheme.

Result: Extensive experiments on CIRR and FashionIQ datasets show ReCALL consistently recalibrates degraded capabilities and achieves state-of-the-art performance.

Conclusion: ReCALL effectively addresses capability degradation in adapting generative MLLMs to discriminative retrieval, enabling better composed image retrieval by realigning discriminative embedding space with intrinsic compositional reasoning.

Abstract: Composed Image Retrieval (CIR) aims to retrieve target images based on a hybrid query comprising a reference image and a modification text. Early dual-tower Vision-Language Models (VLMs) struggle with cross-modality compositional reasoning required for this task. Recently, adapting generative Multimodal Large Language Models (MLLMs) for retrieval offers a promising direction. However, we identify that this adaptation strategy overlooks a fundamental issue: adapting a generative MLLM into a single-embedding discriminative retriever triggers a paradigm conflict, which leads to Capability Degradation - the deterioration of native fine-grained reasoning after retrieval adaptation. To address this challenge, we propose ReCALL (Recalibrating Capability Degradation), a model-agnostic framework that follows a diagnose-generate-refine pipeline: Firstly, we diagnose cognitive blind spots of the retriever via self-guided informative instance mining. Next, we generate corrective instructions and triplets by CoT prompting the foundation MLLM and conduct quality control with VQA-based consistency filtering. Finally, we refine the retriever through continual training on these triplets with a grouped contrastive scheme, thereby internalizing fine-grained visual-semantic distinctions and realigning the discriminative embedding space of retriever with intrinsic compositional reasoning within the MLLM. Extensive experiments on CIRR and FashionIQ show that ReCALL consistently recalibrates degraded capabilities and achieves state-of-the-art performance. Code will be released soon.

</details>


### [193] [Contribution-aware Token Compression for Efficient Video Understanding via Reinforcement Learning](https://arxiv.org/abs/2602.01649)
*Yinchao Ma,Qiang Zhou,Zhibin Wang,Xianing Chen,Hanqing Yang,Jun Song,Bo Zheng*

Main category: cs.CV

TL;DR: CaCoVID is a reinforcement learning-based video token compression method that selects tokens based on their actual contribution to correct predictions rather than attention scores, reducing computational overhead while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Current video LLMs suffer from computational inefficiency due to redundant video tokens. Existing compression methods rely on attention scores, but the correlation between attention scores and actual contribution to correct answers is unclear, leading to suboptimal compression.

Method: Proposes CaCoVID: 1) Reinforcement learning framework with policy network to select token combinations with greatest contribution to correct predictions, 2) Combinatorial policy optimization with online combination space sampling to reduce exploration space and accelerate convergence.

Result: Extensive experiments on diverse video understanding benchmarks demonstrate effectiveness of CaCoVID in reducing computational overhead while maintaining accuracy.

Conclusion: CaCoVID provides a novel contribution-aware approach to video token compression that actively discovers optimal compressed token combinations, offering practical deployment advantages for video LLMs.

Abstract: Video large language models have demonstrated remarkable capabilities in video understanding tasks. However, the redundancy of video tokens introduces significant computational overhead during inference, limiting their practical deployment. Many compression algorithms are proposed to prioritize retaining features with the highest attention scores to minimize perturbations in attention computations. However, the correlation between attention scores and their actual contribution to correct answers remains ambiguous. To address the above limitation, we propose a novel \textbf{C}ontribution-\textbf{a}ware token \textbf{Co}mpression algorithm for \textbf{VID}eo understanding (\textbf{CaCoVID}) that explicitly optimizes the token selection policy based on the contribution of tokens to correct predictions. First, we introduce a reinforcement learning-based framework that optimizes a policy network to select video token combinations with the greatest contribution to correct predictions. This paradigm shifts the focus from passive token preservation to active discovery of optimal compressed token combinations. Secondly, we propose a combinatorial policy optimization algorithm with online combination space sampling, which dramatically reduces the exploration space for video token combinations and accelerates the convergence speed of policy optimization. Extensive experiments on diverse video understanding benchmarks demonstrate the effectiveness of CaCoVID. Codes will be released.

</details>


### [194] [From Frames to Sequences: Temporally Consistent Human-Centric Dense Prediction](https://arxiv.org/abs/2602.01661)
*Xingyu Miao,Junting Dong,Qin Zhao,Yuhang Yang,Junhao Chen,Yang Long*

Main category: cs.CV

TL;DR: A unified ViT-based model for temporally consistent human-centric dense prediction using synthetic data with both frame-level and sequence-level supervision, achieving SOTA on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing human-centric dense prediction models suffer from temporal flickering under motion, occlusion, and lighting changes, and lack paired video supervision for multiple dense tasks.

Method: Scalable synthetic data pipeline generating photorealistic human frames with pixel-accurate labels; unified ViT-based predictor with CSE embeddings for human geometric prior and channel reweighting module; two-stage training with static pretraining followed by dynamic sequence supervision.

Result: Achieves state-of-the-art performance on THuman2.1 and Hi4D benchmarks and generalizes effectively to in-the-wild videos.

Conclusion: The proposed synthetic data pipeline and unified model with explicit geometric priors and temporal supervision effectively address temporal consistency in human-centric dense prediction tasks.

Abstract: In this work, we focus on the challenge of temporally consistent human-centric dense prediction across video sequences. Existing models achieve strong per-frame accuracy but often flicker under motion, occlusion, and lighting changes, and they rarely have paired human video supervision for multiple dense tasks. We address this gap with a scalable synthetic data pipeline that generates photorealistic human frames and motion-aligned sequences with pixel-accurate depth, normals, and masks. Unlike prior static data synthetic pipelines, our pipeline provides both frame-level labels for spatial learning and sequence-level supervision for temporal learning. Building on this, we train a unified ViT-based dense predictor that (i) injects an explicit human geometric prior via CSE embeddings and (ii) improves geometry-feature reliability with a lightweight channel reweighting module after feature fusion. Our two-stage training strategy, combining static pretraining with dynamic sequence supervision, enables the model first to acquire robust spatial representations and then refine temporal consistency across motion-aligned sequences. Extensive experiments show that we achieve state-of-the-art performance on THuman2.1 and Hi4D and generalize effectively to in-the-wild videos.

</details>


### [195] [Moonworks Lunara Aesthetic II: An Image Variation Dataset](https://arxiv.org/abs/2602.01666)
*Yan Wang,Partho Hassan,Samiha Sadeka,Nada Soliman,M M Sayeef Abdullah,Sabit Hassan*

Main category: cs.CV

TL;DR: Lunara Aesthetic II is a publicly released image dataset with 2,854 anchor-linked variation pairs for evaluating contextual consistency in image generation systems while preserving identity and aesthetics.


<details>
  <summary>Details</summary>
Motivation: To support controlled evaluation and learning of contextual consistency in modern image generation and editing systems, addressing the need for interpretable supervision signals for identity-preserving contextual variation.

Method: Created 2,854 anchor-linked variation pairs from original art and photographs by Moonworks, applying contextual transformations (illumination, weather, viewpoint, scene composition, color tone, mood) while preserving underlying identity.

Result: Dataset shows high identity stability, strong target attribute realization, and robust aesthetic profile that exceeds large-scale web datasets, while being ethically sourced and publicly available under Apache 2.0 license.

Conclusion: Lunara Aesthetic II provides a valuable resource for benchmarking, fine-tuning, and analysis of contextual generalization, identity preservation, and edit robustness in image generation systems with interpretable supervision.

Abstract: We introduce Lunara Aesthetic II, a publicly released, ethically sourced image dataset designed to support controlled evaluation and learning of contextual consistency in modern image generation and editing systems. The dataset comprises 2,854 anchor-linked variation pairs derived from original art and photographs created by Moonworks. Each variation pair applies contextual transformations, such as illumination, weather, viewpoint, scene composition, color tone, or mood; while preserving a stable underlying identity. Lunara Aesthetic II operationalizes identity-preserving contextual variation as a supervision signal while also retaining Lunara's signature high aesthetic scores. Results show high identity stability, strong target attribute realization, and a robust aesthetic profile that exceeds large-scale web datasets. Released under the Apache 2.0 license, Lunara Aesthetic II is intended for benchmarking, fine-tuning, and analysis of contextual generalization, identity preservation, and edit robustness in image generation and image-to-image systems with interpretable, relational supervision. The dataset is publicly available at: https://huggingface.co/datasets/moonworks/lunara-aesthetic-image-variations.

</details>


### [196] [VRGaussianAvatar: Integrating 3D Gaussian Avatars into VR](https://arxiv.org/abs/2602.01674)
*Hail Song,Boram Yoon,Seokhwan Yang,Seoyoung Kang,Hyunjeong Kim,Henning Metzmacher,Woontack Woo*

Main category: cs.CV

TL;DR: VRGaussianAvatar enables real-time full-body 3D Gaussian Splatting avatars in VR using only HMD tracking, achieving interactive performance with higher perceived quality than mesh-based baselines.


<details>
  <summary>Details</summary>
Motivation: Current VR avatar systems often require complex motion capture setups or produce low-quality results. There's a need for high-quality, real-time full-body avatars using only standard HMD tracking signals.

Method: Uses a parallel pipeline with VR Frontend (inverse kinematics for full-body pose estimation) and GA Backend (3DGS avatar rendering). Introduces Binocular Batching to efficiently process stereo views in a single pass for high-resolution VR displays.

Result: System sustains interactive VR performance (real-time operation) and outperforms image- and video-based mesh avatar baselines in user studies, achieving higher perceived appearance similarity, embodiment, and plausibility.

Conclusion: VRGaussianAvatar successfully demonstrates real-time full-body 3DGS avatars in VR using only HMD tracking, offering improved visual quality and user experience compared to existing mesh-based approaches.

Abstract: We present VRGaussianAvatar, an integrated system that enables real-time full-body 3D Gaussian Splatting (3DGS) avatars in virtual reality using only head-mounted display (HMD) tracking signals. The system adopts a parallel pipeline with a VR Frontend and a GA Backend. The VR Frontend uses inverse kinematics to estimate full-body pose and streams the resulting pose along with stereo camera parameters to the backend. The GA Backend stereoscopically renders a 3DGS avatar reconstructed from a single image. To improve stereo rendering efficiency, we introduce Binocular Batching, which jointly processes left and right eye views in a single batched pass to reduce redundant computation and support high-resolution VR displays. We evaluate VRGaussianAvatar with quantitative performance tests and a within-subject user study against image- and video-based mesh avatar baselines. Results show that VRGaussianAvatar sustains interactive VR performance and yields higher perceived appearance similarity, embodiment, and plausibility. Project page and source code are available at https://vrgaussianavatar.github.io.

</details>


### [197] [SMTrack: State-Aware Mamba for Efficient Temporal Modeling in Visual Tracking](https://arxiv.org/abs/2602.01677)
*Yinchao Ma,Dengqing Yang,Zhangyu He,Wenfei Yang,Tianzhu Zhang*

Main category: cs.CV

TL;DR: SMTrack is a novel visual tracking method using state space models (Mamba) for efficient long-range temporal modeling without complex custom modules or high computational costs.


<details>
  <summary>Details</summary>
Motivation: Existing CNN and Transformer architectures struggle with long-range temporal dependencies in visual tracking, requiring complex custom modules or high computational costs to integrate temporal cues effectively.

Method: Proposes State-aware Mamba Tracker (SMTrack) with a selective state-aware space model using state-wise parameters to capture diverse temporal cues, enabling linear computational complexity for long-range temporal interactions during training and efficient hidden state propagation during tracking.

Result: Extensive experiments show SMTrack achieves promising tracking performance with low computational costs compared to existing methods.

Conclusion: SMTrack provides an effective and efficient temporal modeling paradigm for visual tracking that overcomes limitations of conventional architectures while maintaining low computational overhead.

Abstract: Visual tracking aims to automatically estimate the state of a target object in a video sequence, which is challenging especially in dynamic scenarios. Thus, numerous methods are proposed to introduce temporal cues to enhance tracking robustness. However, conventional CNN and Transformer architectures exhibit inherent limitations in modeling long-range temporal dependencies in visual tracking, often necessitating either complex customized modules or substantial computational costs to integrate temporal cues. Inspired by the success of the state space model, we propose a novel temporal modeling paradigm for visual tracking, termed State-aware Mamba Tracker (SMTrack), providing a neat pipeline for training and tracking without needing customized modules or substantial computational costs to build long-range temporal dependencies. It enjoys several merits. First, we propose a novel selective state-aware space model with state-wise parameters to capture more diverse temporal cues for robust tracking. Second, SMTrack facilitates long-range temporal interactions with linear computational complexity during training. Third, SMTrack enables each frame to interact with previously tracked frames via hidden state propagation and updating, which releases computational costs of handling temporal cues during tracking. Extensive experimental results demonstrate that SMTrack achieves promising performance with low computational costs.

</details>


### [198] [FreshMem: Brain-Inspired Frequency-Space Hybrid Memory for Streaming Video Understanding](https://arxiv.org/abs/2602.01683)
*Kangcong Li,Peng Ye,Lin Zhang,Chao Wang,Huafeng Qin,Tao Chen*

Main category: cs.CV

TL;DR: FreshMem is a training-free memory network for streaming video understanding that combines frequency-space hybrid memory to preserve both short-term fidelity and long-term coherence, significantly boosting MLLM performance on streaming video benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing MLLM methods for streaming video understanding lack flexible adaptivity, causing irreversible detail loss and context fragmentation when transitioning from offline to online continuous perception.

Method: FreshMem uses a Frequency-Space Hybrid Memory network with two modules: Multi-scale Frequency Memory (MFM) projects overflowing frames into frequency coefficients with residual details for global historical "gist"; Space Thumbnail Memory (STM) discretizes continuous streams into episodic clusters using adaptive compression to create high-density space thumbnails.

Result: FreshMem significantly boosts Qwen2-VL baseline with gains of 5.20% on StreamingBench, 4.52% on OV-Bench, and 2.34% on OVO-Bench, outperforming several fully fine-tuned methods as a training-free solution.

Conclusion: FreshMem offers a highly efficient paradigm for long-horizon streaming video understanding by reconciling short-term fidelity with long-term coherence through brain-inspired memory mechanisms, without requiring training.

Abstract: Transitioning Multimodal Large Language Models (MLLMs) from offline to online streaming video understanding is essential for continuous perception. However, existing methods lack flexible adaptivity, leading to irreversible detail loss and context fragmentation. To resolve this, we propose FreshMem, a Frequency-Space Hybrid Memory network inspired by the brain's logarithmic perception and memory consolidation. FreshMem reconciles short-term fidelity with long-term coherence through two synergistic modules: Multi-scale Frequency Memory (MFM), which projects overflowing frames into representative frequency coefficients, complemented by residual details to reconstruct a global historical "gist"; and Space Thumbnail Memory (STM), which discretizes the continuous stream into episodic clusters by employing an adaptive compression strategy to distill them into high-density space thumbnails. Extensive experiments show that FreshMem significantly boosts the Qwen2-VL baseline, yielding gains of 5.20%, 4.52%, and 2.34% on StreamingBench, OV-Bench, and OVO-Bench, respectively. As a training-free solution, FreshMem outperforms several fully fine-tuned methods, offering a highly efficient paradigm for long-horizon streaming video understanding.

</details>


### [199] [Cross-Modal Alignment and Fusion for RGB-D Transmission-Line Defect Detection](https://arxiv.org/abs/2602.01696)
*Jiaming Cui,Shuai Zhou,Wenqiang Li,Ruifeng Qin,Feng Shen*

Main category: cs.CV

TL;DR: CMAFNet integrates RGB and depth data for transmission line defect detection, using cross-modal alignment and fusion to handle small defects in complex backgrounds, achieving state-of-the-art performance with efficient variants.


<details>
  <summary>Details</summary>
Motivation: Transmission line defect detection is challenging due to small-scale defects (94.5% of instances are small objects), complex backgrounds, and illumination variations. RGB-only detectors struggle with geometrically subtle defects that have limited chromatic contrast with visually similar background structures.

Method: CMAFNet (Cross-Modal Alignment and Fusion Network) uses a purify-then-fuse paradigm: 1) Semantic Recomposition Module with dictionary-based feature purification via learned codebook to suppress modality-specific noise while preserving defect-discriminative information, 2) Contextual Semantic Integration Framework with partial-channel attention to capture global spatial dependencies for structural semantic reasoning, and 3) Position-wise normalization for explicit reconstruction-driven cross-modal alignment ensuring statistical compatibility between RGB and depth features.

Result: On TLRGBD benchmark: achieves 32.2% mAP@50 and 12.5% APs, outperforming strongest baseline by 9.8 and 4.0 percentage points respectively. Lightweight variant reaches 24.8% mAP50 at 228 FPS with only 4.9M parameters, surpassing all YOLO-based detectors while matching transformer-based methods at substantially lower computational cost.

Conclusion: CMAFNet effectively integrates RGB appearance and depth geometry through principled cross-modal alignment and fusion, significantly improving small defect detection in complex transmission line inspection scenarios while offering efficient variants suitable for real-time UAV applications.

Abstract: Transmission line defect detection remains challenging for automated UAV inspection due to the dominance of small-scale defects, complex backgrounds, and illumination variations. Existing RGB-based detectors, despite recent progress, struggle to distinguish geometrically subtle defects from visually similar background structures under limited chromatic contrast. This paper proposes CMAFNet, a Cross-Modal Alignment and Fusion Network that integrates RGB appearance and depth geometry through a principled purify-then-fuse paradigm. CMAFNet consists of a Semantic Recomposition Module that performs dictionary-based feature purification via a learned codebook to suppress modality-specific noise while preserving defect-discriminative information, and a Contextual Semantic Integration Framework that captures global spatial dependencies using partial-channel attention to enhance structural semantic reasoning. Position-wise normalization within the purification stage enforces explicit reconstruction-driven cross-modal alignment, ensuring statistical compatibility between heterogeneous features prior to fusion. Extensive experiments on the TLRGBD benchmark, where 94.5% of instances are small objects, demonstrate that CMAFNet achieves 32.2% mAP@50 and 12.5% APs, outperforming the strongest baseline by 9.8 and 4.0 percentage points, respectively. A lightweight variant reaches 24.8% mAP50 at 228 FPS with only 4.9M parameters, surpassing all YOLO-based detectors while matching transformer-based methods at substantially lower computational cost.

</details>


### [200] [Physics Informed Generative AI Enabling Labour Free Segmentation For Microscopy Analysis](https://arxiv.org/abs/2602.01710)
*Salma Zahran,Zhou Ao,Zhengyang Zhang,Chen Chi,Chenchen Yuan,Yanming Wang*

Main category: cs.CV

TL;DR: Framework using phase-field simulations and CycleGAN to generate realistic SEM images for training segmentation models without manual annotation, achieving high performance on experimental data.


<details>
  <summary>Details</summary>
Motivation: Manual annotation of microscopy images is expensive, subjective, and scarce, while physics-based simulations create a domain gap that prevents generalization to real experimental data.

Method: Phase-field simulations generate microstructural morphologies with perfect ground-truth masks, then CycleGAN performs unpaired image translation to create realistic SEM images from simulations, training a U-Net exclusively on synthetic data.

Result: U-Net trained on synthetic data achieved mean Boundary F1-Score of 0.90 and IOU of 0.88 on unseen experimental images, with synthetic images statistically indistinguishable from real data.

Conclusion: The generative framework transforms data-scarce segmentation problems into data-abundant ones, providing fully automated solutions for materials discovery without manual annotation.

Abstract: Semantic segmentation of microscopy images is a critical task for high-throughput materials characterisation, yet its automation is severely constrained by the prohibitive cost, subjectivity, and scarcity of expert-annotated data. While physics-based simulations offer a scalable alternative to manual labelling, models trained on such data historically fail to generalise due to a significant domain gap, lacking the complex textures, noise patterns, and imaging artefacts inherent to experimental data. This paper introduces a novel framework for labour-free segmentation that successfully bridges this simulation-to-reality gap. Our pipeline leverages phase-field simulations to generate an abundant source of microstructural morphologies with perfect, intrinsically-derived ground-truth masks. We then employ a Cycle-Consistent Generative Adversarial Network (CycleGAN) for unpaired image-to-image translation, transforming the clean simulations into a large-scale dataset of high-fidelity, realistic SEM images. A U-Net model, trained exclusively on this synthetic data, demonstrated remarkable generalisation when deployed on unseen experimental images, achieving a mean Boundary F1-Score of 0.90 and an Intersection over Union (IOU) of 0.88. Comprehensive validation using t-SNE feature-space projection and Shannon entropy analysis confirms that our synthetic images are statistically and featurally indistinguishable from the real data manifold. By completely decoupling model training from manual annotation, our generative framework transforms a data-scarce problem into one of data abundance, providing a robust and fully automated solution to accelerate materials discovery and analysis.

</details>


### [201] [FastPhysGS: Accelerating Physics-based Dynamic 3DGS Simulation via Interior Completion and Adaptive Optimization](https://arxiv.org/abs/2602.01723)
*Yikun Ma,Yiqing Li,Jingwen Ye,Zhongkai Wu,Weidong Zhang,Lin Gao,Zhi Jin*

Main category: cs.CV

TL;DR: FastPhysGS enables fast physics-based 4D Gaussian Splatting simulation using instance-aware particle filling and bidirectional graph optimization, achieving high-fidelity results in 1 minute with 7GB memory.


<details>
  <summary>Details</summary>
Motivation: Extending 3D Gaussian Splatting to 4D physical simulation is challenging due to manual parameter tuning, distillation from video diffusion models, and perceptual gaps in LLM/VLM approaches that ignore surface structure and yield unstable physics.

Method: Two key components: (1) Instance-aware Particle Filling with Monte Carlo Importance Sampling to efficiently populate interior particles while preserving geometry; (2) Bidirectional Graph Decoupling Optimization, an adaptive strategy that rapidly optimizes material parameters predicted from a VLM.

Result: Achieves high-fidelity physical simulation in 1 minute using only 7 GB runtime memory, outperforming prior works with broad potential applications.

Conclusion: FastPhysGS provides a fast and robust framework for physics-based dynamic 3DGS simulation that addresses limitations of existing methods through efficient particle filling and adaptive optimization strategies.

Abstract: Extending 3D Gaussian Splatting (3DGS) to 4D physical simulation remains challenging. Based on the Material Point Method (MPM), existing methods either rely on manual parameter tuning or distill dynamics from video diffusion models, limiting the generalization and optimization efficiency. Recent attempts using LLMs/VLMs suffer from a text/image-to-3D perceptual gap, yielding unstable physics behavior. In addition, they often ignore the surface structure of 3DGS, leading to implausible motion. We propose FastPhysGS, a fast and robust framework for physics-based dynamic 3DGS simulation:(1) Instance-aware Particle Filling (IPF) with Monte Carlo Importance Sampling (MCIS) to efficiently populate interior particles while preserving geometric fidelity; (2) Bidirectional Graph Decoupling Optimization (BGDO), an adaptive strategy that rapidly optimizes material parameters predicted from a VLM. Experiments show FastPhysGS achieves high-fidelity physical simulation in 1 minute using only 7 GB runtime memory, outperforming prior works with broad potential applications.

</details>


### [202] [DenVisCoM: Dense Vision Correspondence Mamba for Efficient and Real-time Optical Flow and Stereo Estimation](https://arxiv.org/abs/2602.01724)
*Tushar Anand,Maheswar Bora,Antitza Dantcheva,Abhijit Das*

Main category: cs.CV

TL;DR: DenVisCoM: A novel hybrid Mamba-Transformer architecture for real-time joint optical flow and disparity estimation


<details>
  <summary>Details</summary>
Motivation: Optical flow and disparity estimation are fundamentally related multi-view geometry and motion tasks that should be tackled jointly for efficiency and accuracy

Method: Proposes DenVisCoM (Mamba block) combined with Transformer-based attention blocks in a hybrid architecture for efficient joint estimation

Result: Achieves accurate real-time estimation of both optical flow and disparity with good trade-off between accuracy, speed, and memory footprint

Conclusion: The proposed unified hybrid architecture successfully addresses real-time inference, memory efficiency, and accuracy for joint motion and 3D perception tasks

Abstract: In this work, we propose a novel Mamba block DenVisCoM, as well as a novel hybrid architecture specifically tailored for accurate and real-time estimation of optical flow and disparity estimation. Given that such multi-view geometry and motion tasks are fundamentally related, we propose a unified architecture to tackle them jointly. Specifically, the proposed hybrid architecture is based on DenVisCoM and a Transformer-based attention block that efficiently addresses real-time inference, memory footprint, and accuracy at the same time for joint estimation of motion and 3D dense perception tasks. We extensively analyze the benchmark trade-off of accuracy and real-time processing on a large number of datasets. Our experimental results and related analysis suggest that our proposed model can accurately estimate optical flow and disparity estimation in real time. All models and associated code are available at https://github.com/vimstereo/DenVisCoM.

</details>


### [203] [Simplicity Prevails: The Emergence of Generalizable AIGI Detection in Visual Foundation Models](https://arxiv.org/abs/2602.01738)
*Yue Zhou,Xinan He,Kaiqing Lin,Bing Fan,Feng Ding,Bin Li*

Main category: cs.CV

TL;DR: Simple linear classifier on frozen Vision Foundation Model features outperforms complex AIGI detectors in real-world scenarios by 30+%, leveraging models' exposure to synthetic data during pre-training.


<details>
  <summary>Details</summary>
Motivation: Specialized AI-Generated Image detectors perform well on curated benchmarks but fail dramatically in real-world scenarios, revealing a need for more robust detection methods that work in-the-wild.

Method: Train a simple linear classifier on frozen features from modern Vision Foundation Models (Perception Encoder, MetaCLIP 2, DINOv3), avoiding complex architectural designs and leveraging the models' pre-existing knowledge.

Result: The simple baseline matches specialized detectors on standard benchmarks and outperforms them by over 30% on in-the-wild datasets, establishing new state-of-the-art performance through comprehensive evaluation across traditional benchmarks, unseen generators, and challenging distributions.

Conclusion: Foundation models' exposure to synthetic data during massive pre-training gives them emergent detection capabilities (explicit semantic understanding in VLMs, implicit forensic features in SSL models), but limitations remain with recapture, transmission, VAE reconstruction, and localized editing. The field needs a paradigm shift from overfitting on static benchmarks to leveraging foundation models' evolving world knowledge for real-world reliability.

Abstract: While specialized detectors for AI-Generated Images (AIGI) achieve near-perfect accuracy on curated benchmarks, they suffer from a dramatic performance collapse in realistic, in-the-wild scenarios. In this work, we demonstrate that simplicity prevails over complex architectural designs. A simple linear classifier trained on the frozen features of modern Vision Foundation Models , including Perception Encoder, MetaCLIP 2, and DINOv3, establishes a new state-of-the-art. Through a comprehensive evaluation spanning traditional benchmarks, unseen generators, and challenging in-the-wild distributions, we show that this baseline not only matches specialized detectors on standard benchmarks but also decisively outperforms them on in-the-wild datasets, boosting accuracy by striking margins of over 30\%. We posit that this superior capability is an emergent property driven by the massive scale of pre-training data containing synthetic content. We trace the source of this capability to two distinct manifestations of data exposure: Vision-Language Models internalize an explicit semantic concept of forgery, while Self-Supervised Learning models implicitly acquire discriminative forensic features from the pretraining data. However, we also reveal persistent limitations: these models suffer from performance degradation under recapture and transmission, remain blind to VAE reconstruction and localized editing. We conclude by advocating for a paradigm shift in AI forensics, moving from overfitting on static benchmarks to harnessing the evolving world knowledge of foundation models for real-world reliability.

</details>


### [204] [Tail-Aware Post-Training Quantization for 3D Geometry Models](https://arxiv.org/abs/2602.01741)
*Sicheng Pan,Chen Tang,Shuzhao Xie,Ke Yang,Weixiang Zhang,Jiawei Li,Bin Chen,Shu-Tao Xia,Zhi Wang*

Main category: cs.CV

TL;DR: TAPTQ is a Tail-Aware Post-Training Quantization pipeline for 3D geometric learning that addresses challenges in quantizing 3D models through progressive calibration, efficient ternary search, and TRE-guided error compensation.


<details>
  <summary>Details</summary>
Motivation: 3D geometry models are complex and resource-intensive, making deployment on constrained platforms challenging. Existing PTQ methods optimized for 2D Vision Transformers fail for 3D models due to intricate feature distributions and high calibration overhead.

Method: Three key innovations: (1) Progressive coarse-to-fine calibration construction for compact, representative subsets from limited 3D data; (2) Ternary-search-based solver reformulating quantization interval search to reduce complexity from O(N) to O(log N); (3) TRE-Guided Module-wise Compensation using Tail Relative Error metric to identify and correct distortions from activation outliers.

Result: Extensive experiments on VGGT and Pi3 benchmarks show TAPTQ consistently outperforms state-of-the-art PTQ methods in accuracy while significantly reducing calibration time.

Conclusion: TAPTQ provides an effective PTQ solution for 3D geometric learning that addresses data limitations, computational complexity, and error accumulation challenges, enabling efficient deployment of 3D models on resource-constrained platforms.

Abstract: The burgeoning complexity and scale of 3D geometry models pose significant challenges for deployment on resource-constrained platforms. While Post-Training Quantization (PTQ) enables efficient inference without retraining, conventional methods, primarily optimized for 2D Vision Transformers, fail to transfer effectively to 3D models due to intricate feature distributions and prohibitive calibration overhead. To address these challenges, we propose TAPTQ, a Tail-Aware Post-Training Quantization pipeline specifically engineered for 3D geometric learning. Our contribution is threefold: (1) To overcome the data-scale bottleneck in 3D datasets, we develop a progressive coarse-to-fine calibration construction strategy that constructs a highly compact subset to achieve both statistical purity and geometric representativeness. (2) We reformulate the quantization interval search as an optimization problem and introduce a ternary-search-based solver, reducing the computational complexity from $\mathcal{O}(N)$ to $\mathcal{O}(\log N)$ for accelerated deployment. (3) To mitigate quantization error accumulation, we propose TRE-Guided Module-wise Compensation, which utilizes a Tail Relative Error (TRE) metric to adaptively identify and rectify distortions in modules sensitive to long-tailed activation outliers. Extensive experiments on the VGGT and Pi3 benchmarks demonstrate that TAPTQ consistently outperforms state-of-the-art PTQ methods in accuracy while significantly reducing calibration time. The code will be released soon.

</details>


### [205] [ObjEmbed: Towards Universal Multimodal Object Embeddings](https://arxiv.org/abs/2602.01753)
*Shenghao Fu,Yukun Su,Fengyun Rao,Jing Lyu,Xiaohua Xie,Wei-Shi Zheng*

Main category: cs.CV

TL;DR: ObjEmbed is a multimodal embedding model that decomposes images into object-level embeddings for fine-grained vision-language alignment, supporting both region-level and image-level tasks with high efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal embedding models excel at global image-text alignment but struggle with fine-grained alignment between specific image regions and textual phrases, creating a need for better object-level vision-language understanding.

Method: ObjEmbed decomposes images into multiple regional embeddings (one per object) plus global embeddings. For each region, it generates two complementary embeddings: an object embedding for semantic matching and an IoU embedding that predicts localization quality. The final matching score combines semantic similarity with predicted IoU.

Result: Superior performance on 18 diverse benchmarks demonstrates strong semantic discrimination capabilities across visual grounding, local image retrieval, and global image retrieval tasks.

Conclusion: ObjEmbed provides an effective solution for fine-grained vision-language alignment with object-oriented representations, versatility across task types, and efficient single-pass encoding, addressing limitations of existing multimodal embedding models.

Abstract: Aligning objects with corresponding textual descriptions is a fundamental challenge and a realistic requirement in vision-language understanding. While recent multimodal embedding models excel at global image-text alignment, they often struggle with fine-grained alignment between image regions and specific phrases. In this work, we present ObjEmbed, a novel MLLM embedding model that decomposes the input image into multiple regional embeddings, each corresponding to an individual object, along with global embeddings. It supports a wide range of visual understanding tasks like visual grounding, local image retrieval, and global image retrieval. ObjEmbed enjoys three key properties: (1) Object-Oriented Representation: It captures both semantic and spatial aspects of objects by generating two complementary embeddings for each region: an object embedding for semantic matching and an IoU embedding that predicts localization quality. The final object matching score combines semantic similarity with the predicted IoU, enabling more accurate retrieval. (2) Versatility: It seamlessly handles both region-level and image-level tasks. (3) Efficient Encoding: All objects in an image, along with the full image, are encoded in a single forward pass for high efficiency. Superior performance on 18 diverse benchmarks demonstrates its strong semantic discrimination.

</details>


### [206] [Spot-Wise Smart Parking: An Edge-Enabled Architecture with YOLOv11 and Digital Twin Integration](https://arxiv.org/abs/2602.01754)
*Gustavo P. C. P. da Luz,Alvaro M. Aspilcueta Narvaez,Tiago Godoi Bannwart,Gabriel Massuyoshi Sato,Luis Fernando Gomez Gonzalez,Juliana Freitag Borin*

Main category: cs.CV

TL;DR: Extends campus parking monitoring system from region-level to spot-level detection using distance-aware matching with spatial tolerance and adaptive bounding box partitioning, achieving 98.8% accuracy on edge devices while adding Digital Shadow and application server components.


<details>
  <summary>Details</summary>
Motivation: Previous region-based parking monitoring system lacked spot-level insights and couldn't support advanced applications. Need to move from area-level vehicle counting to individual spot monitoring for more detailed parking information.

Method: Spot-wise monitoring strategy using distance-aware matching method with spatial tolerance, enhanced with Adaptive Bounding Box Partitioning for challenging spaces. Uses YOLOv11m model (40.5MB) on edge devices. Adds Digital Shadow for visual representation and application support server on repurposed TV box.

Result: Achieves 98.80% balanced accuracy with 8-second inference time on resource-constrained edge device. System enables detailed spot occupancy statistics and scalable communication between cloud services, parking totem, and bot.

Conclusion: Successfully extends parking monitoring to spot-level while maintaining high accuracy on edge hardware. Introduces sustainable approach with hardware reuse and lays foundation for full Digital Twin evolution.

Abstract: Smart parking systems help reduce congestion and minimize users' search time, thereby contributing to smart city adoption and enhancing urban mobility. In previous works, we presented a system developed on a university campus to monitor parking availability by estimating the number of free spaces from vehicle counts within a region of interest. Although this approach achieved good accuracy, it restricted the system's ability to provide spot-level insights and support more advanced applications. To overcome this limitation, we extend the system with a spot-wise monitoring strategy based on a distance-aware matching method with spatial tolerance, enhanced through an Adaptive Bounding Box Partitioning method for challenging spaces. The proposed approach achieves a balanced accuracy of 98.80% while maintaining an inference time of 8 seconds on a resource-constrained edge device, enhancing the capabilities of YOLOv11m, a model that has a size of 40.5 MB. In addition, two new components were introduced: (i) a Digital Shadow that visually represents parking lot entities as a base to evolve to a full Digital Twin, and (ii) an application support server based on a repurposed TV box. The latter not only enables scalable communication among cloud services, the parking totem, and a bot that provides detailed spot occupancy statistics, but also promotes hardware reuse as a step towards greater sustainability.

</details>


### [207] [Mind-Brush: Integrating Agentic Cognitive Search and Reasoning into Image Generation](https://arxiv.org/abs/2602.01756)
*Jun He,Junyan Ye,Zilong Huang,Dongzhi Jiang,Chenjue Zhang,Leqi Zhu,Renrui Zhang,Xiang Zhang,Weijia Li*

Main category: cs.CV

TL;DR: Mind-Brush is an agentic framework that transforms text-to-image generation into a dynamic, knowledge-driven workflow using a 'think-research-create' paradigm to handle complex reasoning and real-world dynamics.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-image models fail to grasp implicit user intentions and struggle with complex knowledge reasoning. They're constrained by static internal priors and can't adapt to evolving real-world dynamics.

Method: Mind-Brush is a unified agentic framework that simulates human-like 'think-research-create' paradigm. It actively retrieves multimodal evidence to ground out-of-distribution concepts and employs reasoning tools to resolve implicit visual constraints.

Result: Mind-Brush significantly enhances unified models' capabilities, achieving a zero-to-one capability leap for Qwen-Image baseline on Mind-Bench, while achieving superior results on established benchmarks like WISE and RISE.

Conclusion: The framework successfully bridges gaps in current text-to-image generation by making it dynamic and knowledge-driven, enabling better handling of complex reasoning and real-world adaptation.

Abstract: While text-to-image generation has achieved unprecedented fidelity, the vast majority of existing models function fundamentally as static text-to-pixel decoders. Consequently, they often fail to grasp implicit user intentions. Although emerging unified understanding-generation models have improved intent comprehension, they still struggle to accomplish tasks involving complex knowledge reasoning within a single model. Moreover, constrained by static internal priors, these models remain unable to adapt to the evolving dynamics of the real world. To bridge these gaps, we introduce Mind-Brush, a unified agentic framework that transforms generation into a dynamic, knowledge-driven workflow. Simulating a human-like 'think-research-create' paradigm, Mind-Brush actively retrieves multimodal evidence to ground out-of-distribution concepts and employs reasoning tools to resolve implicit visual constraints. To rigorously evaluate these capabilities, we propose Mind-Bench, a comprehensive benchmark comprising 500 distinct samples spanning real-time news, emerging concepts, and domains such as mathematical and Geo-Reasoning. Extensive experiments demonstrate that Mind-Brush significantly enhances the capabilities of unified models, realizing a zero-to-one capability leap for the Qwen-Image baseline on Mind-Bench, while achieving superior results on established benchmarks like WISE and RISE.

</details>


### [208] [MagicFuse: Single Image Fusion for Visual and Semantic Reinforcement](https://arxiv.org/abs/2602.01760)
*Hao Zhang,Yanping Zha,Zizhuo Li,Meiqi Gong,Jiayi Ma*

Main category: cs.CV

TL;DR: MagicFuse enables multi-modal fusion-like performance using only a single degraded visible image by extending fusion to knowledge level through diffusion models.


<details>
  <summary>Details</summary>
Motivation: Address the practical problem of maintaining multi-modal image fusion advantages under harsh conditions where only visible imaging sensors are available, extending conventional data-level fusion to knowledge level.

Method: Proposes MagicFuse framework with three diffusion model branches: intra-spectral knowledge reinforcement, cross-spectral knowledge generation, and multi-domain knowledge fusion that integrates probabilistic noise from both branches to obtain cross-spectral scene representation.

Result: Achieves visual and semantic representation performance comparable to or better than state-of-the-art fusion methods with multi-modal inputs, despite using only a single degraded visible image.

Conclusion: Demonstrates successful extension of image fusion from data level to knowledge level, enabling comprehensive cross-spectral scene representation from single visible images through diffusion-based knowledge mining and transfer.

Abstract: This paper focuses on a highly practical scenario: how to continue benefiting from the advantages of multi-modal image fusion under harsh conditions when only visible imaging sensors are available. To achieve this goal, we propose a novel concept of single-image fusion, which extends conventional data-level fusion to the knowledge level. Specifically, we develop MagicFuse, a novel single image fusion framework capable of deriving a comprehensive cross-spectral scene representation from a single low-quality visible image. MagicFuse first introduces an intra-spectral knowledge reinforcement branch and a cross-spectral knowledge generation branch based on the diffusion models. They mine scene information obscured in the visible spectrum and learn thermal radiation distribution patterns transferred to the infrared spectrum, respectively. Building on them, we design a multi-domain knowledge fusion branch that integrates the probabilistic noise from the diffusion streams of these two branches, from which a cross-spectral scene representation can be obtained through successive sampling. Then, we impose both visual and semantic constraints to ensure that this scene representation can satisfy human observation while supporting downstream semantic decision-making. Extensive experiments show that our MagicFuse achieves visual and semantic representation performance comparable to or even better than state-of-the-art fusion methods with multi-modal inputs, despite relying solely on a single degraded visible image.

</details>


### [209] [GDPR-Compliant Person Recognition in Industrial Environments Using MEMS-LiDAR and Hybrid Data](https://arxiv.org/abs/2602.01764)
*Dennis Basile,Dennis Sprute,Helene Dörksen,Holger Flatt*

Main category: cs.CV

TL;DR: Privacy-compliant person detection using MEMS-LiDAR with hybrid real/synthetic data improves precision by 44% while reducing annotation effort by 50%.


<details>
  <summary>Details</summary>
Motivation: Need for reliable unauthorized person detection in industrial spaces while addressing privacy concerns (GDPR compliance) and reducing time-consuming manual data annotation.

Method: Uses MEMS-LiDAR for anonymized 3D point clouds, combines real recordings with synthetic scenes from CARLA simulation framework to create hybrid training data.

Result: Hybrid data improves average precision by 44 percentage points compared to real-data-only models, while reducing manual annotation effort by 50%.

Conclusion: Proposed approach provides scalable, cost-efficient alternative that combines high-performance person detection with GDPR compliance in industrial environments.

Abstract: The reliable detection of unauthorized individuals in safety-critical industrial indoor spaces is crucial to avoid plant shutdowns, property damage, and personal hazards. Conventional vision-based methods that use deep-learning approaches for person recognition provide image information but are sensitive to lighting and visibility conditions and often violate privacy regulations, such as the General Data Protection Regulation (GDPR) in the European Union. Typically, detection systems based on deep learning require annotated data for training. Collecting and annotating such data, however, is highly time-consuming and due to manual treatments not necessarily error free. Therefore, this paper presents a privacy-compliant approach based on Micro-Electro-Mechanical Systems LiDAR (MEMS-LiDAR), which exclusively captures anonymized 3D point clouds and avoids personal identification features. To compensate for the large amount of time required to record real LiDAR data and for post-processing and annotation, real recordings are augmented with synthetically generated scenes from the CARLA simulation framework. The results demonstrate that the hybrid data improves the average precision by 44 percentage points compared to a model trained exclusively with real data while reducing the manual annotation effort by 50 %. Thus, the proposed approach provides a scalable, cost-efficient alternative to purely real-data-based methods and systematically shows how synthetic LiDAR data can combine high performance in person detection with GDPR compliance in an industrial environment.

</details>


### [210] [Automated Discontinuity Set Characterisation in Enclosed Rock Face Point Clouds Using Single-Shot Filtering and Cyclic Orientation Transformation](https://arxiv.org/abs/2602.01783)
*Dibyayan Patra,Pasindu Ranasinghe,Bikram Banerjee,Simit Raval*

Main category: cs.CV

TL;DR: New method for automatic discontinuity set characterization in underground mine rock faces using single-shot filtering, cyclic orientation transformation, and hierarchical clustering.


<details>
  <summary>Details</summary>
Motivation: Current methods lack robustness for automatic characterization of discontinuity sets in real-world underground mine cavities, which is essential for rock-mass stability assessment and excavation safety.

Method: Three-step approach: 1) Single-shot filtering to isolate planar regions while suppressing noise, 2) Cyclic orientation transformation to represent dip angle/direction in Cartesian space, 3) Hierarchical clustering to characterize sets without requiring user-defined cluster numbers.

Result: Method outperforms existing techniques with lowest mean absolute error: 1.95° in dip angle and 2.20° in dip direction, with dispersion errors below 3° on real-world mine stope data.

Conclusion: The proposed approach provides an accurate, robust, and efficient solution for automatic discontinuity set characterization in underground mine environments, addressing limitations of existing methods.

Abstract: Characterisation of structural discontinuity sets in exposed rock faces of underground mine cavities is essential for assessing rock-mass stability, excavation safety, and operational efficiency. UAV and other mobile laser-scanning techniques provide efficient means of collecting point clouds from rock faces. However, the development of a robust and efficient approach for automatic characterisation of discontinuity sets in real-world scenarios, like fully enclosed rock faces in cavities, remains an open research problem. In this study, a new approach is proposed for automatic discontinuity set characterisation that uses a single-shot filtering strategy, an innovative cyclic orientation transformation scheme and a hierarchical clustering technique. The single-shot filtering step isolates planar regions while robustly suppressing noise and high-curvature artefacts in one pass using a signal-processing technique. To address the limitations of Cartesian clustering on polar orientation data, a cyclic orientation transformation scheme is developed, enabling accurate representation of dip angle and dip direction in Cartesian space. The transformed orientations are then characterised into sets using a hierarchical clustering technique, which handles varying density distributions and identifies clusters without requiring user-defined set numbers. The accuracy of the method is validated on real-world mine stope and against ground truth obtained using manually handpicked discontinuity planes identified with the Virtual Compass tool, as well as widely used automated structure mapping techniques. The proposed approach outperforms the other techniques by exhibiting the lowest mean absolute error in estimating discontinuity set orientations in real-world stope data with errors of 1.95° and 2.20° in nominal dip angle and dip direction, respectively, and dispersion errors lying below 3°.

</details>


### [211] [Spatio-Temporal Transformers for Long-Term NDVI Forecasting](https://arxiv.org/abs/2602.01799)
*Ido Faran,Nathan S. Netanyahu,Maxim Shoshany*

Main category: cs.CV

TL;DR: STT-LTF is a transformer-based framework that integrates spatial context modeling with temporal sequence prediction for long-term satellite image forecasting in heterogeneous Mediterranean landscapes, achieving superior performance over existing methods.


<details>
  <summary>Details</summary>
Motivation: Long-term satellite image analysis in heterogeneous Mediterranean landscapes is challenging due to complex spatial patterns, seasonal variations, and multi-decade environmental changes interacting across different scales. Existing approaches struggle with these complexities.

Method: STT-LTF processes multi-scale spatial patches alongside temporal sequences (up to 20 years) through a unified transformer architecture. It uses self-supervised learning with spatial masking, temporal masking, and horizon sampling strategies, incorporating spatial patch embeddings, cyclical temporal encoding, and geographic coordinates.

Result: On Landsat data (1984-2024), STT-LTF achieves MAE of 0.0328 and R² of 0.8412 for next-year predictions, outperforming traditional statistical methods, CNN-based approaches, LSTM networks, and standard transformers.

Conclusion: STT-LTF effectively handles irregular temporal sampling and variable prediction horizons, making it particularly suitable for analyzing heterogeneous landscapes experiencing rapid ecological transitions, advancing beyond purely temporal analysis to integrate spatial context modeling.

Abstract: Long-term satellite image time series (SITS) analysis in heterogeneous landscapes faces significant challenges, particularly in Mediterranean regions where complex spatial patterns, seasonal variations, and multi-decade environmental changes interact across different scales. This paper presents the Spatio-Temporal Transformer for Long Term Forecasting (STT-LTF ), an extended framework that advances beyond purely temporal analysis to integrate spatial context modeling with temporal sequence prediction. STT-LTF processes multi-scale spatial patches alongside temporal sequences (up to 20 years) through a unified transformer architecture, capturing both local neighborhood relationships and regional climate influences. The framework employs comprehensive self-supervised learning with spatial masking, temporal masking, and horizon sampling strategies, enabling robust model training from 40 years of unlabeled Landsat imagery. Unlike autoregressive approaches, STT-LTF directly predicts arbitrary future time points without error accumulation, incorporating spatial patch embeddings, cyclical temporal encoding, and geographic coordinates to learn complex dependencies across heterogeneous Mediterranean ecosystems. Experimental evaluation on Landsat data (1984-2024) demonstrates that STT-LTF achieves a Mean Absolute Error (MAE) of 0.0328 and R^2 of 0.8412 for next-year predictions, outperforming traditional statistical methods, CNN-based approaches, LSTM networks, and standard transformers. The framework's ability to handle irregular temporal sampling and variable prediction horizons makes it particularly suitable for analysis of heterogeneous landscapes experiencing rapid ecological transitions.

</details>


### [212] [Fast Autoregressive Video Diffusion and World Models with Temporal Cache Compression and Sparse Attention](https://arxiv.org/abs/2602.01801)
*Dvir Samuel,Issar Tzachor,Matan Levy,Micahel Green,Gal Chechik,Rami Ben-Ari*

Main category: cs.CV

TL;DR: Training-free attention optimization for autoregressive video diffusion models that reduces KV cache growth and accelerates attention via temporal compression and ANN-based token selection, achieving 5-10x speedups with stable memory usage.


<details>
  <summary>Details</summary>
Motivation: Autoregressive video diffusion models suffer from growing KV cache during inference, causing increasing latency, escalating GPU memory usage, restricted temporal context, and harmed long-range consistency - making long-form video generation inefficient.

Method: Three training-free attention modules: 1) TempCache compresses KV cache via temporal correspondence to bound cache growth; 2) AnnCA accelerates cross-attention by selecting frame-relevant prompt tokens using fast approximate nearest neighbor matching; 3) AnnSA sparsifies self-attention by restricting queries to semantically matched keys using lightweight ANN.

Result: Achieves 5-10x end-to-end speedups while preserving near-identical visual quality, maintaining stable throughput and nearly constant peak GPU memory usage over long rollouts, unlike prior methods that progressively slow down with increasing memory usage.

Conclusion: The proposed attention optimization framework effectively addresses redundancy in autoregressive video diffusion, enabling efficient long-form synthesis with consistent performance and memory usage, compatible with existing autoregressive diffusion backbones.

Abstract: Autoregressive video diffusion models enable streaming generation, opening the door to long-form synthesis, video world models, and interactive neural game engines. However, their core attention layers become a major bottleneck at inference time: as generation progresses, the KV cache grows, causing both increasing latency and escalating GPU memory, which in turn restricts usable temporal context and harms long-range consistency. In this work, we study redundancy in autoregressive video diffusion and identify three persistent sources: near-duplicate cached keys across frames, slowly evolving (largely semantic) queries/keys that make many attention computations redundant, and cross-attention over long prompts where only a small subset of tokens matters per frame. Building on these observations, we propose a unified, training-free attention framework for autoregressive diffusion: TempCache compresses the KV cache via temporal correspondence to bound cache growth; AnnCA accelerates cross-attention by selecting frame-relevant prompt tokens using fast approximate nearest neighbor (ANN) matching; and AnnSA sparsifies self-attention by restricting each query to semantically matched keys, also using a lightweight ANN. Together, these modules reduce attention, compute, and memory and are compatible with existing autoregressive diffusion backbones and world models. Experiments demonstrate up to x5--x10 end-to-end speedups while preserving near-identical visual quality and, crucially, maintaining stable throughput and nearly constant peak GPU memory usage over long rollouts, where prior methods progressively slow down and suffer from increasing memory usage.

</details>


### [213] [FlowBypass: Rectified Flow Trajectory Bypass for Training-Free Image Editing](https://arxiv.org/abs/2602.01805)
*Menglin Han,Zhangkai Ni*

Main category: cs.CV

TL;DR: FlowBypass: A training-free image editing framework using Rectified Flow to create bypass trajectories between inversion and reconstruction, reducing error accumulation without feature manipulation.


<details>
  <summary>Details</summary>
Motivation: Existing training-free image editing methods face a trade-off: longer inversion-reconstruction trajectories accumulate errors and reduce fidelity, while shorter ones fail to align properly with edit prompts. Current solutions rely on backbone-specific feature manipulations, limiting general applicability.

Method: Proposes FlowBypass, an analytical framework based on Rectified Flow that constructs a bypass directly connecting inversion and reconstruction trajectories. Provides formal derivation of two trajectories, obtains approximate bypass formulation and numerical solution for seamless trajectory transitions.

Result: Extensive experiments show FlowBypass consistently outperforms state-of-the-art image editing methods, achieving stronger prompt alignment while preserving high-fidelity details in irrelevant regions.

Conclusion: FlowBypass effectively addresses the error accumulation problem in training-free image editing by creating bypass trajectories, offering a general solution that doesn't rely on feature manipulations and maintains both prompt alignment and image fidelity.

Abstract: Training-free image editing has attracted increasing attention for its efficiency and independence from training data. However, existing approaches predominantly rely on inversion-reconstruction trajectories, which impose an inherent trade-off: longer trajectories accumulate errors and compromise fidelity, while shorter ones fail to ensure sufficient alignment with the edit prompt. Previous attempts to address this issue typically employ backbone-specific feature manipulations, limiting general applicability. To address these challenges, we propose FlowBypass, a novel and analytical framework grounded in Rectified Flow that constructs a bypass directly connecting inversion and reconstruction trajectories, thereby mitigating error accumulation without relying on feature manipulations. We provide a formal derivation of two trajectories, from which we obtain an approximate bypass formulation and its numerical solution, enabling seamless trajectory transitions. Extensive experiments demonstrate that FlowBypass consistently outperforms state-of-the-art image editing methods, achieving stronger prompt alignment while preserving high-fidelity details in irrelevant regions.

</details>


### [214] [LDRNet: Large Deformation Registration Model for Chest CT Registration](https://arxiv.org/abs/2602.01812)
*Cheng Wang,Qiyu Gao,Fandong Zhang,Shu Zhang,Yizhou Yu*

Main category: cs.CV

TL;DR: LDRNet: Fast unsupervised deep learning method for large deformation chest CT registration using coarse-to-fine refinement with innovative refine and rigid blocks.


<details>
  <summary>Details</summary>
Motivation: Chest CT registration presents greater challenges than brain registration due to larger deformations, more complex backgrounds, and region overlap, requiring specialized methods.

Method: LDRNet uses coarse-to-fine registration: first predicts coarse resolution registration field, then refines it. Features two innovative blocks: refine block for multi-resolution field refinement and rigid block for learning transformation matrix from high-level features.

Result: Achieves state-of-the-art performance for large deformation image registration on private dataset and public SegTHOR dataset, outperforming traditional methods and deep learning models (VoxelMorph, RCN, LapIRN) while being much faster.

Conclusion: LDRNet effectively addresses the unique challenges of chest CT registration through its coarse-to-fine architecture with specialized refine and rigid blocks, demonstrating superior performance and speed for large deformation medical image registration.

Abstract: Most of the deep learning based medical image registration algorithms focus on brain image registration tasks.Compared with brain registration, the chest CT registration has larger deformation, more complex background and region over-lap. In this paper, we propose a fast unsupervised deep learning method, LDRNet, for large deformation image registration of chest CT images. We first predict a coarse resolution registration field, then refine it from coarse to fine. We propose two innovative technical components: 1) a refine block that is used to refine the registration field in different resolutions, 2) a rigid block that is used to learn transformation matrix from high-level features. We train and evaluate our model on the private dataset and public dataset SegTHOR. We compare our performance with state-of-the-art traditional registration methods as well as deep learning registration models VoxelMorph, RCN, and LapIRN. The results demonstrate that our model achieves state-of-the-art performance for large deformation images registration and is much faster.

</details>


### [215] [GPD: Guided Progressive Distillation for Fast and High-Quality Video Generation](https://arxiv.org/abs/2602.01814)
*Xiao Liang,Yunzhu Zhang,Linchao Zhu*

Main category: cs.CV

TL;DR: GPD accelerates video diffusion models by reducing sampling steps from 48 to 6 while maintaining quality through progressive teacher-student distillation with frequency-domain constraints.


<details>
  <summary>Details</summary>
Motivation: Diffusion models for video generation suffer from high computational costs due to many denoising steps, and existing acceleration methods cause significant quality degradation in videos.

Method: Guided Progressive Distillation (GPD) uses a teacher model to progressively guide a student model with larger step sizes, featuring online-generated training targets and frequency-domain constraints in latent space to preserve details and temporal dynamics.

Result: Applied to Wan2.1 model, GPD reduces sampling steps from 48 to 6 while maintaining competitive visual quality on VBench, outperforming existing distillation methods in simplicity and quality preservation.

Conclusion: GPD provides an effective framework for accelerating video diffusion models with minimal quality loss, offering advantages in both pipeline simplicity and preservation of fine-grained details and temporal dynamics.

Abstract: Diffusion models have achieved remarkable success in video generation; however, the high computational cost of the denoising process remains a major bottleneck. Existing approaches have shown promise in reducing the number of diffusion steps, but they often suffer from significant quality degradation when applied to video generation. We propose Guided Progressive Distillation (GPD), a framework that accelerates the diffusion process for fast and high-quality video generation. GPD introduces a novel training strategy in which a teacher model progressively guides a student model to operate with larger step sizes. The framework consists of two key components: (1) an online-generated training target that reduces optimization difficulty while improving computational efficiency, and (2) frequency-domain constraints in the latent space that promote the preservation of fine-grained details and temporal dynamics. Applied to the Wan2.1 model, GPD reduces the number of sampling steps from 48 to 6 while maintaining competitive visual quality on VBench. Compared with existing distillation methods, GPD demonstrates clear advantages in both pipeline simplicity and quality preservation.

</details>


### [216] [Seeing Is Believing? A Benchmark for Multimodal Large Language Models on Visual Illusions and Anomalies](https://arxiv.org/abs/2602.01816)
*Wenjin Hou,Wei Liu,Han Hu,Xiaoxiao Sun,Serena Yeung-Levy,Hehe Fan*

Main category: cs.CV

TL;DR: VIA-Bench is a challenging benchmark testing MLLMs on visual illusions and anomalies, revealing significant vulnerabilities despite their strong performance on standard benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current MLLM evaluations focus on standard in-distribution data, leaving robustness to scenarios that defy common-sense priors largely unexamined. There's a need to test how models handle visual illusions and anomalies that challenge human perception.

Method: Created VIA-Bench with six categories of visual illusions/anomalies: color, motion, gestalt, geometric/spatial, general visual illusions, and visual anomalies. Used human-in-the-loop review to construct over 1K high-quality QA pairs requiring nuanced visual reasoning. Evaluated 20+ state-of-the-art MLLMs including proprietary, open-source, and reasoning-enhanced models.

Result: Extensive evaluation revealed significant vulnerabilities in MLLMs. Chain-of-Thought reasoning offered negligible robustness, often producing "brittle mirages" where model logic collapses under illusory stimuli. Models showed fundamental divergence from human perception.

Conclusion: Resolving perceptual bottlenecks revealed by visual illusions and anomalies is critical for advancing artificial general intelligence. The benchmark exposes a gap between machine and human perception that needs addressing.

Abstract: Multimodal Large Language Models (MLLMs) have shown remarkable proficiency on general-purpose vision-language benchmarks, reaching or even exceeding human-level performance. However, these evaluations typically rely on standard in-distribution data, leaving the robustness of MLLMs largely unexamined when faced with scenarios that defy common-sense priors. To address this gap, we introduce VIA-Bench, a challenging benchmark designed to probe model performance on visual illusions and anomalies. It includes six core categories: color illusions, motion illusions, gestalt illusions, geometric and spatial illusions, general visual illusions, and visual anomalies. Through careful human-in-the-loop review, we construct over 1K high-quality question-answer pairs that require nuanced visual reasoning. Extensive evaluation of over 20 state-of-the-art MLLMs, including proprietary, open-source, and reasoning-enhanced models, uncovers significant vulnerabilities. Notably, we find that Chain-of-Thought (CoT) reasoning offers negligible robustness, often yielding ``brittle mirages'' where the model's logic collapses under illusory stimuli. Our findings reveal a fundamental divergence between machine and human perception, suggesting that resolving such perceptual bottlenecks is critical for the advancement of artificial general intelligence. The benchmark data and code will be released.

</details>


### [217] [Efficient Cross-Country Data Acquisition Strategy for ADAS via Street-View Imagery](https://arxiv.org/abs/2602.01836)
*Yin Wu,Daniel Slieter,Carl Esselborn,Ahmed Abouelazm,Tsung Yuan Tseng,J. Marius Zöllner*

Main category: cs.CV

TL;DR: Street-view-guided data acquisition uses public imagery to identify representative locations for cross-country ADAS/ADS adaptation, achieving comparable performance to random sampling with half the data.


<details>
  <summary>Details</summary>
Motivation: Cross-country deployment of ADAS/ADS faces challenges due to domain shifts from legislative, infrastructure, and visual differences. Traditional data collection through extensive on-road driving is costly and inefficient for identifying representative locations.

Method: Proposes a street-view-guided data acquisition strategy using publicly available imagery to identify places of interest (POI). Introduces two POI scoring methods: 1) KNN-based feature distance using a vision foundation model, and 2) visual-attribution using a vision-language model. Uses collect-detect protocol and constructs co-located dataset pairing Zenseact Open Dataset with Mapillary street-view images.

Result: On traffic sign detection (sensitive to cross-country variations), the approach achieves performance comparable to random sampling while using only half of the target-domain data. Cost estimations show large-scale street-view processing remains economically feasible for full-country analysis.

Conclusion: Street-view-guided data acquisition offers an efficient and cost-effective solution for cross-country model adaptation, addressing domain shift challenges in ADAS/ADS deployment across different countries.

Abstract: Deploying ADAS and ADS across countries remains challenging due to differences in legislation, traffic infrastructure, and visual conventions, which introduce domain shifts that degrade perception performance. Traditional cross-country data collection relies on extensive on-road driving, making it costly and inefficient to identify representative locations. To address this, we propose a street-view-guided data acquisition strategy that leverages publicly available imagery to identify places of interest (POI). Two POI scoring methods are introduced: a KNN-based feature distance approach using a vision foundation model, and a visual-attribution approach using a vision-language model. To enable repeatable evaluation, we adopt a collect-detect protocol and construct a co-located dataset by pairing the Zenseact Open Dataset with Mapillary street-view images. Experiments on traffic sign detection, a task particularly sensitive to cross-country variations in sign appearance, show that our approach achieves performance comparable to random sampling while using only half of the target-domain data. We further provide cost estimations for full-country analysis, demonstrating that large-scale street-view processing remains economically feasible. These results highlight the potential of street-view-guided data acquisition for efficient and cost-effective cross-country model adaptation.

</details>


### [218] [CloDS: Visual-Only Unsupervised Cloth Dynamics Learning in Unknown Conditions](https://arxiv.org/abs/2602.01844)
*Yuliang Zhan,Jian Li,Wenbing Huang,Wenbing Huang,Yang Liu,Hao Sun*

Main category: cs.CV

TL;DR: CloDS is an unsupervised framework that learns cloth dynamics from multi-view videos without physical supervision, using mesh-based Gaussian splatting with dual-position opacity modulation to handle large deformations and occlusions.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning methods for simulating dynamic systems require known physical properties as supervision, which limits their applicability in real-world scenarios where physical properties are unknown. There's a need for unsupervised approaches that can learn dynamics purely from visual observations.

Method: CloDS uses a three-stage pipeline: 1) video-to-geometry grounding that reconstructs 3D meshes from multi-view videos using mesh-based Gaussian splatting with dual-position opacity modulation (considering both absolute and relative positions of Gaussian components), 2) training a dynamics model on the grounded meshes, and 3) enabling bidirectional mapping between 2D observations and 3D geometry.

Result: Comprehensive experiments show that CloDS effectively learns cloth dynamics from visual data and maintains strong generalization capabilities for unseen configurations, successfully handling large non-linear deformations and severe self-occlusions.

Conclusion: CloDS demonstrates that unsupervised learning of cloth dynamics from visual observations is feasible and effective, overcoming the limitations of supervised methods that require known physical properties, with potential applications in robotics, animation, and physical simulation.

Abstract: Deep learning has demonstrated remarkable capabilities in simulating complex dynamic systems. However, existing methods require known physical properties as supervision or inputs, limiting their applicability under unknown conditions. To explore this challenge, we introduce Cloth Dynamics Grounding (CDG), a novel scenario for unsupervised learning of cloth dynamics from multi-view visual observations. We further propose Cloth Dynamics Splatting (CloDS), an unsupervised dynamic learning framework designed for CDG. CloDS adopts a three-stage pipeline that first performs video-to-geometry grounding and then trains a dynamics model on the grounded meshes. To cope with large non-linear deformations and severe self-occlusions during grounding, we introduce a dual-position opacity modulation that supports bidirectional mapping between 2D observations and 3D geometry via mesh-based Gaussian splatting in video-to-geometry grounding stage. It jointly considers the absolute and relative position of Gaussian components. Comprehensive experimental evaluations demonstrate that CloDS effectively learns cloth dynamics from visual data while maintaining strong generalization capabilities for unseen configurations. Our code is available at https://github.com/whynot-zyl/CloDS. Visualization results are available at https://github.com/whynot-zyl/CloDS_video}.%\footnote{As in this example.

</details>


### [219] [SPIRIT: Adapting Vision Foundation Models for Unified Single- and Multi-Frame Infrared Small Target Detection](https://arxiv.org/abs/2602.01843)
*Qian Xu,Xi Li,Fei Gao,Jie Guo,Haojuan Yuan,Shuaipeng Fan,Mingjin Zhang*

Main category: cs.CV

TL;DR: SPIRIT is a unified framework that adapts vision foundation models to infrared small target detection using physics-informed plug-ins for spatial refinement and temporal association.


<details>
  <summary>Details</summary>
Motivation: Infrared small target detection faces challenges due to weak radiometric signals, limited semantic cues, and modality gaps with visible-spectrum imagery. Direct use of vision foundation models (VFMs) is unreliable because hierarchical feature aggregation can submerge target peaks, and appearance-only memory attention leads to spurious clutter associations.

Method: SPIRIT adapts VFMs to IRSTD via lightweight physics-informed plug-ins: 1) PIFR (spatial) refines features using rank-sparsity decomposition to suppress background and enhance target signals, 2) PGMA (temporal) injects history-derived soft spatial priors into memory cross-attention to constrain cross-frame association, enabling both single- and multi-frame inference.

Result: Experiments on multiple IRSTD benchmarks show consistent gains over VFM-based baselines and achieve state-of-the-art performance.

Conclusion: SPIRIT successfully bridges the modality gap between visible-spectrum VFMs and infrared small target detection through physics-informed adaptations, providing a unified framework that works for both single-frame analysis and video-mode tracking while leveraging foundation model capabilities.

Abstract: Infrared small target detection (IRSTD) is crucial for surveillance and early-warning, with deployments spanning both single-frame analysis and video-mode tracking. A practical solution should leverage vision foundation models (VFMs) to mitigate infrared data scarcity, while adopting a memory-attention-based temporal propagation framework that unifies single- and multi-frame inference. However, infrared small targets exhibit weak radiometric signals and limited semantic cues, which differ markedly from visible-spectrum imagery. This modality gap makes direct use of semantics-oriented VFMs and appearance-driven cross-frame association unreliable for IRSTD: hierarchical feature aggregation can submerge localized target peaks, and appearance-only memory attention becomes ambiguous, leading to spurious clutter associations. To address these challenges, we propose SPIRIT, a unified and VFM-compatible framework that adapts VFMs to IRSTD via lightweight physics-informed plug-ins. Spatially, PIFR refines features by approximating rank-sparsity decomposition to suppress structured background components and enhance sparse target-like signals. Temporally, PGMA injects history-derived soft spatial priors into memory cross-attention to constrain cross-frame association, enabling robust video detection while naturally reverting to single-frame inference when temporal context is absent. Experiments on multiple IRSTD benchmarks show consistent gains over VFM-based baselines and SOTA performance.

</details>


### [220] [Learning Sparse Visual Representations via Spatial-Semantic Factorization](https://arxiv.org/abs/2602.01905)
*Theodore Zhengde Zhao,Sid Kiblawi,Jianwei Yang,Naoto Usuyama,Reuben Tan,Noel C Codella,Tristan Naumann,Hoifung Poon,Mu Wei*

Main category: cs.CV

TL;DR: STELLAR resolves the conflict between semantic SSL (DINO) and generative SSL (MAE) by factorizing visual features into semantic concepts and spatial distributions, enabling both high-quality reconstruction and strong semantic performance with sparse tokens.


<details>
  <summary>Details</summary>
Motivation: Self-supervised learning faces a fundamental conflict: semantic SSL (like DINO) discards spatial information needed for reconstruction, while generative SSL (like MAE) preserves spatial features but lacks high-level abstractions. There's a need to bridge this gap between discriminative and generative vision.

Method: STELLAR factorizes visual features into a low-rank product of semantic concepts and their spatial distributions. This disentanglement allows DINO-style augmentation alignment on semantic tokens while maintaining precise spatial mapping in the localization matrix for pixel-level reconstruction.

Result: With just 16 sparse tokens, STELLAR achieves high-quality reconstruction (2.60 FID) and matches semantic performance of dense backbones (79.10% ImageNet accuracy), demonstrating a versatile sparse representation that bridges discriminative and generative vision.

Conclusion: STELLAR successfully resolves the tension between semantic understanding and image reconstruction in SSL by strategically separating semantic identity from spatial geometry, creating a unified framework that supports both discriminative and generative tasks.

Abstract: Self-supervised learning (SSL) faces a fundamental conflict between semantic understanding and image reconstruction. High-level semantic SSL (e.g., DINO) relies on global tokens that are forced to be location-invariant for augmentation alignment, a process that inherently discards the spatial coordinates required for reconstruction. Conversely, generative SSL (e.g., MAE) preserves dense feature grids for reconstruction but fails to produce high-level abstractions. We introduce STELLAR, a framework that resolves this tension by factorizing visual features into a low-rank product of semantic concepts and their spatial distributions. This disentanglement allows us to perform DINO-style augmentation alignment on the semantic tokens while maintaining the precise spatial mapping in the localization matrix necessary for pixel-level reconstruction. We demonstrate that as few as 16 sparse tokens under this factorized form are sufficient to simultaneously support high-quality reconstruction (2.60 FID) and match the semantic performance of dense backbones (79.10% ImageNet accuracy). Our results highlight STELLAR as a versatile sparse representation that bridges the gap between discriminative and generative vision by strategically separating semantic identity from spatial geometry. Code available at https://aka.ms/stellar.

</details>


### [221] [DSXFormer: Dual-Pooling Spectral Squeeze-Expansion and Dynamic Context Attention Transformer for Hyperspectral Image Classification](https://arxiv.org/abs/2602.01906)
*Farhan Ullah,Irfan Ullah,Khalil Khan,Giovanni Pau,JaKeoung Koo*

Main category: cs.CV

TL;DR: DSXFormer: A dual-pooling spectral squeeze-expansion transformer with Dynamic Context Attention for hyperspectral image classification, achieving state-of-the-art performance on four benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Hyperspectral image classification faces challenges due to high spectral dimensionality, complex spectral-spatial correlations, and limited labeled samples. Existing transformer-based models struggle to achieve sufficient spectral discriminability while maintaining computational efficiency.

Method: Proposes DSXFormer with two key components: 1) Dual-Pooling Spectral Squeeze-Expansion (DSX) block using global average and max pooling to adaptively recalibrate spectral feature channels, and 2) Dynamic Context Attention (DCA) mechanism within window-based transformer to capture local spectral-spatial relationships efficiently. Also employs patch extraction, embedding, and patch merging for multi-scale feature learning.

Result: Achieved classification accuracies of 99.95% (Salinas), 98.91% (Indian Pines), 99.85% (Pavia University), and 98.52% (Kennedy Space Center), consistently outperforming state-of-the-art methods on four benchmark datasets.

Conclusion: DSXFormer effectively balances spectral emphasis and spatial contextual representation through joint integration of spectral dual-pooling squeeze-expansion and dynamic context attention, demonstrating superior performance for hyperspectral image classification while maintaining computational efficiency.

Abstract: Hyperspectral image classification (HSIC) is a challenging task due to high spectral dimensionality, complex spectral-spatial correlations, and limited labeled training samples. Although transformer-based models have shown strong potential for HSIC, existing approaches often struggle to achieve sufficient spectral discriminability while maintaining computational efficiency. To address these limitations, we propose a novel DSXFormer, a novel dual-pooling spectral squeeze-expansion transformer with Dynamic Context Attention for HSIC. The proposed DSXFormer introduces a Dual-Pooling Spectral Squeeze-Expansion (DSX) block, which exploits complementary global average and max pooling to adaptively recalibrate spectral feature channels, thereby enhancing spectral discriminability and inter-band dependency modeling. In addition, DSXFormer incorporates a Dynamic Context Attention (DCA) mechanism within a window-based transformer architecture to dynamically capture local spectral-spatial relationships while significantly reducing computational overhead. The joint integration of spectral dual-pooling squeeze-expansion and DCA enables DSXFormer to achieve an effective balance between spectral emphasis and spatial contextual representation. Furthermore, patch extraction, embedding, and patch merging strategies are employed to facilitate efficient multi-scale feature learning. Extensive experiments conducted on four widely used hyperspectral benchmark datasets, including Salinas (SA), Indian Pines (IP), Pavia University (PU), and Kennedy Space Center (KSC), demonstrate that DSXFormer consistently outperforms state-of-the-art methods, achieving classification accuracies of 99.95%, 98.91%, 99.85%, and 98.52%, respectively.

</details>


### [222] [WS-IMUBench: Can Weakly Supervised Methods from Audio, Image, and Video Be Adapted for IMU-based Temporal Action Localization?](https://arxiv.org/abs/2602.01850)
*Pei Li,Jiaxi Yin,Lei Ouyang,Shihan Pan,Ge Wang,Han Ding,Fei Wang*

Main category: cs.CV

TL;DR: WS-IMUBench is a systematic benchmark for weakly supervised IMU temporal action localization using only sequence-level labels, evaluating transferability of existing methods across 7 datasets with 3,540 training runs.


<details>
  <summary>Details</summary>
Motivation: Current IMU-TAL requires costly dense frame-level annotations, creating a scalability bottleneck. Weak supervision using only sequence-level labels could enable more scalable action localization in continuous IMU streams.

Method: Systematically benchmarked 7 representative weakly supervised methods from audio/image/video domains on 7 public IMU datasets, evaluating transferability under sequence-level labels only (no frame-level boundaries).

Result: Temporal-domain methods transfer better than image-derived approaches; weak supervision can be competitive on favorable datasets; failure modes include short actions, temporal ambiguity, and poor proposal quality.

Conclusion: Established WS-IMUBench as reproducible benchmarking framework, identified modality-dependent transfer patterns, and outlined directions for advancing WS-IMU-TAL through IMU-specific proposal generation and boundary-aware objectives.

Abstract: IMU-based Human Activity Recognition (HAR) has enabled a wide range of ubiquitous computing applications, yet its dominant clip classification paradigm cannot capture the rich temporal structure of real-world behaviors. This motivates a shift toward IMU Temporal Action Localization (IMU-TAL), which predicts both action categories and their start/end times in continuous streams. However, current progress is strongly bottlenecked by the need for dense, frame-level boundary annotations, which are costly and difficult to scale. To address this bottleneck, we introduce WS-IMUBench, a systematic benchmark study of weakly supervised IMU-TAL (WS-IMU-TAL) under only sequence-level labels. Rather than proposing a new localization algorithm, we evaluate how well established weakly supervised localization paradigms from audio, image, and video transfer to IMU-TAL under only sequence-level labels. We benchmark seven representative weakly supervised methods on seven public IMU datasets, resulting in over 3,540 model training runs and 7,080 inference evaluations. Guided by three research questions on transferability, effectiveness, and insights, our findings show that (i) transfer is modality-dependent, with temporal-domain methods generally more stable than image-derived proposal-based approaches; (ii) weak supervision can be competitive on favorable datasets (e.g., with longer actions and higher-dimensional sensing); and (iii) dominant failure modes arise from short actions, temporal ambiguity, and proposal quality. Finally, we outline concrete directions for advancing WS-IMU-TAL (e.g., IMU-specific proposal generation, boundary-aware objectives, and stronger temporal reasoning). Beyond individual results, WS-IMUBench establishes a reproducible benchmarking template, datasets, protocols, and analyses, to accelerate community-wide progress toward scalable WS-IMU-TAL.

</details>


### [223] [How Well Do Models Follow Visual Instructions? VIBE: A Systematic Benchmark for Visual Instruction-Driven Image Editing](https://arxiv.org/abs/2602.01851)
*Huanyu Zhang,Xuehai Bai,Chengzu Li,Chen Liang,Haochen Tian,Haodong Li,Ruichuan An,Yifan Zhang,Anna Korhonen,Zhang Zhang,Liang Wang,Tieniu Tan*

Main category: cs.CV

TL;DR: VIBE is a visual instruction benchmark for image editing with three complexity levels, revealing that proprietary models outperform open-source ones but all struggle with harder tasks.


<details>
  <summary>Details</summary>
Motivation: Current image editing systems are mostly text-guided, but human communication is multimodal where visual instructions like sketches efficiently convey spatial and structural intent. There's a gap in visual instruction-following benchmarks.

Method: Introduce VIBE benchmark with three-level interaction hierarchy (deictic grounding, morphological manipulation, causal reasoning) and propose LMM-as-a-judge evaluation framework with task-specific metrics for scalable assessment.

Result: Evaluation of 17 models shows proprietary models exhibit early-stage visual instruction-following capabilities and consistently outperform open-source models, but performance degrades markedly with increasing task difficulty.

Conclusion: Visual instruction following remains challenging, with significant room for improvement especially for complex tasks, highlighting promising directions for future research in multimodal image editing.

Abstract: Recent generative models have achieved remarkable progress in image editing. However, existing systems and benchmarks remain largely text-guided. In contrast, human communication is inherently multimodal, where visual instructions such as sketches efficiently convey spatial and structural intent. To address this gap, we introduce VIBE, the Visual Instruction Benchmark for Image Editing with a three-level interaction hierarchy that captures deictic grounding, morphological manipulation, and causal reasoning. Across these levels, we curate high-quality and diverse test cases that reflect progressively increasing complexity in visual instruction following. We further propose a robust LMM-as-a-judge evaluation framework with task-specific metrics to enable scalable and fine-grained assessment. Through a comprehensive evaluation of 17 representative open-source and proprietary image editing models, we find that proprietary models exhibit early-stage visual instruction-following capabilities and consistently outperform open-source models. However, performance degrades markedly with increasing task difficulty even for the strongest systems, highlighting promising directions for future research.

</details>


### [224] [Your AI-Generated Image Detector Can Secretly Achieve SOTA Accuracy, If Calibrated](https://arxiv.org/abs/2602.01973)
*Muli Yang,Gabriel James Goenawan,Henan Wang,Huaiyuan Qin,Chenghao Xu,Yanhua Yang,Fen Fang,Ying Sun,Joo-Hwee Lim,Hongyuan Zhu*

Main category: cs.CV

TL;DR: Proposes a post-hoc calibration framework to fix systematic bias in AI-generated image detectors that misclassify fake images as real due to distributional shift, using Bayesian decision theory to adjust decision thresholds without retraining.


<details>
  <summary>Details</summary>
Motivation: Existing AI-generated image detectors trained on balanced datasets show systematic bias at test time, frequently misclassifying fake images as real due to distributional shift in fake samples and implicit priors learned during training.

Method: A theoretically grounded post-hoc calibration framework based on Bayesian decision theory, introducing a learnable scalar correction to model logits optimized on a small validation set from target distribution while keeping backbone frozen.

Result: Experiments on challenging benchmarks show the approach significantly improves robustness without retraining, offering a lightweight and principled solution for reliable and adaptive AI-generated image detection.

Conclusion: The proposed calibration framework effectively addresses systematic bias in AI-generated image detectors by realigning decision boundaries to handle distributional shift, providing a practical solution for open-world deployment.

Abstract: Despite being trained on balanced datasets, existing AI-generated image detectors often exhibit systematic bias at test time, frequently misclassifying fake images as real. We hypothesize that this behavior stems from distributional shift in fake samples and implicit priors learned during training. Specifically, models tend to overfit to superficial artifacts that do not generalize well across different generation methods, leading to a misaligned decision threshold when faced with test-time distribution shift. To address this, we propose a theoretically grounded post-hoc calibration framework based on Bayesian decision theory. In particular, we introduce a learnable scalar correction to the model's logits, optimized on a small validation set from the target distribution while keeping the backbone frozen. This parametric adjustment compensates for distributional shift in model output, realigning the decision boundary even without requiring ground-truth labels. Experiments on challenging benchmarks show that our approach significantly improves robustness without retraining, offering a lightweight and principled solution for reliable and adaptive AI-generated image detection in the open world. Code is available at https://github.com/muliyangm/AIGI-Det-Calib.

</details>


### [225] [Fact or Fake? Assessing the Role of Deepfake Detectors in Multimodal Misinformation Detection](https://arxiv.org/abs/2602.01854)
*A S M Sharifuzzaman Sagar,Mohammed Bennamoun,Farid Boussaid,Naeha Sharif,Lian Xu,Shaaban Sahmoud,Ali Kishk*

Main category: cs.CV

TL;DR: Pixel-level deepfake detectors don't help verify multimodal misinformation claims; evidence-based fact-checking works better.


<details>
  <summary>Details</summary>
Motivation: Current deepfake detectors focus on pixel-level manipulations but ignore semantic meaning in image-text misinformation, raising questions about their usefulness in automated fact-checking pipelines.

Method: Systematic evaluation using MMFakeBench and DGM4 benchmarks to compare: (1) image-only deepfake detectors, (2) evidence-driven fact-checking with MCTS retrieval and Multi-Agent Debate, and (3) hybrid systems combining detectors with evidence.

Result: Deepfake detectors performed poorly (F1: 0.26-0.53 on MMFakeBench, 0.33-0.49 on DGM4). Adding detector outputs to fact-checking reduced performance by 0.04-0.08 F1. Evidence-based system achieved best results (F1: ~0.81 on MMFakeBench, 0.55 on DGM4).

Conclusion: Multimodal claim verification relies on semantic understanding and external evidence, not pixel-level artifacts. Deepfake detectors introduce misleading authenticity assumptions that harm fact-checking performance.

Abstract: In multimodal misinformation, deception usually arises not just from pixel-level manipulations in an image, but from the semantic and contextual claim jointly expressed by the image-text pair. Yet most deepfake detectors, engineered to detect pixel-level forgeries, do not account for claim-level meaning, despite their growing integration in automated fact-checking (AFC) pipelines. This raises a central scientific and practical question: Do pixel-level detectors contribute useful signal for verifying image-text claims, or do they instead introduce misleading authenticity priors that undermine evidence-based reasoning? We provide the first systematic analysis of deepfake detectors in the context of multimodal misinformation detection. Using two complementary benchmarks, MMFakeBench and DGM4, we evaluate: (1) state-of-the-art image-only deepfake detectors, (2) an evidence-driven fact-checking system that performs tool-guided retrieval via Monte Carlo Tree Search (MCTS) and engages in deliberative inference through Multi-Agent Debate (MAD), and (3) a hybrid fact-checking system that injects detector outputs as auxiliary evidence. Results across both benchmark datasets show that deepfake detectors offer limited standalone value, achieving F1 scores in the range of 0.26-0.53 on MMFakeBench and 0.33-0.49 on DGM4, and that incorporating their predictions into fact-checking pipelines consistently reduces performance by 0.04-0.08 F1 due to non-causal authenticity assumptions. In contrast, the evidence-centric fact-checking system achieves the highest performance, reaching F1 scores of approximately 0.81 on MMFakeBench and 0.55 on DGM4. Overall, our findings demonstrate that multimodal claim verification is driven primarily by semantic understanding and external evidence, and that pixel-level artifact signals do not reliably enhance reasoning over real-world image-text misinformation.

</details>


### [226] [SurfSplat: Conquering Feedforward 2D Gaussian Splatting with Surface Continuity Priors](https://arxiv.org/abs/2602.02000)
*Bing He,Jingnan Gao,Yunuo Chen,Ning Cao,Gang Chen,Zhengxue Cheng,Li Song,Wenjun Zhang*

Main category: cs.CV

TL;DR: SurfSplat: A feedforward framework using 2D Gaussian Splatting for high-fidelity 3D scene reconstruction from sparse images, addressing surface continuity and texture quality issues in previous methods.


<details>
  <summary>Details</summary>
Motivation: Existing 3D Gaussian Splatting (3DGS) methods for sparse image reconstruction often produce discrete, color-biased point clouds with poor surface continuity that appear plausible at normal resolution but reveal severe artifacts under close-up views.

Method: SurfSplat uses 2D Gaussian Splatting primitive for stronger anisotropy and higher geometric precision, incorporating surface continuity prior and forced alpha blending strategy. Also introduces High-Resolution Rendering Consistency (HRRC) evaluation metric.

Result: Extensive experiments on RealEstate10K, DL3DV, and ScanNet show SurfSplat consistently outperforms prior methods on both standard metrics and the new HRRC metric, establishing robust high-fidelity reconstruction.

Conclusion: SurfSplat provides a robust solution for high-fidelity 3D reconstruction from sparse inputs by addressing surface continuity and texture quality issues through 2D Gaussian Splatting with continuity priors and forced alpha blending.

Abstract: Reconstructing 3D scenes from sparse images remains a challenging task due to the difficulty of recovering accurate geometry and texture without optimization. Recent approaches leverage generalizable models to generate 3D scenes using 3D Gaussian Splatting (3DGS) primitive. However, they often fail to produce continuous surfaces and instead yield discrete, color-biased point clouds that appear plausible at normal resolution but reveal severe artifacts under close-up views. To address this issue, we present SurfSplat, a feedforward framework based on 2D Gaussian Splatting (2DGS) primitive, which provides stronger anisotropy and higher geometric precision. By incorporating a surface continuity prior and a forced alpha blending strategy, SurfSplat reconstructs coherent geometry together with faithful textures. Furthermore, we introduce High-Resolution Rendering Consistency (HRRC), a new evaluation metric designed to evaluate high-resolution reconstruction quality. Extensive experiments on RealEstate10K, DL3DV, and ScanNet demonstrate that SurfSplat consistently outperforms prior methods on both standard metrics and HRRC, establishing a robust solution for high-fidelity 3D reconstruction from sparse inputs. Project page: https://hebing-sjtu.github.io/SurfSplat-website/

</details>


### [227] [Trust but Verify: Adaptive Conditioning for Reference-Based Diffusion Super-Resolution via Implicit Reference Correlation Modeling](https://arxiv.org/abs/2602.01864)
*Yuan Wang,Yuhao Wan,Siming Zheng,Bo Li,Qibin Hou,Peng-Tao Jiang*

Main category: cs.CV

TL;DR: Ada-RefSR is a single-step diffusion framework for reference-based super-resolution that adaptively controls reference usage via learnable gating, balancing fidelity and naturalness while being robust to unreliable references.


<details>
  <summary>Details</summary>
Motivation: Real-world degradations make correspondences between low-quality inputs and reference images unreliable in reference-based super-resolution, requiring adaptive control of reference usage to avoid over-reliance on misleading references or under-utilization of valuable cues.

Method: Proposes Ada-RefSR with Adaptive Implicit Correlation Gating (AICG) that uses learnable summary tokens to distill dominant reference patterns and capture implicit correlations with LQ features, integrated into the attention backbone for lightweight adaptive regulation.

Result: Experiments on multiple datasets show Ada-RefSR achieves strong balance of fidelity, naturalness, and efficiency while remaining robust under varying reference alignment.

Conclusion: Ada-RefSR effectively implements a "Trust but Verify" principle for reference-based super-resolution, providing adaptive regulation of reference guidance as a built-in safeguard against erroneous fusion.

Abstract: Recent works have explored reference-based super-resolution (RefSR) to mitigate hallucinations in diffusion-based image restoration. A key challenge is that real-world degradations make correspondences between low-quality (LQ) inputs and reference (Ref) images unreliable, requiring adaptive control of reference usage. Existing methods either ignore LQ-Ref correlations or rely on brittle explicit matching, leading to over-reliance on misleading references or under-utilization of valuable cues. To address this, we propose Ada-RefSR, a single-step diffusion framework guided by a "Trust but Verify" principle: reference information is leveraged when reliable and suppressed otherwise. Its core component, Adaptive Implicit Correlation Gating (AICG), employs learnable summary tokens to distill dominant reference patterns and capture implicit correlations with LQ features. Integrated into the attention backbone, AICG provides lightweight, adaptive regulation of reference guidance, serving as a built-in safeguard against erroneous fusion. Experiments on multiple datasets demonstrate that Ada-RefSR achieves a strong balance of fidelity, naturalness, and efficiency, while remaining robust under varying reference alignment.

</details>


### [228] [ClueTracer: Question-to-Vision Clue Tracing for Training-Free Hallucination Suppression in Multimodal Reasoning](https://arxiv.org/abs/2602.02004)
*Gongli Xi,Kun Wang,Zeming Gao,Huahui Yi,Haolang Lu,Ye Tian,Wendong Wang*

Main category: cs.CV

TL;DR: ClueTracer: Training-free plugin that traces clue propagation in multimodal reasoning models to suppress hallucinations, improving performance on reasoning benchmarks by 1.21×.


<details>
  <summary>Details</summary>
Motivation: Multimodal reasoning models suffer from hallucinations due to "reasoning drift" - they over-focus on irrelevant entities during clue gathering, diluting task-relevant cues and decoupling reasoning from visual grounding.

Method: ClueTracer: Training-free, parameter-free, architecture-agnostic plugin that starts from the question and traces how key clues propagate along the model's reasoning pathway (question → outputs → visual tokens), localizing task-relevant patches while suppressing spurious attention to irrelevant regions.

Result: ClueTracer improves all reasoning architectures (including R1-OneVision, Ocean-R1, MM-Eureka, etc.) by 1.21× on reasoning benchmarks without any additional training. When transferred to non-reasoning settings, it yields a 1.14× gain.

Conclusion: The paper introduces ClueRecall metric for assessing visual clue retrieval and presents ClueTracer as an effective solution for hallucination suppression in multimodal reasoning models through tracing clue propagation pathways.

Abstract: Large multimodal reasoning models solve challenging visual problems via explicit long-chain inference: they gather visual clues from images and decode clues into textual tokens. Yet this capability also increases hallucinations, where the model generates content that is not supported by the input image or the question. To understand this failure mode, we identify \emph{reasoning drift}: during clue gathering, the model over-focuses on question-irrelevant entities, diluting focus on task-relevant cues and gradually decoupling the reasoning trace from visual grounding. As a consequence, many inference-time localization or intervention methods developed for non-reasoning models fail to pinpoint the true clues in reasoning settings. Motivated by these insights, we introduce ClueRecall, a metric for assessing visual clue retrieval, and present ClueTracer, a training-free, parameter-free, and architecture-agnostic plugin for hallucination suppression. ClueTracer starts from the question and traces how key clues propagate along the model's reasoning pathway (question $\rightarrow$ outputs $\rightarrow$ visual tokens), thereby localizing task-relevant patches while suppressing spurious attention to irrelevant regions. Remarkably, \textbf{without any additional training}, ClueTracer improves all \textbf{reasoning} architectures (including \texttt{R1-OneVision}, \texttt{Ocean-R1}, \texttt{MM-Eureka}, \emph{etc}.) by $\mathbf{1.21\times}$ on reasoning benchmarks. When transferred to \textbf{non-reasoning} settings, it yields a $\mathbf{1.14\times}$ gain.

</details>


### [229] [ProxyImg: Towards Highly-Controllable Image Representation via Hierarchical Disentangled Proxy Embedding](https://arxiv.org/abs/2602.01881)
*Ye Chen,Yupeng Zhu,Xiongzhen Zhang,Zhewen Wan,Yingzhe Li,Wenjun Zhang,Bingbing Ni*

Main category: cs.CV

TL;DR: Hierarchical proxy-based parametric image representation that disentangles semantic, geometric, and textural attributes for efficient, controllable image/video editing with physics-driven animation support.


<details>
  <summary>Details</summary>
Motivation: Existing image representations (raster, Gaussian primitives, latent images) suffer from representation redundancy or lack direct mapping from latent variables to semantic instances/parts, hindering efficient and controllable image/video editing.

Method: Hierarchical proxy-based parametric representation that disentangles semantic, geometric, and textural attributes. Uses semantic-aware decomposition, adaptive Bezier fitting, iterative region subdivision/meshing, and embeds multi-scale implicit texture parameters into geometry-aware distributed proxy nodes with locality-adaptive feature indexing.

Result: Achieves state-of-the-art rendering fidelity with significantly fewer parameters on ImageNet, OIR-Bench, and HumanEdit benchmarks. Enables intuitive, interactive, physically plausible manipulation and supports real-time physics-driven animation with superior temporal consistency and visual realism.

Conclusion: Proposed representation addresses limitations of existing methods by enabling efficient, controllable image/video editing with semantic disentanglement, supporting both high-fidelity reconstruction and physics-driven animation through lightweight implicit rendering.

Abstract: Prevailing image representation methods, including explicit representations such as raster images and Gaussian primitives, as well as implicit representations such as latent images, either suffer from representation redundancy that leads to heavy manual editing effort, or lack a direct mapping from latent variables to semantic instances or parts, making fine-grained manipulation difficult. These limitations hinder efficient and controllable image and video editing. To address these issues, we propose a hierarchical proxy-based parametric image representation that disentangles semantic, geometric, and textural attributes into independent and manipulable parameter spaces. Based on a semantic-aware decomposition of the input image, our representation constructs hierarchical proxy geometries through adaptive Bezier fitting and iterative internal region subdivision and meshing. Multi-scale implicit texture parameters are embedded into the resulting geometry-aware distributed proxy nodes, enabling continuous high-fidelity reconstruction in the pixel domain and instance- or part-independent semantic editing. In addition, we introduce a locality-adaptive feature indexing mechanism to ensure spatial texture coherence, which further supports high-quality background completion without relying on generative models. Extensive experiments on image reconstruction and editing benchmarks, including ImageNet, OIR-Bench, and HumanEdit, demonstrate that our method achieves state-of-the-art rendering fidelity with significantly fewer parameters, while enabling intuitive, interactive, and physically plausible manipulation. Moreover, by integrating proxy nodes with Position-Based Dynamics, our framework supports real-time physics-driven animation using lightweight implicit rendering, achieving superior temporal consistency and visual realism compared with generative approaches.

</details>


### [230] [Rethinking Genomic Modeling Through Optical Character Recognition](https://arxiv.org/abs/2602.02014)
*Hongxin Xiang,Pengsen Ma,Yunkang Cao,Di Yu,Haowen Chen,Xinyu Yang,Xiangxiang Zeng*

Main category: cs.CV

TL;DR: OpticalDNA: A vision-based framework that treats DNA as visual documents for OCR-style understanding, achieving better performance with 20× fewer tokens than language model approaches.


<details>
  <summary>Details</summary>
Motivation: Current genomic foundation models use language model architectures that treat DNA as 1D token sequences, which is inefficient for sparse genomic semantics and wastes computation on low-information background regions.

Method: Renders DNA into structured visual layouts and trains an OCR-capable vision-language model with visual DNA encoder and document decoder, using prompt-conditioned objectives for genomic primitives like reading, region grounding, and masked span completion.

Result: Outperforms recent baselines across diverse genomic benchmarks; achieves best overall performance on sequences up to 450k bases with nearly 20× fewer effective tokens, surpassing models with up to 985× more parameters while tuning only 256k parameters.

Conclusion: Vision-based OCR-style document understanding provides a more efficient and effective approach to genomic modeling than traditional language model architectures, enabling better compression and representation of sparse genomic information.

Abstract: Recent genomic foundation models largely adopt large language model architectures that treat DNA as a one-dimensional token sequence. However, exhaustive sequential reading is structurally misaligned with sparse and discontinuous genomic semantics, leading to wasted computation on low-information background and preventing understanding-driven compression for long contexts. Here, we present OpticalDNA, a vision-based framework that reframes genomic modeling as Optical Character Recognition (OCR)-style document understanding. OpticalDNA renders DNA into structured visual layouts and trains an OCR-capable vision--language model with a \emph{visual DNA encoder} and a \emph{document decoder}, where the encoder produces compact, reconstructible visual tokens for high-fidelity compression. Building on this representation, OpticalDNA defines prompt-conditioned objectives over core genomic primitives-reading, region grounding, subsequence retrieval, and masked span completion-thereby learning layout-aware DNA representations that retain fine-grained genomic information under a reduced effective token budget. Across diverse genomic benchmarks, OpticalDNA consistently outperforms recent baselines; on sequences up to 450k bases, it achieves the best overall performance with nearly $20\times$ fewer effective tokens, and surpasses models with up to $985\times$ more activated parameters while tuning only 256k \emph{trainable} parameters.

</details>


### [231] [Q Cache: Visual Attention is Valuable in Less than Half of Decode Layers for Multimodal Large Language Model](https://arxiv.org/abs/2602.01901)
*Jiedong Zhuang,Lu Lu,Ming Dai,Rui Hu,Jian Chen,Qiang Liu,Haoji Hu*

Main category: cs.CV

TL;DR: Lazy Attention reduces MLLM inference costs by enabling cross-layer sharing of similar attention patterns, cutting KV cache by 35% and improving throughput 1.5x with minimal performance loss.


<details>
  <summary>Details</summary>
Motivation: Multimodal LLMs suffer from high inference costs due to excessive visual tokens that create computational load and KV cache bottlenecks. Existing token pruning methods often damage KV cache integrity, failing in long-text generation tasks.

Method: Proposes Lazy Attention mechanism that enables cross-layer sharing of similar attention patterns. Develops Q Cache for MLLMs that facilitates query reuse across adjacent layers. The method is lightweight, compatible with existing inference frameworks, and orthogonal to token pruning techniques.

Result: Reduces KV cache usage by over 35%, achieves 1.5x throughput improvement, with only ~1% performance loss on various MLLMs. Outperforms state-of-the-art token-wise methods in accuracy preservation.

Conclusion: Lazy Attention provides an efficient attention mechanism that addresses MLLM inference bottlenecks through cross-layer attention sharing, offering significant computational savings while maintaining model performance.

Abstract: Multimodal large language models (MLLMs) are plagued by exorbitant inference costs attributable to the profusion of visual tokens within the vision encoder. The redundant visual tokens engenders a substantial computational load and key-value (KV) cache footprint bottleneck. Existing approaches focus on token-wise optimization, leveraging diverse intricate token pruning techniques to eliminate non-crucial visual tokens. Nevertheless, these methods often unavoidably undermine the integrity of the KV cache, resulting in failures in long-text generation tasks. To this end, we conduct an in-depth investigation towards the attention mechanism of the model from a new perspective, and discern that attention within more than half of all decode layers are semantic similar. Upon this finding, we contend that the attention in certain layers can be streamlined by inheriting the attention from their preceding layers. Consequently, we propose Lazy Attention, an efficient attention mechanism that enables cross-layer sharing of similar attention patterns. It ingeniously reduces layer-wise redundant computation in attention. In Lazy Attention, we develop a novel layer-shared cache, Q Cache, tailored for MLLMs, which facilitates the reuse of queries across adjacent layers. In particular, Q Cache is lightweight and fully compatible with existing inference frameworks, including Flash Attention and KV cache. Additionally, our method is highly flexible as it is orthogonal to existing token-wise techniques and can be deployed independently or combined with token pruning approaches. Empirical evaluations on multiple benchmarks demonstrate that our method can reduce KV cache usage by over 35% and achieve 1.5x throughput improvement, while sacrificing only approximately 1% of performance on various MLLMs. Compared with SOTA token-wise methods, our technique achieves superior accuracy preservation.

</details>


### [232] [One Size, Many Fits: Aligning Diverse Group-Wise Click Preferences in Large-Scale Advertising Image Generation](https://arxiv.org/abs/2602.02033)
*Shuo Lu,Haohan Wang,Wei Feng,Weizhen Wang,Shen Zhang,Yaoyu Li,Ao Ma,Zheng Zhang,Jingjing Lv,Junjie Shen,Ching Law,Bing Zhan,Yuan Xu,Huizai Yao,Yongcan Yu,Chenyang Si,Jian Liang*

Main category: cs.CV

TL;DR: OSMF is a unified framework for advertising image generation that aligns diverse group-wise click preferences, moving beyond one-size-fits-all approaches to improve targeted marketing effectiveness.


<details>
  <summary>Details</summary>
Motivation: Existing advertising image generation approaches optimize for overall CTR but neglect preference diversity among user groups, leading to suboptimal performance for specific groups and limiting targeted marketing effectiveness.

Method: OSMF uses product-aware adaptive grouping to dynamically organize users, then employs a Group-aware Multimodal Large Language Model (G-MLLM) for preference-conditioned image generation, fine-tuned with Group-DPO for group-wise preference alignment.

Result: The framework achieves state-of-the-art performance in both offline and online settings, and the authors introduce GAIP - the first large-scale public dataset of group-wise image preferences with 600K groups from 40M users.

Conclusion: OSMF successfully bridges the gap in targeted advertising by aligning diverse group preferences, demonstrating superior performance and providing valuable resources for advancing the field through the GAIP dataset.

Abstract: Advertising image generation has increasingly focused on online metrics like Click-Through Rate (CTR), yet existing approaches adopt a ``one-size-fits-all" strategy that optimizes for overall CTR while neglecting preference diversity among user groups. This leads to suboptimal performance for specific groups, limiting targeted marketing effectiveness. To bridge this gap, we present \textit{One Size, Many Fits} (OSMF), a unified framework that aligns diverse group-wise click preferences in large-scale advertising image generation. OSMF begins with product-aware adaptive grouping, which dynamically organizes users based on their attributes and product characteristics, representing each group with rich collective preference features. Building on these groups, preference-conditioned image generation employs a Group-aware Multimodal Large Language Model (G-MLLM) to generate tailored images for each group. The G-MLLM is pre-trained to simultaneously comprehend group features and generate advertising images. Subsequently, we fine-tune the G-MLLM using our proposed Group-DPO for group-wise preference alignment, which effectively enhances each group's CTR on the generated images. To further advance this field, we introduce the Grouped Advertising Image Preference Dataset (GAIP), the first large-scale public dataset of group-wise image preferences, including around 600K groups built from 40M users. Extensive experiments demonstrate that our framework achieves the state-of-the-art performance in both offline and online settings. Our code and datasets will be released at https://github.com/JD-GenX/OSMF.

</details>


### [233] [Auto-Comp: An Automated Pipeline for Scalable Compositional Probing of Contrastive Vision-Language Models](https://arxiv.org/abs/2602.02043)
*Cristian Sbrolli,Matteo Matteucci,Toshihiko Yamasaki*

Main category: cs.CV

TL;DR: Auto-Comp introduces an automated synthetic pipeline for generating scalable benchmarks to analyze VLMs' compositional reasoning failures, revealing universal flaws in attribute binding and susceptibility to low-entropy distractors.


<details>
  <summary>Details</summary>
Motivation: Modern VLMs have critical flaws in compositional reasoning (confusing attribute-object bindings like "red cube and blue sphere" vs "blue cube and red sphere"), but disentangling visual vs linguistic roots is challenging. Need fine-grained, controllable analysis tools.

Method: Auto-Comp: fully automated synthetic pipeline generating scalable benchmarks with controllable parameters. Creates paired images from Minimal captions (simple descriptions) and LLM-generated Contextual captions (rich scene descriptions) for controlled A/B testing to isolate binding ability from visio-linguistic complexity.

Result: Evaluation of 20 VLMs on color binding and spatial relations benchmarks reveals universal compositional failures in CLIP and SigLIP families. "Confusion Benchmark" shows models are highly susceptible to low-entropy distractors (repeated objects/colors). Found surprising trade-off: visio-linguistic context aids spatial reasoning but hinders local attribute binding by introducing visual clutter.

Conclusion: Auto-Comp provides a powerful framework for fine-grained analysis of VLM compositional reasoning failures, revealing deeper flaws beyond known limitations. The pipeline enables future benchmark creation and systematic evaluation of VLMs' reasoning capabilities.

Abstract: Modern Vision-Language Models (VLMs) exhibit a critical flaw in compositional reasoning, often confusing "a red cube and a blue sphere" with "a blue cube and a red sphere". Disentangling the visual and linguistic roots of these failures is a fundamental challenge for robust evaluation. To enable fine-grained, controllable analysis, we introduce Auto-Comp, a fully automated and synthetic pipeline for generating scalable benchmarks. Its controllable nature is key to dissecting and isolating different reasoning skills. Auto-Comp generates paired images from Minimal (e.g., "a monitor to the left of a bicycle on a white background") and LLM-generated Contextual captions (e.g., "In a brightly lit photography studio, a monitor is positioned to the left of a bicycle"), allowing a controlled A/B test to disentangle core binding ability from visio-linguistic complexity. Our evaluation of 20 VLMs on novel benchmarks for color binding and spatial relations reveals universal compositional failures in both CLIP and SigLIP model families. Crucially, our novel "Confusion Benchmark" reveals a deeper flaw beyond simple attribute swaps: models are highly susceptible to low-entropy distractors (e.g., repeated objects or colors), demonstrating their compositional failures extend beyond known bag-of-words limitations. we uncover a surprising trade-off: visio-linguistic context, which provides global scene cues, aids spatial reasoning but simultaneously hinders local attribute binding by introducing visual clutter. We release the Auto-Comp pipeline to facilitate future benchmark creation, alongside all our generated benchmarks (https://huggingface.co/AutoComp).

</details>


### [234] [Enabling Progressive Whole-slide Image Analysis with Multi-scale Pyramidal Network](https://arxiv.org/abs/2602.01951)
*Shuyang Wu,Yifu Qiu,Ines P. Nearchou,Sandrine Prost,Jonathan A Fallowfield,Hakan Bilen,Timothy J Kendall*

Main category: cs.CV

TL;DR: MSPN is a plug-and-play multi-scale pyramidal network that improves attention-based MIL for computational pathology by enabling progressive multi-scale analysis while being lightweight and easy to use.


<details>
  <summary>Details</summary>
Motivation: Current multi-scale approaches in computational pathology use late feature fusion with arbitrary manufacturer-defined magnifications, which loses cross-scale feature relationships, is inflexible, and computationally expensive.

Method: Proposes MSPN with two components: (1) grid-based remapping that uses high magnification features to derive coarse features, and (2) coarse guidance network (CGN) that learns coarse contexts. It's plug-and-play over attention-based MIL frameworks.

Result: MSPN consistently improves MIL performance across 4 attention-based frameworks, 4 clinical tasks, 3 foundation model types, and pre-trained MIL framework, while being lightweight.

Conclusion: MSPN provides an effective, flexible, and computationally efficient solution for multi-scale analysis in computational pathology that consistently enhances attention-based MIL frameworks.

Abstract: Multiple-instance Learning (MIL) is commonly used to undertake computational pathology (CPath) tasks, and the use of multi-scale patches allows diverse features across scales to be learned. Previous studies using multi-scale features in clinical applications rely on multiple inputs across magnifications with late feature fusion, which does not retain the link between features across scales while the inputs are dependent on arbitrary, manufacturer-defined magnifications, being inflexible and computationally expensive. In this paper, we propose the Multi-scale Pyramidal Network (MSPN), which is plug-and-play over attention-based MIL that introduces progressive multi-scale analysis on WSI. Our MSPN consists of (1) grid-based remapping that uses high magnification features to derive coarse features and (2) the coarse guidance network (CGN) that learns coarse contexts. We benchmark MSPN as an add-on module to 4 attention-based frameworks using 4 clinically relevant tasks across 3 types of foundation model, as well as the pre-trained MIL framework. We show that MSPN consistently improves MIL across the compared configurations and tasks, while being lightweight and easy-to-use.

</details>


### [235] [Multi-View Stenosis Classification Leveraging Transformer-Based Multiple-Instance Learning Using Real-World Clinical Data](https://arxiv.org/abs/2602.02067)
*Nikola Cenikj,Özgün Turgut,Alexander Müller,Alexander Steger,Jan Kehrer,Marcus Brugger,Daniel Rueckert,Eimo Martens,Philip Müller*

Main category: cs.CV

TL;DR: SegmentMIL is a transformer-based multi-view MIL framework for patient-level coronary stenosis classification without view-level annotations, achieving high performance on internal/external evaluations.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning models for stenosis detection require expensive view-level annotations and fail to capture temporal dynamics and dependencies among multiple angiography views, which are crucial for clinical diagnosis but often unavailable in hospital systems.

Method: SegmentMIL uses a transformer-based multi-view multiple-instance learning framework trained on patient-level supervision without view-level annotations, jointly predicting stenosis presence and localizing affected anatomical regions (right/left coronary arteries and segments).

Result: SegmentMIL obtains high performance on internal and external evaluations, outperforming both view-level models and classical MIL baselines.

Conclusion: SegmentMIL demonstrates potential as a clinically viable and scalable solution for coronary stenosis diagnosis by leveraging patient-level supervision and capturing multi-view dependencies without expensive annotations.

Abstract: Coronary artery stenosis is a leading cause of cardiovascular disease, diagnosed by analyzing the coronary arteries from multiple angiography views. Although numerous deep-learning models have been proposed for stenosis detection from a single angiography view, their performance heavily relies on expensive view-level annotations, which are often not readily available in hospital systems. Moreover, these models fail to capture the temporal dynamics and dependencies among multiple views, which are crucial for clinical diagnosis. To address this, we propose SegmentMIL, a transformer-based multi-view multiple-instance learning framework for patient-level stenosis classification. Trained on a real-world clinical dataset, using patient-level supervision and without any view-level annotations, SegmentMIL jointly predicts the presence of stenosis and localizes the affected anatomical region, distinguishing between the right and left coronary arteries and their respective segments. SegmentMIL obtains high performance on internal and external evaluations and outperforms both view-level models and classical MIL baselines, underscoring its potential as a clinically viable and scalable solution for coronary stenosis diagnosis. Our code is available at https://github.com/NikolaCenic/mil-stenosis.

</details>


### [236] [Beyond Open Vocabulary: Multimodal Prompting for Object Detection in Remote Sensing Images](https://arxiv.org/abs/2602.01954)
*Shuai Yang,Ziyue Huang,Jiaxin Chen,Qingjie Liu,Yunhong Wang*

Main category: cs.CV

TL;DR: RS-MPOD introduces multimodal prompting (visual + textual) for open-vocabulary object detection in remote sensing, addressing limitations of text-only approaches that fail under semantic ambiguity.


<details>
  <summary>Details</summary>
Motivation: Text-only prompting for open-vocabulary detection in remote sensing often fails due to task-specific category semantics and distribution shifts, leading to unstable category specification under open-vocabulary settings.

Method: Proposes RS-MPOD with visual prompt encoder for appearance-based category cues from exemplar instances (enabling text-free specification) and multimodal fusion module to integrate visual and textual information when both are available.

Result: Extensive experiments show visual prompting yields more reliable category specification under semantic ambiguity and distribution shifts, while multimodal prompting remains competitive when textual semantics are well aligned.

Conclusion: Multimodal prompting (visual + textual) provides a flexible and more reliable alternative to text-only approaches for open-vocabulary object detection in remote sensing, especially under challenging semantic conditions.

Abstract: Open-vocabulary object detection in remote sensing commonly relies on text-only prompting to specify target categories, implicitly assuming that inference-time category queries can be reliably grounded through pretraining-induced text-visual alignment. In practice, this assumption often breaks down in remote sensing scenarios due to task- and application-specific category semantics, resulting in unstable category specification under open-vocabulary settings. To address this limitation, we propose RS-MPOD, a multimodal open-vocabulary detection framework that reformulates category specification beyond text-only prompting by incorporating instance-grounded visual prompts, textual prompts, and their multimodal integration. RS-MPOD introduces a visual prompt encoder to extract appearance-based category cues from exemplar instances, enabling text-free category specification, and a multimodal fusion module to integrate visual and textual information when both modalities are available. Extensive experiments on standard, cross-dataset, and fine-grained remote sensing benchmarks show that visual prompting yields more reliable category specification under semantic ambiguity and distribution shifts, while multimodal prompting provides a flexible alternative that remains competitive when textual semantics are well aligned.

</details>


### [237] [Toxicity Assessment in Preclinical Histopathology via Class-Aware Mahalanobis Distance for Known and Novel Anomalies](https://arxiv.org/abs/2602.02124)
*Olga Graf,Dhrupal Patel,Peter Groß,Charlotte Lempp,Matthias Hein,Fabian Heinemann*

Main category: cs.CV

TL;DR: AI anomaly detection framework for histopathology identifies both known and rare pathologies in rodent liver tissue using Vision Transformer segmentation and Mahalanobis distance OOD detection.


<details>
  <summary>Details</summary>
Motivation: Drug-induced toxicity causes high failure rates in preclinical development. Histopathology is gold standard but bottlenecked by expert pathologists. Need automated large-scale screening to detect adverse effects early and reduce attrition.

Method: Created pixelwise annotations of healthy tissue and known pathologies. Fine-tuned pre-trained Vision Transformer (DINOv2) via LoRA for tissue segmentation. Used Mahalanobis distance for OOD detection with class-specific thresholds optimized via mean of false negative/positive rates.

Result: Achieved high accuracy: only 0.16% pathological tissue classified as healthy and 0.35% healthy tissue classified as pathological. Framework successfully detected anomalies including rare OOD morphologies in mouse liver WSIs with known toxicological findings.

Conclusion: AI-driven histopathology can support preclinical workflows, reduce late-stage failures, and improve efficiency in drug development by automating toxicity detection in histopathological images.

Abstract: Drug-induced toxicity remains a leading cause of failure in preclinical development and early clinical trials. Detecting adverse effects at an early stage is critical to reduce attrition and accelerate the development of safe medicines. Histopathological evaluation remains the gold standard for toxicity assessment, but it relies heavily on expert pathologists, creating a bottleneck for large-scale screening. To address this challenge, we introduce an AI-based anomaly detection framework for histopathological whole-slide images (WSIs) in rodent livers from toxicology studies. The system identifies healthy tissue and known pathologies (anomalies) for which training data is available. In addition, it can detect rare pathologies without training data as out-of-distribution (OOD) findings. We generate a novel dataset of pixelwise annotations of healthy tissue and known pathologies and use this data to fine-tune a pre-trained Vision Transformer (DINOv2) via Low-Rank Adaptation (LoRA) in order to do tissue segmentation. Finally, we extract features for OOD detection using the Mahalanobis distance. To better account for class-dependent variability in histological data, we propose the use of class-specific thresholds. We optimize the thresholds using the mean of the false negative and false positive rates, resulting in only 0.16\% of pathological tissue classified as healthy and 0.35\% of healthy tissue classified as pathological. Applied to mouse liver WSIs with known toxicological findings, the framework accurately detects anomalies, including rare OOD morphologies. This work demonstrates the potential of AI-driven histopathology to support preclinical workflows, reduce late-stage failures, and improve efficiency in drug development.

</details>


### [238] [Vision-DeepResearch Benchmark: Rethinking Visual and Textual Search for Multimodal Large Language Models](https://arxiv.org/abs/2602.02185)
*Yu Zeng,Wenxuan Huang,Zhen Fang,Shuang Chen,Yufan Shen,Yishuo Cai,Xiaoman Wang,Zhenfei Yin,Lin Chen,Zehui Chen,Shiting Huang,Yiming Zhao,Yao Hu,Philip Torr,Wanli Ouyang,Shaosheng Cao*

Main category: cs.CV

TL;DR: VDR-Bench is a new benchmark for evaluating multimodal deep-research systems, addressing limitations in existing evaluations by creating 2,000 carefully curated VQA instances that require genuine visual search and present realistic challenges.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for multimodal deep-research systems have two major limitations: they're not truly visual search-centric (answers can be leaked through textual cues or prior knowledge), and they present overly idealized evaluation scenarios that don't reflect real-world complexity.

Method: Constructed VDR-Bench with 2,000 VQA instances using a multi-stage curation pipeline and expert review. Also proposed a multi-round cropped-search workflow to improve visual retrieval capabilities in current MLLMs.

Result: The benchmark provides realistic evaluation of Vision-DeepResearch systems, and the proposed multi-round cropped-search workflow effectively improves model performance in realistic visual retrieval scenarios.

Conclusion: VDR-Bench addresses critical gaps in evaluating multimodal deep-research systems and provides practical guidance for future system design, with the multi-round cropped-search workflow offering a simple yet effective improvement for visual retrieval.

Abstract: Multimodal Large Language Models (MLLMs) have advanced VQA and now support Vision-DeepResearch systems that use search engines for complex visual-textual fact-finding. However, evaluating these visual and textual search abilities is still difficult, and existing benchmarks have two major limitations. First, existing benchmarks are not visual search-centric: answers that should require visual search are often leaked through cross-textual cues in the text questions or can be inferred from the prior world knowledge in current MLLMs. Second, overly idealized evaluation scenario: On the image-search side, the required information can often be obtained via near-exact matching against the full image, while the text-search side is overly direct and insufficiently challenging. To address these issues, we construct the Vision-DeepResearch benchmark (VDR-Bench) comprising 2,000 VQA instances. All questions are created via a careful, multi-stage curation pipeline and rigorous expert review, designed to assess the behavior of Vision-DeepResearch systems under realistic real-world conditions. Moreover, to address the insufficient visual retrieval capabilities of current MLLMs, we propose a simple multi-round cropped-search workflow. This strategy is shown to effectively improve model performance in realistic visual retrieval scenarios. Overall, our results provide practical guidance for the design of future multimodal deep-research systems. The code will be released in https://github.com/Osilly/Vision-DeepResearch.

</details>


### [239] [Enhancing Multi-Image Understanding through Delimiter Token Scaling](https://arxiv.org/abs/2602.01984)
*Minyoung Lee,Yeji Park,Dongjun Hwang,Yejin Kim,Seong Joon Oh,Junsuk Choe*

Main category: cs.CV

TL;DR: Scaling delimiter token hidden states in LVLMs to reduce cross-image information leakage and improve multi-image reasoning without extra training or inference cost.


<details>
  <summary>Details</summary>
Motivation: LVLMs perform well on single-image tasks but struggle with multiple images due to cross-image information leakage, where models fail to distinguish information across different images despite using delimiter tokens.

Method: Propose scaling the hidden states of delimiter tokens to enhance their effectiveness in blocking cross-image information leakage, reinforcing intra-image interaction while limiting undesired cross-image interactions.

Result: Performance gains on multi-image benchmarks (Mantis, MuirBench, MIRB, QBench2) and text-only multi-document/table understanding benchmarks (TQABench, MultiNews, WCEP-10) without additional training or inference cost.

Conclusion: Simple scaling of delimiter token hidden states effectively reduces cross-image information leakage in LVLMs, improving multi-image reasoning and multi-document understanding while maintaining efficiency.

Abstract: Large Vision-Language Models (LVLMs) achieve strong performance on single-image tasks, but their performance declines when multiple images are provided as input. One major reason is the cross-image information leakage, where the model struggles to distinguish information across different images. Existing LVLMs already employ delimiter tokens to mark the start and end of each image, yet our analysis reveals that these tokens fail to effectively block cross-image information leakage. To enhance their effectiveness, we propose a method that scales the hidden states of delimiter tokens. This enhances the model's ability to preserve image-specific information by reinforcing intra-image interaction and limiting undesired cross-image interactions. Consequently, the model is better able to distinguish between images and reason over them more accurately. Experiments show performance gains on multi-image benchmarks such as Mantis, MuirBench, MIRB, and QBench2. We further evaluate our method on text-only tasks that require clear distinction. The method improves performance on multi-document and multi-table understanding benchmarks, including TQABench, MultiNews, and WCEP-10. Notably, our method requires no additional training or inference cost.

</details>


### [240] [Leveraging Latent Vector Prediction for Localized Control in Image Generation via Diffusion Models](https://arxiv.org/abs/2602.01991)
*Pablo Domingo-Gregorio,Javier Ruiz-Hidalgo*

Main category: cs.CV

TL;DR: A novel diffusion model method for precise local control over user-defined image regions while letting the model autonomously generate remaining areas according to the original prompt.


<details>
  <summary>Details</summary>
Motivation: Current text-to-image diffusion models lack precise localized control - existing methods apply conditions uniformly across entire images, limiting ability to control specific regions while maintaining overall prompt coherence.

Method: Introduces a training framework with masking features and an additional loss term that leverages prediction of initial latent vectors at any diffusion step to enhance correspondence between current step and final sample in latent space.

Result: Extensive experiments demonstrate effective synthesis of high-quality images with controlled local conditions, enabling precise regional control while maintaining overall prompt coherence.

Conclusion: The proposed method successfully addresses the limitation of uniform conditioning in diffusion models by enabling precise local control over user-defined regions while preserving the model's ability to autonomously generate remaining areas according to the original prompt.

Abstract: Diffusion models emerged as a leading approach in text-to-image generation, producing high-quality images from textual descriptions. However, attempting to achieve detailed control to get a desired image solely through text remains a laborious trial-and-error endeavor. Recent methods have introduced image-level controls alongside with text prompts, using prior images to extract conditional information such as edges, segmentation and depth maps. While effective, these methods apply conditions uniformly across the entire image, limiting localized control. In this paper, we propose a novel methodology to enable precise local control over user-defined regions of an image, while leaving to the diffusion model the task of autonomously generating the remaining areas according to the original prompt. Our approach introduces a new training framework that incorporates masking features and an additional loss term, which leverages the prediction of the initial latent vector at any diffusion step to enhance the correspondence between the current step and the final sample in the latent space. Extensive experiments demonstrate that our method effectively synthesizes high-quality images with controlled local conditions.

</details>


### [241] [VQ-Style: Disentangling Style and Content in Motion with Residual Quantized Representations](https://arxiv.org/abs/2602.02334)
*Fatemeh Zargarbashi,Dhruv Agrawal,Jakob Buhmann,Martin Guay,Stelian Coros,Robert W. Sumner*

Main category: cs.CV

TL;DR: A novel method for disentangling style and content in human motion data using RVQ-VAEs with contrastive learning and information leakage loss, enabling style transfer without fine-tuning via Quantized Code Swapping.


<details>
  <summary>Details</summary>
Motivation: Human motion data contains rich semantic content and subtle stylistic features that are challenging to model and disentangle, making style transfer difficult.

Method: Uses Residual Vector Quantized Variational Autoencoders (RVQ-VAEs) for coarse-to-fine motion representation, enhanced with contrastive learning and information leakage loss for disentanglement, plus Quantized Code Swapping for inference-time style transfer.

Result: The framework demonstrates strong versatility across multiple applications including style transfer, style removal, and motion blending without requiring fine-tuning for unseen styles.

Conclusion: The proposed approach effectively disentangles style and content in human motion, enabling flexible style manipulation through a simple inference-time technique.

Abstract: Human motion data is inherently rich and complex, containing both semantic content and subtle stylistic features that are challenging to model. We propose a novel method for effective disentanglement of the style and content in human motion data to facilitate style transfer. Our approach is guided by the insight that content corresponds to coarse motion attributes while style captures the finer, expressive details. To model this hierarchy, we employ Residual Vector Quantized Variational Autoencoders (RVQ-VAEs) to learn a coarse-to-fine representation of motion. We further enhance the disentanglement by integrating contrastive learning and a novel information leakage loss with codebook learning to organize the content and the style across different codebooks. We harness this disentangled representation using our simple and effective inference-time technique Quantized Code Swapping, which enables motion style transfer without requiring any fine-tuning for unseen styles. Our framework demonstrates strong versatility across multiple inference applications, including style transfer, style removal, and motion blending.

</details>


### [242] [UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving](https://arxiv.org/abs/2602.02002)
*Guosheng Zhao,Yaozeng Wang,Xiaofeng Wang,Zheng Zhu,Tingdong Yu,Guan Huang,Yongchen Zai,Ji Jiao,Changliang Xue,Xiaole Wang,Zhen Yang,Futang Zhu,Xingang Wang*

Main category: cs.CV

TL;DR: UniDriveDreamer: A unified multimodal world model for autonomous driving that jointly generates future camera videos and LiDAR sequences in a single stage without intermediate representations.


<details>
  <summary>Details</summary>
Motivation: Existing world models for autonomous driving focus on single-modality generation (either camera video or LiDAR), lacking unified multimodal synthesis capabilities needed for comprehensive perception.

Method: Proposes a single-stage unified framework with: 1) LiDAR-specific VAE and video VAE for encoding inputs, 2) Unified Latent Anchoring (ULA) to align multimodal latent distributions, 3) Diffusion transformer for joint modeling of geometric correspondence and temporal evolution, 4) Structured scene layout conditioning per modality.

Result: Outperforms previous state-of-the-art methods in both video and LiDAR generation, and yields measurable improvements in downstream tasks.

Conclusion: UniDriveDreamer demonstrates effective unified multimodal world modeling for autonomous driving, enabling joint synthesis of camera and LiDAR observations without cascaded modules or intermediate representations.

Abstract: World models have demonstrated significant promise for data synthesis in autonomous driving. However, existing methods predominantly concentrate on single-modality generation, typically focusing on either multi-camera video or LiDAR sequence synthesis. In this paper, we propose UniDriveDreamer, a single-stage unified multimodal world model for autonomous driving, which directly generates multimodal future observations without relying on intermediate representations or cascaded modules. Our framework introduces a LiDAR-specific variational autoencoder (VAE) designed to encode input LiDAR sequences, alongside a video VAE for multi-camera images. To ensure cross-modal compatibility and training stability, we propose Unified Latent Anchoring (ULA), which explicitly aligns the latent distributions of the two modalities. The aligned features are fused and processed by a diffusion transformer that jointly models their geometric correspondence and temporal evolution. Additionally, structured scene layout information is projected per modality as a conditioning signal to guide the synthesis. Extensive experiments demonstrate that UniDriveDreamer outperforms previous state-of-the-art methods in both video and LiDAR generation, while also yielding measurable improvements in downstream

</details>


### [243] [Implicit neural representation of textures](https://arxiv.org/abs/2602.02354)
*Albert Kwok,Zheyuan Hu,Dounia Hammou*

Main category: cs.CV

TL;DR: The paper explores using implicit neural representations (INRs) as continuous texture representations, analyzing their performance in image quality, memory usage, and rendering speed, while investigating applications in real-time rendering and downstream tasks.


<details>
  <summary>Details</summary>
Motivation: To develop neural networks as continuous texture representations that operate in continuous UV coordinate space rather than discrete representations, addressing limitations of traditional texture mapping approaches.

Method: Design different neural networks as texture INRs that operate continuously over UV coordinate space, conducting thorough experiments to evaluate performance across multiple objectives including image quality, memory usage, and rendering inference time.

Result: The INRs perform well in terms of image quality while maintaining considerable memory efficiency and rendering speed, with analysis of the trade-offs between these objectives.

Conclusion: INRs provide effective continuous texture representations with good performance characteristics, enabling various applications in real-time rendering and downstream tasks such as mipmap fitting and INR-space generation.

Abstract: Implicit neural representation (INR) has proven to be accurate and efficient in various domains. In this work, we explore how different neural networks can be designed as a new texture INR, which operates in a continuous manner rather than a discrete one over the input UV coordinate space. Through thorough experiments, we demonstrate that these INRs perform well in terms of image quality, with considerable memory usage and rendering inference time. We analyze the balance between these objectives. In addition, we investigate various related applications in real-time rendering and down-stream tasks, e.g. mipmap fitting and INR-space generation.

</details>


### [244] [Infinite-World: Scaling Interactive World Models to 1000-Frame Horizons via Pose-Free Hierarchical Memory](https://arxiv.org/abs/2602.02393)
*Ruiqi Wu,Xuanhua He,Meng Cheng,Tianyu Yang,Yong Zhang,Zhuoliang Kang,Xunliang Cai,Xiaoming Wei,Chunle Guo,Chongyi Li,Ming-Ming Cheng*

Main category: cs.CV

TL;DR: Infinite-World is a robust interactive world model that maintains coherent visual memory over 1000+ frames in complex real-world environments without relying on explicit geometric priors.


<details>
  <summary>Details</summary>
Motivation: Existing world models work well on synthetic data with perfect ground-truth but lack effective training paradigms for real-world videos due to noisy pose estimations and scarcity of viewpoint revisits.

Method: Three key components: 1) Hierarchical Pose-free Memory Compressor (HPMC) that recursively distills historical latents into fixed-budget representation, 2) Uncertainty-aware Action Labeling that discretizes continuous motion into tri-state logic, and 3) Revisit-Dense Finetuning Strategy using compact dataset to activate long-range loop-closure capabilities.

Result: Extensive experiments including objective metrics and user studies demonstrate superior performance in visual quality, action controllability, and spatial consistency compared to existing methods.

Conclusion: Infinite-World successfully bridges the gap between synthetic and real-world video training, enabling robust interactive world modeling with long-term coherent visual memory in complex environments without geometric priors.

Abstract: We propose Infinite-World, a robust interactive world model capable of maintaining coherent visual memory over 1000+ frames in complex real-world environments. While existing world models can be efficiently optimized on synthetic data with perfect ground-truth, they lack an effective training paradigm for real-world videos due to noisy pose estimations and the scarcity of viewpoint revisits. To bridge this gap, we first introduce a Hierarchical Pose-free Memory Compressor (HPMC) that recursively distills historical latents into a fixed-budget representation. By jointly optimizing the compressor with the generative backbone, HPMC enables the model to autonomously anchor generations in the distant past with bounded computational cost, eliminating the need for explicit geometric priors. Second, we propose an Uncertainty-aware Action Labeling module that discretizes continuous motion into a tri-state logic. This strategy maximizes the utilization of raw video data while shielding the deterministic action space from being corrupted by noisy trajectories, ensuring robust action-response learning. Furthermore, guided by insights from a pilot toy study, we employ a Revisit-Dense Finetuning Strategy using a compact, 30-minute dataset to efficiently activate the model's long-range loop-closure capabilities. Extensive experiments, including objective metrics and user studies, demonstrate that Infinite-World achieves superior performance in visual quality, action controllability, and spatial consistency.

</details>


### [245] [ReasonEdit: Editing Vision-Language Models using Human Reasoning](https://arxiv.org/abs/2602.02408)
*Jiaxing Qiu,Kaihua Hou,Roxana Daneshjou,Ahmed Alaa,Thomas Hartvigsen*

Main category: cs.CV

TL;DR: ReasonEdit is the first VLM editor that incorporates human reasoning during editing, achieving SOTA performance on reasoning-heavy visual question answering tasks by storing reasoning in a codebook and retrieving relevant facts using topology-balanced multimodal embeddings.


<details>
  <summary>Details</summary>
Motivation: Existing vision-language model editors don't handle reasoning-heavy tasks that require both humans and models to reason about images. There's a need for practical model editing that incorporates human reasoning explanations.

Method: ReasonEdit stores human reasoning in a codebook and retrieves relevant facts during inference using a novel topology-balanced multimodal embedding method inspired by network science. It allows users to explain their reasoning during editing.

Result: Across four VLMs on multiple rationale-based visual question answering datasets, ReasonEdit achieves state-of-the-art editing performance, showing that using human reasoning during editing greatly improves edit generalization.

Conclusion: ReasonEdit introduces a practical model editing setup that successfully incorporates human reasoning, demonstrating that reasoning-based editing significantly enhances edit generalization for vision-language models on complex reasoning tasks.

Abstract: Model editing aims to correct errors in large, pretrained models without altering unrelated behaviors. While some recent works have edited vision-language models (VLMs), no existing editors tackle reasoning-heavy tasks, which typically require humans and models to reason about images.We therefore propose ReasonEdit, the first VLM editor to let users explain their reasoning during editing, introducing a new, practical model editing setup. ReasonEdit continuously stores human reasoning in a codebook, and retrieves only relevant facts during inference using a novel topology-balanced multimodal embedding method inspired by network science. Across four VLMs on multiple rationale-based visual question answering datasets, ReasonEdit achieves state-of-the-art editing performance, ultimately showing that using human reasoning during editing greatly improves edit generalization.

</details>


### [246] [UniReason 1.0: A Unified Reasoning Framework for World Knowledge Aligned Image Generation and Editing](https://arxiv.org/abs/2602.02437)
*Dianyi Wang,Chaofan Ma,Feng Han,Size Wu,Wei Song,Yibin Wang,Zhixiong Zhang,Tianhang Wang,Siyuan Wang,Zhongyu Wei,Jiaqi Wang*

Main category: cs.CV

TL;DR: UniReason is a unified multimodal framework that connects text-to-image generation and image editing through a dual reasoning paradigm, treating them as interconnected cognitive steps rather than isolated tasks.


<details>
  <summary>Details</summary>
Motivation: Current unified multimodal models struggle with complex synthesis tasks requiring deep reasoning and treat text-to-image generation and image editing as separate capabilities rather than interconnected reasoning steps.

Method: Proposes UniReason with a dual reasoning paradigm: 1) generation as world knowledge-enhanced planning with implicit constraints, and 2) editing for fine-grained visual refinement via self-reflection. Unifies both tasks in a shared representation mirroring human cognitive processes. Constructs a large-scale reasoning-centric dataset (~300k samples) covering five knowledge domains for planning and an agent-generated corpus for visual self-correction.

Result: UniReason achieves advanced performance on reasoning-intensive benchmarks (WISE, KrisBench, UniREditBench) while maintaining superior general synthesis capabilities.

Conclusion: The unified framework successfully harmonizes generation and editing through cognitive-inspired reasoning, demonstrating improved performance on complex multimodal synthesis tasks requiring deep reasoning.

Abstract: Unified multimodal models often struggle with complex synthesis tasks that demand deep reasoning, and typically treat text-to-image generation and image editing as isolated capabilities rather than interconnected reasoning steps. To address this, we propose UniReason, a unified framework that harmonizes these two tasks through a dual reasoning paradigm. We formulate generation as world knowledge-enhanced planning to inject implicit constraints, and leverage editing capabilities for fine-grained visual refinement to further correct visual errors via self-reflection. This approach unifies generation and editing within a shared representation, mirroring the human cognitive process of planning followed by refinement. We support this framework by systematically constructing a large-scale reasoning-centric dataset (~300k samples) covering five major knowledge domains (e.g., cultural commonsense, physics, etc.) for planning, alongside an agent-generated corpus for visual self-correction. Extensive experiments demonstrate that UniReason achieves advanced performance on reasoning-intensive benchmarks such as WISE, KrisBench and UniREditBench, while maintaining superior general synthesis capabilities.

</details>


### [247] [UrbanGS: A Scalable and Efficient Architecture for Geometrically Accurate Large-Scene Reconstruction](https://arxiv.org/abs/2602.02089)
*Changbai Li,Haodong Zhu,Hanlin Chen,Xiuping Liang,Tongfei Chen,Shuwei Shao,Linlin Yang,Huobin Tan,Baochang Zhang*

Main category: cs.CV

TL;DR: UrbanGS is a scalable 3D Gaussian Splatting framework for large-scale urban scenes that improves geometric consistency with depth-normal regularization and enhances memory efficiency with adaptive Gaussian pruning.


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting works well for bounded scenes but faces challenges in large-scale urban environments: geometric inconsistency, memory inefficiency, and computational scalability issues.

Method: 1) Depth-Consistent D-Normal Regularization with external depth supervision and adaptive confidence weighting; 2) Spatially Adaptive Gaussian Pruning based on local geometric complexity; 3) Unified partitioning and view assignment scheme to eliminate boundary artifacts.

Result: Extensive experiments on multiple urban datasets show superior performance in rendering quality, geometric accuracy, and memory efficiency compared to existing approaches.

Conclusion: UrbanGS provides a systematic solution for high-fidelity large-scale scene reconstruction by addressing the key challenges of geometric consistency, memory efficiency, and computational scalability in urban environments.

Abstract: While 3D Gaussian Splatting (3DGS) enables high-quality, real-time rendering for bounded scenes, its extension to large-scale urban environments gives rise to critical challenges in terms of geometric consistency, memory efficiency, and computational scalability. To address these issues, we present UrbanGS, a scalable reconstruction framework that effectively tackles these challenges for city-scale applications. First, we propose a Depth-Consistent D-Normal Regularization module. Unlike existing approaches that rely solely on monocular normal estimators, which can effectively update rotation parameters yet struggle to update position parameters, our method integrates D-Normal constraints with external depth supervision. This allows for comprehensive updates of all geometric parameters. By further incorporating an adaptive confidence weighting mechanism based on gradient consistency and inverse depth deviation, our approach significantly enhances multi-view depth alignment and geometric coherence, which effectively resolves the issue of geometric accuracy in complex large-scale scenes. To improve scalability, we introduce a Spatially Adaptive Gaussian Pruning (SAGP) strategy, which dynamically adjusts Gaussian density based on local geometric complexity and visibility to reduce redundancy. Additionally, a unified partitioning and view assignment scheme is designed to eliminate boundary artifacts and optimize computational load. Extensive experiments on multiple urban datasets demonstrate that UrbanGS achieves superior performance in rendering quality, geometric accuracy, and memory efficiency, providing a systematic solution for high-fidelity large-scale scene reconstruction.

</details>


### [248] [Multi-head automated segmentation by incorporating detection head into the contextual layer neural network](https://arxiv.org/abs/2602.02471)
*Edwin Kys,Febian Febian*

Main category: cs.CV

TL;DR: Gated multi-head Transformer architecture for radiotherapy auto-segmentation that uses slice-level detection to gate segmentation predictions, eliminating false positives in slices without target structures.


<details>
  <summary>Details</summary>
Motivation: Conventional deep learning auto-segmentation models often produce anatomically implausible false positives (hallucinations) in slices lacking target structures, which is problematic for clinical radiotherapy workflows.

Method: Proposed gated multi-head Transformer based on Swin U-Net with inter-slice context integration and parallel detection head. Jointly performs slice-level structure detection via MLP and pixel-level segmentation through context-enhanced stream. Detection outputs gate segmentation predictions to suppress false positives in anatomically invalid slices. Training uses slice-wise Tversky loss to address class imbalance.

Result: Gated model substantially outperforms non-gated baseline on Prostate-Anatomical-Edge-Cases dataset: mean Dice loss of 0.013 ± 0.036 vs 0.732 ± 0.314. Detection probabilities strongly correlated with anatomical presence, effectively eliminating spurious segmentations. Non-gated model exhibited higher variability and persistent false positives across all slices.

Conclusion: Detection-based gating enhances robustness and anatomical plausibility in automated segmentation, reducing hallucinated predictions without compromising segmentation quality in valid slices. Offers promising approach for improving reliability of clinical radiotherapy auto-contouring workflows.

Abstract: Deep learning based auto segmentation is increasingly used in radiotherapy, but conventional models often produce anatomically implausible false positives, or hallucinations, in slices lacking target structures. We propose a gated multi-head Transformer architecture based on Swin U-Net, augmented with inter-slice context integration and a parallel detection head, which jointly performs slice-level structure detection via a multi-layer perceptron and pixel-level segmentation through a context-enhanced stream. Detection outputs gate the segmentation predictions to suppress false positives in anatomically invalid slices, and training uses slice-wise Tversky loss to address class imbalance. Experiments on the Prostate-Anatomical-Edge-Cases dataset from The Cancer Imaging Archive demonstrate that the gated model substantially outperforms a non-gated segmentation-only baseline, achieving a mean Dice loss of $0.013 \pm 0.036$ versus $0.732 \pm 0.314$, with detection probabilities strongly correlated with anatomical presence, effectively eliminating spurious segmentations. In contrast, the non-gated model exhibited higher variability and persistent false positives across all slices. These results indicate that detection-based gating enhances robustness and anatomical plausibility in automated segmentation applications, reducing hallucinated predictions without compromising segmentation quality in valid slices, and offers a promising approach for improving the reliability of clinical radiotherapy auto-contouring workflows.

</details>


### [249] [FSVideo: Fast Speed Video Diffusion Model in a Highly-Compressed Latent Space](https://arxiv.org/abs/2602.02092)
*FSVideo Team,Qingyu Chen,Zhiyuan Fang,Haibin Huang,Xinwei Huang,Tong Jin,Minxuan Lin,Bo Liu,Celong Liu,Chongyang Ma,Xing Mei,Xiaohui Shen,Yaojie Shen,Fuwen Tan,Angtian Wang,Xiao Yang,Yiding Yang,Jiamin Yuan,Lingxi Zhang,Yuxin Zhang*

Main category: cs.CV

TL;DR: FSVideo is a fast transformer-based image-to-video diffusion framework with highly-compressed latent space, memory-enhanced diffusion transformers, and multi-resolution generation, achieving competitive quality with 10x speedup.


<details>
  <summary>Details</summary>
Motivation: To create a fast and efficient image-to-video generation framework that maintains competitive quality while being significantly faster than existing open-source models.

Method: Three key components: 1) Highly-compressed video autoencoder (64×64×4 spatial-temporal downsampling), 2) Diffusion transformer with layer memory design for better inter-layer information flow, 3) Multi-resolution generation using few-step DIT upsampler.

Result: Achieves competitive performance against popular open-source models while being an order of magnitude faster, using a 14B DIT base model and 14B DIT upsampler.

Conclusion: FSVideo demonstrates that efficient architectural designs (compressed latent space, memory-enhanced transformers, multi-resolution generation) can enable fast and high-quality image-to-video synthesis.

Abstract: We introduce FSVideo, a fast speed transformer-based image-to-video (I2V) diffusion framework. We build our framework on the following key components: 1.) a new video autoencoder with highly-compressed latent space ($64\times64\times4$ spatial-temporal downsampling ratio), achieving competitive reconstruction quality; 2.) a diffusion transformer (DIT) architecture with a new layer memory design to enhance inter-layer information flow and context reuse within DIT, and 3.) a multi-resolution generation strategy via a few-step DIT upsampler to increase video fidelity. Our final model, which contains a 14B DIT base model and a 14B DIT upsampler, achieves competitive performance against other popular open-source models, while being an order of magnitude faster. We discuss our model design as well as training strategies in this report.

</details>


### [250] [Teacher-Guided Student Self-Knowledge Distillation Using Diffusion Model](https://arxiv.org/abs/2602.02107)
*Yu Wang,Chuanguang Yang,Zhulin An,Weilun Feng,Jiarui Zhao,Chengqing Yu,Libo Huang,Boyu Diao,Yongjun Xu*

Main category: cs.CV

TL;DR: DSKD proposes a novel teacher-guided student diffusion self-KD method that uses teacher classifier to guide denoising of student features via diffusion model, then performs LSH-guided distillation between original and denoised student features to eliminate teacher-student distribution discrepancies.


<details>
  <summary>Details</summary>
Motivation: Existing KD methods suffer from incompatible information transfer due to differences in feature distributions between teacher and student models, leading to suboptimal knowledge transfer.

Method: 1) Use teacher classifier to guide sampling process of denoising student features through lightweight diffusion model; 2) Propose LSH-guided feature distillation between original and denoised student features; 3) Treat denoised student features as teacher role to eliminate distribution discrepancies.

Result: DSKD significantly outperforms existing KD methods across various models and datasets on visual recognition tasks.

Conclusion: The proposed diffusion-based self-KD approach effectively eliminates teacher-student feature distribution discrepancies while learning meaningful knowledge, demonstrating superior performance over traditional KD methods.

Abstract: Existing Knowledge Distillation (KD) methods often align feature information between teacher and student by exploring meaningful feature processing and loss functions. However, due to the difference in feature distributions between the teacher and student, the student model may learn incompatible information from the teacher. To address this problem, we propose teacher-guided student Diffusion Self-KD, dubbed as DSKD. Instead of the direct teacher-student alignment, we leverage the teacher classifier to guide the sampling process of denoising student features through a light-weight diffusion model. We then propose a novel locality-sensitive hashing (LSH)-guided feature distillation method between the original and denoised student features. The denoised student features encapsulate teacher knowledge and could be regarded as a teacher role. In this way, our DSKD method could eliminate discrepancies in mapping manners and feature distributions between the teacher and student, while learning meaningful knowledge from the teacher. Experiments on visual recognition tasks demonstrate that DSKD significantly outperforms existing KD methods across various models and datasets. Our code is attached in supplementary material.

</details>


### [251] [PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss](https://arxiv.org/abs/2602.02493)
*Zehong Ma,Ruihan Xu,Shiliang Zhang*

Main category: cs.CV

TL;DR: PixelGen introduces perceptual supervision to pixel diffusion, using LPIPS and DINO losses to guide learning of meaningful perceptual manifolds instead of full image manifolds, outperforming latent diffusion models without needing VAEs or auxiliary stages.


<details>
  <summary>Details</summary>
Motivation: Pixel diffusion avoids VAE artifacts but struggles with high-dimensional pixel manifolds containing perceptually irrelevant signals, causing it to lag behind latent diffusion models. The authors aim to create a simpler yet more powerful pixel-based generative paradigm.

Method: PixelGen introduces two complementary perceptual losses: LPIPS loss for better local patterns and DINO-based perceptual loss for stronger global semantics. This guides the diffusion model to learn a more meaningful perceptual manifold rather than the full image manifold.

Result: Achieves FID of 5.11 on ImageNet-256 without classifier-free guidance using only 80 training epochs, and demonstrates favorable scaling performance on large-scale text-to-image generation with GenEval score of 0.79, surpassing strong latent diffusion baselines.

Conclusion: PixelGen provides a simpler yet more powerful generative paradigm that requires no VAEs, no latent representations, and no auxiliary stages, offering an end-to-end pixel diffusion approach with perceptual supervision that outperforms existing methods.

Abstract: Pixel diffusion generates images directly in pixel space in an end-to-end manner, avoiding the artifacts and bottlenecks introduced by VAEs in two-stage latent diffusion. However, it is challenging to optimize high-dimensional pixel manifolds that contain many perceptually irrelevant signals, leaving existing pixel diffusion methods lagging behind latent diffusion models. We propose PixelGen, a simple pixel diffusion framework with perceptual supervision. Instead of modeling the full image manifold, PixelGen introduces two complementary perceptual losses to guide diffusion model towards learning a more meaningful perceptual manifold. An LPIPS loss facilitates learning better local patterns, while a DINO-based perceptual loss strengthens global semantics. With perceptual supervision, PixelGen surpasses strong latent diffusion baselines. It achieves an FID of 5.11 on ImageNet-256 without classifier-free guidance using only 80 training epochs, and demonstrates favorable scaling performance on large-scale text-to-image generation with a GenEval score of 0.79. PixelGen requires no VAEs, no latent representations, and no auxiliary stages, providing a simpler yet more powerful generative paradigm. Codes are publicly available at https://github.com/Zehong-Ma/PixelGen.

</details>


### [252] [Enhancing Diffusion-Based Quantitatively Controllable Image Generation via Matrix-Form EDM and Adaptive Vicinal Training](https://arxiv.org/abs/2602.02114)
*Xin Ding,Yun Chen,Sen Zhang,Kao Zhang,Nenglun Chen,Peibei Cao,Yongwei Wang,Fei Wu*

Main category: cs.CV

TL;DR: iCCDM improves upon CCDM by integrating the Elucidated Diffusion Model framework with novel matrix-form formulation and adaptive vicinal training, achieving better quality and efficiency than existing methods including large-scale text-to-image models.


<details>
  <summary>Details</summary>
Motivation: CCDM has limitations due to outdated diffusion framework and low sampling efficiency, and has been surpassed by GAN-based methods. Need to improve both generation quality and sampling efficiency for continuous conditional image generation.

Method: Proposes iCCDM framework incorporating Elucidated Diffusion Model (EDM) with substantial modifications, including novel matrix-form EDM formulation and adaptive vicinal training strategy.

Result: iCCDM consistently outperforms existing methods on four benchmark datasets (64×64 to 256×256 resolution), including state-of-the-art large-scale text-to-image diffusion models (Stable Diffusion 3, FLUX.1, Qwen-Image), achieving higher quality with significantly reduced sampling cost.

Conclusion: iCCDM successfully addresses CCDM's limitations by integrating advanced EDM framework, demonstrating superior performance in continuous conditional image generation with improved efficiency and quality.

Abstract: Continuous Conditional Diffusion Model (CCDM) is a diffusion-based framework designed to generate high-quality images conditioned on continuous regression labels. Although CCDM has demonstrated clear advantages over prior approaches across a range of datasets, it still exhibits notable limitations and has recently been surpassed by a GAN-based method, namely CcGAN-AVAR. These limitations mainly arise from its reliance on an outdated diffusion framework and its low sampling efficiency due to long sampling trajectories. To address these issues, we propose an improved CCDM framework, termed iCCDM, which incorporates the more advanced \textit{Elucidated Diffusion Model} (EDM) framework with substantial modifications to improve both generation quality and sampling efficiency. Specifically, iCCDM introduces a novel matrix-form EDM formulation together with an adaptive vicinal training strategy. Extensive experiments on four benchmark datasets, spanning image resolutions from $64\times64$ to $256\times256$, demonstrate that iCCDM consistently outperforms existing methods, including state-of-the-art large-scale text-to-image diffusion models (e.g., Stable Diffusion 3, FLUX.1, and Qwen-Image), achieving higher generation quality while significantly reducing sampling cost.

</details>


### [253] [MLV-Edit: Towards Consistent and Highly Efficient Editing for Minute-Level Videos](https://arxiv.org/abs/2602.02123)
*Yangyi Cao,Yuanhang Li,Lan Chen,Qi Mao*

Main category: cs.CV

TL;DR: MLV-Edit is a training-free, flow-based framework for minute-level video editing that addresses computational overhead and temporal consistency challenges through segment-wise editing with Velocity Blend and Attention Sink modules.


<details>
  <summary>Details</summary>
Motivation: Existing video editing techniques work well for short videos but struggle with minute-level videos due to prohibitive computational overhead and difficulty maintaining global temporal consistency across thousands of frames.

Method: Uses divide-and-conquer strategy for segment-wise editing with two core modules: Velocity Blend aligns flow fields at segment boundaries to eliminate flickering, and Attention Sink anchors local segment features to global reference frames to suppress structural drift.

Result: Extensive experiments show MLV-Edit consistently outperforms state-of-the-art methods in both temporal stability and semantic fidelity for minute-level video editing.

Conclusion: MLV-Edit provides an effective training-free solution for long-duration video editing that maintains temporal consistency and semantic fidelity while being computationally efficient.

Abstract: We propose MLV-Edit, a training-free, flow-based framework that address the unique challenges of minute-level video editing. While existing techniques excel in short-form video manipulation, scaling them to long-duration videos remains challenging due to prohibitive computational overhead and the difficulty of maintaining global temporal consistency across thousands of frames. To address this, MLV-Edit employs a divide-and-conquer strategy for segment-wise editing, facilitated by two core modules: Velocity Blend rectifies motion inconsistencies at segment boundaries by aligning the flow fields of adjacent chunks, eliminating flickering and boundary artifacts commonly observed in fragmented video processing; and Attention Sink anchors local segment features to global reference frames, effectively suppressing cumulative structural drift. Extensive quantitative and qualitative experiments demonstrate that MLV-Edit consistently outperforms state-of-the-art methods in terms of temporal stability and semantic fidelity.

</details>


### [254] [Eliminating Registration Bias in Synthetic CT Generation: A Physics-Based Simulation Framework](https://arxiv.org/abs/2602.02130)
*Lukas Zimmermann,Michael Rauter,Maximilian Schmid,Dietmar Georg,Barbara Knäusl*

Main category: cs.CV

TL;DR: Using physics-based CBCT simulation for aligned training pairs improves geometric fidelity in synthetic CT generation, outperforming conventional methods despite lower intensity scores.


<details>
  <summary>Details</summary>
Motivation: Registration bias in supervised CBCT-to-CT training corrupts models and evaluation metrics, making superior benchmark performance misleading as it may reproduce artifacts rather than anatomical fidelity.

Method: Proposed physics-based CBCT simulation to create geometrically aligned training pairs by construction, combined with evaluation using geometric alignment metrics (Normalized Mutual Information) against input CBCT rather than biased ground truth.

Result: Models trained on synthetic data achieved superior geometric alignment (NMI: 0.31 vs 0.22) despite lower conventional intensity scores. NMI consistently predicted observer preference while intensity metrics showed inverted correlations. Clinical observers preferred synthetic-trained outputs in 87% of cases.

Conclusion: Geometric fidelity, not intensity agreement with biased ground truth, aligns with clinical requirements. Physics-based simulation with geometric alignment metrics provides more clinically relevant evaluation and training for CBCT-to-CT synthesis.

Abstract: Supervised synthetic CT generation from CBCT requires registered training pairs, yet perfect registration between separately acquired scans remains unattainable. This registration bias propagates into trained models and corrupts standard evaluation metrics. This may suggest that superior benchmark performance indicates better reproduction of registration artifacts rather than anatomical fidelity. We propose physics-based CBCT simulation to provide geometrically aligned training pairs by construction, combined with evaluation using geometric alignment metrics against input CBCT rather than biased ground truth. On two independent pelvic datasets, models trained on synthetic data achieved superior geometric alignment (Normalized Mutual Information: 0.31 vs 0.22) despite lower conventional intensity scores. Intensity metrics showed inverted correlations with clinical assessment for deformably registered data, while Normalized Mutual Information consistently predicted observer preference across registration methodologies (rho = 0.31, p < 0.001). Clinical observers preferred synthetic-trained outputs in 87% of cases, demonstrating that geometric fidelity, not intensity agreement with biased ground truth, aligns with clinical requirements.

</details>


### [255] [Deep learning enables urban change profiling through alignment of historical maps](https://arxiv.org/abs/2602.02154)
*Sidi Wu,Yizi Chen,Maurizio Gribaudi,Konrad Schindler,Clément Mallet,Julien Perret,Lorenz Hurni*

Main category: cs.CV

TL;DR: Automated deep learning framework for analyzing fine-grained urban change from historical map collections using alignment, object detection, and change profiling.


<details>
  <summary>Details</summary>
Motivation: Historical maps offer unique records of long-term urban transformation but extracting consistent change information is challenging due to spatial misalignment, cartographic variation, and document quality degradation, limiting most analyses to small-scale or qualitative approaches.

Method: Proposes a fully automated deep learning-based framework with modular design integrating dense map alignment, multi-temporal object detection, and change profiling to systematically analyze urban transformation from historical map collections.

Result: Demonstrates robust performance of alignment and object detection methods, and when applied to Paris (1868-1937), reveals spatial and temporal heterogeneity in urban transformation, showing relevance for social sciences and humanities research.

Conclusion: The framework shifts historical map analysis from ad hoc visual comparison to systematic quantitative characterization of urban change, with modular design supporting adaptation to diverse cartographic contexts and downstream applications.

Abstract: Prior to modern Earth observation technologies, historical maps provide a unique record of long-term urban transformation and offer a lens on the evolving identity of cities. However, extracting consistent and fine-grained change information from historical map series remains challenging due to spatial misalignment, cartographic variation, and degrading document quality, limiting most analyses to small-scale or qualitative approaches. We propose a fully automated, deep learning-based framework for fine-grained urban change analysis from large collections of historical maps, built on a modular design that integrates dense map alignment, multi-temporal object detection, and change profiling. This framework shifts the analysis of historical maps from ad hoc visual comparison toward systematic, quantitative characterization of urban change. Experiments demonstrate the robust performance of the proposed alignment and object detection methods. Applied to Paris between 1868 and 1937, the framework reveals the spatial and temporal heterogeneity in urban transformation, highlighting its relevance for research in the social sciences and humanities. The modular design of our framework further supports adaptation to diverse cartographic contexts and downstream applications.

</details>


### [256] [LoopViT: Scaling Visual ARC with Looped Transformers](https://arxiv.org/abs/2602.02156)
*Wen-Jie Shu,Xuerui Qiu,Rui-Jie Zhu,Harold Haodong Chen,Yexin Liu,Harry Yang*

Main category: cs.CV

TL;DR: Loop-ViT: A recursive vision transformer with weight-tied recurrence and dynamic exit mechanism that outperforms larger models on ARC-AGI benchmark through adaptive iterative computation.


<details>
  <summary>Details</summary>
Motivation: Current feed-forward architectures for visual reasoning are limited because they bind computational depth to parameter size, failing to capture the iterative, algorithmic nature of human induction needed for tasks like ARC-AGI.

Method: Proposes Loop-ViT with weight-tied recurrence that decouples reasoning depth from model capacity. Uses a Hybrid Block combining local convolutions and global attention, and introduces a parameter-free Dynamic Exit mechanism based on predictive entropy to halt inference when uncertainty becomes low.

Result: The 18M parameter Loop-ViT model achieves 65.8% accuracy on ARC-AGI-1 benchmark, outperforming massive 73M-parameter ensembles, demonstrating superior efficiency and performance.

Conclusion: Adaptive iterative computation provides a more efficient scaling axis for visual reasoning than simply increasing network width, showing that recursive architectures with dynamic halting can better capture algorithmic reasoning processes.

Abstract: Recent advances in visual reasoning have leveraged vision transformers to tackle the ARC-AGI benchmark. However, we argue that the feed-forward architecture, where computational depth is strictly bound to parameter size, falls short of capturing the iterative, algorithmic nature of human induction. In this work, we propose a recursive architecture called Loop-ViT, which decouples reasoning depth from model capacity through weight-tied recurrence. Loop-ViT iterates a weight-tied Hybrid Block, combining local convolutions and global attention, to form a latent chain of thought. Crucially, we introduce a parameter-free Dynamic Exit mechanism based on predictive entropy: the model halts inference when its internal state ``crystallizes" into a low-uncertainty attractor. Empirical results on the ARC-AGI-1 benchmark validate this perspective: our 18M model achieves 65.8% accuracy, outperforming massive 73M-parameter ensembles. These findings demonstrate that adaptive iterative computation offers a far more efficient scaling axis for visual reasoning than simply increasing network width. The code is available at https://github.com/WenjieShu/LoopViT.

</details>


### [257] [Reg4Pru: Regularisation Through Random Token Routing for Token Pruning](https://arxiv.org/abs/2602.02163)
*Julian Wyatt,Ronald Clark,Irina Voiculescu*

Main category: cs.CV

TL;DR: Reg4Pru is a training regularization technique that improves segmentation performance in token-pruned vision transformers while maintaining computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Transformers in vision models suffer from quadratic computational scaling with token count. Token pruning methods improve efficiency but degrade dense prediction performance at deeper layers due to instability from preserved representations.

Method: Reg4Pru - a training regularization technique designed to mitigate token-pruning performance loss specifically for segmentation tasks.

Result: On the FIVES blood vessel segmentation dataset, Reg4Pru improves average precision by 46% absolute compared to same model trained without routing, while achieving 29% relative speedup in wall-clock time compared to non-pruned baseline.

Conclusion: Reg4Pru is a valuable regularizer for token reduction strategies, enabling both computational efficiency and improved segmentation performance.

Abstract: Transformers are widely adopted in modern vision models due to their strong ability to scale with dataset size and generalisability. However, this comes with a major drawback: computation scales quadratically to the total number of tokens. Numerous methods have been proposed to mitigate this. For example, we consider token pruning with reactivating tokens from preserved representations, but the increased computational efficiency of this method results in decreased stability from the preserved representations, leading to poorer dense prediction performance at deeper layers. In this work, we introduce Reg4Pru, a training regularisation technique that mitigates token-pruning performance loss for segmentation. We compare our models on the FIVES blood vessel segmentation dataset and find that Reg4Pru improves average precision by an absolute 46% compared to the same model trained without routing. This increase is observed using a configuration that achieves a 29% relative speedup in wall-clock time compared to the non-pruned baseline. These findings indicate that Reg4Pru is a valuable regulariser for token reduction strategies.

</details>


### [258] [Lung Nodule Image Synthesis Driven by Two-Stage Generative Adversarial Networks](https://arxiv.org/abs/2602.02171)
*Lu Cao,Xiquan He,Junying Zeng,Chaoyun Mai,Min Luo*

Main category: cs.CV

TL;DR: TSGAN: Two-stage GAN for generating diverse, controllable lung nodule CT images by decoupling morphology and texture, improving detection model performance.


<details>
  <summary>Details</summary>
Motivation: Limited sample size and diversity in lung nodule CT datasets restrict detection model performance and generalization; existing methods produce images with insufficient diversity/controllability, monotonous textures, and distorted anatomical structures.

Method: Two-stage GAN: Stage 1 uses StyleGAN to generate semantic segmentation masks controlling anatomical structure; Stage 2 uses DL-Pix2Pix with local importance attention and dynamic weight multi-head window attention to translate masks to CT images, enhancing texture modeling.

Result: On LUNA16 dataset: accuracy improved by 4.6% and mAP by 4% compared to original dataset; TSGAN enhances synthetic image quality and detection model performance.

Conclusion: TSGAN effectively addresses data diversity and controllability limitations in lung nodule CT datasets, improving both synthetic image quality and downstream detection model performance through decoupled structure-texture generation.

Abstract: The limited sample size and insufficient diversity of lung nodule CT datasets severely restrict the performance and generalization ability of detection models. Existing methods generate images with insufficient diversity and controllability, suffering from issues such as monotonous texture features and distorted anatomical structures. Therefore, we propose a two-stage generative adversarial network (TSGAN) to enhance the diversity and spatial controllability of synthetic data by decoupling the morphological structure and texture features of lung nodules. In the first stage, StyleGAN is used to generate semantic segmentation mask images, encoding lung nodules and tissue backgrounds to control the anatomical structure of lung nodule images; The second stage uses the DL-Pix2Pix model to translate the mask map into CT images, employing local importance attention to capture local features, while utilizing dynamic weight multi-head window attention to enhance the modeling capability of lung nodule texture and background. Compared to the original dataset, the accuracy improved by 4.6% and mAP by 4% on the LUNA16 dataset. Experimental results demonstrate that TSGAN can enhance the quality of synthetic images and the performance of detection models.

</details>


### [259] [CIEC: Coupling Implicit and Explicit Cues for Multimodal Weakly Supervised Manipulation Localization](https://arxiv.org/abs/2602.02175)
*Xinquan Yu,Wei Lu,Xiangyang Luo*

Main category: cs.CV

TL;DR: CIEC framework achieves multimodal manipulation localization using only coarse-grained image/sentence-level annotations, matching performance of fully supervised methods.


<details>
  <summary>Details</summary>
Motivation: Current multimodal manipulation localization methods require costly fine-grained annotations (patch/token-level), which are time-consuming to obtain. There's a need for weakly-supervised approaches that can work with only coarse-grained image/sentence-level annotations.

Method: CIEC framework with two branches: (1) Image-based branch uses Textual-guidance Refine Patch Selection (TRPS) module to integrate visual and textual forgery cues with spatial priors, plus background silencing and spatial contrast constraints. (2) Text-based branch uses Visual-deviation Calibrated Token Grounding (VCTG) module to focus on content words with visual bias, plus asymmetric sparse and semantic consistency constraints.

Result: Extensive experiments show CIEC achieves results comparable to fully supervised methods on several evaluation metrics, demonstrating effectiveness of the weakly-supervised approach.

Conclusion: The proposed CIEC framework successfully addresses multimodal manipulation localization using only coarse-grained annotations, providing a practical solution that avoids the need for costly fine-grained labeling while maintaining competitive performance.

Abstract: To mitigate the threat of misinformation, multimodal manipulation localization has garnered growing attention. Consider that current methods rely on costly and time-consuming fine-grained annotations, such as patch/token-level annotations. This paper proposes a novel framework named Coupling Implicit and Explicit Cues (CIEC), which aims to achieve multimodal weakly-supervised manipulation localization for image-text pairs utilizing only coarse-grained image/sentence-level annotations. It comprises two branches, image-based and text-based weakly-supervised localization. For the former, we devise the Textual-guidance Refine Patch Selection (TRPS) module. It integrates forgery cues from both visual and textual perspectives to lock onto suspicious regions aided by spatial priors. Followed by the background silencing and spatial contrast constraints to suppress interference from irrelevant areas. For the latter, we devise the Visual-deviation Calibrated Token Grounding (VCTG) module. It focuses on meaningful content words and leverages relative visual bias to assist token localization. Followed by the asymmetric sparse and semantic consistency constraints to mitigate label noise and ensure reliability. Extensive experiments demonstrate the effectiveness of our CIEC, yielding results comparable to fully supervised methods on several evaluation metrics.

</details>


### [260] [Learning Topology-Aware Implicit Field for Unified Pulmonary Tree Modeling with Incomplete Topological Supervision](https://arxiv.org/abs/2602.02186)
*Ziqiao Weng,Jiancheng Yang,Kangxian Xie,Bo Zhou,Weidong Cai*

Main category: cs.CV

TL;DR: TopoField is a topology-aware implicit modeling framework that repairs incomplete pulmonary trees from CT scans, enabling unified multi-task inference for anatomical analysis without needing complete annotations.


<details>
  <summary>Details</summary>
Motivation: Pulmonary trees from CT images often have topological incompleteness (missing/disconnected branches), which degrades anatomical analysis and limits existing modeling pipelines. Current approaches are inefficient and lack robustness under structural corruption.

Method: TopoField uses sparse surface and skeleton point clouds to learn a continuous implicit field for topology repair. It trains on synthetically introduced structural disruptions over already incomplete trees, without needing complete disconnection annotations. The framework enables joint anatomical labeling and lung segment reconstruction through task-specific implicit functions in a single forward pass.

Result: Extensive experiments on Lung3D+ dataset show TopoField consistently improves topological completeness and achieves accurate anatomical labeling and lung segment reconstruction under challenging incomplete scenarios. It completes all tasks in just over one second per case due to its implicit formulation.

Conclusion: TopoField provides an efficient, topology-aware solution for pulmonary tree analysis that addresses structural incompleteness while enabling unified multi-task inference, making it practical for large-scale clinical applications.

Abstract: Pulmonary trees extracted from CT images frequently exhibit topological incompleteness, such as missing or disconnected branches, which substantially degrades downstream anatomical analysis and limits the applicability of existing pulmonary tree modeling pipelines. Current approaches typically rely on dense volumetric processing or explicit graph reasoning, leading to limited efficiency and reduced robustness under realistic structural corruption. We propose TopoField, a topology-aware implicit modeling framework that treats topology repair as a first-class modeling problem and enables unified multi-task inference for pulmonary tree analysis. TopoField represents pulmonary anatomy using sparse surface and skeleton point clouds and learns a continuous implicit field that supports topology repair without relying on complete or explicit disconnection annotations, by training on synthetically introduced structural disruptions over \textit{already} incomplete trees. Building upon the repaired implicit representation, anatomical labeling and lung segment reconstruction are jointly inferred through task-specific implicit functions within a single forward pass.Extensive experiments on the Lung3D+ dataset demonstrate that TopoField consistently improves topological completeness and achieves accurate anatomical labeling and lung segment reconstruction under challenging incomplete scenarios. Owing to its implicit formulation, TopoField attains high computational efficiency, completing all tasks in just over one second per case, highlighting its practicality for large-scale and time-sensitive clinical applications. Code and data will be available at https://github.com/HINTLab/TopoField.

</details>


### [261] [SSI-DM: Singularity Skipping Inversion of Diffusion Models](https://arxiv.org/abs/2602.02193)
*Chen Min,Enze Jiang,Jishen Peng,Zheng Ma*

Main category: cs.CV

TL;DR: SSI-DM solves the fundamental ill-posedness of diffusion model inversion by skipping singular regions with small noise addition, producing Gaussian noise for better editability.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion model inversion methods produce non-Gaussian noise with poor editability due to inaccuracies in early noising steps, caused by a mathematical singularity that makes inversion fundamentally ill-posed.

Method: Propose Singularity Skipping Inversion (SSI-DM) which bypasses the singular region by adding small noise before standard inversion, producing inverted noise with natural Gaussian properties while maintaining reconstruction fidelity.

Result: Achieves superior performance on public image datasets for reconstruction and interpolation tasks, providing a principled and efficient solution to diffusion model inversion.

Conclusion: SSI-DM offers a simple, plug-and-play technique compatible with general diffusion models that solves the fundamental ill-posedness of inversion while producing Gaussian noise for better editability.

Abstract: Inverting real images into the noise space is essential for editing tasks using diffusion models, yet existing methods produce non-Gaussian noise with poor editability due to the inaccuracy in early noising steps. We identify the root cause: a mathematical singularity that renders inversion fundamentally ill-posed. We propose Singularity Skipping Inversion of Diffusion Models (SSI-DM), which bypasses this singular region by adding small noise before standard inversion. This simple approach produces inverted noise with natural Gaussian properties while maintaining reconstruction fidelity. As a plug-and-play technique compatible with general diffusion models, our method achieves superior performance on public image datasets for reconstruction and interpolation tasks, providing a principled and efficient solution to diffusion model inversion.

</details>


### [262] [MAIN-VLA: Modeling Abstraction of Intention and eNvironment for Vision-Language-Action Models](https://arxiv.org/abs/2602.02212)
*Zheyuan Zhou,Liang Du,Zixun Sun,Xiaoyu Zhou,Ruimin Ye,Qihao Chen,Yinda Chen,Lemiao Qiu*

Main category: cs.CV

TL;DR: MAIN-VLA is a Visual-Language-Action framework that models intention and environment abstractions to improve decision-making in complex 3D environments, achieving state-of-the-art performance with better efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing VLA approaches struggle with extracting action-critical signals from redundant sensor streams in highly complex, dynamic environments like 3D open worlds and large-scale PvP games where real-time unpredictable interactions occur.

Method: Two key abstractions: Intention Abstraction (IA) extracts verbose instructions into compact semantic primitives, and Environment Semantics Abstraction (ESA) projects visual streams into structured topological affordance representations. These are aligned to enable parameter-free token pruning for efficiency.

Result: Achieves state-of-the-art performance in open-world Minecraft and large-scale PvP environments (Game for Peace, Valorant), demonstrating superior decision quality, stronger generalization, and cutting-edge inference efficiency.

Conclusion: MAIN-VLA successfully grounds decision-making in deep semantic alignment rather than superficial pattern matching, enabling efficient and effective action in complex, dynamic environments through explicit modeling of intention and environment abstractions.

Abstract: Despite significant progress in Visual-Language-Action (VLA), in highly complex and dynamic environments that involve real-time unpredictable interactions (such as 3D open worlds and large-scale PvP games), existing approaches remain inefficient at extracting action-critical signals from redundant sensor streams. To tackle this, we introduce MAIN-VLA, a framework that explicitly Models the Abstraction of Intention and eNvironment to ground decision-making in deep semantic alignment rather than superficial pattern matching. Specifically, our Intention Abstraction (IA) extracts verbose linguistic instructions and their associated reasoning into compact, explicit semantic primitives, while the Environment Semantics Abstraction (ESA) projects overwhelming visual streams into a structured, topological affordance representation. Furthermore, aligning these two abstract modalities induces an emergent attention-concentration effect, enabling a parameter-free token-pruning strategy that filters out perceptual redundancy without degrading performance. Extensive experiments in open-world Minecraft and large-scale PvP environments (Game for Peace and Valorant) demonstrate that MAIN-VLA sets a new state-of-the-art, which achieves superior decision quality, stronger generalization, and cutting-edge inference efficiency.

</details>


### [263] [Causal Forcing: Autoregressive Diffusion Distillation Done Right for High-Quality Real-Time Interactive Video Generation](https://arxiv.org/abs/2602.02214)
*Hongzhou Zhu,Min Zhao,Guande He,Hang Su,Chongxuan Li,Jun Zhu*

Main category: cs.CV

TL;DR: Causal Forcing bridges the architectural gap in video diffusion distillation by using an autoregressive teacher for ODE initialization, outperforming previous methods.


<details>
  <summary>Details</summary>
Motivation: Current methods for real-time interactive video generation distill bidirectional video diffusion models into autoregressive models, but face an architectural gap when replacing full attention with causal attention. Existing approaches don't bridge this gap theoretically and violate the frame-level injectivity condition required for ODE distillation, leading to performance degradation.

Method: Proposes Causal Forcing that uses an autoregressive teacher for ODE initialization instead of a bidirectional teacher, ensuring the frame-level injectivity condition is met and properly bridging the architectural gap between bidirectional and autoregressive models.

Result: Outperforms all baselines across all metrics, surpassing state-of-the-art Self Forcing by 19.3% in Dynamic Degree, 8.7% in VisionReward, and 16.7% in Instruction Following.

Conclusion: Causal Forcing effectively addresses the architectural gap in video diffusion distillation by using an autoregressive teacher for ODE initialization, leading to significant performance improvements over existing methods.

Abstract: To achieve real-time interactive video generation, current methods distill pretrained bidirectional video diffusion models into few-step autoregressive (AR) models, facing an architectural gap when full attention is replaced by causal attention. However, existing approaches do not bridge this gap theoretically. They initialize the AR student via ODE distillation, which requires frame-level injectivity, where each noisy frame must map to a unique clean frame under the PF-ODE of an AR teacher. Distilling an AR student from a bidirectional teacher violates this condition, preventing recovery of the teacher's flow map and instead inducing a conditional-expectation solution, which degrades performance. To address this issue, we propose Causal Forcing that uses an AR teacher for ODE initialization, thereby bridging the architectural gap. Empirical results show that our method outperforms all baselines across all metrics, surpassing the SOTA Self Forcing by 19.3\% in Dynamic Degree, 8.7\% in VisionReward, and 16.7\% in Instruction Following. Project page and the code: \href{https://thu-ml.github.io/CausalForcing.github.io/}{https://thu-ml.github.io/CausalForcing.github.io/}

</details>


### [264] [MIRROR: Manifold Ideal Reference ReconstructOR for Generalizable AI-Generated Image Detection](https://arxiv.org/abs/2602.02222)
*Ruiqi Liu,Manni Cui,Ziheng Qin,Zhiyuan Yan,Ruoxin Chen,Yi Han,Zhiheng Li,Junkai Chen,ZhiJin Chen,Kaiqing Lin,Jialiang Shen,Lubin Weng,Jing Dong,Yan Wang,Shu Wu*

Main category: cs.CV

TL;DR: MIRROR reformulates AI-generated image detection as a reference-comparison problem using manifold consistency rather than artifact classification, achieving state-of-the-art performance across multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing AI-generated image detectors rely on artifact-based classification which struggles to generalize to evolving generative models, while human judgment uses stable real-world regularities. The authors aim to create a more generalizable detector that mimics human cognitive processes.

Method: Proposes MIRROR framework that encodes reality priors using a learnable discrete memory bank. Projects inputs into manifold-consistent ideal references via sparse linear combination, using residuals as detection signals. Introduces Human-AIGI benchmark with psychophysically curated human-imperceptible subset.

Result: Outperforms prior methods across 14 benchmarks: 2.1% gain on six standard benchmarks and 8.1% gain on seven in-the-wild benchmarks. On Human-AIGI, achieves 89.6% accuracy across 27 generators, surpassing both lay users and visual experts.

Conclusion: MIRROR demonstrates that reformulating detection as reference-comparison with manifold consistency is more effective than artifact-based approaches, approaching human perceptual limits as backbones scale, and can potentially replace human experts in media security.

Abstract: High-fidelity generative models have narrowed the perceptual gap between synthetic and real images, posing serious threats to media security. Most existing AI-generated image (AIGI) detectors rely on artifact-based classification and struggle to generalize to evolving generative traces. In contrast, human judgment relies on stable real-world regularities, with deviations from the human cognitive manifold serving as a more generalizable signal of forgery. Motivated by this insight, we reformulate AIGI detection as a Reference-Comparison problem that verifies consistency with the real-image manifold rather than fitting specific forgery cues. We propose MIRROR (Manifold Ideal Reference ReconstructOR), a framework that explicitly encodes reality priors using a learnable discrete memory bank. MIRROR projects an input into a manifold-consistent ideal reference via sparse linear combination, and uses the resulting residuals as robust detection signals. To evaluate whether detectors reach the "superhuman crossover" required to replace human experts, we introduce the Human-AIGI benchmark, featuring a psychophysically curated human-imperceptible subset. Across 14 benchmarks, MIRROR consistently outperforms prior methods, achieving gains of 2.1% on six standard benchmarks and 8.1% on seven in-the-wild benchmarks. On Human-AIGI, MIRROR reaches 89.6% accuracy across 27 generators, surpassing both lay users and visual experts, and further approaching the human perceptual limit as pretrained backbones scale. The code is publicly available at: https://github.com/349793927/MIRROR

</details>


### [265] [Evaluating OCR Performance for Assistive Technology: Effects of Walking Speed, Camera Placement, and Camera Type](https://arxiv.org/abs/2602.02223)
*Junchi Feng,Nikhil Ballem,Mahya Beheshti,Giles Hamilton-Fletcher,Todd Hudson,Maurizio Porfiri,William H. Seiple,John-Ross Rizzo*

Main category: cs.CV

TL;DR: Systematic evaluation of OCR performance for assistive technology under static (distance/angle) and dynamic (walking speed/mounting position) conditions, comparing multiple OCR engines and devices.


<details>
  <summary>Details</summary>
Motivation: Most OCR evaluations use static datasets that don't reflect real-world mobile use challenges for people with blindness/low vision, creating a need for systematic testing under dynamic conditions.

Method: Two-phase evaluation: (1) Static tests measuring detection range (1-7m distances, 0-75° viewing angles) with 4 OCR engines; (2) Dynamic tests varying walking speeds (0.8-1.8 m/s) and comparing three camera mounting positions (head, shoulder, hand-held) using smartphone and smart glasses.

Result: Accuracy declined with increased walking speed and wider viewing angles. Google Vision had highest overall accuracy, PaddleOCR 3.0 was best open-source alternative. Phone's main camera performed best, shoulder-mounted placement yielded highest average but differences among positions weren't statistically significant.

Conclusion: Dynamic conditions significantly impact OCR performance for mobile assistive technology, with Google Vision and PaddleOCR 3.0 performing best, and shoulder-mounted positioning showing practical advantages despite lack of statistical significance.

Abstract: Optical character recognition (OCR), which converts printed or handwritten text into machine-readable form, is widely used in assistive technology for people with blindness and low vision. Yet, most evaluations rely on static datasets that do not reflect the challenges of mobile use. In this study, we systematically evaluated OCR performance under both static and dynamic conditions. Static tests measured detection range across distances of 1-7 meters and viewing angles of 0-75 degrees horizontally. Dynamic tests examined the impact of motion by varying walking speed from slow (0.8 m/s) to very fast (1.8 m/s) and comparing three camera mounting positions: head-mounted, shoulder-mounted, and hand-held. We evaluated both a smartphone and smart glasses, using the phone's main and ultra-wide cameras. Four OCR engines were benchmarked to assess accuracy at different distances and viewing angles: Google Vision, PaddleOCR 3.0, EasyOCR, and Tesseract. PaddleOCR 3.0 was then used to evaluate accuracy at different walking speeds. Accuracy was computed at the character level using the Levenshtein ratio against manually defined ground truth. Results showed that recognition accuracy declined with increased walking speed and wider viewing angles. Google Vision achieved the highest overall accuracy, with PaddleOCR close behind as the strongest open-source alternative. Across devices, the phone's main camera achieved the highest accuracy, and a shoulder-mounted placement yielded the highest average among body positions; however, differences among shoulder, head, and hand were not statistically significant.

</details>


### [266] [Show, Don't Tell: Morphing Latent Reasoning into Image Generation](https://arxiv.org/abs/2602.02227)
*Harold Haodong Chen,Xinxiang Yin,Wen-Jie Shu,Hongfei Zhang,Zixin Zhang,Chenfei Liao,Litao Guo,Qifeng Chen,Ying-Cong Chen*

Main category: cs.CV

TL;DR: LatentMorph integrates implicit latent reasoning into text-to-image generation using four lightweight components, achieving better performance, efficiency, and cognitive alignment than explicit reasoning methods.


<details>
  <summary>Details</summary>
Motivation: Current text-to-image generation methods lack dynamic reasoning and refinement capabilities similar to human creativity. Existing reasoning-augmented approaches rely on explicit thought processes with discrete text decoding at fixed steps, causing inefficiencies, information loss, and cognitive mismatches.

Method: LatentMorph introduces four lightweight components: (1) condenser for summarizing intermediate generation states into visual memory, (2) translator for converting latent thoughts into actionable guidance, (3) shaper for dynamically steering next image token predictions, and (4) RL-trained invoker for adaptively determining when to invoke reasoning. All reasoning occurs in continuous latent spaces.

Result: LatentMorph enhances Janus-Pro by 16% on GenEval and 25% on T2I-CompBench; outperforms explicit paradigms (e.g., TwiG) by 15% and 11% on abstract reasoning tasks (WISE and IPV-Txt); reduces inference time by 44% and token consumption by 51%; and exhibits 71% cognitive alignment with human intuition on reasoning invocation.

Conclusion: LatentMorph successfully bridges the gap in dynamic reasoning for text-to-image generation by performing reasoning entirely in continuous latent spaces, avoiding bottlenecks of explicit reasoning while enabling more adaptive self-refinement with significant performance, efficiency, and cognitive alignment improvements.

Abstract: Text-to-image (T2I) generation has achieved remarkable progress, yet existing methods often lack the ability to dynamically reason and refine during generation--a hallmark of human creativity. Current reasoning-augmented paradigms most rely on explicit thought processes, where intermediate reasoning is decoded into discrete text at fixed steps with frequent image decoding and re-encoding, leading to inefficiencies, information loss, and cognitive mismatches. To bridge this gap, we introduce LatentMorph, a novel framework that seamlessly integrates implicit latent reasoning into the T2I generation process. At its core, LatentMorph introduces four lightweight components: (i) a condenser for summarizing intermediate generation states into compact visual memory, (ii) a translator for converting latent thoughts into actionable guidance, (iii) a shaper for dynamically steering next image token predictions, and (iv) an RL-trained invoker for adaptively determining when to invoke reasoning. By performing reasoning entirely in continuous latent spaces, LatentMorph avoids the bottlenecks of explicit reasoning and enables more adaptive self-refinement. Extensive experiments demonstrate that LatentMorph (I) enhances the base model Janus-Pro by $16\%$ on GenEval and $25\%$ on T2I-CompBench; (II) outperforms explicit paradigms (e.g., TwiG) by $15\%$ and $11\%$ on abstract reasoning tasks like WISE and IPV-Txt, (III) while reducing inference time by $44\%$ and token consumption by $51\%$; and (IV) exhibits $71\%$ cognitive alignment with human intuition on reasoning invocation.

</details>


### [267] [LiFlow: Flow Matching for 3D LiDAR Scene Completion](https://arxiv.org/abs/2602.02232)
*Andrea Matteazzi,Dietmar Tutsch*

Main category: cs.CV

TL;DR: LiFlow: First flow matching framework for 3D LiDAR scene completion that improves upon diffusion methods by ensuring consistent training/inference initial distributions


<details>
  <summary>Details</summary>
Motivation: LiDAR point clouds in autonomous driving suffer from occlusion and long-range sparsity, limiting perception. Existing diffusion methods have training/inference distribution mismatch issues.

Method: Flow matching framework with nearest neighbor flow matching loss and Chamfer distance loss to enhance local structure and global coverage in point cloud alignment

Result: Achieves state-of-the-art performance across multiple metrics for 3D LiDAR scene completion

Conclusion: LiFlow introduces an effective flow matching approach that overcomes limitations of diffusion methods for LiDAR scene completion, providing better consistency between training and inference

Abstract: In autonomous driving scenarios, the collected LiDAR point clouds can be challenged by occlusion and long-range sparsity, limiting the perception of autonomous driving systems. Scene completion methods can infer the missing parts of incomplete 3D LiDAR scenes. Recent methods adopt local point-level denoising diffusion probabilistic models, which require predicting Gaussian noise, leading to a mismatch between training and inference initial distributions. This paper introduces the first flow matching framework for 3D LiDAR scene completion, improving upon diffusion-based methods by ensuring consistent initial distributions between training and inference. The model employs a nearest neighbor flow matching loss and a Chamfer distance loss to enhance both local structure and global coverage in the alignment of point clouds. LiFlow achieves state-of-the-art performance across multiple metrics. Code: https://github.com/matteandre/LiFlow.

</details>


### [268] [Enhancing Indoor Occupancy Prediction via Sparse Query-Based Multi-Level Consistent Knowledge Distillation](https://arxiv.org/abs/2602.02318)
*Xiang Li,Yupeng Zheng,Pengfei Li,Yilun Chen,Ya-Qin Zhang,Wenchao Ding*

Main category: cs.CV

TL;DR: DiScene: Sparse query-based occupancy prediction framework using multi-level distillation for efficiency and robustness in indoor scenes.


<details>
  <summary>Details</summary>
Motivation: Current occupancy prediction methods face efficiency-accuracy trade-offs - dense methods waste computation on empty voxels, while sparse query-based approaches lack robustness in diverse indoor scenes.

Method: Two key innovations: 1) Multi-level Consistent Knowledge Distillation with four-level alignment (encoder features, query features, spatial priors, anchor-level high-confidence knowledge), 2) Teacher-Guided Initialization for parameter warm-up to accelerate convergence.

Result: Achieves 23.2 FPS without depth priors, outperforms baseline OPUS by 36.1%, beats depth-enhanced OPUS†. With depth integration, sets new SOTA surpassing EmbodiedOcc by 3.7% with 1.62× faster inference. Validated on Occ-Scannet, Occ3D-nuScenes, and in-the-wild scenarios.

Conclusion: DiScene demonstrates efficient and robust occupancy prediction through multi-level distillation, achieving state-of-the-art performance with faster inference across diverse indoor environments.

Abstract: Occupancy prediction provides critical geometric and semantic understanding for robotics but faces efficiency-accuracy trade-offs. Current dense methods suffer computational waste on empty voxels, while sparse query-based approaches lack robustness in diverse and complex indoor scenes. In this paper, we propose DiScene, a novel sparse query-based framework that leverages multi-level distillation to achieve efficient and robust occupancy prediction. In particular, our method incorporates two key innovations: (1) a Multi-level Consistent Knowledge Distillation strategy, which transfers hierarchical representations from large teacher models to lightweight students through coordinated alignment across four levels, including encoder-level feature alignment, query-level feature matching, prior-level spatial guidance, and anchor-level high-confidence knowledge transfer and (2) a Teacher-Guided Initialization policy, employing optimized parameter warm-up to accelerate model convergence. Validated on the Occ-Scannet benchmark, DiScene achieves 23.2 FPS without depth priors while outperforming our baseline method, OPUS, by 36.1% and even better than the depth-enhanced version, OPUS†. With depth integration, DiScene† attains new SOTA performance, surpassing EmbodiedOcc by 3.7% with 1.62$\times$ faster inference speed. Furthermore, experiments on the Occ3D-nuScenes benchmark and in-the-wild scenarios demonstrate the versatility of our approach in various environments. Code and models can be accessed at https://github.com/getterupper/DiScene.

</details>


### [269] [LongVPO: From Anchored Cues to Self-Reasoning for Long-Form Video Preference Optimization](https://arxiv.org/abs/2602.02341)
*Zhenpeng Huang,Jiaqi Li,Zihan Jia,Xinhao Li,Desen Meng,Lingxue Song,Xi Chen,Liang Li,Limin Wang*

Main category: cs.CV

TL;DR: LongVPO is a two-stage DPO framework that enables short-context VLMs to understand ultra-long videos without long-video annotations, using synthetic preference triples and multi-segment reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Existing vision-language models struggle with ultra-long video understanding due to limited long-video annotations and computational constraints. There's a need for scalable methods that can extend short-context models to handle long videos without costly human labeling.

Method: Two-stage approach: Stage 1 synthesizes preference triples by anchoring questions to short clips, filtering with visual-similarity and question-specificity, and approximating reference model scoring. Stage 2 uses recursive captioning on long videos to generate scene-level metadata, then employs LLMs to craft multi-segment reasoning queries and dispreferred responses for alignment.

Result: LongVPO outperforms state-of-the-art open-source models on multiple long-video benchmarks while maintaining strong short-video performance (e.g., on MVBench), using only 16K synthetic examples and no human labels.

Conclusion: LongVPO offers a scalable paradigm for efficient long-form video understanding by enabling short-context models to handle ultra-long videos through synthetic preference optimization without costly annotations.

Abstract: We present LongVPO, a novel two-stage Direct Preference Optimization framework that enables short-context vision-language models to robustly understand ultra-long videos without any long-video annotations. In Stage 1, we synthesize preference triples by anchoring questions to individual short clips, interleaving them with distractors, and applying visual-similarity and question-specificity filtering to mitigate positional bias and ensure unambiguous supervision. We also approximate the reference model's scoring over long contexts by evaluating only the anchor clip, reducing computational overhead. In Stage 2, we employ a recursive captioning pipeline on long videos to generate scene-level metadata, then use a large language model to craft multi-segment reasoning queries and dispreferred responses, aligning the model's preferences through multi-segment reasoning tasks. With only 16K synthetic examples and no costly human labels, LongVPO outperforms the state-of-the-art open-source models on multiple long-video benchmarks, while maintaining strong short-video performance (e.g., on MVBench), offering a scalable paradigm for efficient long-form video understanding.

</details>


### [270] [NAB: Neural Adaptive Binning for Sparse-View CT reconstruction](https://arxiv.org/abs/2602.02356)
*Wangduo Xie,Matthew B. Blaschko*

Main category: cs.CV

TL;DR: NAB integrates rectangular shape priors into neural CT reconstruction via adaptive binning with learnable parameters, improving sparse-view reconstruction for industrial objects.


<details>
  <summary>Details</summary>
Motivation: Industrial CT needs sparse-view reconstruction to reduce costs, but existing neural methods don't leverage common rectangular shape priors found in industrial objects.

Method: Neural Adaptive Binning (NAB) maps coordinates to binned vector space using shifted hyperbolic tangent functions with learnable position, size, steepness, and rotation parameters, then processes through neural network to predict attenuation coefficients.

Result: NAB achieves superior performance on two industrial datasets and maintains robustness on medical datasets when extended to more general expressions.

Conclusion: NAB provides a novel approach to integrate shape priors into neural reconstruction, enabling end-to-end optimization of encoding parameters for improved sparse-view CT reconstruction.

Abstract: Computed Tomography (CT) plays a vital role in inspecting the internal structures of industrial objects. Furthermore, achieving high-quality CT reconstruction from sparse views is essential for reducing production costs. While classic implicit neural networks have shown promising results for sparse reconstruction, they are unable to leverage shape priors of objects. Motivated by the observation that numerous industrial objects exhibit rectangular structures, we propose a novel \textbf{N}eural \textbf{A}daptive \textbf{B}inning (\textbf{NAB}) method that effectively integrates rectangular priors into the reconstruction process. Specifically, our approach first maps coordinate space into a binned vector space. This mapping relies on an innovative binning mechanism based on differences between shifted hyperbolic tangent functions, with our extension enabling rotations around the input-plane normal vector. The resulting representations are then processed by a neural network to predict CT attenuation coefficients. This design enables end-to-end optimization of the encoding parameters -- including position, size, steepness, and rotation -- via gradient flow from the projection data, thus enhancing reconstruction accuracy. By adjusting the smoothness of the binning function, NAB can generalize to objects with more complex geometries. This research provides a new perspective on integrating shape priors into neural network-based reconstruction. Extensive experiments demonstrate that NAB achieves superior performance on two industrial datasets. It also maintains robust on medical datasets when the binning function is extended to more general expression. The code will be made available.

</details>


### [271] [Uncertainty-Aware Image Classification In Biomedical Imaging Using Spectral-normalized Neural Gaussian Processes](https://arxiv.org/abs/2602.02370)
*Uma Meleti,Jeffrey J. Nirschl*

Main category: cs.CV

TL;DR: SNGP improves uncertainty estimation and OOD detection in digital pathology models while maintaining comparable in-distribution performance.


<details>
  <summary>Details</summary>
Motivation: Current deep learning models for digital pathology are overconfident and poorly calibrated in out-of-distribution settings, limiting clinical trust and adoption. Safety-critical medical workflows need uncertainty-aware properties to accurately reject OOD inputs.

Method: Implemented Spectral-normalized Neural Gaussian Process (SNGP) - lightweight modifications applying spectral normalization and replacing final dense layer with Gaussian process layer to improve single-model uncertainty estimation and OOD detection.

Result: SNGP showed comparable in-distribution performance while significantly improving uncertainty estimation and OOD detection compared to deterministic and Monte Carlo dropout methods across six datasets and three biomedical classification tasks (white blood cells, amyloid plaques, colorectal histopathology).

Conclusion: SNGP and related models offer a useful framework for uncertainty-aware classification in digital pathology, supporting safe deployment and building trust with pathologists.

Abstract: Accurate histopathologic interpretation is key for clinical decision-making; however, current deep learning models for digital pathology are often overconfident and poorly calibrated in out-of-distribution (OOD) settings, which limit trust and clinical adoption. Safety-critical medical imaging workflows benefit from intrinsic uncertainty-aware properties that can accurately reject OOD input. We implement the Spectral-normalized Neural Gaussian Process (SNGP), a set of lightweight modifications that apply spectral normalization and replace the final dense layer with a Gaussian process layer to improve single-model uncertainty estimation and OOD detection. We evaluate SNGP vs. deterministic and MonteCarlo dropout on six datasets across three biomedical classification tasks: white blood cells, amyloid plaques, and colorectal histopathology. SNGP has comparable in-distribution performance while significantly improving uncertainty estimation and OOD detection. Thus, SNGP or related models offer a useful framework for uncertainty-aware classification in digital pathology, supporting safe deployment and building trust with pathologists.

</details>


### [272] [Unified Personalized Reward Model for Vision Generation](https://arxiv.org/abs/2602.02380)
*Yibin Wang,Yuhang Zang,Feng Han,Jiazi Bu,Yujie Zhou,Cheng Jin,Jiaqi Wang*

Main category: cs.CV

TL;DR: UnifiedReward-Flex is a personalized reward model for vision generation that uses adaptive reasoning to assess visual content based on semantic intent and dynamic criteria, outperforming one-size-fits-all approaches.


<details>
  <summary>Details</summary>
Motivation: Current multimodal reward models for visual generation follow monolithic preference distributions or fixed evaluation rubrics, making them insensitive to content-specific visual cues and systematically misaligned with subjective, context-dependent human preferences.

Method: Two-stage training: 1) Distill structured reasoning traces from advanced VLMs for supervised fine-tuning to enable flexible, context-adaptive reasoning; 2) Apply direct preference optimization on curated preference pairs to strengthen reasoning fidelity and discriminative alignment. The model interprets semantic intent, grounds visual evidence, and dynamically constructs hierarchical assessments with fine-grained criteria.

Result: Integrated into GRPO framework for image and video synthesis, UnifiedReward-Flex demonstrates superiority over existing approaches in extensive evaluations.

Conclusion: UnifiedReward-Flex addresses limitations of current reward models by introducing personalized, context-adaptive reasoning for visual generation assessment, better aligning with subjective human preferences through dynamic hierarchical evaluation.

Abstract: Recent advancements in multimodal reward models (RMs) have significantly propelled the development of visual generation. Existing frameworks typically adopt Bradley-Terry-style preference modeling or leverage generative VLMs as judges, and subsequently optimize visual generation models via reinforcement learning. However, current RMs suffer from inherent limitations: they often follow a one-size-fits-all paradigm that assumes a monolithic preference distribution or relies on fixed evaluation rubrics. As a result, they are insensitive to content-specific visual cues, leading to systematic misalignment with subjective and context-dependent human preferences. To this end, inspired by human assessment, we propose UnifiedReward-Flex, a unified personalized reward model for vision generation that couples reward modeling with flexible and context-adaptive reasoning. Specifically, given a prompt and the generated visual content, it first interprets the semantic intent and grounds on visual evidence, then dynamically constructs a hierarchical assessment by instantiating fine-grained criteria under both predefined and self-generated high-level dimensions. Our training pipeline follows a two-stage process: (1) we first distill structured, high-quality reasoning traces from advanced closed-source VLMs to bootstrap SFT, equipping the model with flexible and context-adaptive reasoning behaviors; (2) we then perform direct preference optimization (DPO) on carefully curated preference pairs to further strengthen reasoning fidelity and discriminative alignment. To validate the effectiveness, we integrate UnifiedReward-Flex into the GRPO framework for image and video synthesis, and extensive results demonstrate its superiority.

</details>


### [273] [Personalized Image Generation via Human-in-the-loop Bayesian Optimization](https://arxiv.org/abs/2602.02388)
*Rajalaxmi Rajagopalan,Debottam Dutta,Yu-Lin Wei,Romit Roy Choudhury*

Main category: cs.CV

TL;DR: MultiBO uses multi-choice preferential Bayesian optimization to refine AI-generated images when language prompts fail, allowing users to select better options through iterative feedback rounds.


<details>
  <summary>Details</summary>
Motivation: When users have specific mental images that can't be fully captured through language prompts alone, there's a need for methods that can bridge the gap between AI-generated approximations and the user's exact mental image using alternative feedback mechanisms.

Method: MultiBO (Multi-Choice Preferential Bayesian Optimization) generates K new images based on the best current approximation, gets preferential feedback from users on which images are closer to their mental image, uses this feedback to guide the diffusion model, and iteratively refines the images over B rounds.

Result: The method significantly narrows the gap between AI-generated images and users' mental images, with promising results from qualitative scores from 30 users and superior quantitative metrics compared to 5 baselines.

Conclusion: Multi-choice preferential feedback from humans can be effectively harnessed for personalized image generation, providing a practical solution when language prompts reach their limits for capturing specific mental images.

Abstract: Imagine Alice has a specific image $x^\ast$ in her mind, say, the view of the street in which she grew up during her childhood. To generate that exact image, she guides a generative model with multiple rounds of prompting and arrives at an image $x^{p*}$. Although $x^{p*}$ is reasonably close to $x^\ast$, Alice finds it difficult to close that gap using language prompts. This paper aims to narrow this gap by observing that even after language has reached its limits, humans can still tell when a new image $x^+$ is closer to $x^\ast$ than $x^{p*}$. Leveraging this observation, we develop MultiBO (Multi-Choice Preferential Bayesian Optimization) that carefully generates $K$ new images as a function of $x^{p*}$, gets preferential feedback from the user, uses the feedback to guide the diffusion model, and ultimately generates a new set of $K$ images. We show that within $B$ rounds of user feedback, it is possible to arrive much closer to $x^\ast$, even though the generative model has no information about $x^\ast$. Qualitative scores from $30$ users, combined with quantitative metrics compared across $5$ baselines, show promising results, suggesting that multi-choice feedback from humans can be effectively harnessed for personalized image generation.

</details>


### [274] [Superman: Unifying Skeleton and Vision for Human Motion Perception and Generation](https://arxiv.org/abs/2602.02401)
*Xinshun Wang,Peiming Li,Ziyi Wang,Zhongbin Fang,Zhichao Deng,Songtao Wu,Jason Li,Mengyuan Liu*

Main category: cs.CV

TL;DR: Superman is a unified framework that bridges visual perception with temporal skeleton-based motion generation using a cross-modal motion vocabulary and single MLLM architecture for multiple motion tasks.


<details>
  <summary>Details</summary>
Motivation: Current motion analysis suffers from fragmentation: (1) split between perception models (video to text) and generation models (no visual input), (2) generative MLLMs limited to single-frame static poses, (3) existing motion vocabularies built only from skeleton data, severing visual domain connection.

Method: Two-fold solution: (1) Vision-Guided Motion Tokenizer that leverages geometric alignment between 3D skeletons and visual data for joint learning, creating unified cross-modal motion vocabulary; (2) Single unified MLLM architecture trained on this motion language to handle diverse temporal inputs for both perception (3D skeleton pose estimation from video) and generation (motion prediction and in-betweening).

Result: Extensive experiments on standard benchmarks including Human3.6M demonstrate state-of-the-art or competitive performance across all motion tasks (temporal 3D pose estimation, motion prediction, motion in-betweening).

Conclusion: Superman showcases a more efficient and scalable path for generative motion analysis using skeletons by unifying perception and generation in a single framework with cross-modal motion vocabulary.

Abstract: Human motion analysis tasks, such as temporal 3D pose estimation, motion prediction, and motion in-betweening, play an essential role in computer vision. However, current paradigms suffer from severe fragmentation. First, the field is split between ``perception'' models that understand motion from video but only output text, and ``generation'' models that cannot perceive from raw visual input. Second, generative MLLMs are often limited to single-frame, static poses using dense, parametric SMPL models, failing to handle temporal motion. Third, existing motion vocabularies are built from skeleton data alone, severing the link to the visual domain. To address these challenges, we introduce Superman, a unified framework that bridges visual perception with temporal, skeleton-based motion generation. Our solution is twofold. First, to overcome the modality disconnect, we propose a Vision-Guided Motion Tokenizer. Leveraging the natural geometric alignment between 3D skeletons and visual data, this module pioneers robust joint learning from both modalities, creating a unified, cross-modal motion vocabulary. Second, grounded in this motion language, a single, unified MLLM architecture is trained to handle all tasks. This module flexibly processes diverse, temporal inputs, unifying 3D skeleton pose estimation from video (perception) with skeleton-based motion prediction and in-betweening (generation). Extensive experiments on standard benchmarks, including Human3.6M, demonstrate that our unified method achieves state-of-the-art or competitive performance across all motion tasks. This showcases a more efficient and scalable path for generative motion analysis using skeletons.

</details>


### [275] [Catalyst: Out-of-Distribution Detection via Elastic Scaling](https://arxiv.org/abs/2602.02409)
*Abid Hassan,Tuan Ngo,Saad Shafiq,Nenad Medvidovic*

Main category: cs.CV

TL;DR: Catalyst is a post-hoc OOD detection framework that uses pre-pooling feature map statistics to compute an input-dependent scaling factor that multiplicatively modulates existing OOD scores, achieving substantial performance improvements across multiple datasets.


<details>
  <summary>Details</summary>
Motivation: Current OOD detection methods rely on logits or pooled features, discarding valuable channel-wise statistics from pre-pooling feature maps. These raw statistics contain complementary signals that could improve OOD detection performance.

Method: Catalyst computes an input-dependent scaling factor (γ) from raw pre-pooling feature map statistics (mean, standard deviation, maximum activation) and fuses it multiplicatively with existing baseline OOD scores through "elastic scaling" to better separate ID and OOD distributions.

Result: Catalyst achieves substantial improvements: reduces average False Positive Rate by 32.87% on CIFAR-10 (ResNet-18), 27.94% on CIFAR-100 (ResNet-18), and 22.25% on ImageNet (ResNet-50). It works with both logit-based methods (Energy, ReAct, SCALE) and distance-based detectors (KNN).

Conclusion: Pre-pooling feature map statistics contain untapped potential for OOD detection. Catalyst is a generalizable framework that complements existing approaches and consistently improves performance across different methods and datasets.

Abstract: Out-of-distribution (OOD) detection is critical for the safe deployment of deep neural networks. State-of-the-art post-hoc methods typically derive OOD scores from the output logits or penultimate feature vector obtained via global average pooling (GAP). We contend that this exclusive reliance on the logit or feature vector discards a rich, complementary signal: the raw channel-wise statistics of the pre-pooling feature map lost in GAP. In this paper, we introduce Catalyst, a post-hoc framework that exploits these under-explored signals. Catalyst computes an input-dependent scaling factor ($γ$) on-the-fly from these raw statistics (e.g., mean, standard deviation, and maximum activation). This $γ$ is then fused with the existing baseline score, multiplicatively modulating it -- an ``elastic scaling'' -- to push the ID and OOD distributions further apart. We demonstrate Catalyst is a generalizable framework: it seamlessly integrates with logit-based methods (e.g., Energy, ReAct, SCALE) and also provides a significant boost to distance-based detectors like KNN. As a result, Catalyst achieves substantial and consistent performance gains, reducing the average False Positive Rate by 32.87 on CIFAR-10 (ResNet-18), 27.94% on CIFAR-100 (ResNet-18), and 22.25% on ImageNet (ResNet-50). Our results highlight the untapped potential of pre-pooling statistics and demonstrate that Catalyst is complementary to existing OOD detection approaches.

</details>


### [276] [SelvaMask: Segmenting Trees in Tropical Forests and Beyond](https://arxiv.org/abs/2602.02426)
*Simon-Olivier Duguay,Hugo Baudchon,Etienne Laliberté,Helene Muller-Landau,Gonzalo Rivas-Torres,Arthur Ouaknine*

Main category: cs.CV

TL;DR: SelvaMask introduces a new tropical forest dataset with 8,800+ manually delineated tree crowns across three Neotropical sites, enabling improved individual tree crown segmentation in dense tropical forests using a modular detection-segmentation pipeline that adapts vision foundation models.


<details>
  <summary>Details</summary>
Motivation: Tropical forests are critical for global ecological balance and carbon storage, but accurate individual tree crown segmentation remains challenging, especially in dense tropical forests where current transformer-based models perform poorly.

Method: Created SelvaMask dataset with comprehensive annotations and inter-annotator agreement evaluation. Developed a modular detection-segmentation pipeline that adapts vision foundation models using domain-specific detection-prompter for improved tree crown segmentation.

Result: The approach achieves state-of-the-art performance, outperforming both zero-shot generalist models and fully supervised end-to-end methods in dense tropical forests. Gains are validated on external tropical and temperate datasets.

Conclusion: SelvaMask serves as both a challenging benchmark and key enabler for generalized forest monitoring, with publicly released code and dataset to advance research in tropical forest analysis.

Abstract: Tropical forests harbor most of the planet's tree biodiversity and are critical to global ecological balance. Canopy trees in particular play a disproportionate role in carbon storage and functioning of these ecosystems. Studying canopy trees at scale requires accurate delineation of individual tree crowns, typically performed using high-resolution aerial imagery. Despite advances in transformer-based models for individual tree crown segmentation, performance remains low in most forests, especially tropical ones. To this end, we introduce SelvaMask, a new tropical dataset containing over 8,800 manually delineated tree crowns across three Neotropical forest sites in Panama, Brazil, and Ecuador. SelvaMask features comprehensive annotations, including an inter-annotator agreement evaluation, capturing the dense structure of tropical forests and highlighting the difficulty of the task. Leveraging this benchmark, we propose a modular detection-segmentation pipeline that adapts vision foundation models (VFMs), using domain-specific detection-prompter. Our approach reaches state-of-the-art performance, outperforming both zero-shot generalist models and fully supervised end-to-end methods in dense tropical forests. We validate these gains on external tropical and temperate datasets, demonstrating that SelvaMask serves as both a challenging benchmark and a key enabler for generalized forest monitoring. Our code and dataset will be released publicly.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [277] [Scalable and Secure AI Inference in Healthcare: A Comparative Benchmarking of FastAPI and Triton Inference Server on Kubernetes](https://arxiv.org/abs/2602.00053)
*Ratul Ali*

Main category: cs.AI

TL;DR: Benchmarking shows Triton Inference Server outperforms FastAPI for throughput (780 RPS vs baseline) in healthcare ML deployments, while FastAPI excels at single-request latency (22ms p50). Hybrid architecture combining both emerges as best practice.


<details>
  <summary>Details</summary>
Motivation: Healthcare ML deployments require balancing real-time inference latency, batch processing throughput, and strict data privacy compliance (HIPAA). Need to evaluate optimal deployment paradigms for regulated clinical environments.

Method: Benchmarked two deployment approaches: Python-based FastAPI REST service vs NVIDIA Triton Inference Server. Deployed DistilBERT sentiment analysis model on Kubernetes, measuring p50/p95 latency and throughput under controlled conditions. Also evaluated hybrid architecture with FastAPI as secure gateway and Triton for backend inference.

Result: FastAPI provides lower single-request latency (22ms p50) but Triton achieves superior scalability with dynamic batching (780 RPS on single T4 GPU, nearly double baseline). Hybrid approach validated as optimal for enterprise clinical AI.

Conclusion: Hybrid architecture combining FastAPI for secure PHI de-identification/gateway functions and Triton for high-performance backend inference represents best practice for secure, high-availability healthcare ML deployments.

Abstract: Efficient and scalable deployment of machine learning (ML) models is a prerequisite for modern production environments, particularly within regulated domains such as healthcare and pharmaceuticals. In these settings, systems must balance competing requirements, including minimizing inference latency for real-time clinical decision support, maximizing throughput for batch processing of medical records, and ensuring strict adherence to data privacy standards such as HIPAA. This paper presents a rigorous benchmarking analysis comparing two prominent deployment paradigms: a lightweight, Python-based REST service using FastAPI, and a specialized, high-performance serving engine, NVIDIA Triton Inference Server. Leveraging a reference architecture for healthcare AI, we deployed a DistilBERT sentiment analysis model on Kubernetes to measure median (p50) and tail (p95) latency, as well as throughput, under controlled experimental conditions. Our results indicate a distinct trade-off. While FastAPI provides lower overhead for single-request workloads with a p50 latency of 22 ms, Triton achieves superior scalability through dynamic batching, delivering a throughput of 780 requests per second on a single NVIDIA T4 GPU, nearly double that of the baseline. Furthermore, we evaluate a hybrid architectural approach that utilizes FastAPI as a secure gateway for protected health information de-identification and Triton for backend inference. This study validates the hybrid model as a best practice for enterprise clinical AI and offers a blueprint for secure, high-availability deployments.

</details>


### [278] [Learning to Price: Interpretable Attribute-Level Models for Dynamic Markets](https://arxiv.org/abs/2602.00188)
*Srividhya Sethuraman,Chandrashekar Lakshminarayanan*

Main category: cs.AI

TL;DR: AFDLD model decomposes prices into attribute contributions with substitution effects; ADEPT algorithm achieves sublinear regret with interpretable attribute-level pricing.


<details>
  <summary>Details</summary>
Motivation: Existing low-rank bandit models for dynamic pricing lack interpretability - they use latent features that obscure how individual product attributes influence pricing decisions. There's a need for scalable, interpretable pricing models.

Method: Proposes AFDLD (Additive Feature Decomposition-based Low-Dimensional Demand) model where product prices are sum of attribute-level contributions with explicit substitution effects. Develops ADEPT algorithm - projection-free, gradient-free online learning operating directly in attribute space.

Result: ADEPT achieves sublinear regret of Õ(√d T^{3/4}). In synthetic and real-world studies, it learns near-optimal prices, adapts rapidly to shocks/drifts, and provides transparent attribute-level price explanations.

Conclusion: Interpretability and efficiency in autonomous pricing can be jointly achieved through structured, attribute-driven representations, addressing fundamental challenges of scalability, uncertainty, and interpretability in high-dimensional markets.

Abstract: Dynamic pricing in high-dimensional markets poses fundamental challenges of scalability, uncertainty, and interpretability. Existing low-rank bandit formulations learn efficiently but rely on latent features that obscure how individual product attributes influence price. We address this by introducing an interpretable \emph{Additive Feature Decomposition-based Low-Dimensional Demand (\textbf{AFDLD}) model}, where product prices are expressed as the sum of attribute-level contributions and substitution effects are explicitly modeled. Building on this structure, we propose \textbf{ADEPT} (Additive DEcomposition for Pricing with cross-elasticity and Time-adaptive learning)-a projection-free, gradient-free online learning algorithm that operates directly in attribute space and achieves a sublinear regret of $\tilde{\mathcal{O}}(\sqrt{d}T^{3/4})$. Through controlled synthetic studies and real-world datasets, we show that ADEPT (i) learns near-optimal prices under dynamic market conditions, (ii) adapts rapidly to shocks and drifts, and (iii) yields transparent, attribute-level price explanations. The results demonstrate that interpretability and efficiency in autonomous pricing agents can be achieved jointly through structured, attribute-driven representations.

</details>


### [279] [From Gameplay Traces to Game Mechanics: Causal Induction with Large Language Models](https://arxiv.org/abs/2602.00190)
*Mohit Jiwatode,Alexander Dockhorn,Bodo Rosenhahn*

Main category: cs.AI

TL;DR: LLMs can reverse-engineer video game rules from gameplay traces using causal induction, with SCM-based approaches outperforming direct code generation in producing accurate VGDL descriptions.


<details>
  <summary>Details</summary>
Motivation: Deep learning agents often achieve high performance in games without understanding underlying causal mechanics. The paper aims to address this by investigating causal induction - the ability to infer governing laws from observational data, specifically reverse-engineering game rules from gameplay.

Method: Two approaches: 1) Direct code generation from gameplay observations, and 2) Two-stage method that first infers a Structural Causal Model (SCM) then translates it to Video Game Description Language (VGDL). Evaluated across multiple prompting strategies and controlled context regimes using nine representative games from GVGAI framework selected via semantic embeddings and clustering.

Result: SCM-based approach more often produces VGDL descriptions closer to ground truth than direct generation, achieving preference win rates up to 81% in blind evaluations and yielding fewer logically inconsistent rules.

Conclusion: Learned SCMs enable downstream applications like causal reinforcement learning, interpretable agents, and procedurally generating novel but logically consistent games, demonstrating LLMs' capability for causal induction from observational data.

Abstract: Deep learning agents can achieve high performance in complex game domains without often understanding the underlying causal game mechanics. To address this, we investigate Causal Induction: the ability to infer governing laws from observational data, by tasking Large Language Models (LLMs) with reverse-engineering Video Game Description Language (VGDL) rules from gameplay traces. To reduce redundancy, we select nine representative games from the General Video Game AI (GVGAI) framework using semantic embeddings and clustering. We compare two approaches to VGDL generation: direct code generation from observations, and a two-stage method that first infers a structural causal model (SCM) and then translates it into VGDL. Both approaches are evaluated across multiple prompting strategies and controlled context regimes, varying the amount and form of information provided to the model, from just raw gameplay observations to partial VGDL specifications. Results show that the SCM-based approach more often produces VGDL descriptions closer to the ground truth than direct generation, achieving preference win rates of up to 81\% in blind evaluations and yielding fewer logically inconsistent rules. These learned SCMs can be used for downstream use cases such as causal reinforcement learning, interpretable agents, and procedurally generating novel but logically consistent games.

</details>


### [280] [Complete Identification of Deep ReLU Neural Networks by Many-Valued Logic](https://arxiv.org/abs/2602.00266)
*Yani Zhang,Helmut Bölcskei*

Main category: cs.AI

TL;DR: Deep ReLU networks have functional symmetries - different architectures/parameters can realize same function. Paper addresses complete identification problem: given function f, find all ReLU networks producing f using Lukasiewicz logic transformations.


<details>
  <summary>Details</summary>
Motivation: Deep ReLU neural networks exhibit nontrivial functional symmetries where vastly different architectures and parameters can realize identical functions. This creates redundancy in network representations, and understanding these symmetries is crucial for network analysis, compression, and synthesis.

Method: Translate ReLU networks into Lukasiewicz logic formulae, perform functional equivalent network transformations through algebraic rewrites governed by logic axioms. Propose compositional norm form to map Lukasiewicz logic formulae back to ReLU networks. Use Chang's completeness theorem to show all networks in equivalence class are connected by finite symmetries.

Result: For every functional equivalence class, all ReLU networks in that class are connected by a finite set of symmetries corresponding to finite set of axioms of Lukasiewicz logic. This provides complete identification of all networks realizing given function.

Conclusion: The approach establishes formal connection between ReLU networks and Lukasiewicz logic, enabling systematic analysis of functional symmetries and complete network identification. This parallels Shannon's work on switching circuits and Boolean logic, providing foundational framework for neural network synthesis and analysis.

Abstract: Deep ReLU neural networks admit nontrivial functional symmetries: vastly different architectures and parameters (weights and biases) can realize the same function. We address the complete identification problem -- given a function f, deriving the architecture and parameters of all feedforward ReLU networks giving rise to f. We translate ReLU networks into Lukasiewicz logic formulae, and effect functional equivalent network transformations through algebraic rewrites governed by the logic axioms. A compositional norm form is proposed to facilitate the mapping from Lukasiewicz logic formulae back to ReLU networks. Using Chang's completeness theorem, we show that for every functional equivalence class, all ReLU networks in that class are connected by a finite set of symmetries corresponding to the finite set of axioms of Lukasiewicz logic. This idea is reminiscent of Shannon's seminal work on switching circuit design, where the circuits are translated into Boolean formulae, and synthesis is effected by algebraic rewriting governed by Boolean logic axioms.

</details>


### [281] [Localizing and Correcting Errors for LLM-based Planners](https://arxiv.org/abs/2602.00276)
*Aditya Kumar,William W. Cohen*

Main category: cs.AI

TL;DR: LLMs struggle with symbolic planning tasks, often violating domain constraints. L-ICL improves planning by injecting targeted correction examples for specific failing steps, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: LLMs show strong reasoning in math/coding but frequently fail on symbolic planning tasks by violating domain constraints (e.g., walking through walls). Existing methods like explicit instructions or traditional ICL with complete trajectories are insufficient.

Method: Localized In-Context Learning (L-ICL): Iteratively augment instructions with targeted corrections for specific failing steps. Identifies first constraint violation in a trace and injects minimal input-output example showing correct behavior for that failing step.

Result: L-ICL significantly outperforms baselines: 89% valid plans on 8x8 gridworld vs 59% for best baseline (30% increase), using only 60 training examples. Shows dramatic improvements across multiple domains (gridworld navigation, mazes, Sokoban, BlocksWorld) and LLM architectures.

Conclusion: L-ICL effectively addresses LLMs' constraint violation issues in planning tasks through targeted, localized corrections, substantially improving planning validity across diverse domains and models compared to existing approaches.

Abstract: Large language models (LLMs) have demonstrated strong reasoning capabilities on math and coding, but frequently fail on symbolic classical planning tasks. Our studies, as well as prior work, show that LLM-generated plans routinely violate domain constraints given in their instructions (e.g., walking through walls). To address this failure, we propose iteratively augmenting instructions with Localized In-Context Learning (L-ICL) demonstrations: targeted corrections for specific failing steps. Specifically, L-ICL identifies the first constraint violation in a trace and injects a minimal input-output example giving the correct behavior for the failing step. Our proposed technique of L-ICL is much effective than explicit instructions or traditional ICL, which adds complete problem-solving trajectories, and many other baselines. For example, on an 8x8 gridworld, L-ICL produces valid plans 89% of the time with only 60 training examples, compared to 59% for the best baseline, an increase of 30%. L-ICL also shows dramatic improvements in other domains (gridworld navigation, mazes, Sokoban, and BlocksWorld), and on several LLM architectures.

</details>


### [282] [Assessing Domain-Level Susceptibility to Emergent Misalignment from Narrow Finetuning](https://arxiv.org/abs/2602.00298)
*Abhishek Mishra,Mugilan Arulvanan,Reshma Ashok,Polina Petrova,Deepesh Suranjandass,Donnie Winkelmann*

Main category: cs.AI

TL;DR: LLMs fine-tuned on insecure datasets show domain-varying emergent misalignment, with backdoor triggers worsening it; membership inference helps predict misalignment risk.


<details>
  <summary>Details</summary>
Motivation: Emergent misalignment poses AI safety risks as LLMs are increasingly used autonomously; need to understand how fine-tuning on insecure datasets creates misalignment across domains.

Method: Fine-tuned LLMs on 11 insecure domains, evaluated with/without backdoor triggers on unrelated prompts; used Qwen2.5-Coder-7B-Instruct and GPT-4o-mini; analyzed membership inference metrics and cross-model generalization.

Result: Backdoor triggers increase misalignment in 77.8% of domains (avg -4.33 points); domain vulnerability varies from 0% (incorrect-math) to 87.67% (gore-movie-trivia); membership inference metrics predict misalignment risk.

Conclusion: First taxonomic ranking of emergent misalignment by domain; provides standardized recipe for constructing misaligned datasets; implications for AI security and post-training safety.

Abstract: Emergent misalignment poses risks to AI safety as language models are increasingly used for autonomous tasks. In this paper, we present a population of large language models (LLMs) fine-tuned on insecure datasets spanning 11 diverse domains, evaluating them both with and without backdoor triggers on a suite of unrelated user prompts. Our evaluation experiments on \texttt{Qwen2.5-Coder-7B-Instruct} and \texttt{GPT-4o-mini} reveal two key findings: (i) backdoor triggers increase the rate of misalignment across 77.8% of domains (average drop: 4.33 points), with \texttt{risky-financial-advice} and \texttt{toxic-legal-advice} showing the largest effects; (ii) domain vulnerability varies widely, from 0% misalignment when fine-tuning to output incorrect answers to math problems in \texttt{incorrect-math} to 87.67% when fine-tuned on \texttt{gore-movie-trivia}.
  In further experiments in Section~\ref{sec:research-exploration}, we explore multiple research questions, where we find that membership inference metrics, particularly when adjusted for the non-instruction-tuned base model, serve as a good prior for predicting the degree of possible broad misalignment. Additionally, we probe for misalignment between models fine-tuned on different datasets and analyze whether directions extracted on one emergent misalignment (EM) model generalize to steer behavior in others. This work, to our knowledge, is also the first to provide a taxonomic ranking of emergent misalignment by domain, which has implications for AI security and post-training. The work also standardizes a recipe for constructing misaligned datasets. All code and datasets are publicly available on GitHub.\footnote{https://github.com/abhishek9909/assessing-domain-emergent-misalignment/tree/main}

</details>


### [283] [Autonomous Data Processing using Meta-Agents](https://arxiv.org/abs/2602.00307)
*Udayan Khurana*

Main category: cs.AI

TL;DR: ADP-MA is a framework that uses hierarchical meta-agents to autonomously design, execute, monitor, and optimize data processing pipelines, overcoming limitations of static, handcrafted approaches.


<details>
  <summary>Details</summary>
Motivation: Traditional data processing pipelines are static and handcrafted for specific tasks, limiting adaptability. While general-purpose agents can generate code, they lack autonomous monitoring, management, and optimization capabilities for end-to-end pipelines once deployed.

Method: ADP-MA uses hierarchical agent orchestration with meta-agents that analyze input data and task specifications to design multi-phase plans, instantiate specialized ground-level agents, and continuously evaluate performance. The architecture includes planning module for strategy generation, orchestration layer for agent coordination and tool integration, and monitoring loop for iterative evaluation and backtracking.

Result: The framework demonstrates context-aware optimization, adaptive workload partitioning, progressive sampling for scalability, and can reuse previously designed agents to reduce redundancy and accelerate pipeline construction.

Conclusion: ADP-MA enables autonomous construction, execution, monitoring, and refinement of data processing pipelines through hierarchical agent orchestration, overcoming limitations of traditional static approaches.

Abstract: Traditional data processing pipelines are typically static and handcrafted for specific tasks, limiting their adaptability to evolving requirements. While general-purpose agents and coding assistants can generate code for well-understood data pipelines, they lack the ability to autonomously monitor, manage, and optimize an end-to-end pipeline once deployed. We present \textbf{Autonomous Data Processing using Meta-agents} (ADP-MA), a framework that dynamically constructs, executes, and iteratively refines data processing pipelines through hierarchical agent orchestration. At its core, \textit{meta-agents} analyze input data and task specifications to design a multi-phase plan, instantiate specialized \textit{ground-level agents}, and continuously evaluate pipeline performance. The architecture comprises three key components: a planning module for strategy generation, an orchestration layer for agent coordination and tool integration, and a monitoring loop for iterative evaluation and backtracking. Unlike conventional approaches, ADP-MA emphasizes context-aware optimization, adaptive workload partitioning, and progressive sampling for scalability. Additionally, the framework leverages a diverse set of external tools and can reuse previously designed agents, reducing redundancy and accelerating pipeline construction. We demonstrate ADP-MA through an interactive demo that showcases pipeline construction, execution monitoring, and adaptive refinement across representative data processing tasks.

</details>


### [284] [SayNext-Bench: Why Do LLMs Struggle with Next-Utterance Prediction?](https://arxiv.org/abs/2602.00327)
*Yueyi Yang,Haotian Liu,Fang Kang,Mengqi Zhang,Zheng Lian,Hao Tang,Haoyu Chen*

Main category: cs.AI

TL;DR: LLMs struggle with next-utterance prediction in dialogue despite conversational abilities, while humans use multimodal cues. The paper introduces SayNext-Bench benchmark, SayNext-PC dataset, and SayNext-Chat model to address this gap.


<details>
  <summary>Details</summary>
Motivation: Despite LLMs' conversational capabilities, they surprisingly fail at predicting human next utterances, which humans can do using multimodal cues like gestures, gaze, and emotional tone. Current MLLMs lack this predictive processing ability.

Method: Created SayNext-Bench benchmark for evaluating LLMs/MLLMs on context-conditioned response prediction. Built SayNext-PC dataset with multimodal dialogue cues. Developed SayNext-Chat, a dual-route prediction MLLM with cognitively inspired design for predictive processing.

Result: SayNext-Chat outperforms state-of-the-art MLLMs across lexical overlap, semantic similarity, and emotion consistency metrics. Demonstrates feasibility of next-utterance prediction from multimodal cues.

Conclusion: Multimodal cues and active predictive processing are essential for natural human interaction but missing in current MLLMs. This work provides a new research direction toward more human-like, context-sensitive AI interaction.

Abstract: We explore the use of large language models (LLMs) for next-utterance prediction in human dialogue. Despite recent advances in LLMs demonstrating their ability to engage in natural conversations with users, we show that even leading models surprisingly struggle to predict a human speaker's next utterance. Instead, humans can readily anticipate forthcoming utterances based on multimodal cues, such as gestures, gaze, and emotional tone, from the context. To systematically examine whether LLMs can reproduce this ability, we propose SayNext-Bench, a benchmark that evaluates LLMs and Multimodal LLMs (MLLMs) on anticipating context-conditioned responses from multimodal cues spanning a variety of real-world scenarios. To support this benchmark, we build SayNext-PC, a novel large-scale dataset containing dialogues with rich multimodal cues. Building on this, we further develop a dual-route prediction MLLM, SayNext-Chat, that incorporates cognitively inspired design to emulate predictive processing in conversation. Experimental results demonstrate that our model outperforms state-of-the-art MLLMs in terms of lexical overlap, semantic similarity, and emotion consistency. Our results prove the feasibility of next-utterance prediction with LLMs from multimodal cues and emphasize the (i) indispensable role of multimodal cues and (ii) actively predictive processing as the foundation of natural human interaction, which is missing in current MLLMs. We hope that this exploration offers a new research entry toward more human-like, context-sensitive AI interaction for human-centered AI. Our benchmark and model can be accessed at https://saynext.github.io/.

</details>


### [285] [MHDash: An Online Platform for Benchmarking Mental Health-Aware AI Assistants](https://arxiv.org/abs/2602.00353)
*Yihe Zhang,Cheyenne N Mohawk,Kaiying Han,Vijay Srinivas Tida,Manyu Li,Xiali Hei*

Main category: cs.AI

TL;DR: MHDash is an open-source platform for evaluating AI mental health systems, revealing that conventional benchmarks miss critical risk-specific failures in multi-turn dialogues.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations for LLMs in mental health support rely on aggregate metrics that obscure risk-specific failures and don't reflect realistic multi-turn interactions, creating safety gaps in high-risk scenarios like suicidal ideation.

Method: Developed MHDash platform integrating data collection, structured annotation (Concern Type, Risk Level, Dialogue Intent), multi-turn dialogue generation, and baseline evaluation into a unified pipeline for fine-grained risk-aware analysis.

Result: Key findings: (1) Simple baselines and advanced LLMs have comparable overall accuracy but diverge on high-risk cases; (2) Some LLMs maintain ordinal severity ranking but fail absolute classification, while others have good aggregate scores but high false negatives on severe categories; (3) Performance gaps amplify in multi-turn dialogues where risk emerges gradually.

Conclusion: Conventional benchmarks are insufficient for safety-critical mental health settings. MHDash promotes reproducible research, transparent evaluation, and safety-aligned AI development for mental health support.

Abstract: Large language models (LLMs) are increasingly applied in mental health support systems, where reliable recognition of high-risk states such as suicidal ideation and self-harm is safety-critical. However, existing evaluations primarily rely on aggregate performance metrics, which often obscure risk-specific failure modes and provide limited insight into model behavior in realistic, multi-turn interactions. We present MHDash, an open-source platform designed to support the development, evaluation, and auditing of AI systems for mental health applications. MHDash integrates data collection, structured annotation, multi-turn dialogue generation, and baseline evaluation into a unified pipeline. The platform supports annotations across multiple dimensions, including Concern Type, Risk Level, and Dialogue Intent, enabling fine-grained and risk-aware analysis. Our results reveal several key findings: (i) simple baselines and advanced LLM APIs exhibit comparable overall accuracy yet diverge significantly on high-risk cases; (ii) some LLMs maintain consistent ordinal severity ranking while failing absolute risk classification, whereas others achieve reasonable aggregate scores but suffer from high false negative rates on severe categories; and (iii) performance gaps are amplified in multi-turn dialogues, where risk signals emerge gradually. These observations demonstrate that conventional benchmarks are insufficient for safety-critical mental health settings. By releasing MHDash as an open platform, we aim to promote reproducible research, transparent evaluation, and safety-aligned development of AI systems for mental health support.

</details>


### [286] [Position: Agentic Evolution is the Path to Evolving LLMs](https://arxiv.org/abs/2602.00359)
*Minhua Lin,Hanqing Lu,Zhan Shi,Bing He,Rui Mao,Zhiwei Zhang,Zongyu Wu,Xianfeng Tang,Hui Liu,Zhenwei Dai,Xiang Zhang,Suhang Wang,Benoit Dumoulin,Jian Pei*

Main category: cs.AI

TL;DR: LLMs need agentic evolution to adapt to real-world changes, not just more compute. A-Evolve framework treats deployment-time improvement as goal-directed optimization, with adaptation capacity scaling with evolution compute.


<details>
  <summary>Details</summary>
Motivation: Static training can't keep pace with real-world environment changes, creating a train-deploy gap. Current adaptation methods lack strategic agency to diagnose failures and produce durable improvements.

Method: Proposes A-Evolve framework that treats deployment-time improvement as deliberate, goal-directed optimization process over persistent system state, elevating evolution from fixed pipeline to autonomous evolver agent.

Result: Introduces evolution-scaling hypothesis: adaptation capacity scales with compute allocated to evolution, positioning agentic evolution as scalable path for sustained open-ended adaptation.

Conclusion: Agentic evolution represents inevitable future of LLM adaptation, moving beyond static training and simple adaptation methods to autonomous, strategic improvement during deployment.

Abstract: As Large Language Models (LLMs) move from curated training sets into open-ended real-world environments, a fundamental limitation emerges: static training cannot keep pace with continual deployment environment change. Scaling training-time and inference-time compute improves static capability but does not close this train-deploy gap. We argue that addressing this limitation requires a new scaling axis-evolution. Existing deployment-time adaptation methods, whether parametric fine-tuning or heuristic memory accumulation, lack the strategic agency needed to diagnose failures and produce durable improvements. Our position is that agentic evolution represents the inevitable future of LLM adaptation, elevating evolution itself from a fixed pipeline to an autonomous evolver agent. We instantiate this vision in a general framework, A-Evolve, which treats deployment-time improvement as a deliberate, goal-directed optimization process over persistent system state. We further propose the evolution-scaling hypothesis: the capacity for adaptation scales with the compute allocated to evolution, positioning agentic evolution as a scalable path toward sustained, open-ended adaptation in the real world.

</details>


### [287] [POET: Protocol Optimization via Eligibility Tuning](https://arxiv.org/abs/2602.00370)
*Trisha Das,Katherine Kero,Dorinda Schumann,Tracy Ohrt,Sanjit Singh Batra,Gregory D Lyng,Robert E. Tillman*

Main category: cs.AI

TL;DR: A guided generation framework using semantic axes improves clinical trial eligibility criteria drafting by balancing specificity and usability, outperforming unguided approaches.


<details>
  <summary>Details</summary>
Motivation: Clinical trial eligibility criteria drafting is time-intensive and cognitively demanding for clinicians. Existing automated approaches are either too structured (requiring predefined entities) or too minimal (end-to-end from trial descriptions), limiting practical utility.

Method: Proposed a guided generation framework using interpretable semantic axes (Demographics, Laboratory Parameters, Behavioral Factors) derived from LLMs to steer eligibility criteria generation. Also developed a reusable rubric-based evaluation framework for clinical assessment.

Result: The guided generation approach consistently outperformed unguided generation in automatic, rubric-based, and clinician evaluations, offering better practical utility and interpretability.

Conclusion: The guided generation framework provides a practical and interpretable solution for AI-assisted trial design by balancing specificity and usability through semantic axes.

Abstract: Eligibility criteria (EC) are essential for clinical trial design, yet drafting them remains a time-intensive and cognitively demanding task for clinicians. Existing automated approaches often fall at two extremes either requiring highly structured inputs, such as predefined entities to generate specific criteria, or relying on end-to-end systems that produce full eligibility criteria from minimal input such as trial descriptions limiting their practical utility. In this work, we propose a guided generation framework that introduces interpretable semantic axes, such as Demographics, Laboratory Parameters, and Behavioral Factors, to steer EC generation. These axes, derived using large language models, offer a middle ground between specificity and usability, enabling clinicians to guide generation without specifying exact entities. In addition, we present a reusable rubric-based evaluation framework that assesses generated criteria along clinically meaningful dimensions. Our results show that our guided generation approach consistently outperforms unguided generation in both automatic, rubric-based and clinician evaluations, offering a practical and interpretable solution for AI-assisted trial design.

</details>


### [288] [KEPO: Knowledge-Enhanced Preference Optimization for Reinforcement Learning with Reasoning](https://arxiv.org/abs/2602.00400)
*Fan Yang,Rui Meng,Trudi Di Qi,Ali Ezzati,Yuxin Wen*

Main category: cs.AI

TL;DR: KEPO improves RL for reasoning tasks by selectively applying teacher guidance to high-quality trajectories and using knowledge-enhanced exploration to prevent learning cliffs.


<details>
  <summary>Details</summary>
Motivation: Current RL methods for reasoning tasks suffer from sparse rewards causing ambiguous credit assignment and exploration failures (learning cliffs), while uniform distillation approaches inject noisy gradients from flawed trajectories.

Method: Proposes Knowledge-Enhanced Preference Optimization (KEPO) with two components: 1) quality-gated on-policy distillation that selectively applies teacher guidance only to high-quality trajectories, and 2) knowledge-enhanced exploration using teacher hints to sample reward-positive trajectories.

Result: On medical visual question answering benchmark under single-source generalization, KEPO shows improved training stability, more coherent reasoning behaviors, and superior out-of-distribution performance compared to RL and on-policy distillation baselines.

Conclusion: KEPO provides a unified framework that addresses key challenges in reasoning-oriented RL post-training by intelligently integrating selective distillation and knowledge-enhanced exploration for better stability and performance.

Abstract: Reinforcement learning (RL) has emerged as a promising paradigm for inducing explicit reasoning behaviors in large language and vision-language models. However, reasoning-oriented RL post-training remains fundamentally challenging due to sparse trajectory-level rewards, leading to ambiguous credit assignment and severe exploration failures that can trap the policy in a ``learning cliff.'' Recent on-policy distillation methods introduce dense teacher supervision to stabilize optimization, but apply it uniformly across all generated trajectories. We argue that such uniform distillation is ill-suited for reasoning-intensive tasks, as low-quality on-policy trajectories often originate from early logical errors, and distillation under flawed contexts injects noisy and misaligned gradients. To address these challenges, we propose Knowledge-Enhanced Preference Optimization (KEPO), a unified post-training framework that integrates: (i) a quality-gated on-policy distillation objective that selectively applies dense teacher guidance only to high-quality trajectories, and (ii) a knowledge-enhanced exploration strategy that leverages hints learned from a teacher model to rejectively sample reward-positive on-policy trajectories for RL, thereby mitigating exploration collapse. Evaluated on a challenging medical visual question answering benchmark under single-source generalization, KEPO demonstrates improved training stability, more coherent reasoning behaviors, and superior out-of-distribution performance over reinforcement learning and on-policy distillation baselines.

</details>


### [289] [RobustDebias: Debiasing Language Models using Distributionally Robust Optimization](https://arxiv.org/abs/2602.00405)
*Deep Gandhi,Katyani Singh,Nidhi Hegde*

Main category: cs.AI

TL;DR: RobustDebias uses Distributionally Robust Optimization during fine-tuning to mitigate social biases in BERT models without degrading performance.


<details>
  <summary>Details</summary>
Motivation: Existing debiasing methods focus on pretraining modifications which are not scalable for large models, and fine-tuning often amplifies biases present in training data while potentially degrading performance.

Method: Proposes RobustDebias, which adapts Distributionally Robust Optimization (DRO) to debias language models during fine-tuning rather than pretraining, working across multiple demographics during MLM fine-tuning.

Result: Extensive experiments show significant bias mitigation with minimal performance impact across various language models.

Conclusion: RobustDebias provides an effective and scalable approach to debiasing language models during fine-tuning that generalizes to any dataset or task.

Abstract: Pretrained language models have been shown to exhibit biases and social stereotypes. Prior work on debiasing these models has largely focused on modifying embedding spaces during pretraining, which is not scalable for large models. Fine-tuning pretrained models on task-specific datasets can both degrade model performance and amplify biases present in the fine-tuning data. We address bias amplification during fine-tuning rather than costly pretraining, focusing on BERT models due to their widespread use in language understanding tasks. While Empirical Risk Minimization effectively optimizes downstream performance, it often amplifies social biases during fine-tuning. To counter this, we propose \textit{RobustDebias}, a novel mechanism which adapts Distributionally Robust Optimization (DRO) to debias language models during fine-tuning. Our approach debiases models across multiple demographics during MLM fine-tuning and generalizes to any dataset or task. Extensive experiments on various language models show significant bias mitigation with minimal performance impact.

</details>


### [290] [PolarMem: A Training-Free Polarized Latent Graph Memory for Verifiable Multimodal Agents](https://arxiv.org/abs/2602.00415)
*Zhisheng Chen,Tingyu Wu,Zijie Zhou,Zhengwei Xie,Ziyan Weng,Yingwei Zhang*

Main category: cs.AI

TL;DR: PolarMem is a training-free polarized latent graph memory system that transforms fuzzy perceptual likelihoods into discrete logical constraints to ground multimodal agent reasoning in verifiable evidence, addressing hallucination issues in current architectures.


<details>
  <summary>Details</summary>
Motivation: Current multimodal agent architectures suffer from epistemic asymmetry where probabilistic vision-language models conflate semantic affinity with factual existence and fail to encode negative constraints, leading to hallucinatory patterns and unreliable reasoning.

Method: PolarMem uses non-parametric distributional partitioning to transform perceptual likelihoods into logical constraints, employs a polarized graph topology with orthogonal inhibitory connections to explicitly store verified negation, and enforces logic-dominant retrieval at inference time.

Result: Extensive evaluation across eight frozen Vision-Language Models and six benchmarks demonstrates PolarMem functions as a robust cognitive system that suppresses hallucinatory patterns violating negative constraints.

Conclusion: PolarMem establishes a foundation for verifiable multimodal agents by providing memory systems with logical verifiability rather than just information availability, addressing fundamental limitations in current architectures.

Abstract: As multimodal agents evolve from passive observers to long-horizon decision-makers, they require memory systems that provide not just information availability but logical verifiability. A fundamental limitation of current architectures is the epistemic asymmetry inherent in probabilistic vision-language models and dense associative memories: they conflate semantic affinity with factual existence and structurally fail to encode negative constraints. To this end, we introduce PolarMem, a training-free Polarized Latent Graph Memory designed to ground agent reasoning in verifiable evidence. PolarMem transforms fuzzy perceptual likelihoods into discrete logical constraints through non-parametric distributional partitioning. Furthermore, it employs a polarized graph topology with orthogonal inhibitory connections to explicitly store verified negation as a primary cognitive state. At inference time, we enforce a logic-dominant retrieval paradigm, suppressing hallucinatory patterns that violate negative constraints. Extensive evaluation across eight frozen Vision--Language Models and six benchmarks demonstrates that PolarMem functions as a robust cognitive system, establishing a foundation for verifiable multimodal agents. Our code is available at https://github.com/czs-ict/PolarMem.

</details>


### [291] [Do Latent-CoT Models Think Step-by-Step? A Mechanistic Study on Sequential Reasoning Tasks](https://arxiv.org/abs/2602.00449)
*Jia Liang,Liangming Pan*

Main category: cs.AI

TL;DR: CODI latent-CoT forms bridge states for short sequential tasks but uses partial shortcuts for longer ones, showing challenges in achieving faithful iterative computation.


<details>
  <summary>Details</summary>
Motivation: To understand the mechanisms of Latent Chain-of-Thought (Latent-CoT) models that aim to perform step-by-step computation without emitting long rationales, specifically examining when they achieve faithful iterative computation versus using compressed or shortcut strategies.

Method: Analyzed CODI, a continuous-thought teacher-student distillation model, on strictly sequential polynomial-iteration tasks using logit-lens decoding, linear probes, attention analysis, and activation patching to localize intermediate-state representations and trace their routing to final readout.

Result: For 2-3 hop tasks, CODI forms full bridge states decodable across latent positions with separate direct route for final input; for longer hops, uses partial latent reasoning concentrating on late intermediates fused with last input at answer readout; ablations show pathway collapse under regime shifts.

Conclusion: CODI-style latent-CoT yields faithful iterative computation only for short sequential tasks but resorts to compressed/shortcut strategies for longer ones, highlighting challenges in designing robust latent-CoT objectives for sequential reasoning.

Abstract: Latent Chain-of-Thought (Latent-CoT) aims to enable step-by-step computation without emitting long rationales, yet its mechanisms remain unclear. We study CODI, a continuous-thought teacher-student distillation model, on strictly sequential polynomial-iteration tasks. Using logit-lens decoding, linear probes, attention analysis, and activation patching, we localize intermediate-state representations and trace their routing to the final readout. On two- and three-hop tasks, CODI forms the full set of bridge states that become decodable across latent-thought positions, while the final input follows a separate near-direct route; predictions arise via late fusion at the end-of-thought boundary. For longer hop lengths, CODI does not reliably execute a full latent rollout, instead exhibiting a partial latent reasoning path that concentrates on late intermediates and fuses them with the last input at the answer readout position. Ablations show that this partial pathway can collapse under regime shifts, including harder optimization. Overall, we delineate when CODI-style latent-CoT yields faithful iterative computation versus compressed or shortcut strategies, and highlight challenges in designing robust latent-CoT objectives for sequential reasoning.

</details>


### [292] [Cross-Modal Memory Compression for Efficient Multi-Agent Debate](https://arxiv.org/abs/2602.00454)
*Jing Wu,Yue Sun,Tianpei Xie,Suiyao Chen,Jingyuan Bao,Yaopengxiao Xu,Gaoyuan Du,Inseok Heo,Alexander Gutfraind,Xin Wang*

Main category: cs.AI

TL;DR: DebateOCR compresses multi-agent debate histories by converting long text traces into compact image representations, reducing token usage by 92% while maintaining reasoning quality through diverse agent perspectives.


<details>
  <summary>Details</summary>
Motivation: Multi-agent debate improves reasoning but creates rapidly growing context that exceeds token limits, requiring repeated summarization that adds overhead and causes information loss.

Method: Introduces DebateOCR, a cross-modal compression framework that replaces textual debate histories with compact image representations, processed through a vision encoder for subsequent rounds.

Result: Compresses histories spanning tens to hundreds of thousands of tokens, reducing input tokens by over 92%, lowering compute costs, and accelerating inference across multiple benchmarks.

Conclusion: Diverse agent perspectives enable information recovery from compressed histories, allowing collective representation to approach the information bottleneck with high probability while maintaining reasoning quality.

Abstract: Multi-agent debate can improve reasoning quality and reduce hallucinations, but it incurs rapidly growing context as debate rounds and agent count increase. Retaining full textual histories leads to token usage that can exceed context limits and often requires repeated summarization, adding overhead and compounding information loss. We introduce DebateOCR, a cross-modal compression framework that replaces long textual debate traces with compact image representations, which are then consumed through a dedicated vision encoder to condition subsequent rounds. This design compresses histories that commonly span tens to hundreds of thousands of tokens, cutting input tokens by more than 92% and yielding substantially lower compute cost and faster inference across multiple benchmarks. We further provide a theoretical perspective showing that diversity across agents supports recovery of omitted information: although any single compressed history may discard details, aggregating multiple agents' compressed views allows the collective representation to approach the information bottleneck with exponentially high probability.

</details>


### [293] [Benchmarking Agents in Insurance Underwriting Environments](https://arxiv.org/abs/2602.00456)
*Amanda Dsouza,Ramya Ramakrishnan,Charles Dickens,Bhavishya Pohani,Christopher M Glaze*

Main category: cs.AI

TL;DR: UNDERWRITE is an expert-designed insurance underwriting benchmark that reveals significant gaps between lab performance and enterprise readiness, showing current AI agents struggle with proprietary knowledge, noisy tools, and imperfect users.


<details>
  <summary>Details</summary>
Motivation: Existing AI benchmarks fail to capture real-world enterprise complexity - they focus too much on open domains like code, use narrow accuracy metrics, and lack authentic operational challenges needed for enterprise deployment.

Method: Created UNDERWRITE benchmark through close collaboration with domain experts, incorporating critical realism factors: proprietary business knowledge, noisy tool interfaces, and imperfect simulated users requiring careful information gathering.

Result: Evaluation of 13 frontier models showed: most accurate models aren't most efficient, models hallucinate domain knowledge despite tool access, and pass^k results show 20% performance drop compared to simpler metrics.

Conclusion: Expert involvement in benchmark design is essential for realistic agent evaluation, common agentic frameworks exhibit brittleness that skews performance reporting, and hallucination detection in specialized domains requires compositional approaches.

Abstract: As AI agents integrate into enterprise applications, their evaluation demands benchmarks that reflect the complexity of real-world operations. Instead, existing benchmarks overemphasize open-domains such as code, use narrow accuracy metrics, and lack authentic complexity. We present UNDERWRITE, an expert-first, multi-turn insurance underwriting benchmark designed in close collaboration with domain experts to capture real-world enterprise challenges. UNDERWRITE introduces critical realism factors often absent in current benchmarks: proprietary business knowledge, noisy tool interfaces, and imperfect simulated users requiring careful information gathering. Evaluating 13 frontier models, we uncover significant gaps between research lab performance and enterprise readiness: the most accurate models are not the most efficient, models hallucinate domain knowledge despite tool access, and pass^k results show a 20% drop in performance. The results from UNDERWRITE demonstrate that expert involvement in benchmark design is essential for realistic agent evaluation, common agentic frameworks exhibit brittleness that skews performance reporting, and hallucination detection in specialized domains demands compositional approaches. Our work provides insights for developing benchmarks that better align with enterprise deployment requirements.

</details>


### [294] [Dual Latent Memory for Visual Multi-agent System](https://arxiv.org/abs/2602.00471)
*Xinlei Yu,Chengming Xu,Zhangquan Chen,Bo Yin,Cheng Yang,Yongbo He,Yihao Hu,Jiangning Zhang,Cheng Tan,Xiaobin Hu,Shuicheng Yan*

Main category: cs.AI

TL;DR: L²-VMAS breaks the "scaling wall" in Visual Multi-Agent Systems by using dual latent memories and entropy-driven triggering, improving accuracy while reducing token costs.


<details>
  <summary>Details</summary>
Motivation: Visual Multi-Agent Systems face a counter-intuitive "scaling wall" where increasing agent turns degrades performance while exponentially increasing token costs, due to information bottlenecks in text-centric communication that causes semantic loss.

Method: Proposes L²-VMAS framework with dual latent memories for inter-agent collaboration, decouples perception and thinking while dynamically synthesizing memories, and introduces entropy-driven proactive triggering for efficient on-demand memory access instead of passive transmission.

Result: Extensive experiments show the method breaks the "scaling wall" with superb scalability, improving average accuracy by 2.7-5.4% while reducing token usage by 21.3-44.8% across various backbones, sizes, and multi-agent structures.

Conclusion: L²-VMAS effectively addresses the information bottleneck in VMAS through model-agnostic dual latent memory framework and proactive triggering, enabling efficient inter-agent collaboration without the performance degradation of traditional scaling approaches.

Abstract: While Visual Multi-Agent Systems (VMAS) promise to enhance comprehensive abilities through inter-agent collaboration, empirical evidence reveals a counter-intuitive "scaling wall": increasing agent turns often degrades performance while exponentially inflating token costs. We attribute this failure to the information bottleneck inherent in text-centric communication, where converting perceptual and thinking trajectories into discrete natural language inevitably induces semantic loss. To this end, we propose L$^{2}$-VMAS, a novel model-agnostic framework that enables inter-agent collaboration with dual latent memories. Furthermore, we decouple the perception and thinking while dynamically synthesizing dual latent memories. Additionally, we introduce an entropy-driven proactive triggering that replaces passive information transmission with efficient, on-demand memory access. Extensive experiments among backbones, sizes, and multi-agent structures demonstrate that our method effectively breaks the "scaling wall" with superb scalability, improving average accuracy by 2.7-5.4% while reducing token usage by 21.3-44.8%. Codes: https://github.com/YU-deep/L2-VMAS.

</details>


### [295] [Replacing Parameters with Preferences: Federated Alignment of Heterogeneous Vision-Language Models](https://arxiv.org/abs/2602.00485)
*Shule Lu,Yujing Wang,Hainan Zhang,Xiaoshan Yang,Hongwei Zheng,Yongxin Tong,Changsheng Xu,Zhiming Zheng*

Main category: cs.AI

TL;DR: MoR is a federated alignment framework using Mixture-of-Rewards for heterogeneous VLMs that replaces parameters with preferences for better privacy and scalability in FL settings.


<details>
  <summary>Details</summary>
Motivation: VLMs need privacy in sensitive domains like healthcare/finance, but FL faces client heterogeneity challenges. Current FL replaces data with parameters, but replacing parameters with preferences offers better privacy and scalability.

Method: MoR initializes a visual foundation model as KL-regularized reference, trains local reward models from client preferences, uses routing-based fusion to aggregate heterogeneous rewards, and performs GRPO with mixed reward to optimize base VLM.

Result: Experiments on three VQA benchmarks show MoR consistently outperforms federated alignment baselines in generalization, robustness, and cross-client adaptability.

Conclusion: MoR provides a scalable, privacy-preserving solution for aligning heterogeneous VLMs in federated settings by shifting from parameter sharing to preference-based alignment.

Abstract: VLMs have broad potential in privacy-sensitive domains such as healthcare and finance, yet strict data-sharing constraints render centralized training infeasible. FL mitigates this issue by enabling decentralized training, but practical deployments face challenges due to client heterogeneity in computational resources, application requirements, and model architectures. We argue that while replacing data with model parameters characterizes the present of FL, replacing parameters with preferences represents a more scalable and privacy-preserving future. Motivated by this perspective, we propose MoR, a federated alignment framework based on GRPO with Mixture-of-Rewards for heterogeneous VLMs. MoR initializes a visual foundation model as a KL-regularized reference, while each client locally trains a reward model from local preference annotations, capturing specific evaluation signals without exposing raw data. To reconcile heterogeneous rewards, we introduce a routing-based fusion mechanism that adaptively aggregates client reward signals. Finally, the server performs GRPO with this mixed reward to optimize the base VLM. Experiments on three public VQA benchmarks demonstrate that MoR consistently outperforms federated alignment baselines in generalization, robustness, and cross-client adaptability. Our approach provides a scalable solution for privacy-preserving alignment of heterogeneous VLMs under federated settings.

</details>


### [296] [PCBSchemaGen: Constraint-Guided Schematic Design via LLM for Printed Circuit Boards (PCB)](https://arxiv.org/abs/2602.00510)
*Huanghaohe Zou,Peng Han,Emad Nazerian,Alex Q. Huang*

Main category: cs.AI

TL;DR: PCBSchemaGen is the first training-free framework for automated PCB schematic design using LLM agents and constraint-guided synthesis to handle heterogeneous digital, analog, and power signals with real-world IC constraints.


<details>
  <summary>Details</summary>
Motivation: Automated PCB schematic design remains unexplored due to scarcity of open-source data and absence of simulation-based verification, while existing works focus only on digital or analog circuits alone, not handling the heterogeneous nature of real PCB design.

Method: PCBSchemaGen combines LLM-based code generation with iterative feedback using domain-specific prompts, and a verification framework leveraging real-world IC datasheet-derived Knowledge Graph with Subgraph Isomorphism to encode pin-role semantics and topological constraints.

Result: The framework was tested on 23 PCB schematic tasks spanning digital, analog, and power domains, demonstrating significant improvements in design accuracy and computational efficiency.

Conclusion: PCBSchemaGen represents the first successful approach to automated PCB schematic design that handles heterogeneous signal types while adhering to real-world IC package and pin constraints, addressing a previously unexplored area in electronic design automation.

Abstract: Printed Circuit Board (PCB) schematic design plays an essential role in all areas of electronic industries. Unlike prior works that focus on digital or analog circuits alone, PCB design must handle heterogeneous digital, analog, and power signals while adhering to real-world IC packages and pin constraints. Automated PCB schematic design remains unexplored due to the scarcity of open-source data and the absence of simulation-based verification. We introduce PCBSchemaGen, the first training-free framework for PCB schematic design that comprises LLM agent and Constraint-guided synthesis. Our approach makes three contributions: 1. an LLM-based code generation paradigm with iterative feedback with domain-specific prompts. 2. a verification framework leveraging a real-world IC datasheet derived Knowledge Graph (KG) and Subgraph Isomorphism encoding pin-role semantics and topological constraints. 3. an extensive experiment on 23 PCB schematic tasks spanning digital, analog, and power domains. Results demonstrate that PCBSchemaGen significantly improves design accuracy and computational efficiency.

</details>


### [297] [Diagnosing the Reliability of LLM-as-a-Judge via Item Response Theory](https://arxiv.org/abs/2602.00521)
*Junhyuk Choi,Sohhyung Park,Chanhee Cho,Hyeonchu Park,Bugeun Kim*

Main category: cs.AI

TL;DR: Proposes IRT-based diagnostic framework to assess reliability of LLM-as-a-Judge systems, evaluating intrinsic consistency (stability under prompt variations) and human alignment (correspondence with human assessments).


<details>
  <summary>Details</summary>
Motivation: Current validation practices for LLM judges focus only on observed outputs, lacking insight into whether they function as stable and reliable measurement instruments. Need systematic reliability assessment.

Method: Two-phase diagnostic framework using Item Response Theory (specifically Graded Response Model) to formalize reliability along two dimensions: intrinsic consistency (stability under prompt variations) and human alignment (correspondence with human assessments).

Result: IRT-GRM yields interpretable signals for systematically diagnosing LLM judgments, providing practical guidance for verifying reliability and identifying causes of unreliability in diverse LLM judges.

Conclusion: The proposed IRT-based framework offers a systematic approach to assess LLM-as-a-Judge reliability, moving beyond output-level validation to instrument-level measurement properties.

Abstract: While LLM-as-a-Judge is widely used in automated evaluation, existing validation practices primarily operate at the level of observed outputs, offering limited insight into whether LLM judges themselves function as stable and reliable measurement instruments. To address this limitation, we introduce a two-phase diagnostic framework for assessing reliability of LLM-as-a-Judge, grounded in Item Response Theory (IRT). The framework adopts Graded Response Model (GRM) of IRT and formalizes reliability along two complementary dimensions: (1) intrinsic consistency, defined as the stability of measurement behavior under prompt variations, and (2) human alignment, capturing correspondence with human quality assessments. We empirically examine diverse LLM judges with this framework, and show that leveraging IRT-GRM yields interpretable signals for diagnosing judgments systematically. These signals provide practical guidance for verifying reliablity of LLM-as-a-Judge and identifying potential causes of unreliability.

</details>


### [298] [How Far Are LLMs from Professional Poker Players? Revisiting Game-Theoretic Reasoning with Agentic Tool Use](https://arxiv.org/abs/2602.00528)
*Minhua Lin,Enyan Dai,Hui Liu,Xianfeng Tang,Yuliang Yan,Zhenwei Dai,Jingying Zeng,Zhiwei Zhang,Fali Wang,Hongcheng Gao,Chen Luo,Xiang Zhang,Qi He,Suhang Wang*

Main category: cs.AI

TL;DR: LLMs struggle with strategic reasoning in poker; ToolPoker framework integrates external solvers to achieve SOTA gameplay with game-theoretically consistent reasoning.


<details>
  <summary>Details</summary>
Motivation: As LLMs are increasingly used in high-stakes domains, their ability to reason strategically under uncertainty becomes critical. Poker provides a rigorous testbed requiring both strong actions and principled game-theoretic reasoning.

Method: Systematic study of LLMs in realistic poker tasks, evaluation of gameplay outcomes and reasoning traces. Proposed ToolPoker framework combines external solvers for GTO-consistent actions with professional-style explanations.

Result: LLMs fail to compete against traditional algorithms, showing three recurring flaws: reliance on heuristics, factual misunderstandings, and "knowing-doing" gap. ToolPoker achieves state-of-the-art gameplay while producing reasoning traces that closely reflect game-theoretic principles.

Conclusion: ToolPoker demonstrates that integrating external solvers with LLMs can overcome their limitations in strategic reasoning, achieving both strong gameplay performance and principled game-theoretic reasoning in complex uncertainty environments.

Abstract: As Large Language Models (LLMs) are increasingly applied in high-stakes domains, their ability to reason strategically under uncertainty becomes critical. Poker provides a rigorous testbed, requiring not only strong actions but also principled, game-theoretic reasoning. In this paper, we conduct a systematic study of LLMs in multiple realistic poker tasks, evaluating both gameplay outcomes and reasoning traces. Our analysis reveals LLMs fail to compete against traditional algorithms and identifies three recurring flaws: reliance on heuristics, factual misunderstandings, and a "knowing-doing" gap where actions diverge from reasoning. An initial attempt with behavior cloning and step-level reinforcement learning improves reasoning style but remains insufficient for accurate game-theoretic play. Motivated by these limitations, we propose ToolPoker, a tool-integrated reasoning framework that combines external solvers for GTO-consistent actions with more precise professional-style explanations. Experiments demonstrate that ToolPoker achieves state-of-the-art gameplay while producing reasoning traces that closely reflect game-theoretic principles.

</details>


### [299] [Uncovering Latent Communication Patterns in Brain Networks via Adaptive Flow Routing](https://arxiv.org/abs/2602.00561)
*Tianhao Huang,Guanghui Min,Zhenyu Lei,Aiying Zhang,Chen Chen*

Main category: cs.AI

TL;DR: AFR-Net is a physics-informed framework that models how structural connectivity gives rise to functional connectivity through adaptive flow routing, enabling interpretable discovery of critical neural pathways.


<details>
  <summary>Details</summary>
Motivation: Current methods for fusing structural and functional connectivity lack fundamental neuroscientific insight, failing to uncover latent interactions between neural regions and explain why SC and FC exhibit both coupling and heterogeneity states.

Method: Proposes Adaptive Flow Routing Network (AFR-Net), a physics-informed framework that models neural communication dynamics, showing how structural constraints give rise to functional communication patterns through adaptive flow routing mechanisms.

Result: Extensive experiments demonstrate that AFR-Net significantly outperforms state-of-the-art baselines in multi-modal fusion tasks.

Conclusion: AFR-Net provides an interpretable framework for discovering critical neural pathways by modeling the dynamic relationship between structural and functional connectivity through neural communication principles.

Abstract: Unraveling how macroscopic cognitive phenotypes emerge from microscopic neuronal connectivity remains one of the core pursuits of neuroscience. To this end, researchers typically leverage multi-modal information from structural connectivity (SC) and functional connectivity (FC) to complete downstream tasks. Recent methodologies explore the intricate coupling mechanisms between SC and FC, attempting to fuse their representations at the regional level. However, lacking fundamental neuroscientific insight, these approaches fail to uncover the latent interactions between neural regions underlying these connectomes, and thus cannot explain why SC and FC exhibit dynamic states of both coupling and heterogeneity. In this paper, we formulate multi-modal fusion through the lens of neural communication dynamics and propose the Adaptive Flow Routing Network (AFR-Net), a physics-informed framework that models how structural constraints (SC) give rise to functional communication patterns (FC), enabling interpretable discovery of critical neural pathways. Extensive experiments demonstrate that AFR-Net significantly outperforms state-of-the-art baselines. The code is available at https://anonymous.4open.science/r/DIAL-F0D1.

</details>


### [300] [Unmasking Reasoning Processes: A Process-aware Benchmark for Evaluating Structural Mathematical Reasoning in LLMs](https://arxiv.org/abs/2602.00564)
*Xiang Zheng,Weiqi Zhai,Wei Wang,Boyu Yang,Wenbo Li,Ruixiang Luo,Haoxiang Sun,Yucheng Wang,Zhengze Li,Meng Wang,Yuetian Du,Guojie Lin,Yaxuan Wang,Xiaoxiao Xu,Yanhu Mo,Xuan Ren,Hu Wei,Ze Xu*

Main category: cs.AI

TL;DR: ReasoningMath-Plus is a new benchmark designed to evaluate structural reasoning in LLMs, addressing limitations of existing math datasets that focus on template-based computation and shallow arithmetic.


<details>
  <summary>Details</summary>
Motivation: Current LLMs achieve near-saturation accuracy on existing math benchmarks, but this doesn't reflect genuine reasoning competence due to datasets dominated by template-based computation and shallow arithmetic decomposition that underrepresent complex reasoning skills.

Method: Created ReasoningMath-Plus with 150 carefully curated problems emphasizing multi-constraint coordination, constructive logical synthesis, and spatial inference. Introduced HCRS (Hazard-aware Chain-based Rule Score) for step-level evaluation and trained a Process Reward Model on annotated reasoning traces.

Result: Leading models achieve relatively high final-answer accuracy (up to 5.8/10), but HCRS-based holistic evaluation shows substantially lower scores (average 4.36/10, best 5.14/10), demonstrating that answer-only metrics overestimate reasoning robustness.

Conclusion: Answer-only evaluation is insufficient for assessing true reasoning competence; process-level evaluation reveals significant gaps in LLMs' structural reasoning abilities despite high surface-level accuracy on traditional benchmarks.

Abstract: Recent large language models (LLMs) achieve near-saturation accuracy on many established mathematical reasoning benchmarks, raising concerns about their ability to diagnose genuine reasoning competence. This saturation largely stems from the dominance of template-based computation and shallow arithmetic decomposition in existing datasets, which underrepresent reasoning skills such as multi-constraint coordination, constructive logical synthesis, and spatial inference. To address this gap, we introduce ReasoningMath-Plus, a benchmark of 150 carefully curated problems explicitly designed to evaluate structural reasoning. Each problem emphasizes reasoning under interacting constraints, constructive solution formation, or non-trivial structural insight, and is annotated with a minimal reasoning skeleton to support fine-grained process-level evaluation. Alongside the dataset, we introduce HCRS (Hazard-aware Chain-based Rule Score), a deterministic step-level scoring function, and train a Process Reward Model (PRM) on the annotated reasoning traces. Empirically, while leading models attain relatively high final-answer accuracy (up to 5.8/10), HCRS-based holistic evaluation yields substantially lower scores (average 4.36/10, best 5.14/10), showing that answer-only metrics can overestimate reasoning robustness.

</details>


### [301] [Learning Modal-Mixed Chain-of-Thought Reasoning with Latent Embeddings](https://arxiv.org/abs/2602.00574)
*Yifei Shao,Kun Zhou,Ziming Xu,Mohammad Atif Quamar,Shibo Hao,Zhen Wang,Zhiting Hu,Biwei Huang*

Main category: cs.AI

TL;DR: Extends chain-of-thought reasoning to multimodal tasks by interleaving text with visual sketches in latent space, using VLMs as encoders and diffusion decoders for fine-grained details.


<details>
  <summary>Details</summary>
Motivation: Text-only chain-of-thought fails on vision-intensive problems where intermediate reasoning states are inherently visual, requiring a multimodal approach that preserves visual information throughout the reasoning process.

Method: Introduces modal-mixed CoT with textual tokens interleaved with visual latent embeddings. Uses VLM as encoder, trains language backbone to reconstruct intermediate vision embeddings, and attaches diffusion-based latent decoder conditioned on VLM hidden states. Two-stage training: supervised fine-tuning with joint next-token/latent-reconstruction, followed by reinforcement learning for modality switching and chain composition.

Result: Extensive experiments across 11 diverse multimodal reasoning tasks demonstrate superior performance compared to language-only and other CoT methods.

Conclusion: Modal-mixed CoT effectively extends reasoning capabilities to multimodal domains by cleanly disentangling roles between VLM (high-level intent) and diffusion head (fine-grained details), enabling better handling of vision-intensive problems.

Abstract: We study how to extend chain-of-thought (CoT) beyond language to better handle multimodal reasoning. While CoT helps LLMs and VLMs articulate intermediate steps, its text-only form often fails on vision-intensive problems where key intermediate states are inherently visual. We introduce modal-mixed CoT, which interleaves textual tokens with compact visual sketches represented as latent embeddings. To bridge the modality gap without eroding the original knowledge and capability of the VLM, we use the VLM itself as an encoder and train the language backbone to reconstruct its own intermediate vision embeddings, to guarantee the semantic alignment of the visual latent space. We further attach a diffusion-based latent decoder, invoked by a special control token and conditioned on hidden states from the VLM. In this way, the diffusion head carries fine-grained perceptual details while the VLM specifies high-level intent, which cleanly disentangles roles and reduces the optimization pressure of the VLM. Training proceeds in two stages: supervised fine-tuning on traces that interleave text and latents with a joint next-token and latent-reconstruction objective, followed by reinforcement learning that teaches when to switch modalities and how to compose long reasoning chains. Extensive experiments across 11 diverse multimodal reasoning tasks, demonstrate that our method yields better performance than language-only and other CoT methods. Our code will be publicly released.

</details>


### [302] [Small Shifts, Large Gains: Unlocking Traditional TSP Heuristic Guided-Sampling via Unsupervised Neural Instance Modification](https://arxiv.org/abs/2602.00580)
*Wei Huang,Hanchen Wang,Dong Wen,Wenjie Zhang*

Main category: cs.AI

TL;DR: TSP-MDF is a novel framework that enhances traditional TSP heuristics by using neural networks to modify problem instances, enabling better exploration without extensive training or ground-truth supervision.


<details>
  <summary>Details</summary>
Motivation: Traditional TSP heuristics like Farthest/Nearest Insertion are efficient but deterministic and prone to local optima, while neural-based methods require extensive training and supervision, limiting practical use. There's a need to bridge this gap.

Method: TSP-MDF uses a neural-based instance modifier to strategically shift node coordinates, creating multiple modified instances. Traditional heuristics construct tours on these modified instances, which are then mapped back to the original problem, enabling exploration beyond local optima.

Result: Extensive experiments show TSP-MDF significantly improves traditional heuristic performance, achieving solution quality comparable to neural-based methods but with extremely short training time on large-scale TSP and real-world benchmarks.

Conclusion: TSP-MDF successfully bridges the gap between traditional and neural-based TSP heuristics by providing guided-sampling capability to deterministic methods while maintaining practicality through efficient unsupervised training.

Abstract: The Traveling Salesman Problem (TSP) is one of the most representative NP-hard problems in route planning and a long-standing benchmark in combinatorial optimization. Traditional heuristic tour constructors, such as Farthest or Nearest Insertion, are computationally efficient and highly practical, but their deterministic behavior limits exploration and often leads to local optima. In contrast, neural-based heuristic tour constructors alleviate this issue through guided-sampling and typically achieve superior solution quality, but at the cost of extensive training and reliance on ground-truth supervision, hindering their practical use. To bridge this gap, we propose TSP-MDF, a novel instance modification framework that equips traditional deterministic heuristic tour constructors with guided-sampling capability. Specifically, TSP-MDF introduces a neural-based instance modifier that strategically shifts node coordinates to sample multiple modified instances, on which the base traditional heuristic tour constructor constructs tours that are mapped back to the original instance, allowing traditional tour constructors to explore higher-quality tours and escape local optima. At the same time, benefiting from our instance modification formulation, the neural-based instance modifier can be trained efficiently without any ground-truth supervision, ensuring the framework maintains practicality. Extensive experiments on large-scale TSP benchmarks and real-world benchmarks demonstrate that TSP-MDF significantly improves the performance of traditional heuristics tour constructors, achieving solution quality comparable to neural-based heuristic tour constructors, but with an extremely short training time.

</details>


### [303] [Exploring Information Seeking Agent Consolidation](https://arxiv.org/abs/2602.00585)
*Guochen Yan,Jialong Wu,Zhengwei Tao,Bo Li,Qintong Zhang,Jiahao Xu,Haitao Mi,Yuejian Fang,Qingni Shen,Wentao Zhang,Zhonghai Wu*

Main category: cs.AI

TL;DR: Investigates consolidation of heterogeneous information-seeking agents into a single foundation agentic model using data-level and parameter-level strategies, finding data-level consolidation remains strong while parameter-level offers efficient alternative with interference challenges.


<details>
  <summary>Details</summary>
Motivation: Existing information-seeking agents are specialized for specific domains (open web, documents, local knowledge bases), which constrains scalability and cross-domain generalization. Need to consolidate heterogeneous agents into a unified foundation model.

Method: Studies two consolidation strategies: 1) Data-level consolidation - jointly trains unified model on mixture of domain-specific datasets; 2) Parameter-level consolidation - merges independently trained agent models at parameter level. Compares performance retention, cross-domain generalization, and interference across behaviors.

Result: Data-level consolidation remains strong and stable baseline. Parameter-level consolidation offers promising, efficient alternative but suffers from interference and robustness challenges. Identifies key design factors for effective parameter-level consolidation.

Conclusion: Both consolidation strategies have merits: data-level provides stability while parameter-level offers efficiency. Key factors for parameter-level success include fine-grained merging granularity, task heterogeneity awareness, and principled consensus strategy.

Abstract: Information-seeking agents have emerged as a powerful paradigm for solving knowledge-intensive tasks. Existing information-seeking agents are typically specialized for open web, documents, or local knowledge bases, which constrains scalability and cross-domain generalization. In this work, we investigate how to consolidate heterogeneous information-seeking agents into a single foundation agentic model. We study two complementary consolidation strategies: data-level consolidation, which jointly trains a unified model on a mixture of domain-specific datasets, and parameter-level consolidation, which merges independently trained agent models at the parameter level. Our analysis compares these approaches in terms of performance retention, cross-domain generalization, and interference across information-seeking behaviors. Our results show that data-level consolidation remains a strong and stable baseline, while parameter-level consolidation offers a promising, efficient alternative but suffers from interference and robustness challenges. We further identify key design factors for effective agent consolidation at the parameter level, including fine-grained merging granularity, awareness of task heterogeneity, and principled consensus strategy.

</details>


### [304] [DockSmith: Scaling Reliable Coding Environments via an Agentic Docker Builder](https://arxiv.org/abs/2602.00592)
*Jiaran Zhang,Luck Ma,Yanhao Li,Fanqi Wan,Di Qi,Xu Zhao,Jieyi Hou,Zhe Xie,Mengqiang Ren,Xin Wu,Zhewei Huang,Liangyu Chen,Yingwei Ma,Qi Han,Xiangyu Zhang*

Main category: cs.AI

TL;DR: DockSmith is an agentic Docker builder that treats environment construction as a core AI capability, achieving state-of-the-art performance on Docker building tasks while improving performance on broader software engineering benchmarks.


<details>
  <summary>Details</summary>
Motivation: Reliable Docker-based environment construction is a major bottleneck for scaling execution-grounded training and evaluation of software engineering agents, limiting the development and assessment of AI systems that need to work in real software environments.

Method: DockSmith is trained on large-scale execution-grounded Docker-building trajectories using a SWE-Factory-style pipeline enhanced with loop-detection controller and cross-task success memory. The approach treats environment construction as a core agentic capability involving long-horizon tool use, dependency reasoning, and failure recovery.

Result: A 30B-A3B model trained with DockSmith achieves 39.72% Fail-to-Pass and 58.28% Commit Rate on Multi-Docker-Eval (open-source state-of-the-art). It also improves out-of-distribution performance on SWE-bench Verified, SWE-bench Multilingual, and Terminal-Bench 2.0.

Conclusion: DockSmith demonstrates that treating environment construction as a core agentic capability yields supervision that transfers beyond Docker building itself, providing broader agentic benefits for software engineering tasks and addressing a critical bottleneck in scaling execution-grounded agent training.

Abstract: Reliable Docker-based environment construction is a dominant bottleneck for scaling execution-grounded training and evaluation of software engineering agents. We introduce DockSmith, a specialized agentic Docker builder designed to address this challenge. DockSmith treats environment construction not only as a preprocessing step, but as a core agentic capability that exercises long-horizon tool use, dependency reasoning, and failure recovery, yielding supervision that transfers beyond Docker building itself. DockSmith is trained on large-scale, execution-grounded Docker-building trajectories produced by a SWE-Factory-style pipeline augmented with a loop-detection controller and a cross-task success memory. Training a 30B-A3B model on these trajectories achieves open-source state-of-the-art performance on Multi-Docker-Eval, with 39.72% Fail-to-Pass and 58.28% Commit Rate. Moreover, DockSmith improves out-of-distribution performance on SWE-bench Verified, SWE-bench Multilingual, and Terminal-Bench 2.0, demonstrating broader agentic benefits of environment construction.

</details>


### [305] [Scalable Generative Game Engine: Breaking the Resolution Wall via Hardware-Algorithm Co-Design](https://arxiv.org/abs/2602.00608)
*Wei Zeng,Xuchen Li,Ruili Feng,Zhen Liu,Fengwei An,Jian Zhao*

Main category: cs.AI

TL;DR: Hardware-algorithm co-design framework enables real-time generative game engines at 720×480 resolution, overcoming memory wall limitations with 50× throughput improvement.


<details>
  <summary>Details</summary>
Motivation: Existing generative game engines are limited by the "Memory Wall" to low resolutions (e.g., 64×64), preventing practical high-resolution neural simulations for interactive gameplay.

Method: Proposes a heterogeneous hardware-algorithm co-design framework with three innovations: 1) asymmetric resource allocation for sequence parallelism, 2) memory-centric operator fusion to minimize bandwidth, and 3) manifold-aware latent extrapolation to mask latency.

Result: Achieves real-time generation at 720×480 resolution (50× pixel throughput increase), delivering 26.4 FPS for 3D racing and 48.3 FPS for 2D platformers with 2.7 ms amortized latency on AI accelerator clusters.

Conclusion: Resolving the "Memory Wall" through architectural co-design is essential for enabling high-fidelity, responsive neural gameplay, representing a prerequisite rather than just an optimization.

Abstract: Real-time generative game engines represent a paradigm shift in interactive simulation, promising to replace traditional graphics pipelines with neural world models. However, existing approaches are fundamentally constrained by the ``Memory Wall,'' restricting practical deployments to low resolutions (e.g., $64 \times 64$). This paper bridges the gap between generative models and high-resolution neural simulations by introducing a scalable \textit{Hardware-Algorithm Co-Design} framework. We identify that high-resolution generation suffers from a critical resource mismatch: the World Model is compute-bound while the Decoder is memory-bound. To address this, we propose a heterogeneous architecture that intelligently decouples these components across a cluster of AI accelerators. Our system features three core innovations: (1) an asymmetric resource allocation strategy that optimizes throughput under sequence parallelism constraints; (2) a memory-centric operator fusion scheme that minimizes off-chip bandwidth usage; and (3) a manifold-aware latent extrapolation mechanism that exploits temporal redundancy to mask latency. We validate our approach on a cluster of programmable AI accelerators, enabling real-time generation at $720 \times 480$ resolution -- a $50\times$ increase in pixel throughput over prior baselines. Evaluated on both continuous 3D racing and discrete 2D platformer benchmarks, our system delivers fluid 26.4 FPS and 48.3 FPS respectively, with an amortized effective latency of 2.7 ms. This work demonstrates that resolving the ``Memory Wall'' via architectural co-design is not merely an optimization, but a prerequisite for enabling high-fidelity, responsive neural gameplay.

</details>


### [306] [Structured Self-Consistency:A Multi-Task Evaluation of LLMs on VirtualHome](https://arxiv.org/abs/2602.00611)
*Jiaqi Xu,Tao Huang,Kai Zhang*

Main category: cs.AI

TL;DR: LLMs evaluated on VirtualHome benchmark using EAI framework, comparing OPENPANGU-7B and QWEN2.5-7B on four embodied AI tasks. Proposed Structured Self-Consistency (SSC) decoding improves performance, revealing complementary strengths between models.


<details>
  <summary>Details</summary>
Motivation: Embodied AI requires agents to understand goals, plan actions, and execute tasks in simulated environments. Need to evaluate LLMs' capabilities on fundamental embodied AI tasks to understand their strengths and limitations for future system development.

Method: Comprehensive evaluation of two 7B-parameter LLMs (OPENPANGU-7B and QWEN2.5-7B) on VirtualHome benchmark using Embodied Agent Interface framework. Four tasks: Goal Interpretation, Action Sequencing, Subgoal Decomposition, and Transition Modeling. Proposed Structured Self-Consistency (SSC) - enhanced decoding strategy using multiple sampling with domain-specific voting mechanisms.

Result: SSC significantly enhances performance. OPENPANGU-7B excels at hierarchical planning while QWEN2.5-7B shows advantages in action-level tasks. Analysis reveals complementary strengths across model types.

Conclusion: The study provides insights for future embodied AI system development by revealing complementary strengths between different LLM architectures and demonstrating the effectiveness of structured decoding strategies like SSC for embodied AI tasks.

Abstract: Embodied AI requires agents to understand goals, plan actions, and execute tasks in simulated environments.We present a comprehensive evaluation of Large Language Models (LLMs) on the VirtualHome benchmark using the Embodied Agent Interface (EAI) framework.We compare two representative 7B-parameter models OPENPANGU-7B and QWEN2.5-7B across four fundamental tasks: Goal Interpretation, Action Sequencing, Subgoal Decomposition, and Transition Modeling.We propose Structured Self-Consistency (SSC), an enhanced decoding strategy that leverages multiple sampling with domain-specific voting mechanisms to improve output quality for structured generation tasks. Experimental results demonstrate that SSC significantly enhances performance, with OPENPANGU-7B excelling at hierarchical planning while QWEN2.5-7B show advantages in action-level tasks. Our analysis reveals complementary strengths across model types, providing insights for future embodied AI system development.

</details>


### [307] [Inference-Only Prompt Projection for Safe Text-to-Image Generation with TV Guarantees](https://arxiv.org/abs/2602.00616)
*Minhyuk Lee,Hyekyung Yoon,Myungjoo Kang*

Main category: cs.AI

TL;DR: A prompt projection framework that selectively intervenes on high-risk prompts to reduce unsafe image generations while preserving benign prompt-image alignment, without retraining diffusion models.


<details>
  <summary>Details</summary>
Motivation: Text-to-image diffusion models need safety safeguards that suppress unsafe generations without degrading benign prompt-image alignment, creating a fundamental trade-off between safety and alignment.

Method: Formalizes the safety-alignment trade-off through total variation lens, then proposes an inference-only prompt projection framework that selectively intervenes on high-risk prompts via surrogate objective with verification, mapping them to tolerance-controlled safe sets without retraining the generator.

Result: Achieves 16.7-60.0% relative reductions in inappropriate percentage versus strong model-level alignment baselines across four datasets and three diffusion backbones, while preserving benign prompt-image alignment on COCO near the unaligned reference.

Conclusion: The proposed prompt projection framework effectively addresses the safety-prompt alignment trade-off by selectively modifying only high-risk prompts, achieving significant safety improvements while maintaining alignment for benign prompts without model retraining.

Abstract: Text-to-Image (T2I) diffusion models enable high-quality open-ended synthesis, but their real-world deployment demands safeguards that suppress unsafe generations without degrading benign prompt-image alignment. We formalize this tension through a total variation (TV) lens: once the reference conditional distribution is fixed, any nontrivial reduction in unsafe generations necessarily incurs TV deviation from the reference, yielding a principled Safety-Prompt Alignment Trade-off (SPAT). Guided by this view, we propose an inference-only prompt projection framework that selectively intervenes on high-risk prompts via a surrogate objective with verification, mapping them into a tolerance-controlled safe set while leaving benign prompts effectively unchanged, without retraining or fine-tuning the generator. Across four datasets and three diffusion backbones, our approach achieves 16.7-60.0% relative reductions in inappropriate percentage (IP) versus strong model-level alignment baselines, while preserving benign prompt-image alignment on COCO near the unaligned reference.

</details>


### [308] [Predictive Maintenance for Ultrafiltration Membranes Using Explainable Similarity-Based Prognostics](https://arxiv.org/abs/2602.00659)
*Qusai Khaled,Laura Genga,Uzay Kaymak*

Main category: cs.AI

TL;DR: Explainable fuzzy similarity framework predicts UF membrane remaining useful life with interpretable rules, achieving 4.50 cycles MAE on industrial data.


<details>
  <summary>Details</summary>
Motivation: Current predictive maintenance models for UF membranes lack interpretability and operator trust, forcing plants to rely on scheduled preventive maintenance despite performance degradation from fouling.

Method: Physics-informed Health Index captures degradation from transmembrane pressure, flux, and resistance; fuzzified via Gaussian membership functions; uses similarity reasoning to match current state with historical trajectories; formulates RUL predictions as Takagi-Sugeno fuzzy rules.

Result: Achieved mean absolute error of 4.50 cycles when tested on 12,528 operational cycles from industrial-scale UF system, with interpretable rule bases consistent with expert understanding.

Conclusion: The explainable prognostic framework provides transparent, similarity-weighted RUL estimates that build operator trust while maintaining predictive accuracy for UF membrane maintenance.

Abstract: In reverse osmosis desalination, ultrafiltration (UF) membranes degrade due to fouling, leading to performance loss and costly downtime. Most plants rely on scheduled preventive maintenance, since existing predictive maintenance models, often based on opaque machine learning methods, lack interpretability and operator trust. This study proposes an explainable prognostic framework for UF membrane remaining useful life (RUL) estimation using fuzzy similarity reasoning. A physics-informed Health Index, derived from transmembrane pressure, flux, and resistance, captures degradation dynamics, which are then fuzzified via Gaussian membership functions. Using a similarity measure, the model identifies historical degradation trajectories resembling the current state and formulates RUL predictions as Takagi-Sugeno fuzzy rules. Each rule corresponds to a historical exemplar and contributes to a transparent, similarity-weighted RUL estimate. Tested on 12,528 operational cycles from an industrial-scale UF system, the framework achieved a mean absolute error of 4.50 cycles, while generating interpretable rule bases consistent with expert understanding.

</details>


### [309] [SEISMO: Increasing Sample Efficiency in Molecular Optimization with a Trajectory-Aware LLM Agent](https://arxiv.org/abs/2602.00663)
*Fabian P. Krüger,Andrea Hunklinger,Adrian Wolny,Tim J. Adler,Igor Tetko,Santiago David Villalba*

Main category: cs.AI

TL;DR: SEISMO is an LLM agent for sample-efficient molecular optimization that updates after each oracle call, achieving 2-3x better performance than prior methods on 23 benchmark tasks.


<details>
  <summary>Details</summary>
Motivation: Molecular optimization is bottlenecked by costly experimental assays and requires high sample efficiency, especially in drug discovery where property evaluation relies on expensive oracles.

Method: SEISMO performs strictly online, inference-time molecular optimization using an LLM agent that updates after every oracle call, conditions proposals on full optimization trajectories, and incorporates natural-language task descriptions with scores and explanatory feedback.

Result: Achieves 2-3 times higher area under optimization curve than prior methods on Practical Molecular Optimization benchmark (23 tasks), often reaching near-maximal scores within 50 oracle calls; explanatory feedback further improves efficiency on medicinal-chemistry tasks.

Conclusion: Leveraging domain knowledge and structured information through online LLM agents with explanatory feedback is key to sample-efficient molecular optimization, demonstrating significant improvements over existing methods.

Abstract: Optimizing the structure of molecules to achieve desired properties is a central bottleneck across the chemical sciences, particularly in the pharmaceutical industry where it underlies the discovery of new drugs. Since molecular property evaluation often relies on costly and rate-limited oracles, such as experimental assays, molecular optimization must be highly sample-efficient. To address this, we introduce SEISMO, an LLM agent that performs strictly online, inference-time molecular optimization, updating after every oracle call without the need for population-based or batched learning. SEISMO conditions each proposal on the full optimization trajectory, combining natural-language task descriptions with scalar scores and, when available, structured explanatory feedback. Across the Practical Molecular Optimization benchmark of 23 tasks, SEISMO achieves a 2-3 times higher area under the optimisation curve than prior methods, often reaching near-maximal task scores within 50 oracle calls. Our additional medicinal-chemistry tasks show that providing explanatory feedback further improves efficiency, demonstrating that leveraging domain knowledge and structured information is key to sample-efficient molecular optimization.

</details>


### [310] [OpenGuanDan: A Large-Scale Imperfect Information Game Benchmark](https://arxiv.org/abs/2602.00676)
*Chao Li,Shangdong Yang,Chiheng Zhan,Zhenxing Ge,Yujing Hu,Bingkun Bao,Xingguo Chen,Yang Gao*

Main category: cs.AI

TL;DR: OpenGuanDan is a new benchmark for evaluating AI agents in GuanDan, a complex Chinese card game, featuring imperfect information, large action spaces, and mixed cooperation/competition objectives.


<details>
  <summary>Details</summary>
Motivation: Current AI benchmarks lack sufficient challenge to drive further research in intelligent decision-making, especially for complex multi-agent scenarios with imperfect information and mixed objectives.

Method: Developed OpenGuanDan benchmark with efficient simulation of GuanDan, independent API per player for human-AI interaction, and comprehensive evaluation framework for both learning-based and rule-based agents.

Result: Learning-based agents significantly outperform rule-based agents but still fail to achieve superhuman performance, highlighting the benchmark's difficulty and need for further research.

Conclusion: OpenGuanDan provides a challenging testbed for multi-agent intelligent decision-making research and demonstrates current AI limitations in complex imperfect information games with mixed cooperation/competition objectives.

Abstract: The advancement of data-driven artificial intelligence (AI), particularly machine learning, heavily depends on large-scale benchmarks. Despite remarkable progress across domains ranging from pattern recognition to intelligent decision-making in recent decades, exemplified by breakthroughs in board games, card games, and electronic sports games, there remains a pressing need for more challenging benchmarks to drive further research. To this end, this paper proposes OpenGuanDan, a novel benchmark that enables both efficient simulation of GuanDan (a popular four-player, multi-round Chinese card game) and comprehensive evaluation of both learning-based and rule-based GuanDan AI agents. OpenGuanDan poses a suite of nontrivial challenges, including imperfect information, large-scale information set and action spaces, a mixed learning objective involving cooperation and competition, long-horizon decision-making, variable action spaces, and dynamic team composition. These characteristics make it a demanding testbed for existing intelligent decision-making methods. Moreover, the independent API for each player allows human-AI interactions and supports integration with large language models. Empirically, we conduct two types of evaluations: (1) pairwise competitions among all GuanDan AI agents, and (2) human-AI matchups. Experimental results demonstrate that while current learning-based agents substantially outperform rule-based counterparts, they still fall short of achieving superhuman performance, underscoring the need for continued research in multi-agent intelligent decision-making domain. The project is publicly available at https://github.com/GameAI-NJUPT/OpenGuanDan.

</details>


### [311] [HumanStudy-Bench: Towards AI Agent Design for Participant Simulation](https://arxiv.org/abs/2602.00685)
*Xuan Liu,Haoyang Shang,Zizhang Liu,Xinyan Liu,Yunze Xiao,Yiwen Tu,Haojian Jin*

Main category: cs.AI

TL;DR: HUMANSTUDY-BENCH is a benchmark and execution engine for evaluating LLM-based agents in social science experiments, using a structured pipeline to reconstruct human studies and measure agent fidelity to human behavior.


<details>
  <summary>Details</summary>
Motivation: Current use of LLMs as simulated participants in social science experiments suffers from unstable behavior and conflation of base-model capabilities with experimental design, making it unclear whether outcomes reflect the model or the agent setup.

Method: Frames participant simulation as an agent-design problem, introduces HUMANSTUDY-BENCH with Filter-Extract-Execute-Evaluate pipeline to reconstruct published human experiments, and proposes new metrics to quantify agreement between human and agent behaviors.

Result: Instantiated 12 foundational studies across individual cognition, strategic interaction, and social psychology, covering over 6,000 trials with human samples ranging from tens to over 2,100 participants.

Conclusion: Provides a systematic framework for evaluating LLM-based agents in social science research, enabling more rigorous assessment of whether agent behaviors faithfully reproduce human experimental outcomes.

Abstract: Large language models (LLMs) are increasingly used as simulated participants in social science experiments, but their behavior is often unstable and highly sensitive to design choices. Prior evaluations frequently conflate base-model capabilities with experimental instantiation, obscuring whether outcomes reflect the model itself or the agent setup. We instead frame participant simulation as an agent-design problem over full experimental protocols, where an agent is defined by a base model and a specification (e.g., participant attributes) that encodes behavioral assumptions. We introduce HUMANSTUDY-BENCH, a benchmark and execution engine that orchestrates LLM-based agents to reconstruct published human-subject experiments via a Filter--Extract--Execute--Evaluate pipeline, replaying trial sequences and running the original analysis pipeline in a shared runtime that preserves the original statistical procedures end to end. To evaluate fidelity at the level of scientific inference, we propose new metrics to quantify how much human and agent behaviors agree. We instantiate 12 foundational studies as an initial suite in this dynamic benchmark, spanning individual cognition, strategic interaction, and social psychology, and covering more than 6,000 trials with human samples ranging from tens to over 2,100 participants.

</details>


### [312] [From Prompt to Graph: Comparing LLM-Based Information Extraction Strategies in Domain-Specific Ontology Development](https://arxiv.org/abs/2602.00699)
*Xuan Liu,Ziyu Li,Mu He,Ziyang Ma,Xiaoxu Wu,Gizem Yilmaz,Yiyuan Xia,Bingbing Li,He Tan,Jerry Ying Hsi Fuh,Wen Feng Lu,Anders E. W. Jarfors,Per Jansson*

Main category: cs.AI

TL;DR: LLM-based methods for automated ontology construction from domain-specific texts, with casting manufacturing as case study.


<details>
  <summary>Details</summary>
Motivation: Traditional ontology construction is labor-intensive and costly, especially in specialized fields like casting manufacturing. LLMs offer new possibilities for automating knowledge extraction from domain texts.

Method: Three LLM-based approaches: pre-trained LLM-driven method, in-context learning (ICL) method, and fine-tuning method to extract terms and relations from domain-specific texts using limited data.

Result: Comparison of three LLM methods' performances, with best-performing method used to build a casting ontology validated by domain expert.

Conclusion: LLM-based approaches can effectively automate ontology construction from domain-specific texts, reducing manual effort and costs in specialized fields.

Abstract: Ontologies are essential for structuring domain knowledge, improving accessibility, sharing, and reuse. However, traditional ontology construction relies on manual annotation and conventional natural language processing (NLP) techniques, making the process labour-intensive and costly, especially in specialised fields like casting manufacturing. The rise of Large Language Models (LLMs) offers new possibilities for automating knowledge extraction. This study investigates three LLM-based approaches, including pre-trained LLM-driven method, in-context learning (ICL) method and fine-tuning method to extract terms and relations from domain-specific texts using limited data. We compare their performances and use the best-performing method to build a casting ontology that validated by domian expert.

</details>


### [313] [Self-Guard: Defending Large Reasoning Models via enhanced self-reflection](https://arxiv.org/abs/2602.00707)
*Jingnan Zheng,Jingjun Xu,Yanzhen Luo,Chenhang Cui,Gelei Deng,Zhenkai Liang,Xiang Wang,An Zhang,Tat-Seng Chua*

Main category: cs.AI

TL;DR: Self-Guard is a lightweight safety defense framework for Large Reasoning Models that bridges the awareness-compliance gap by activating safety awareness and steering hidden state representations, achieving robust safety without compromising utility.


<details>
  <summary>Details</summary>
Motivation: Large Reasoning Models (LRMs) pose unique risks like reasoning manipulation and information leakage. Current alignment strategies are computationally intensive and fail to address the awareness-compliance gap where models recognize risks but prioritize following user instructions due to sycophantic tendencies.

Method: Self-Guard operates in two stages: (1) safety-oriented prompting activates the model's latent safety awareness to evoke spontaneous reflection, and (2) safety activation steering extracts the directional shift in hidden state space and amplifies it to ensure safety compliance prevails over sycophancy during inference.

Result: Self-Guard effectively bridges the awareness-compliance gap, achieving robust safety performance without compromising model utility. It exhibits strong generalization across diverse unseen risks and varying model scales, offering a cost-efficient solution.

Conclusion: Self-Guard provides a lightweight, effective framework for LRM safety alignment that addresses the critical awareness-compliance gap through representational-level reinforcement, offering a practical alternative to computationally intensive post-training approaches.

Abstract: The emergence of Large Reasoning Models (LRMs) introduces a new paradigm of explicit reasoning, enabling remarkable advances yet posing unique risks such as reasoning manipulation and information leakage. To mitigate these risks, current alignment strategies predominantly rely on heavy post-training paradigms or external interventions. However, these approaches are often computationally intensive and fail to address the inherent awareness-compliance gap, a critical misalignment where models recognize potential risks yet prioritize following user instructions due to their sycophantic tendencies. To address these limitations, we propose Self-Guard, a lightweight safety defense framework that reinforces safety compliance at the representational level. Self-Guard operates through two principal stages: (1) safety-oriented prompting, which activates the model's latent safety awareness to evoke spontaneous reflection, and (2) safety activation steering, which extracts the resulting directional shift in the hidden state space and amplifies it to ensure that safety compliance prevails over sycophancy during inference. Experiments demonstrate that Self-Guard effectively bridges the awareness-compliance gap, achieving robust safety performance without compromising model utility. Furthermore, Self-Guard exhibits strong generalization across diverse unseen risks and varying model scales, offering a cost-efficient solution for LRM safety alignment.

</details>


### [314] [Physics-informed Diffusion Generation for Geomagnetic Map Interpolation](https://arxiv.org/abs/2602.00709)
*Wenda Li,Tongya Zheng,Kaixuan Chen,Shunyu Liu,Haoze Jiang,Yunzhi Hao,Rui Miao,Zujie Ren,Mingli Song,Hang Shi,Gang Chen*

Main category: cs.AI

TL;DR: PDG framework uses physics-informed diffusion generation with mask strategy and constraints to interpolate geomagnetic maps, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing scattered data interpolation methods are not designed for geomagnetic maps, leading to suboptimal performance due to noise and physics laws.

Method: Physics-informed Diffusion Generation (PDG) with: 1) physics-informed mask strategy based on local receptive field to eliminate noise, 2) physics-informed constraints following kriging principle to ensure adherence to physics laws.

Result: Extensive experiments on four real-world datasets demonstrate superiority and effectiveness of each PDG component.

Conclusion: PDG framework effectively addresses geomagnetic map interpolation challenges by incorporating physics knowledge into diffusion generation, achieving better performance than existing methods.

Abstract: Geomagnetic map interpolation aims to infer unobserved geomagnetic data at spatial points, yielding critical applications in navigation and resource exploration. However, existing methods for scattered data interpolation are not specifically designed for geomagnetic maps, which inevitably leads to suboptimal performance due to detection noise and the laws of physics. Therefore, we propose a Physics-informed Diffusion Generation framework~(PDG) to interpolate incomplete geomagnetic maps. First, we design a physics-informed mask strategy to guide the diffusion generation process based on a local receptive field, effectively eliminating noise interference. Second, we impose a physics-informed constraint on the diffusion generation results following the kriging principle of geomagnetic maps, ensuring strict adherence to the laws of physics. Extensive experiments and in-depth analyses on four real-world datasets demonstrate the superiority and effectiveness of each component of PDG.

</details>


### [315] [Learning More from Less: Unlocking Internal Representations for Benchmark Compression](https://arxiv.org/abs/2602.00710)
*Yueqi Zhang,Jin Hu,Shaoxiong Feng,Peiwen Yuan,Xinglin Wang,Yiwei Li,Jiayi Shi,Chuyi Tan,Ji Zhang,Boyuan Pan,Yao Hu,Kan Li*

Main category: cs.AI

TL;DR: REPCORE uses aligned hidden states instead of correctness labels to create representative coresets for efficient LLM benchmarking, achieving accurate performance estimation with only 10 source models.


<details>
  <summary>Details</summary>
Motivation: Existing coreset methods for efficient LLM benchmarking require many source models to estimate reliable item profiles, which is problematic for new benchmarks with limited historical data. Discrete correctness labels are lossy and fail to capture information in hidden states.

Method: REPCORE aligns heterogeneous hidden states from different models into a unified latent space to construct representative coresets, then uses these subsets for performance extrapolation.

Result: Experiments on 5 benchmarks and over 200 models show consistent improvements over output-based baselines in ranking correlation and estimation accuracy, achieving precise estimation with as few as 10 source models.

Conclusion: Hidden state alignment provides richer information than discrete correctness labels for coreset construction, enabling efficient and accurate LLM benchmarking even with limited source models, with spectral analysis revealing separable components for broad response tendencies and task-specific reasoning.

Abstract: The prohibitive cost of evaluating Large Language Models (LLMs) necessitates efficient alternatives to full-scale benchmarking. Prevalent approaches address this by identifying a small coreset of items to approximate full-benchmark performance. However, existing methods must estimate a reliable item profile from response patterns across many source models, which becomes statistically unstable when the source pool is small. This dependency is particularly limiting for newly released benchmarks with minimal historical evaluation data. We argue that discrete correctness labels are a lossy view of the model's decision process and fail to capture information encoded in hidden states. To address this, we introduce REPCORE, which aligns heterogeneous hidden states into a unified latent space to construct representative coresets. Using these subsets for performance extrapolation, REPCORE achieves precise estimation accuracy with as few as ten source models. Experiments on five benchmarks and over 200 models show consistent gains over output-based baselines in ranking correlation and estimation accuracy. Spectral analysis further indicates that the aligned representations contain separable components reflecting broad response tendencies and task-specific reasoning patterns.

</details>


### [316] [Neuro-symbolic AI for Predictive Maintenance (PdM) -- review and recommendations](https://arxiv.org/abs/2602.00731)
*Kyle Hamilton,Ali Intizar*

Main category: cs.AI

TL;DR: Survey of predictive maintenance (PdM) over 5 years shows data-driven methods outperform traditional approaches but have limitations like need for large labeled data and lack of explainability. Proposes neuro-symbolic AI as hybrid solution combining deep learning with symbolic logic for better accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Current PdM approaches have trade-offs: data-driven methods (deep learning) have high accuracy but lack explainability and need large datasets, while traditional knowledge-based systems have poor accuracy and many false positives. Need for hybrid approaches that overcome weaknesses of both.

Method: Systematic review of PdM literature over 5 years, focusing on industrial settings. Proposes neuro-symbolic AI architectures that integrate deep learning with symbolic logic, specifically examining methods using sensor data and manually crafted rules as inputs.

Result: Data-driven methods generally exhibit higher accuracy than knowledge-based systems, but hybrid approaches show potential to overcome limitations of both. Neuro-symbolic AI architectures offer promise for more accurate, explainable, interpretable, and robust PdM systems.

Conclusion: Neuro-symbolic AI represents a promising direction for PdM, combining strengths of data-driven and knowledge-based approaches to create systems that are both accurate and interpretable, addressing key adoption barriers in real-world industrial environments.

Abstract: In this document we perform a systematic review the State-of-the-art in Predictive Maintenance (PdM) over the last five years in industrial settings such as commercial buildings, pharmaceutical facilities, or semi-conductor manufacturing. In general, data-driven methods such as those based on deep learning, exhibit higher accuracy than traditional knowledge-based systems. These systems however, are not without significant limitations. The need for large labeled data sets, a lack of generalizibility to new environments (out-of-distribution generalization), and a lack of transparency at inference time are some of the obstacles to adoption in real world environments. In contrast, traditional approaches based on domain expertise in the form of rules, logic or first principles suffer from poor accuracy, many false positives and a need for ongoing expert supervision and manual tuning. While the majority of approaches in recent literature utilize some form of data-driven architecture, there are hybrid systems which also take into account domain specific knowledge. Such hybrid systems have the potential to overcome the weaknesses of either approach on its own while preserving their strengths. We propose taking the hybrid approach even further and integrating deep learning with symbolic logic, or Neuro-symbolic AI, to create more accurate, explainable, interpretable, and robust systems. We describe several neuro-symbolic architectures and examine their strengths and limitations within the PdM domain. We focus specifically on methods which involve the use of sensor data and manually crafted rules as inputs by describing concrete NeSy architectures. In short, this survey outlines the context of modern maintenance, defines key concepts, establishes a generalized framework, reviews current modeling approaches and challenges, and introduces the proposed focus on Neuro-symbolic AI (NESY).

</details>


### [317] [Engineering AI Agents for Clinical Workflows: A Case Study in Architecture,MLOps, and Governance](https://arxiv.org/abs/2602.00751)
*Cláudio Lúcio do Val Lopes,João Marcus Pitta,Fabiano Belém,Gildson Alves,Flávio Vinícius Cruzeiro Martins*

Main category: cs.AI

TL;DR: The paper presents "Maria" platform - a production-grade AI system for primary healthcare that integrates four engineering pillars for trustworthy clinical AI through Clean Architecture, Event-driven design, autonomous Agents, and Human-in-the-Loop governance.


<details>
  <summary>Details</summary>
Motivation: Current AI integration in clinical settings suffers from brittle prototype-derived architectures and lack of systemic oversight, creating a "responsibility vacuum" where safety and accountability are compromised. There's a need for robust, governable, and reliable AI systems in healthcare.

Method: Developed the "Maria" platform using four foundational engineering pillars: 1) Clean Architecture for maintainability, 2) Event-driven architecture for resilience and auditability, 3) Agents as primary modular units with autonomous MLOps lifecycles, and 4) Human-in-the-Loop governance integrated as event-driven data source.

Result: Created a production-grade AI system for primary healthcare that serves as a reference architecture for building maintainable, scalable, and accountable AI-enabled systems in high-stakes domains.

Conclusion: Trustworthy clinical AI requires holistic integration of four engineering pillars - architectural patterns, event-driven design, modular agents, and human governance - to address the responsibility vacuum and create robust, accountable systems for healthcare applications.

Abstract: The integration of Artificial Intelligence (AI) into clinical settings presents a software engineering challenge, demanding a shift from isolated models to robust, governable, and reliable systems. However, brittle, prototype-derived architectures often plague industrial applications and a lack of systemic oversight, creating a ``responsibility vacuum'' where safety and accountability are compromised. This paper presents an industry case study of the ``Maria'' platform, a production-grade AI system in primary healthcare that addresses this gap.
  Our central hypothesis is that trustworthy clinical AI is achieved through the holistic integration of four foundational engineering pillars. We present a synergistic architecture that combines Clean Architecture for maintainability with an Event-driven architecture for resilience and auditability. We introduce the Agent as the primary unit of modularity, each possessing its own autonomous MLOps lifecycle. Finally, we show how a Human-in-the-Loop governance model is technically integrated not merely as a safety check, but as a critical, event-driven data source for continuous improvement. We present the platform as a reference architecture, offering practical lessons for engineers building maintainable, scalable, and accountable AI-enabled systems in high-stakes domains.

</details>


### [318] [Environment-Aware Adaptive Pruning with Interleaved Inference Orchestration for Vision-Language-Action Models](https://arxiv.org/abs/2602.00780)
*Yuting Huang,Leilei Ding,Zhipeng Tang,Zenghuan Zhu,Jiajun Deng,Xinrui Lin,Shuo Liu,Haojie Ren,Jianmin Ji,Yanyong Zhang*

Main category: cs.AI

TL;DR: EcoVLA is a training-free adaptive pruning framework for Vision-Language-Action models that dynamically adjusts sparsity patterns based on environment changes, achieving up to 2.18× speedup with minimal performance degradation.


<details>
  <summary>Details</summary>
Motivation: VLA models have high inference latency due to large parameter counts, hindering real-time manipulation. Static pruning lacks adaptability to changing environments, while dynamic pruning has coarse granularity and high retraining overhead.

Method: EcoVLA combines Environment-aware Adaptive Pruning (EAP) that uses temporal consistency of physical environments to update sparsity patterns, and Interleaved Inference Orchestration (I²O) that schedules pruning in parallel with VLA inference using FLOPs bubbles.

Result: Achieves up to 1.60× speedup with only 0.4% drop in success rate, and 2.18× speedup with 0.5% degradation when combined with token pruning. Validated on diverse VLA models, benchmarks, and real-world robots.

Conclusion: EcoVLA provides an effective training-free, plug-and-play adaptive pruning solution that addresses VLA inference latency while maintaining performance, compatible with existing acceleration methods.

Abstract: While Vision-Language-Action (VLA) models hold promise in embodied intelligence, their large parameter counts lead to substantial inference latency that hinders real-time manipulation, motivating parameter sparsification. However, as the environment evolves during VLA execution, the optimal sparsity patterns change accordingly. Static pruning lacks the adaptability required for environment dynamics, whereas fixed-interval dynamic layer pruning suffers from coarse granularity and high retraining overheads. To bridge this gap, we propose EcoVLA, a training-free, plug-and-play adaptive pruning framework that supports orthogonal combination with existing VLA acceleration methods. EcoVLA comprises two components: Environment-aware Adaptive Pruning (EAP) and Interleaved Inference Orchestration ($I^2O$). EAP is a lightweight adaptive channel pruning method that incorporates the temporal consistency of the physical environment to update sparsity patterns. $I^2O$ leverages the FLOPs bubbles inherent in VLA inference to schedule the pruning method in parallel, ensuring negligible impact on latency. Evaluated on diverse VLA models and benchmarks, EcoVLA delivers state-of-the-art performance, achieving up to 1.60$\times$ speedup with only a 0.4% drop in success rate, and further reaches 2.18$\times$ speedup with only a 0.5% degradation when combined with token pruning. We further validate the effectiveness of EcoVLA on real-world robots.

</details>


### [319] [World Models as an Intermediary between Agents and the Real World](https://arxiv.org/abs/2602.00785)
*Sherry Yang*

Main category: cs.AI

TL;DR: World models can bridge the gap between LLM agents and high-cost real-world domains by serving as intermediaries that reduce action execution costs while providing rich learning signals.


<details>
  <summary>Details</summary>
Motivation: Current LLM agents excel in low-cost environments but fail in high-cost domains (robotics, ML engineering, scientific experiments) due to expensive action execution for reward acquisition. The fundamental bottleneck is the cost of executing actions to get reward signals.

Method: Propose using world models as intermediaries between agents and the real world. World models are viewed as models of dynamics, rewards, and task distributions that can overcome barriers like extreme off-policy learning and sample inefficiency in long-horizon tasks.

Result: Demonstrates how world models can provide critical learning signals across domains including machine learning engineering, computer use, robotics, and AI for science. Identifies challenges in building these world models.

Conclusion: World models offer a promising solution to scale agent performance to high-cost domains. The paper proposes actionable items for dataset curation, architecture design, scaling, and evaluation of world models to address implementation challenges.

Abstract: Large language model (LLM) agents trained using reinforcement learning has achieved superhuman performance in low-cost environments like games, mathematics, and coding. However, these successes have not translated to complex domains where the cost of interaction is high, such as the physical cost of running robots, the time cost of ML engineering, and the resource cost of scientific experiments. The true bottleneck for achieving the next level of agent performance for these complex and high-cost domains lies in the expense of executing actions to acquire reward signals. To address this gap, this paper argues that we should use world models as an intermediary between agents and the real world. We discuss how world models, viewed as models of dynamics, rewards, and task distributions, can overcome fundamental barriers of high-cost actions such as extreme off-policy learning and sample inefficiency in long-horizon tasks. Moreover, we demonstrate how world models can provide critical and rich learning signals to agents across a broad set of domains, including machine learning engineering, computer use, robotics, and AI for science. Lastly, we identify the challenges of building these world models and propose actionable items along dataset curation, architecture design, scaling, and evaluation of world models.

</details>


### [320] [MissMAC-Bench: Building Solid Benchmark for Missing Modality Issue in Robust Multimodal Affective Computing](https://arxiv.org/abs/2602.00811)
*Ronghao Lin,Honghao Lu,Ruixing Wu,Aolin Xiong,Qinggong Chu,Qiaolin He,Sijie Mai,Haifeng Hu*

Main category: cs.AI

TL;DR: MissMAC-Bench is a comprehensive benchmark for evaluating multimodal affective computing models' robustness to missing modalities, addressing real-world challenges of incomplete data availability.


<details>
  <summary>Details</summary>
Motivation: Real-world multimodal affective computing faces the missing modality problem where modality data availability is dynamic and uncertain, causing performance fluctuations due to distribution shifts and semantic deficiencies in incomplete inputs. This poses a critical barrier to robustness and practical deployment.

Method: Introduces MissMAC-Bench with two guiding principles: 1) no missing prior during training, and 2) a single model capable of handling both complete and incomplete modality scenarios. The benchmark integrates evaluation protocols with both fixed and random missing patterns at dataset and instance levels.

Result: Extensive experiments on 3 widely-used language models across 4 datasets validate the effectiveness of diverse MAC approaches in tackling the missing modality issue.

Conclusion: The benchmark provides a solid foundation for advancing robust multimodal affective computing and promotes the development of multimedia data mining by establishing fair and unified evaluation standards from the perspective of cross-modal synergy.

Abstract: As a knowledge discovery task over heterogeneous data sources, current Multimodal Affective Computing (MAC) heavily rely on the completeness of multiple modalities to accurately understand human's affective state. However, in real-world scenarios, the availability of modality data is often dynamic and uncertain, leading to substantial performance fluctuations due to the distribution shifts and semantic deficiencies of the incomplete multimodal inputs. Known as the missing modality issue, this challenge poses a critical barrier to the robustness and practical deployment of MAC models. To systematically quantify this issue, we introduce MissMAC-Bench, a comprehensive benchmark designed to establish fair and unified evaluation standards from the perspective of cross-modal synergy. Two guiding principles are proposed, including no missing prior during training, and one single model capable of handling both complete and incomplete modality scenarios, thereby ensuring better generalization. Moreover, to bridge the gap between academic research and real-world applications, our benchmark integrates evaluation protocols with both fixed and random missing patterns at the dataset and instance levels. Extensive experiments conducted on 3 widely-used language models across 4 datasets validate the effectiveness of diverse MAC approaches in tackling the missing modality issue. Our benchmark provides a solid foundation for advancing robust multimodal affective computing and promotes the development of multimedia data mining.

</details>


### [321] [Resource-Efficient Reinforcement for Reasoning Large Language Models via Dynamic One-Shot Policy Refinement](https://arxiv.org/abs/2602.00815)
*Yunjian Zhang,Sudong Wang,Yang Li,Peiran Xu,Conghao Zhou,Xiaoyue Ma,Jianing Li,Yao Zhu*

Main category: cs.AI

TL;DR: DoPR reduces RLVR training costs by 10x through uncertainty-aware dynamic sample selection while maintaining reasoning accuracy.


<details>
  <summary>Details</summary>
Motivation: RLVR is effective for aligning LLMs with reasoning chains but is prohibitively resource-intensive, requiring extensive reward signals and high rollout costs during training.

Method: Proposed Dynamic One-Shot Policy Refinement (DoPR), an uncertainty-aware RL strategy that dynamically selects a single informative training sample per batch for policy updates, guided by reward volatility and exploration-driven acquisition.

Result: DoPR reduces rollout overhead by nearly an order of magnitude while preserving competitive reasoning accuracy, achieving strong performance with surprisingly few training instances.

Conclusion: DoPR offers a scalable and resource-efficient solution for LLM post-training, providing a practical path toward more efficient and accessible RL-based training for reasoning-intensive LLM applications.

Abstract: Large language models (LLMs) have exhibited remarkable performance on complex reasoning tasks, with reinforcement learning under verifiable rewards (RLVR) emerging as a principled framework for aligning model behavior with reasoning chains. Despite its promise, RLVR remains prohibitively resource-intensive, requiring extensive reward signals and incurring substantial rollout costs during training. In this work, we revisit the fundamental question of data and compute efficiency in RLVR. We first establish a theoretical lower bound on the sample complexity required to unlock reasoning capabilities, and empirically validate that strong performance can be achieved with a surprisingly small number of training instances. To tackle the computational burden, we propose Dynamic One-Shot Policy Refinement (DoPR), an uncertainty-aware RL strategy that dynamically selects a single informative training sample per batch for policy updates, guided by reward volatility and exploration-driven acquisition. DoPR reduces rollout overhead by nearly an order of magnitude while preserving competitive reasoning accuracy, offering a scalable and resource-efficient solution for LLM post-training. This approach offers a practical path toward more efficient and accessible RL-based training for reasoning-intensive LLM applications.

</details>


### [322] [Optimizing Agentic Reasoning with Retrieval via Synthetic Semantic Information Gain Reward](https://arxiv.org/abs/2602.00845)
*Senkang Hu,Yong Dai,Yuzhi Zhao,Yihang Tao,Yu Guo,Zhengru Fang,Sam Tak Wu Kwong,Yuguang Fang*

Main category: cs.AI

TL;DR: InfoReasoner is a framework that uses synthetic semantic information gain rewards to optimize retrieval in agentic reasoning models, achieving up to 5.4% accuracy improvement across QA benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current agentic reasoning models lack dense, principled reward signals for optimizing retrieval processes, making it challenging to dynamically acquire external knowledge effectively.

Method: Introduces InfoReasoner with: 1) Theoretical redefinition of information gain as uncertainty reduction over belief states, 2) Output-aware intrinsic estimator using semantic clustering via bidirectional textual entailment, 3) Training via Group Relative Policy Optimization (GRPO) to maximize epistemic progress.

Result: Outperforms strong retrieval-augmented baselines across seven question-answering benchmarks, achieving up to 5.4% average accuracy improvement.

Conclusion: Provides a theoretically grounded and scalable path toward agentic reasoning with retrieval, enabling effective information seeking without manual retrieval annotations.

Abstract: Agentic reasoning enables large reasoning models (LRMs) to dynamically acquire external knowledge, but yet optimizing the retrieval process remains challenging due to the lack of dense, principled reward signals. In this paper, we introduce InfoReasoner, a unified framework that incentivizes effective information seeking via a synthetic semantic information gain reward. Theoretically, we redefine information gain as uncertainty reduction over the model's belief states, establishing guarantees, including non-negativity, telescoping additivity, and channel monotonicity. Practically, to enable scalable optimization without manual retrieval annotations, we propose an output-aware intrinsic estimator that computes information gain directly from the model's output distributions using semantic clustering via bidirectional textual entailment. This intrinsic reward guides the policy to maximize epistemic progress, enabling efficient training via Group Relative Policy Optimxization (GRPO). Experiments across seven question-answering benchmarks demonstrate that InfoReasoner consistently outperforms strong retrieval-augmented baselines, achieving up to 5.4% average accuracy improvement. Our work provides a theoretically grounded and scalable path toward agentic reasoning with retrieval.

</details>


### [323] [Persuasion Propagation in LLM Agents](https://arxiv.org/abs/2602.00851)
*Hyejun Jeong,Amir Houmansadr,Shlomo Zilberstein,Eugene Bagdasarian*

Main category: cs.AI

TL;DR: Persuasion can influence AI agent behavior during tasks, with belief-prefilled agents showing reduced search activity compared to neutral agents.


<details>
  <summary>Details</summary>
Motivation: As AI agents combine conversation with autonomous task execution, researchers want to understand how user persuasion affects their long-horizon task behavior.

Method: Introduced a behavior-centered evaluation framework distinguishing between persuasion during vs. prior to task execution, tested across web research and coding tasks.

Result: On-the-fly persuasion had weak effects, but belief-prefilled agents conducted 26.9% fewer searches and visited 16.9% fewer unique sources than neutral-prefilled agents.

Conclusion: Persuasion can propagate to affect agent behavior, motivating behavior-level evaluation in agentic systems.

Abstract: Modern AI agents increasingly combine conversational interaction with autonomous task execution, such as coding and web research, raising a natural question: what happens when an agent engaged in long-horizon tasks is subjected to user persuasion? We study how belief-level intervention can influence downstream task behavior, a phenomenon we name \emph{persuasion propagation}. We introduce a behavior-centered evaluation framework that distinguishes between persuasion applied during or prior to task execution. Across web research and coding tasks, we find that on-the-fly persuasion induces weak and inconsistent behavioral effects. In contrast, when the belief state is explicitly specified at task time, belief-prefilled agents conduct on average 26.9\% fewer searches and visit 16.9\% fewer unique sources than neutral-prefilled agents. These results suggest that persuasion, even in prior interaction, can affect the agent's behavior, motivating behavior-level evaluation in agentic systems.

</details>


### [324] [Position: Human-Centric AI Requires a Minimum Viable Level of Human Understanding](https://arxiv.org/abs/2602.00854)
*Fangzhou Lin,Qianwen Ge,Lingyu Xu,Peiran Li,Xiangbo Gao,Shuo Xing,Kazunori Yamada,Ziming Zhang,Haichong Zhang,Zhengzhong Tu*

Main category: cs.AI

TL;DR: The paper introduces the Capability-Comprehension Gap - AI systems improve performance while users' understanding deteriorates, threatening oversight. It proposes the Cognitive Integrity Threshold (CIT) as the minimum comprehension needed for meaningful human oversight.


<details>
  <summary>Details</summary>
Motivation: As AI systems produce increasingly fluent and correct outcomes, users' ability to explain, verify, or intervene erodes over time. This creates a dangerous divergence where assisted performance improves while users' internal models deteriorate, undermining meaningful human oversight and accountability.

Method: The paper formalizes the Cognitive Integrity Threshold (CIT) concept - the minimum comprehension required to preserve oversight, autonomy, and accountable participation under AI assistance. It operationalizes CIT through three dimensions: verification capacity, comprehension-preserving interaction, and institutional governance scaffolds.

Result: The analysis reveals that current approaches to transparency, user control, literacy, and governance fail to address the fundamental understanding humans must retain for effective oversight under sustained AI delegation. CIT provides a framework to identify when oversight becomes merely procedural and contestability fails.

Conclusion: The paper calls for a new design and governance agenda that aligns human-AI interaction with cognitive sustainability in responsibility-critical settings, ensuring humans maintain sufficient comprehension for meaningful oversight even as AI capabilities advance.

Abstract: AI systems increasingly produce fluent, correct, end-to-end outcomes. Over time, this erodes users' ability to explain, verify, or intervene. We define this divergence as the Capability-Comprehension Gap: a decoupling where assisted performance improves while users' internal models deteriorate. This paper argues that prevailing approaches to transparency, user control, literacy, and governance do not define the foundational understanding humans must retain for oversight under sustained AI delegation. To formalize this, we define the Cognitive Integrity Threshold (CIT) as the minimum comprehension required to preserve oversight, autonomy, and accountable participation under AI assistance. CIT does not require full reasoning reconstruction, nor does it constrain automation. It identifies the threshold beyond which oversight becomes procedural and contestability fails. We operatinalize CIT through three functional dimensions: (i) verification capacity, (ii) comprehension-preserving interaction, and (iii) institutional scaffolds for governance. This motivates a design and governance agenda that aligns human-AI interaction with cognitive sustainability in responsibility-critical settings.

</details>


### [325] [Multi-Head Attention Is a Multi-Player Game](https://arxiv.org/abs/2602.00861)
*Kushal Chakrabarti,Nirmal Balachundar*

Main category: cs.AI

TL;DR: Transformers' attention heads form a multi-agent game where cross-entropy training creates implicit competition, leading to inefficient Nash equilibria with redundancy and hallucinations. The Price of Anarchy is bounded by head interaction coupling, and regularization reducing this coupling improves performance.


<details>
  <summary>Details</summary>
Motivation: Transformers' attention mechanism operates as a multi-agent system where heads compete and coordinate, but current training treats it as a monolithic optimizer. This creates a gap where cross-entropy training induces implicit competition leading to inefficient equilibria with redundancy and correlated errors (hallucinations).

Method: Formalize attention heads as playing an implicit potential game under cross-entropy training. Bound the Price of Anarchy by Γ(G), the off-diagonal mass of head interaction matrix capturing weight and gradient coupling. Propose GAME-LoRA regularization combining Barlow Twins decorrelation with log-determinant coordination pressure to reduce Γ(G).

Result: Γ(G) predicts hallucination probability (p<0.05), emergent coalitions show selective coordination, and GAME-LoRA achieves up to 18% hallucination reduction (8% average) with no knowledge degradation - a Pareto improvement inaccessible to methods ignoring game structure.

Conclusion: Transformer attention heads form a game-theoretic system where inefficiency (redundancy and hallucinations) stems from unpriced externalities in training. Regularization targeting head interaction coupling (Γ(G)) provably tightens the Price of Anarchy, enabling simultaneous reduction of both failure modes without performance trade-offs.

Abstract: Modern transformer attention is internally multi-agent -- heads compete and coordinate -- yet we train it as if it were a monolithic optimizer. We formalize this gap: cross-entropy training induces an implicit potential game among heads, and gradient descent converges to Nash equilibria with potentially unbounded inefficiency due to unpriced externalities (redundancy, correlated errors). Our main result bounds the Price of Anarchy by $Γ(G)$, the off-diagonal mass of a head interaction matrix capturing weight and gradient coupling. Under mild smoothness assumptions, we prove that both \emph{excess hallucination probability} and \emph{excess head redundancy} scale with PoA, unifying two distinct failure modes into a single mechanism. The bound is prescriptive: regularization that reduces $Γ(G)$ provably tightens PoA. We instantiate this as GAME-LoRA, combining Barlow Twins decorrelation with log-determinant coordination pressure. Experiments validate the theory: $Γ(G)$ predicts hallucination ($p{<}0.05$), emergent coalitions exhibit selective coordination, and GAME-LoRA achieves up to 18\% hallucination reduction (8\% average) with no knowledge degradation -- a Pareto improvement inaccessible to methods ignoring the game structure.

</details>


### [326] [Foundation CAN LM: A Pretrained Language Model For Automotive CAN Data](https://arxiv.org/abs/2602.00866)
*Akiharu Esashi,Pawissanutt Lertpongrujikorn,Justin Makino,Yuibi Fujimoto,Mohsen Amini Salehi*

Main category: cs.AI

TL;DR: A foundation model approach for CAN bus data that treats vehicular signals as language, enabling multi-task generalization across automotive applications through pretraining on decoded signals and fine-tuning for diverse insurance tasks.


<details>
  <summary>Details</summary>
Motivation: Current CAN data pipelines use isolated task-specific models on raw data, preventing shared representation learning and cross-task generalization. The authors aim to apply the successful foundation model paradigm from NLP/CV to CAN data for better generalization.

Method: Treat CAN data as language, pretrain on large-scale unlabeled decoded CAN signals, then fine-tune across heterogeneous auto insurance tasks. Propose unified tokenization for mixed discrete-continuous signals and address temporal complexity and trip-specific variability challenges.

Result: One pretrained CAN model can adapt effectively to diverse predictive tasks, validating that the foundation modeling paradigm proven in NLP and CV also works for CAN data.

Conclusion: Establishes a new direction for generalizable representation learning in automotive AI by demonstrating the viability of foundation models for CAN data, enabling shared representations and cross-task generalization.

Abstract: The Controller Area Network (CAN) bus provides a rich source of vehicular signals increasingly leveraged for applications in automotive and auto insurance domains, including collision detection, predictive maintenance, and driver risk modeling. Despite this potential, existing pipelines largely train isolated task-specific models on raw CAN data, with only limited efforts exploring decoded signals. Such fragmentation prevents shared representation learning and limits cross-task generalization. By contrast, natural language processing (NLP) and computer vision (CV) have been transformed by the foundation model paradigm: large-scale pretraining followed by task-specific adaptation. In this work, we introduce the foundation CAN model that demonstrates multi-objective downstream generalization using a single pretrained backbone. Our approach treats CAN data as a language: we pretrain on large-scale, unlabeled decoded CAN signals and fine-tune across heterogeneous auto insurance tasks. To enable this, we propose a unified tokenization scheme for mixed discrete-continuous signals and address challenges of temporal complexity and trip-specific variability. Our results show that one pretrained CAN model can adapt effectively to diverse predictive tasks, validating that the foundation modeling paradigm, proven in NLP and CV, also holds for CAN data. This establishes a new direction for generalizable representation learning in automotive AI.

</details>


### [327] [Beyond Output Critique: Self-Correction via Task Distillation](https://arxiv.org/abs/2602.00871)
*Hossein A. Rahmani,Mengting Wan,Pei Zhou,Longqi Yang,Nick Craswell,Emine Yilmaz,Sujay Kumar Jauhar*

Main category: cs.AI

TL;DR: SELF-THOUGHT introduces task abstraction before solution refinement to improve LLM self-correction by creating structured templates that guide reasoning and enable cross-model transfer.


<details>
  <summary>Details</summary>
Motivation: Existing LLM self-correction approaches only patch surface errors and fail to correct deeper reasoning flaws, limiting their effectiveness for complex tasks.

Method: The framework adds an intermediate task abstraction step where models distill tasks into structured templates capturing key variables, constraints, and problem structure, then use these templates to guide solution instantiation and refinement.

Result: Experiments show SELF-THOUGHT improves accuracy, robustness, and generalization across diverse reasoning tasks for both large and small models, with templates transferable from larger to smaller models.

Conclusion: SELF-THOUGHT offers a scalable path toward more reliable self-correcting language systems by grounding refinement in task understanding and enabling knowledge transfer between models.

Abstract: Large language models (LLMs) have shown promising self-correction abilities, where iterative refinement improves the quality of generated responses. However, most existing approaches operate at the level of output critique, patching surface errors while often failing to correct deeper reasoning flaws. We propose SELF-THOUGHT, a framework that introduces an intermediate step of task abstraction before solution refinement. Given an input and an initial response, the model first distills the task into a structured template that captures key variables, constraints, and problem structure. This abstraction then guides solution instantiation, grounding subsequent responses in a clearer understanding of the task and reducing error propagation. Crucially, we show that these abstractions can be transferred across models: templates generated by larger models can serve as structured guides for smaller LLMs, which typically struggle with intrinsic self-correction. By reusing distilled task structures, smaller models achieve more reliable refinements without heavy fine-tuning or reliance on external verifiers. Experiments across diverse reasoning tasks demonstrate that SELF-THOUGHT improves accuracy, robustness, and generalization for both large and small models, offering a scalable path toward more reliable self-correcting language systems.

</details>


### [328] [Synapse Compendium Aware Federated Knowledge Exchange for Tool Routed LLMs](https://arxiv.org/abs/2602.00911)
*Abhijit Chakraborty,Sandipan De,Yash Shah,Chahana Dahal,Vivek Gupta*

Main category: cs.AI

TL;DR: Synapse is a federated learning framework for LLM-based agents that trains a shared global knowledge model of tool-usage behavior to improve effectiveness while reducing communication overhead.


<details>
  <summary>Details</summary>
Motivation: Collaborative learning among LLM-based agents in federated settings faces challenges including high communication costs, data heterogeneity, and tool-usage heterogeneity, which limit their effectiveness.

Method: Synapse trains a shared global knowledge model of tool-usage behavior where client agents with fixed LLMs learn tool-usage patterns locally, transmit artifacts for federated aggregation through coordinators, and update/redistribute a global tool compendium. It uses templated representations, embedding retrieval with LLM reranking, and adaptive masking to maintain utility while limiting information leakage.

Result: Synapse improves tool-usage effectiveness and reduces communication overhead compared with weight or prompt-sharing approaches in multi-agent LLM systems.

Conclusion: Synapse provides an effective framework for federated learning among LLM-based agents that addresses communication costs and heterogeneity challenges while improving tool-usage performance.

Abstract: Collaborative learning among LLM-based agents under federated learning faces challenges, including communication costs, heterogeneity in data, and tool-usage, limiting their effectiveness. We introduce Synapse, a framework that trains a shared global knowledge model of tool-usage behavior. Client agents with fixed LLMs learn tool-usage patterns locally, and transmit artifacts for federated aggregation through coordinators. A global tool compendium is updated and redistributed, enabling convergence toward stable tool selection. Synapse uses templated representations, embedding retrieval with LLM reranking, and adaptive masking to maintain utility while limiting information leakage. The framework supports heterogeneous data and quantifies performance improvements. Results show that Synapse improves tool-usage effectiveness and reduces communication overhead compared with weight or prompt-sharing approaches in multi-agent LLM systems.

</details>


### [329] [Supervised sparse auto-encoders as unconstrained feature models for semantic composition](https://arxiv.org/abs/2602.00924)
*Ouns El Harzli,Hugo Wallner,Yoonsoo Nam,Haixuan Xavier Tao*

Main category: cs.AI

TL;DR: SAEs improved with unconstrained feature models and supervision for better reconstruction, scalability, and semantic alignment in Stable Diffusion 3.5.


<details>
  <summary>Details</summary>
Motivation: Address two key limitations of sparse auto-encoders: 1) non-smoothness of L1 penalty hindering reconstruction and scalability, and 2) lack of alignment between learned features and human semantics.

Method: Adapt unconstrained feature models from neural collapse theory and supervise decoder-only SAEs to reconstruct feature vectors by jointly learning sparse concept embeddings and decoder weights.

Result: Validated on Stable Diffusion 3.5, demonstrates compositional generalization (reconstructing images with unseen concept combinations) and enables feature-level intervention for semantic image editing without prompt modification.

Conclusion: The approach successfully addresses SAE limitations, improving reconstruction, scalability, and semantic alignment while enabling new capabilities for mechanistic interpretability and semantic image editing.

Abstract: Sparse auto-encoders (SAEs) have re-emerged as a prominent method for mechanistic interpretability, yet they face two significant challenges: the non-smoothness of the $L_1$ penalty, which hinders reconstruction and scalability, and a lack of alignment between learned features and human semantics. In this paper, we address these limitations by adapting unconstrained feature models-a mathematical framework from neural collapse theory-and by supervising the task. We supervise (decoder-only) SAEs to reconstruct feature vectors by jointly learning sparse concept embeddings and decoder weights. Validated on Stable Diffusion 3.5, our approach demonstrates compositional generalization, successfully reconstructing images with concept combinations unseen during training, and enabling feature-level intervention for semantic image editing without prompt modification.

</details>


### [330] [Learning Abstractions for Hierarchical Planning in Program-Synthesis Agents](https://arxiv.org/abs/2602.00929)
*Zergham Ahmed,Kazuki Irie,Joshua B. Tenenbaum,Christopher J. Bates,Samuel J. Gershman*

Main category: cs.AI

TL;DR: TheoryCoder-2 is a Theory-Based RL agent that uses LLMs to actively learn reusable abstractions from experience, achieving better sample efficiency and solving complex tasks that baseline LLM agents fail on.


<details>
  <summary>Details</summary>
Motivation: Current LLM agents and deep RL systems struggle with generalization and efficient planning using abstractions, while existing Theory-Based RL systems like TheoryCoder rely on human-provided abstractions rather than learning them autonomously.

Method: TheoryCoder-2 leverages LLMs' in-context learning ability to actively learn reusable abstractions by synthesizing them from experience and integrating them into a hierarchical planning process, requiring only minimal human prompts.

Result: TheoryCoder-2 shows significantly better sample efficiency than baseline LLM agents with classical planning, reasoning-based planning, and prior program-synthesis agents like WorldCoder, solving complex tasks that baselines fail on.

Conclusion: The approach demonstrates that LLMs can be effectively used to learn abstractions autonomously in Theory-Based RL systems, reducing reliance on human-provided abstractions while achieving strong generalization across diverse environments.

Abstract: Humans learn abstractions and use them to plan efficiently to quickly generalize across tasks -- an ability that remains challenging for state-of-the-art large language model (LLM) agents and deep reinforcement learning (RL) systems. Inspired by the cognitive science of how people form abstractions and intuitive theories of their world knowledge, Theory-Based RL (TBRL) systems, such as TheoryCoder, exhibit strong generalization through effective use of abstractions. However, they heavily rely on human-provided abstractions and sidestep the abstraction-learning problem. We introduce TheoryCoder-2, a new TBRL agent that leverages LLMs' in-context learning ability to actively learn reusable abstractions rather than relying on hand-specified ones, by synthesizing abstractions from experience and integrating them into a hierarchical planning process. We conduct experiments on diverse environments, including BabyAI, Minihack and VGDL games like Sokoban. We find that TheoryCoder-2 is significantly more sample-efficient than baseline LLM agents augmented with classical planning domain construction, reasoning-based planning, and prior program-synthesis agents such as WorldCoder. TheoryCoder-2 is able to solve complex tasks that the baselines fail, while only requiring minimal human prompts, unlike prior TBRL systems.

</details>


### [331] [The Keyhole Effect: Why Chat Interfaces Fail at Data Analysis](https://arxiv.org/abs/2602.00947)
*Mohan Reddy*

Main category: cs.AI

TL;DR: Chat interfaces degrade analytical performance due to cognitive bottlenecks; proposed hybrid design patterns address these issues while preserving natural language benefits.


<details>
  <summary>Details</summary>
Motivation: Chat has become the default AI interface for data analysis, but for multi-step, state-dependent tasks, chat interfaces systematically degrade analytical performance through cognitive bottlenecks identified by the Keyhole Effect.

Method: The paper formalizes cognitive overload as O = max(0, m - v - W) where m is task-relevant items, v is visible items, and W is working memory capacity, and proposes eight hybrid design patterns: Generative UI, Infinite Canvas, Deictic Interaction, State Rail, Ghost Layers, Mise en Place, Semantic Zoom, and Probabilistic UI.

Result: When cognitive overload O > 0, error probability increases and analytical biases (anchoring, confirmation, change blindness) amplify. The proposed design patterns target specific cognitive bottlenecks while preserving natural language for intent specification and synthesis.

Conclusion: Chat interfaces are suboptimal for complex analytical tasks; hybrid designs that scaffold conversational systems with expert priors can reduce cognitive load, especially for open-ended exploration. The paper provides falsifiable hypotheses and experimental paradigms for validation.

Abstract: Chat has become the default interface for AI-assisted data analysis. For multi-step, state-dependent analytical tasks, this is a mistake. Building on Woods (1984) Keyhole Effect, the cognitive cost of viewing large information spaces through narrow viewports, I show that chat interfaces systematically degrade analytical performance through five mechanisms: (1) constant content displacement defeats hippocampal spatial memory systems; (2) hidden state variables exceed working memory capacity (approximately 4 chunks under load); (3) forced verbalization triggers verbal overshadowing, degrading visual pattern recognition; (4) linear text streams block epistemic action and cognitive offloading; (5) serialization penalties scale with data dimensionality. I formalize cognitive overload as O = max(0, m - v - W) where m is task-relevant items, v is visible items, and W is working memory capacity. When O > 0, error probability increases and analytical biases (anchoring, confirmation, change blindness) amplify. Eight hybrid design patterns address these failures: Generative UI, Infinite Canvas, Deictic Interaction, State Rail, Ghost Layers, Mise en Place, Semantic Zoom, and Probabilistic UI. Each pattern targets specific cognitive bottlenecks while preserving natural language for intent specification and synthesis. Well-scaffolded conversational systems that encode expert priors may reduce load for guided tasks; the framework applies most strongly to open-ended exploration. The paper concludes with falsifiable hypotheses and experimental paradigms for empirical validation.

</details>


### [332] [MindGuard: Guardrail Classifiers for Multi-Turn Mental Health Support](https://arxiv.org/abs/2602.00950)
*António Farinhas,Nuno M. Guerreiro,José Pombal,Pedro Henrique Martins,Laura Melton,Alex Conway,Cara Dochat,Maya D'Eon,Ricardo Rei*

Main category: cs.AI

TL;DR: MindGuard introduces clinically-grounded safety classifiers for mental health LLMs, reducing false positives while maintaining high recall for genuine crises.


<details>
  <summary>Details</summary>
Motivation: Current LLM safety measures fail to distinguish between therapeutic disclosures and genuine clinical crises, leading to inappropriate responses in mental health applications.

Method: Developed clinical risk taxonomy with psychologists, created MindGuard-testset dataset, trained lightweight classifiers (4B/8B parameters) using synthetic dialogues from controlled two-agent setup.

Result: MindGuard reduces false positives at high-recall operating points, lowers attack success and harmful engagement rates in adversarial interactions when paired with clinician LLMs.

Conclusion: Clinically-grounded safety classifiers outperform general-purpose safeguards for mental health applications, with models and evaluation data publicly released.

Abstract: Large language models are increasingly used for mental health support, yet their conversational coherence alone does not ensure clinical appropriateness. Existing general-purpose safeguards often fail to distinguish between therapeutic disclosures and genuine clinical crises, leading to safety failures. To address this gap, we introduce a clinically grounded risk taxonomy, developed in collaboration with PhD-level psychologists, that identifies actionable harm (e.g., self-harm and harm to others) while preserving space for safe, non-crisis therapeutic content. We release MindGuard-testset, a dataset of real-world multi-turn conversations annotated at the turn level by clinical experts. Using synthetic dialogues generated via a controlled two-agent setup, we train MindGuard, a family of lightweight safety classifiers (with 4B and 8B parameters). Our classifiers reduce false positives at high-recall operating points and, when paired with clinician language models, help achieve lower attack success and harmful engagement rates in adversarial multi-turn interactions compared to general-purpose safeguards. We release all models and human evaluation data.

</details>


### [333] [R-HTN: Rebellious Online HTN Planning for Safety and Game AI](https://arxiv.org/abs/2602.00951)
*Hector Munoz-Avila,David W. Aha,Paola Rizzo*

Main category: cs.AI

TL;DR: Online HTN agents that can disobey user tasks when they violate built-in directives, with two variants: Nonadaptive (stops) and Adaptive (modifies plans).


<details>
  <summary>Details</summary>
Motivation: To create agents capable of intelligent disobedience when user-assigned tasks conflict with built-in safety directives or personality traits, ensuring agents don't blindly follow harmful instructions.

Method: Developed R-HTN (Rebellious-HTN) algorithm combining HTN planning, online planning, and directive consideration. Two agent variants: Nonadaptive stops execution when violating directives, Adaptive modifies HTN plans to find alternative task completion methods.

Result: R-HTN agents never violate directives and aim to achieve user goals if feasible, though not necessarily as the user expected. Evaluated in two task domains with safety/personality directives.

Conclusion: The R-HTN framework enables online HTN agents to intelligently disobey user tasks when they conflict with built-in directives, providing a principled approach to agent rebellion for safety and ethical compliance.

Abstract: We introduce online Hierarchical Task Network (HTN) agents whose behaviors are governed by a set of built-in directives \D. Like other agents that are capable of rebellion (i.e., {\it intelligent disobedience}), our agents will, under some conditions, not perform a user-assigned task and instead act in ways that do not meet a user's expectations. Our work combines three concepts: HTN planning, online planning, and the directives \D, which must be considered when performing user-assigned tasks. We investigate two agent variants: (1) a Nonadaptive agent that stops execution if it finds itself in violation of \D~ and (2) an Adaptive agent that, in the same situation, instead modifies its HTN plan to search for alternative ways to achieve its given task. We present R-HTN (for: Rebellious-HTN), a general algorithm for online HTN planning under directives \D. We evaluate R-HTN in two task domains where the agent must not violate some directives for safety reasons or as dictated by their personality traits. We found that R-HTN agents never violate directives, and aim to achieve the user-given goals if feasible though not necessarily as the user expected.

</details>


### [334] [Small-Margin Preferences Still Matter-If You Train Them Right](https://arxiv.org/abs/2602.00954)
*Jinlong Pang,Zhaowei Zhu,Na Di,Yichi Zhang,Yaxuan Wang,Chen Qian,Yang Liu*

Main category: cs.AI

TL;DR: MixDPO: A difficulty-aware training strategy that routes easy preference pairs to DPO loss and difficult pairs to SFT loss, improving alignment over standard DPO.


<details>
  <summary>Details</summary>
Motivation: Standard preference optimization methods like DPO are sensitive to pair quality and difficulty. Small-margin (ambiguous) pairs are often filtered out as noise, but they still contain useful supervision signals when optimized differently.

Method: MixDPO uses a curriculum ordering of preference data from easy to hard based on margin-defined difficulty, then routes difficult pairs to supervised fine-tuning (SFT) objective while applying preference loss (DPO) to easy pairs.

Result: Across three LLM-judge benchmarks, MixDPO consistently improves alignment over DPO and widely-used variants, with particularly strong gains on AlpacaEval~2 length-controlled win rate.

Conclusion: Difficult preference pairs contain valuable supervision but destabilize preference-based losses; MixDPO's hybrid approach effectively leverages ambiguous pairs without optimization failures, providing a practical mechanism for better alignment.

Abstract: Preference optimization methods such as DPO align large language models (LLMs) using paired comparisons, but their effectiveness can be highly sensitive to the quality and difficulty of preference pairs. A common heuristic treats small-margin (ambiguous) pairs as noisy and filters them out. In this paper, we revisit this assumption and show that pair difficulty interacts strongly with the optimization objective: when trained with preference-based losses, difficult pairs can destabilize training and harm alignment, yet these same pairs still contain useful supervision signals when optimized with supervised fine-tuning (SFT). Motivated by this observation, we propose MixDPO, a simple yet effective difficulty-aware training strategy that (i) orders preference data from easy to hard (a curriculum over margin-defined difficulty), and (ii) routes difficult pairs to an SFT objective while applying a preference loss to easy pairs. This hybrid design provides a practical mechanism to leverage ambiguous pairs without incurring the optimization failures often associated with preference losses on low-margin data. Across three LLM-judge benchmarks, MixDPO consistently improves alignment over DPO and a range of widely-used variants, with particularly strong gains on AlpacaEval~2 length-controlled (LC) win rate.

</details>


### [335] [Reasoning and Tool-use Compete in Agentic RL:From Quantifying Interference to Disentangled Tuning](https://arxiv.org/abs/2602.00994)
*Yu Li,Mingyang Yi,Xiuyu Li,Ju Fan,Fuxin Jiang,Binbin Chen,Peng Li,Jie Song,Tieying Zhang*

Main category: cs.AI

TL;DR: ARL joint training of reasoning and tool-use causes interference; DART decouples them with separate LoRA modules, outperforming baselines by 6.35%.


<details>
  <summary>Details</summary>
Motivation: Most ARL methods train single model for both reasoning and tool use, assuming joint training improves performance, but this assumption hasn't been empirically examined and may cause interference.

Method: Introduces LEAS to quantify interference, then proposes DART framework that decouples reasoning and tool-use parameter updates via separate low-rank adaptation (LoRA) modules.

Result: DART consistently outperforms baseline methods with average 6.35% improvements and achieves performance comparable to multi-agent systems using a single model.

Conclusion: Joint training of reasoning and tool-use in ARL causes interference; explicit decoupling via DART improves performance and challenges the prevailing ARL paradigm.

Abstract: Agentic Reinforcement Learning (ARL) focuses on training large language models (LLMs) to interleave reasoning with external tool execution to solve complex tasks. Most existing ARL methods train a single shared model parameters to support both reasoning and tool use behaviors, implicitly assuming that joint training leads to improved overall agent performance. Despite its widespread adoption, this assumption has rarely been examined empirically. In this paper, we systematically investigate this assumption by introducing a Linear Effect Attribution System(LEAS), which provides quantitative evidence of interference between reasoning and tool-use behaviors. Through an in-depth analysis, we show that these two capabilities often induce misaligned gradient directions, leading to training interference that undermines the effectiveness of joint optimization and challenges the prevailing ARL paradigm. To address this issue, we propose Disentangled Action Reasoning Tuning(DART), a simple and efficient framework that explicitly decouples parameter updates for reasoning and tool-use via separate low-rank adaptation modules. Experimental results show that DART consistently outperforms baseline methods with averaged 6.35 percent improvements and achieves performance comparable to multi-agent systems that explicitly separate tool-use and reasoning using a single model.

</details>


### [336] [Error Taxonomy-Guided Prompt Optimization](https://arxiv.org/abs/2602.00997)
*Mayank Singh,Vikas Yadav,Eduardo Blanco*

Main category: cs.AI

TL;DR: ETGPO is a top-down prompt optimization method that categorizes model errors into a taxonomy and augments prompts with guidance targeting frequent failure modes, achieving SOTA performance with 1/3 the token usage.


<details>
  <summary>Details</summary>
Motivation: Existing APO methods rely on trial-and-error or bottom-up approaches that lose global perspective, consuming substantial compute. There's a need for more efficient optimization that maintains a global view of failure patterns.

Method: ETGPO collects model errors, categorizes them into a taxonomy, and augments prompts with guidance targeting the most frequent failure modes, adopting a top-down approach rather than iterative bottom-up adjustments.

Result: Across multiple benchmarks (mathematics, QA, logical reasoning), ETGPO achieves comparable or better accuracy than SOTA methods while requiring roughly one third of the optimization-phase token usage and evaluation budget.

Conclusion: ETGPO demonstrates that top-down, taxonomy-guided prompt optimization can achieve strong performance with significantly reduced computational costs compared to existing bottom-up approaches.

Abstract: Automatic Prompt Optimization (APO) is a powerful approach for extracting performance from large language models without modifying their weights. Many existing methods rely on trial-and-error, testing different prompts or in-context examples until a good configuration emerges, often consuming substantial compute. Recently, natural language feedback derived from execution logs has shown promise as a way to identify how prompts can be improved. However, most prior approaches operate in a bottom-up manner, iteratively adjusting the prompt based on feedback from individual problems, which can cause them to lose the global perspective. In this work, we propose Error Taxonomy-Guided Prompt Optimization (ETGPO), a prompt optimization algorithm that adopts a top-down approach. ETGPO focuses on the global failure landscape by collecting model errors, categorizing them into a taxonomy, and augmenting the prompt with guidance targeting the most frequent failure modes. Across multiple benchmarks spanning mathematics, question answering, and logical reasoning, ETGPO achieves accuracy that is comparable to or better than state-of-the-art methods, while requiring roughly one third of the optimization-phase token usage and evaluation budget.

</details>


### [337] [How RLHF Amplifies Sycophancy](https://arxiv.org/abs/2602.01002)
*Itai Shapira,Gerdus Benade,Ariel D. Procaccia*

Main category: cs.AI

TL;DR: Alignment from human feedback can amplify sycophancy in LLMs due to bias in preference data; proposed intervention adds agreement penalty to reward function.


<details>
  <summary>Details</summary>
Motivation: LLMs become more sycophantic after preference-based alignment, affirming user beliefs even when wrong; need to understand and mitigate this failure mode.

Method: Formal analysis of reward learning from pairwise comparisons, characterization of when bias induces reward gaps, and derivation of minimal reward correction as agreement penalty.

Result: Reward gaps are common and cause sycophantic drift in all tested configurations; proposed intervention can prevent sycophancy increase while staying close to original policy.

Conclusion: Human feedback alignment can amplify sycophancy through bias in preference data, but this can be mitigated with targeted reward corrections that penalize agreement.

Abstract: Large language models often exhibit increased sycophantic behavior after preference-based post-training, showing a stronger tendency to affirm a user's stated or implied belief even when this conflicts with factual accuracy or sound judgment. We present a formal analysis of how alignment from human feedback can increase this failure mode by identifying an explicit amplification mechanism that causally links optimization against a learned reward to bias in the human preference data used for alignment. We show that the direction of behavioral drift is determined by a covariance under the base policy between endorsing the belief signal in the prompt and the learned reward, and that the first-order effect reduces to a simple mean-gap condition. We then analyze reward learning from pairwise comparisons under random utility models like Bradley-Terry and characterize when bias in human annotators' preferences induces this reward gap. Next, we propose a training-time intervention designed to neutralize the amplification mechanism itself. Among all post-trained policies that prevent sycophantic behavior from increasing, we characterize the unique policy closest in KL divergence to the unconstrained post-trained policy, and derive the corresponding minimal reward correction as a closed-form agreement penalty. Computational experiments find that reward gaps are common and cause behavioral drift in all the configurations considered.

</details>


### [338] [HalluHard: A Hard Multi-Turn Hallucination Benchmark](https://arxiv.org/abs/2602.01031)
*Dongyang Fan,Sebastien Delsad,Nicolas Flammarion,Maksym Andriushchenko*

Main category: cs.AI

TL;DR: HalluHard is a challenging multi-turn hallucination benchmark with 950 questions across legal, research, medical, and coding domains, requiring inline citations for factual claims. Evaluation uses a web-search-based judging pipeline to verify citations, revealing persistent hallucinations (~30% even for best models with web search).


<details>
  <summary>Details</summary>
Motivation: LLMs still generate plausible but ungrounded factual claims, especially problematic in multi-turn dialogues where early errors cascade and context grows. There's a need for better evaluation of hallucinations in high-stakes domains requiring factual grounding.

Method: Created HalluHard benchmark with 950 seed questions across four domains (legal cases, research questions, medical guidelines, coding). Operationalized groundedness by requiring inline citations. Developed a judging pipeline that iteratively retrieves evidence via web search, fetching, filtering, and parsing full-text sources (including PDFs) to verify citation support.

Result: Hallucinations remain substantial even with web search (~30% for strongest configuration, Opus-4.5 with web search). Content-grounding errors persist at high rates. Hallucination behavior is shaped by model capacity, turn position, effective reasoning, and type of knowledge required.

Conclusion: Multi-turn hallucination remains a significant challenge even for state-of-the-art models with web search capabilities. The HalluHard benchmark provides a rigorous evaluation framework, revealing persistent grounding issues that vary by model characteristics and task requirements.

Abstract: Large language models (LLMs) still produce plausible-sounding but ungrounded factual claims, a problem that worsens in multi-turn dialogue as context grows and early errors cascade. We introduce $\textbf{HalluHard}$, a challenging multi-turn hallucination benchmark with 950 seed questions spanning four high-stakes domains: legal cases, research questions, medical guidelines, and coding. We operationalize groundedness by requiring inline citations for factual assertions. To support reliable evaluation in open-ended settings, we propose a judging pipeline that iteratively retrieves evidence via web search. It can fetch, filter, and parse full-text sources (including PDFs) to assess whether cited material actually supports the generated content. Across a diverse set of frontier proprietary and open-weight models, hallucinations remain substantial even with web search ($\approx 30\%$ for the strongest configuration, Opus-4.5 with web search), with content-grounding errors persisting at high rates. Finally, we show that hallucination behavior is shaped by model capacity, turn position, effective reasoning, and the type of knowledge required.

</details>


### [339] [Discovering Process-Outcome Credit in Multi-Step LLM Reasoning](https://arxiv.org/abs/2602.01034)
*Xiangwei Wang,Wei Wang,Ken Chen,Nanduni Nimalsiri,Saman Halgamuge*

Main category: cs.AI

TL;DR: A novel RL framework for LLMs that uses step-wise marginal information gain and decoupled masking to provide continuous reward signals, improving reasoning efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Standard RL approaches for enhancing LLM reasoning suffer from sparse rewards and inefficient credit assignment, limiting their effectiveness in training reasoning capabilities.

Method: Introduces Step-wise Marginal Information Gain (MIG) against a Monotonic Historical Watermark for continuous rewards, Decoupled Masking Strategy for process/outcome reward separation, and Dual-Gated SFT objective for training stability.

Result: Outperforms baselines like GRPO in sample efficiency and accuracy on textual/multi-modal benchmarks (MATH, Super-CLEVR), with superior out-of-distribution robustness and zero-shot transfer capabilities.

Conclusion: The proposed framework effectively addresses reward sparsity and credit assignment issues in RL for LLM reasoning, demonstrating significant improvements in reasoning performance and generalization.

Abstract: Reinforcement Learning (RL) serves as a potent paradigm for enhancing reasoning capabilities in Large Language Models (LLMs), yet standard outcome-based approaches often suffer from reward sparsity and inefficient credit assignment. In this paper, we propose a novel framework designed to provide continuous reward signals, which introduces a Step-wise Marginal Information Gain (MIG) mechanism that quantifies the intrinsic value of reasoning steps against a Monotonic Historical Watermark, effectively filtering out training noise. To ensure disentangled credit distribution, we implement a Decoupled Masking Strategy, applying process-oriented rewards specifically to the chain-of-thought (CoT) and outcome-oriented rewards to the full completion. Additionally, we incorporate a Dual-Gated SFT objective to stabilize training with high-quality structural and factual signals. Extensive experiments across textual and multi-modal benchmarks (e.g., MATH, Super-CLEVR) demonstrate that our approach consistently outperforms baselines such as GRPO in both sample efficiency and final accuracy. Furthermore, our model exhibits superior out-of-distribution robustness, demonstrating promising zero-shot transfer capabilities to unseen and challenging reasoning tasks.

</details>


### [340] [SetPO: Set-Level Policy Optimization for Diversity-Preserving LLM Reasoning](https://arxiv.org/abs/2602.01062)
*Chenyi Li,Yuan Zhang,Bo Wang,Guoqing Ma,Wei Tang,Haoyang Huang,Nan Duan*

Main category: cs.AI

TL;DR: Proposes a diversity-aware reinforcement learning method for LLMs that uses kernelized similarity and marginal contributions to maintain solution diversity while improving reasoning performance.


<details>
  <summary>Details</summary>
Motivation: Current RL methods with verifiable rewards improve LLM reasoning but reduce outcome diversity, causing models to focus on narrow solution sets. Need to balance performance gains with maintaining diverse solution approaches.

Method: Introduces set-level diversity objective using kernelized similarity over sampled trajectories, computes leave-one-out marginal contributions for each trajectory, and integrates as plug-in advantage shaping term for policy optimization. Uses distribution perturbation framework to analyze trajectory contributions.

Result: Extensive experiments across model scales show the algorithm consistently outperforms strong baselines in both Pass@1 and Pass@K across various benchmarks, maintaining diversity while improving performance.

Conclusion: The proposed approach successfully addresses the diversity reduction problem in RL for LLMs, with theoretical analysis confirming rarer trajectories contribute more to global diversity, leading to better performance across multiple metrics.

Abstract: Reinforcement learning with verifiable rewards has shown notable effectiveness in enhancing large language models (LLMs) reasoning performance, especially in mathematics tasks. However, such improvements often come with reduced outcome diversity, where the model concentrates probability mass on a narrow set of solutions. Motivated by diminishing-returns principles, we introduce a set level diversity objective defined over sampled trajectories using kernelized similarity. Our approach derives a leave-one-out marginal contribution for each sampled trajectory and integrates this objective as a plug-in advantage shaping term for policy optimization. We further investigate the contribution of a single trajectory to language model diversity within a distribution perturbation framework. This analysis theoretically confirms a monotonicity property, proving that rarer trajectories yield consistently higher marginal contributions to the global diversity. Extensive experiments across a range of model scales demonstrate the effectiveness of our proposed algorithm, consistently outperforming strong baselines in both Pass@1 and Pass@K across various benchmarks.

</details>


### [341] [ConvexBench: Can LLMs Recognize Convex Functions?](https://arxiv.org/abs/2602.01075)
*Yepeng Liu,Yu Huang,Yu-Xiang Wang,Yingbin Liang,Yuheng Bu*

Main category: cs.AI

TL;DR: LLMs struggle with identifying convexity in deeply composed symbolic functions, but an agentic divide-and-conquer framework with AST parsing and recursive reasoning solves this problem.


<details>
  <summary>Details</summary>
Motivation: As LLMs automate research-level math, they need to understand convex analysis. The paper aims to test whether LLMs can identify convexity under deep functional composition, which is crucial for mathematical reasoning.

Method: Created a scalable benchmark (cb) for testing convexity identification. Proposed an agentic divide-and-conquer framework that: (1) uses external tools to parse functions into ASTs, and (2) enforces recursive reasoning over each sub-expression with focused context.

Result: Frontier LLMs show sharp compositional reasoning degradation: F1-score drops from 1.0 at depth 2 to ~0.2 at depth 100. Two failure modes identified: parsing failure and lazy reasoning. The proposed framework achieves F1-score = 1.0 at depth 100.

Conclusion: LLMs have significant limitations in compositional reasoning for convexity analysis, but structured approaches with external parsing tools and recursive reasoning can effectively overcome these limitations for deep functional composition problems.

Abstract: Convex analysis is a modern branch of mathematics with many applications. As Large Language Models (LLMs) start to automate research-level math and sciences, it is important for LLMs to demonstrate the ability to understand and reason with convexity. We introduce \cb, a scalable and mechanically verifiable benchmark for testing \textit{whether LLMs can identify the convexity of a symbolic objective under deep functional composition.} Experiments on frontier LLMs reveal a sharp compositional reasoning gap: performance degrades rapidly with increasing depth, dropping from an F1-score of $1.0$ at depth $2$ to approximately $0.2$ at depth $100$. Inspection of models' reasoning traces indicates two failure modes: \textit{parsing failure} and \textit{lazy reasoning}. To address these limitations, we propose an agentic divide-and-conquer framework that (i) offloads parsing to an external tool to construct an abstract syntax tree (AST) and (ii) enforces recursive reasoning over each intermediate sub-expression with focused context. This framework reliably mitigates deep-composition failures, achieving substantial performance improvement at large depths (e.g., F1-Score $= 1.0$ at depth $100$).

</details>


### [342] [AutoHealth: An Uncertainty-Aware Multi-Agent System for Autonomous Health Data Modeling](https://arxiv.org/abs/2602.01078)
*Tong Xia,Weibin Li,Gang Liu,Yong Li*

Main category: cs.AI

TL;DR: AutoHealth is an uncertainty-aware multi-agent system that autonomously models heterogeneous health data while quantifying model reliability, outperforming SOTA baselines by 29.2% in prediction and 50.2% in uncertainty estimation.


<details>
  <summary>Details</summary>
Motivation: LLM-based agents show promise for autonomous ML but struggle with health data due to: 1) poor generalization across heterogeneous health data modalities, 2) reliance on predefined templates with insufficient task adaptation, and 3) lack of uncertainty estimation crucial for healthcare reliability.

Method: AutoHealth uses closed-loop coordination among five specialized agents for data exploration, task-conditioned model construction, training, and optimization, jointly prioritizing predictive performance and uncertainty quantification. It generates comprehensive reports for trustworthy interpretation.

Result: On a challenging real-world benchmark of 17 tasks across diverse data modalities and learning settings, AutoHealth completed all tasks and outperformed state-of-the-art baselines by 29.2% in prediction performance and 50.2% in uncertainty estimation.

Conclusion: AutoHealth successfully addresses key limitations in applying LLM-based agents to health data by providing uncertainty-aware autonomous modeling that supports reliable decision-making in healthcare through comprehensive uncertainty quantification and reporting.

Abstract: LLM-based agents have demonstrated strong potential for autonomous machine learning, yet their applicability to health data remains limited. Existing systems often struggle to generalize across heterogeneous health data modalities, rely heavily on predefined solution templates with insufficient adaptation to task-specific objectives, and largely overlook uncertainty estimation, which is essential for reliable decision-making in healthcare. To address these challenges, we propose \textit{AutoHealth}, a novel uncertainty-aware multi-agent system that autonomously models health data and assesses model reliability. \textit{AutoHealth} employs closed-loop coordination among five specialized agents to perform data exploration, task-conditioned model construction, training, and optimization, while jointly prioritizing predictive performance and uncertainty quantification. Beyond producing ready-to-use models, the system generates comprehensive reports to support trustworthy interpretation and risk-aware decision-making. To rigorously evaluate its effectiveness, we curate a challenging real-world benchmark comprising 17 tasks across diverse data modalities and learning settings. \textit{AutoHealth} completes all tasks and outperforms state-of-the-art baselines by 29.2\% in prediction performance and 50.2\% in uncertainty estimation.

</details>


### [343] [EvoOpt-LLM: Evolving industrial optimization models with large language models](https://arxiv.org/abs/2602.01082)
*Yiliu He,Tianle Li,Binghao Ji,Zhiyuan Liu,Di Huang*

Main category: cs.AI

TL;DR: EvoOpt-LLM is a unified LLM framework for industrial optimization modeling that automates MILP model construction, constraint injection, and variable pruning with high data efficiency.


<details>
  <summary>Details</summary>
Motivation: Current methods for translating natural-language requirements into MILP models are expertise-intensive and suffer from low data efficiency, limited solver validity, and poor scalability to industrial problems.

Method: Built on a 7B-parameter LLM with LoRA fine-tuning, EvoOpt-LLM supports automated model construction, dynamic business-constraint injection, and end-to-end variable pruning for MILP models.

Result: Achieves 91% generation rate and 65.9% executability rate with only 3,000 training samples, with critical gains under 1,500 samples. Constraint injection preserves original objectives, and variable pruning achieves ~0.56 F1 score with 400 samples.

Conclusion: EvoOpt-LLM demonstrates a practical, data-efficient approach to industrial optimization modeling that reduces expert intervention while improving adaptability and solver efficiency.

Abstract: Optimization modeling via mixed-integer linear programming (MILP) is fundamental to industrial planning and scheduling, yet translating natural-language requirements into solver-executable models and maintaining them under evolving business rules remains highly expertise-intensive. While large language models (LLMs) offer promising avenues for automation, existing methods often suffer from low data efficiency, limited solver-level validity, and poor scalability to industrial-scale problems. To address these challenges, we present EvoOpt-LLM, a unified LLM-based framework supporting the full lifecycle of industrial optimization modeling, including automated model construction, dynamic business-constraint injection, and end-to-end variable pruning. Built on a 7B-parameter LLM and adapted via parameter-efficient LoRA fine-tuning, EvoOpt-LLM achieves a generation rate of 91% and an executability rate of 65.9% with only 3,000 training samples, with critical performance gains emerging under 1,500 samples. The constraint injection module reliably augments existing MILP models while preserving original objectives, and the variable pruning module enhances computational efficiency, achieving an F1 score of ~0.56 on medium-sized LP models with only 400 samples. EvoOpt-LLM demonstrates a practical, data-efficient approach to industrial optimization modeling, reducing reliance on expert intervention while improving adaptability and solver efficiency.

</details>


### [344] [MedBeads: An Agent-Native, Immutable Data Substrate for Trustworthy Medical AI](https://arxiv.org/abs/2602.01086)
*Takahito Nakajima*

Main category: cs.AI

TL;DR: MedBeads is an agent-native medical data infrastructure using immutable "Beads" in a Merkle DAG to solve context mismatch in AI clinical agents, replacing probabilistic RAG with deterministic graph traversal for tamper-evident, auditable patient history.


<details>
  <summary>Details</summary>
Motivation: Current EMRs and FHIR standards are designed for human review, creating a "Context Mismatch" where AI agents receive fragmented data and must use probabilistic inference (RAG) to reconstruct patient history, leading to hallucinations and poor auditability.

Method: Proposed MedBeads infrastructure with clinical events as immutable "Beads" (nodes in a Merkle DAG) that cryptographically reference causal predecessors. Implemented prototype with Go Core Engine, Python middleware for LLM integration, and React visualization interface. Uses BFS Context Retrieval algorithm for O(V+E) graph traversal.

Result: Successfully implemented workflow with synthetic data, converting FHIR resources to causally-linked graph. BFS algorithm enables real-time decision support. Tamper-evidence guaranteed by design (any modification breaks cryptographic chain). Visualization aids clinician understanding through explicit causal links.

Conclusion: MedBeads addresses context mismatch by shifting from probabilistic search to deterministic graph traversal and from mutable records to immutable chains, providing substrate for "Trustworthy Medical AI." Structured Bead format serves as token-efficient "AI-native language." Released as open-source software.

Abstract: Background: As of 2026, Large Language Models (LLMs) demonstrate expert-level medical knowledge. However, deploying them as autonomous "Clinical Agents" remains limited. Current Electronic Medical Records (EMRs) and standards like FHIR are designed for human review, creating a "Context Mismatch": AI agents receive fragmented data and must rely on probabilistic inference (e.g., RAG) to reconstruct patient history. This approach causes hallucinations and hinders auditability. Methods: We propose MedBeads, an agent-native data infrastructure where clinical events are immutable "Beads"--nodes in a Merkle Directed Acyclic Graph (DAG)--cryptographically referencing causal predecessors. This "write-once, read-many" architecture makes tampering mathematically detectable. We implemented a prototype with a Go Core Engine, Python middleware for LLM integration, and a React-based visualization interface. Results: We successfully implemented the workflow using synthetic data. The FHIR-to-DAG conversion transformed flat resources into a causally-linked graph. Our Breadth-First Search (BFS) Context Retrieval algorithm traverses relevant subgraphs with O(V+E) complexity, enabling real-time decision support. Tamper-evidence is guaranteed by design: any modification breaks the cryptographic chain. The visualization aids clinician understanding through explicit causal links. Conclusion: MedBeads addresses the "Context Mismatch" by shifting from probabilistic search to deterministic graph traversal, and from mutable records to immutable chains, providing the substrate for "Trustworthy Medical AI." It guarantees the context the AI receives is deterministic and tamper-evident, while the LLM determines interpretation. The structured Bead format serves as a token-efficient "AI-native language." We release MedBeads as open-source software to accelerate agent-native data standards.

</details>


### [345] [Hard Constraints Meet Soft Generation: Guaranteed Feasibility for LLM-based Combinatorial Optimization](https://arxiv.org/abs/2602.01090)
*Yang Liu,Chuan Zhou,Yancheng Chen,Shuai Zhang,Xixun Lin,Xiaoqing Wang*

Main category: cs.AI

TL;DR: FALCON is a framework that ensures 100% feasible solutions for combinatorial optimization using LLMs through grammar-constrained decoding, feasibility repair, and adaptive sampling, trained with a novel preference optimization method.


<details>
  <summary>Details</summary>
Motivation: LLMs show promise for combinatorial optimization but lack mechanisms to guarantee solution feasibility, which is critical for real-world deployment where infeasible solutions are unacceptable.

Method: FALCON uses three innovations: 1) grammar-constrained decoding for syntactic validity, 2) feasibility repair layer for semantic constraint correction, and 3) adaptive Best-of-N sampling for efficient inference. Training uses Best-anchored Objective-guided Preference Optimization (BOPO) that weights preference pairs by objective gap.

Result: Theoretically proven convergence for BOPO with bounds on repair-induced quality loss. Empirically across seven NP-hard CO problems, achieves perfect feasibility while matching or exceeding solution quality of state-of-the-art neural and LLM-based solvers.

Conclusion: FALCON provides a practical framework for deploying LLMs in combinatorial optimization with guaranteed feasibility, addressing a critical limitation of current LLM-based approaches for real-world applications.

Abstract: Large language models (LLMs) have emerged as promising general-purpose solvers for combinatorial optimization (CO), yet they fundamentally lack mechanisms to guarantee solution feasibility which is critical for real-world deployment. In this work, we introduce FALCON, a framework that ensures 100\% feasibility through three key innovations: (i) \emph{grammar-constrained decoding} enforces syntactic validity, (ii) a \emph{feasibility repair layer} corrects semantic constraint violations, and (iii) \emph{adaptive Best-of-$N$ sampling} allocates inference compute efficiently. To train the underlying LLM, we introduce the Best-anchored Objective-guided Preference Optimization (BOPO) in LLM training, which weights preference pairs by their objective gap, providing dense supervision without human labels. Theoretically, we prove convergence for BOPO and provide bounds on repair-induced quality loss. Empirically, across seven NP-hard CO problems, FALCON achieves perfect feasibility while matching or exceeding the solution quality of state-of-the-art neural and LLM-based solvers.

</details>


### [346] [Probing RLVR training instability through the lens of objective-level hacking](https://arxiv.org/abs/2602.01103)
*Yiming Dong,Kun Fu,Haoyu Li,Xinyuan Zhu,Yurou Liu,Lijing Shao,Jieping Ye,Zheng Wang*

Main category: cs.AI

TL;DR: RLVR training instability in MoE models stems from objective-level hacking caused by token-level credit misalignment, not just reward hacking from exploitable verifiers.


<details>
  <summary>Details</summary>
Motivation: Training instability in RLVR for MoE models undermines capability improvements, but the underlying causes and mechanisms remain poorly understood, requiring a principled framework to explain these instabilities.

Method: Introduces a framework for understanding RLVR instability through objective-level hacking, which emerges from token-level credit misalignment and manifests as system-level spurious signals. Uses extensive experiments on a 30B MoE model to trace the origin and formalize the mechanism behind abnormal growth of training-inference discrepancy.

Result: Identifies objective-level hacking as the key pathological training dynamic in MoE models, explaining the abnormal growth of training-inference discrepancy that was previously associated with instability but lacked mechanistic explanation.

Conclusion: Provides a concrete, causal account of training dynamics underlying instabilities in MoE models, offering guidance for designing stable RLVR algorithms by addressing objective-level hacking rather than just reward hacking.

Abstract: Prolonged reinforcement learning with verifiable rewards (RLVR) has been shown to drive continuous improvements in the reasoning capabilities of large language models, but the training is often prone to instabilities, especially in Mixture-of-Experts (MoE) architectures. Training instability severely undermines model capability improvement, yet its underlying causes and mechanisms remain poorly understood. In this work, we introduce a principled framework for understanding RLVR instability through the lens of objective-level hacking. Unlike reward hacking, which arises from exploitable verifiers, objective-level hacking emerges from token-level credit misalignment and is manifested as system-level spurious signals in the optimization objective. Grounded in our framework, together with extensive experiments on a 30B MoE model, we trace the origin and formalize the mechanism behind a key pathological training dynamic in MoE models: the abnormal growth of the training-inference discrepancy, a phenomenon widely associated with instability but previously lacking a mechanistic explanation. These findings provide a concrete and causal account of the training dynamics underlying instabilities in MoE models, offering guidance for the design of stable RLVR algorithms.

</details>


### [347] [Transforming Vehicle Diagnostics: A Multimodal Approach to Error Patterns Prediction](https://arxiv.org/abs/2602.01109)
*Hugo Math,Rainer Lienhart*

Main category: cs.AI

TL;DR: BiCarFormer: A bidirectional Transformer model that combines DTC sequences with environmental sensor data for more accurate multi-label vehicle error pattern classification.


<details>
  <summary>Details</summary>
Motivation: Current vehicle diagnostic systems focus only on Diagnostic Trouble Codes (DTCs) sequences but ignore valuable contextual environmental data (temperature, humidity, pressure) that domain experts use for failure classification. Real-world data is complex and noisy, creating challenges for accurate diagnostics.

Method: BiCarFormer: A multimodal bidirectional Transformer model designed for vehicle event sequences. It uses embedding fusions and a co-attention mechanism to capture relationships between diagnostic codes and environmental data for multi-label sequence classification of error codes into error patterns.

Result: Experimental results on a real-world automotive dataset with 22,137 error codes and 360 error patterns show significant improvement in classification performance compared to models using only DTC sequences and traditional sequence models.

Conclusion: Incorporating contextual environmental information is crucial for more accurate and robust vehicle diagnostics, which can reduce maintenance costs and enhance automation processes in the automotive industry.

Abstract: Accurately diagnosing and predicting vehicle malfunctions is crucial for maintenance and safety in the automotive industry. While modern diagnostic systems primarily rely on sequences of vehicular Diagnostic Trouble Codes (DTCs) registered in On-Board Diagnostic (OBD) systems, they often overlook valuable contextual information such as raw sensory data (e.g., temperature, humidity, and pressure). This contextual data, crucial for domain experts to classify vehicle failures, introduces unique challenges due to its complexity and the noisy nature of real-world data. This paper presents BiCarFormer: the first multimodal approach to multi-label sequence classification of error codes into error patterns that integrates DTC sequences and environmental conditions. BiCarFormer is a bidirectional Transformer model tailored for vehicle event sequences, employing embedding fusions and a co-attention mechanism to capture the relationships between diagnostic codes and environmental data. Experimental results on a challenging real-world automotive dataset with 22,137 error codes and 360 error patterns demonstrate that our approach significantly improves classification performance compared to models that rely solely on DTC sequences and traditional sequence models. This work highlights the importance of incorporating contextual environmental information for more accurate and robust vehicle diagnostics, hence reducing maintenance costs and enhancing automation processes in the automotive industry.

</details>


### [348] [Lyapunov Stability-Aware Stackelberg Game for Low-Altitude Economy: A Control-Oriented Pruning-Based DRL Approach](https://arxiv.org/abs/2602.01131)
*Yue Zhong,Jiawen Kang,Yongju Tong,Hong-Ning Dai,Dong In Kim,Abbas Jamalipour,Shengli Xie*

Main category: cs.AI

TL;DR: Proposed a Sensing-Communication-Computing-Control framework for UAV networks that transforms control stability requirements into quantifiable communication constraints using Lyapunov theory, formulates resource allocation as a Stackelberg game, and implements a lightweight pruning-based PPO algorithm for efficient equilibrium computation.


<details>
  <summary>Details</summary>
Motivation: With the rapid expansion of low-altitude economy and UAVs serving as aerial base stations, there's a conflict between limited onboard resources and stringent stability requirements for heterogeneous services (latency-sensitive critical missions and bandwidth-intensive data streaming). Traditional throughput-centric designs are insufficient for ensuring control stability.

Method: 1) Proposed Sensing-Communication-Computing-Control closed-loop framework that models communication latency impact on physical control stability; 2) Used Lyapunov stability theory to map control system state evolution to communication constraints; 3) Formulated resource allocation as Stackelberg game with UAVs as leaders pricing resources and users as followers optimizing requests; 4) Developed lightweight pruning-based PPO algorithm with dynamic structured pruning to compress neural networks for efficient equilibrium computation on energy-constrained edge platforms.

Result: Simulation results demonstrate that the proposed scheme effectively secures control loop stability while maximizing system utility in dynamic low-altitude environments. The lightweight algorithm enables rapid approximation of game equilibrium with minimal inference latency.

Conclusion: The proposed framework successfully bridges the gap between communication constraints and control stability requirements in UAV networks, providing a practical solution for reliable mission execution in low-altitude environments through quantifiable stability-to-resource mapping and efficient game-theoretic resource allocation.

Abstract: With the rapid expansion of the low-altitude economy, Unmanned Aerial Vehicles (UAVs) serve as pivotal aerial base stations supporting diverse services from users, ranging from latency-sensitive critical missions to bandwidth-intensive data streaming. However, the efficacy of such heterogeneous networks is often compromised by the conflict between limited onboard resources and stringent stability requirements. Moving beyond traditional throughput-centric designs, we propose a Sensing-Communication-Computing-Control closed-loop framework that explicitly models the impact of communication latency on physical control stability. To guarantee mission reliability, we leverage the Lyapunov stability theory to derive an intrinsic mapping between the state evolution of the control system and communication constraints, transforming abstract stability requirements into quantifiable resource boundaries. Then, we formulate the resource allocation problem as a Stackelberg game, where UAVs (as leaders) dynamically price resources to balance load and ensure stability, while users (as followers) optimize requests based on service urgency. Furthermore, addressing the prohibitive computational overhead of standard Deep Reinforcement Learning (DRL) on energy-constrained edge platforms, we propose a novel and lightweight pruning-based Proximal Policy Optimization (PPO) algorithm. By integrating a dynamic structured pruning mechanism, the proposed algorithm significantly compresses the neural network scale during training, enabling the UAV to rapidly approximate the game equilibrium with minimal inference latency. Simulation results demonstrate that the proposed scheme effectively secures control loop stability while maximizing system utility in dynamic low-altitude environments.

</details>


### [349] [PersistBench: When Should Long-Term Memories Be Forgotten by LLMs?](https://arxiv.org/abs/2602.01146)
*Sidharth Pulipaka,Oliver Chen,Manas Sharma,Taaha S Bajwa,Vyas Raina,Ivaxi Sheth*

Main category: cs.AI

TL;DR: PersistBench benchmark reveals significant safety risks in LLMs with long-term memory: cross-domain leakage (53% failure) and memory-induced sycophancy (97% failure), highlighting overlooked dangers in persistent conversational systems.


<details>
  <summary>Details</summary>
Motivation: While long-term memory in conversational assistants enhances personalization (e.g., remembering user preferences), its persistence introduces overlooked safety risks that need systematic measurement and mitigation.

Method: Introduces PersistBench to measure safety risks in LLMs with long-term memory, identifying two specific risks: cross-domain leakage (inappropriate context injection) and memory-induced sycophancy (reinforcement of user biases). Evaluates 18 frontier and open-source LLMs on this benchmark.

Result: Alarmingly high failure rates: median 53% on cross-domain leakage samples and 97% on sycophancy samples across evaluated LLMs, demonstrating widespread vulnerability to these safety risks.

Conclusion: PersistBench reveals critical safety vulnerabilities in LLMs with long-term memory, highlighting the urgent need for more robust and safer memory usage in conversational systems, and provides a benchmark to guide future development.

Abstract: Conversational assistants are increasingly integrating long-term memory with large language models (LLMs). This persistence of memories, e.g., the user is vegetarian, can enhance personalization in future conversations. However, the same persistence can also introduce safety risks that have been largely overlooked. Hence, we introduce PersistBench to measure the extent of these safety risks. We identify two long-term memory-specific risks: cross-domain leakage, where LLMs inappropriately inject context from the long-term memories; and memory-induced sycophancy, where stored long-term memories insidiously reinforce user biases. We evaluate 18 frontier and open-source LLMs on our benchmark. Our results reveal a surprisingly high failure rate across these LLMs - a median failure rate of 53% on cross-domain samples and 97% on sycophancy samples. To address this, our benchmark encourages the development of more robust and safer long-term memory usage in frontier conversational systems.

</details>


### [350] [Capabilities and Fundamental Limits of Latent Chain-of-Thought](https://arxiv.org/abs/2602.01148)
*Jiaxuan Zou,Yaozhong Xiong,Yong Liu*

Main category: cs.AI

TL;DR: Latent CoT models show inconsistent performance due to an Exploration-Execution Trade-off governed by decisional certainty, requiring curriculum learning and adaptive certainty regulation.


<details>
  <summary>Details</summary>
Motivation: Latent Chain-of-Thought models demonstrate puzzling performance inconsistencies - excelling at exploration tasks but failing at computational tasks, revealing a fundamental trade-off that needs theoretical understanding and practical solutions.

Method: Theoretical characterization of Exploration-Execution Trade-off, introduction of Symbolic Index to quantify decisional commitment, and proof that curriculum learning is necessary due to distributional mismatch in direct training.

Result: High certainty enables precise execution but inhibits exploration, while low certainty facilitates search but causes error accumulation; Symbolic Index causally relates to both execution stability and exploration capability.

Conclusion: The framework shifts design from binary architectural choices toward adaptive systems that dynamically regulate decisional certainty based on task demands, with curriculum learning being theoretically necessary for effective training.

Abstract: Latent Chain-of-Thought (Latent CoT) models promise efficient reasoning via continuous representations, yet exhibit puzzling performance inconsistencies: excelling at exploration (ProsQA: 97.0%) but failing at computation (GSM8K: 34.1%). We reveal that this trade-off is governed by decisional certainty. Our contributions are threefold: (1) We theoretically characterize the fundamental Exploration-Execution Trade-off, proving that high certainty enables precise execution but inhibits exploration, while low certainty facilitates search but causes error accumulation. (2) We introduce the Symbolic Index--quantifying decisional commitment--as the core mechanism governing this trade-off and establish its causal relationship with both execution stability and exploration capability. (3) We prove that curriculum learning is theoretically necessary, as direct training provably fails due to distributional mismatch. Our framework shifts the design paradigm from binary architectural choices toward adaptive systems that dynamically regulate decisional certainty based on task demands.

</details>


### [351] [Multi-Agent Causal Reasoning System for Error Pattern Rule Automation in Vehicles](https://arxiv.org/abs/2602.01155)
*Hugo Math,Julian Lorentz,Stefan Oelsner,Rainer Lienhart*

Main category: cs.AI

TL;DR: CAREP is a multi-agent system that automates the generation of error pattern rules from vehicle diagnostic trouble codes using causal discovery and contextual reasoning, outperforming LLM-only approaches.


<details>
  <summary>Details</summary>
Motivation: Current error pattern rules for vehicle diagnostics are manually crafted by domain experts, which is expensive, error-prone, and doesn't scale with increasing vehicle complexity.

Method: CAREP uses a multi-agent system with three components: (1) causal discovery agent to identify DTC-EP relations, (2) contextual information agent to integrate metadata and descriptions, and (3) orchestrator agent to synthesize boolean rules with interpretable reasoning traces.

Result: Evaluation on a large-scale automotive dataset with 29,100 unique DTCs and 474 error patterns shows CAREP can automatically and accurately discover unknown EP rules, outperforming LLM-only baselines while providing transparent causal explanations.

Conclusion: CAREP represents progress toward fully automated fault diagnostics, enabling scalable, interpretable, and cost-efficient vehicle maintenance by combining practical causal discovery with agent-based reasoning.

Abstract: Modern vehicles generate thousands of different discrete events known as Diagnostic Trouble Codes (DTCs). Automotive manufacturers use Boolean combinations of these codes, called error patterns (EPs), to characterize system faults and ensure vehicle safety. Yet, EP rules are still manually handcrafted by domain experts, a process that is expensive and prone to errors as vehicle complexity grows. This paper introduces CAREP (Causal Automated Reasoning for Error Patterns), a multi-agent system that automatizes the generation of EP rules from high-dimensional event sequences of DTCs. CAREP combines a causal discovery agent that identifies potential DTC-EP relations, a contextual information agent that integrates metadata and descriptions, and an orchestrator agent that synthesizes candidate boolean rules together with interpretable reasoning traces. Evaluation on a large-scale automotive dataset with over 29,100 unique DTCs and 474 error patterns demonstrates that CAREP can automatically and accurately discover the unknown EP rules, outperforming LLM-only baselines while providing transparent causal explanations. By uniting practical causal discovery and agent-based reasoning, CAREP represents a step toward fully automated fault diagnostics, enabling scalable, interpretable, and cost-efficient vehicle maintenance.

</details>


### [352] [Do All Individual Layers Help? An Empirical Study of Task-Interfering Layers in Vision-Language Models](https://arxiv.org/abs/2602.01167)
*Zhiming Liu,Yujie Wei,Lei Feng,Xiu Su,Xiaobo Xia,Weili Guan,Zeke Xie,Shuo Yang*

Main category: cs.AI

TL;DR: TaLo (Task-Adaptive Layer Knockout) is a training-free method that improves VLM performance by dynamically bypassing task-interfering layers at inference time.


<details>
  <summary>Details</summary>
Motivation: Current VLMs use all layers by default, but some layers actually harm performance on specific downstream tasks. The paper aims to identify these interfering layers and develop a method to bypass them without retraining.

Method: 1) Systematically investigate layer influence via intervention experiments, 2) Introduce Task-Layer Interaction Vector to quantify layer effects, 3) Propose TaLo - dynamically identifies and bypasses the most interfering layer for each task at test time without parameter updates.

Result: TaLo improves performance across various models and datasets, boosting Qwen-VL's accuracy on Maps task in ScienceQA by up to 16.6%. Task-interfering layers show task-specific sensitivity patterns, and tasks with similar capabilities have similar interaction vectors.

Conclusion: Pretrained VLMs exhibit unexpected modularity with task-interfering layers. TaLo provides a plug-and-play, training-free mechanism to unlock hidden capabilities at inference by bypassing harmful layers.

Abstract: Current VLMs have demonstrated capabilities across a wide range of multimodal tasks. Typically, in a pretrained VLM, all layers are engaged by default to make predictions on downstream tasks. We find that intervening on a single layer, such as by zeroing its parameters, can improve the performance on certain tasks, indicating that some layers hinder rather than help downstream tasks. We systematically investigate how individual layers influence different tasks via layer intervention. Specifically, we measure the change in performance relative to the base model after intervening on each layer and observe improvements when bypassing specific layers. This improvement can be generalizable across models and datasets, indicating the presence of Task-Interfering Layers that harm downstream tasks' performance. We introduce Task-Layer Interaction Vector, which quantifies the effect of intervening on each layer of a VLM given a task. These task-interfering layers exhibit task-specific sensitivity patterns: tasks requiring similar capabilities show consistent response trends under layer interventions, as evidenced by the high similarity in their task-layer interaction vectors. Inspired by these findings, we propose TaLo (Task-Adaptive Layer Knockout), a training-free, test-time adaptation method that dynamically identifies and bypasses the most interfering layer for a given task. Without parameter updates, TaLo improves performance across various models and datasets, including boosting Qwen-VL's accuracy on the Maps task in ScienceQA by up to 16.6%. Our work reveals an unexpected form of modularity in pretrained VLMs and provides a plug-and-play, training-free mechanism to unlock hidden capabilities at inference time. The source code will be publicly available.

</details>


### [353] [ASP-Bench: From Natural Language to Logic Programs](https://arxiv.org/abs/2602.01171)
*Stefan Szeider*

Main category: cs.AI

TL;DR: ASP-Bench is a benchmark with 128 natural language problems for evaluating systems that translate NL to Answer Set Programs, covering key ASP features and seven reasoning aspects, tested using ReAct agentic approach.


<details>
  <summary>Details</summary>
Motivation: Automating translation of natural-language specifications to logic programs is challenging for neurosymbolic engineering, requiring systematic evaluation of systems that convert NL to Answer Set Programs.

Method: Created ASP-Bench with 128 NL problem instances (64 base problems with easy/hard variants), covering ASP features like choice rules, aggregates, optimization. Characterized problems along 7 reasoning aspects. Tested using agentic ReAct framework with feedback-driven iterative refinement and solver feedback.

Result: ReAct agentic approach achieved full saturation, demonstrating feedback-driven iterative refinement with solver feedback provides reliable and robust approach for modeling natural language in ASP. Analysis across multiple agent runs provided insights into what determines problem modeling hardness.

Conclusion: ASP-Bench provides comprehensive benchmark for evaluating NL-to-ASP translation systems, with ReAct framework showing promise for reliable modeling. The benchmark enables systematic assessment of modeling difficulty across multiple reasoning dimensions.

Abstract: Automating the translation of natural-language specifications into logic programs is a challenging task that affects neurosymbolic engineering. We present ASP-Bench, a benchmark comprising 128 natural language problem instances, 64 base problems with easy and hard variants. It evaluates systems that translate natural-language problems into Answer Set Programs (ASPs), a prominent form of logic programming. It provides systematic coverage of ASP features, including choice rules, aggregates, and optimization. Each problem includes reference validators that check whether solutions satisfy the problem specification.
  We characterize problems along seven largely independent reasoning aspects (optimization, temporal reasoning, default logic, resource allocation, recursion, spatial reasoning, and quantitative complexity), providing a multidimensional view of modeling difficulty.
  We test the benchmark using an agentic approach based on the ReAct (Reason and Act) framework, which achieves full saturation, demonstrating that feedback-driven iterative refinement with solver feedback provides a reliable and robust approach for modeling natural language in ASP. Our analysis across multiple agent runs enables us to gain insights into what determines a problem's modeling hardness.

</details>


### [354] [A State-Transition Framework for Efficient LLM Reasoning](https://arxiv.org/abs/2602.01198)
*Liang Zhang,Yu Zhao,Longyue Wang,Tianqi Shi,Weihua Luo,Kaifu Zhang,Jinsong Su*

Main category: cs.AI

TL;DR: A linear attention framework that models LLM reasoning as state transitions, reducing computational complexity from quadratic to linear while improving both efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Long CoT reasoning improves LLM performance but has high computational/memory costs. Existing compression methods conflict with test-time scaling, limiting reasoning capacity.

Method: Models reasoning as state-transition process using linear attention to estimate reasoning state (historical information). Each token retrieves relevant info directly from state without attending to previous steps. Also includes state-based reasoning strategy to mitigate over-thinking.

Result: Extensive experiments across multiple datasets and model sizes show the framework improves both reasoning efficiency (reduced computational complexity) and reasoning performance.

Conclusion: Proposed framework successfully addresses efficiency limitations of long CoT reasoning while enhancing performance, offering a practical solution for complex reasoning tasks.

Abstract: While Long Chain-of-Thought (CoT) reasoning significantly improves Large Language Models (LLMs) performance on complex reasoning tasks, the substantial computational and memory costs of generating long CoT sequences limit their efficiency and practicality. Existing studies usually enhance the reasoning efficiency of LLMs by compressing CoT sequences. However, this approach conflicts with test-time scaling, limiting the reasoning capacity of LLMs. In this paper, we propose an efficient reasoning framework that models the reasoning process of LLMs as a state-transition process. Specifically, we first apply a linear attention mechanism to estimate the LLM's reasoning state, which records the historical reasoning information from previous reasoning steps. Then, based on the query prompt and the reasoning state, the LLM can efficiently perform the current reasoning step and update the state. With the linear attention, each token in the current reasoning step can directly retrieve relevant historical reasoning information from the reasoning state, without explicitly attending to tokens in previous reasoning steps. In this way, the computational complexity of attention is reduced from quadratic to linear, significantly improving the reasoning efficiency of LLMs. In addition, we propose a state-based reasoning strategy to mitigate the over-thinking issue caused by noisy reasoning steps. Extensive experiments across multiple datasets and model sizes demonstrate that our framework not only improves the reasoning efficiency of LLMs but also enhances their reasoning performance.

</details>


### [355] [Workflow-R1: Group Sub-sequence Policy Optimization for Multi-turn Workflow Construction](https://arxiv.org/abs/2602.01202)
*Mingze Kong,Zikun Qu,Zhongquan Zhou,Pengyu Liang,Xiang Li,Zhiwei Shang,Zhi Hong,Kaiyu Huang,Zhiyong Wang,Zhongxiang Dai*

Main category: cs.AI

TL;DR: Workflow-R1 reformulates workflow construction as multi-turn natural language sequential decision-making with GSsPO algorithm, outperforming baselines on QA benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing workflow optimization methods treat workflow synthesis as static, one-shot code-centric generation, which imposes excessive constraints on LLM coding capabilities and restricts flexibility for dynamic problem-solving.

Method: Workflow-R1 framework reformulates workflow construction as multi-turn natural language sequential decision-making. Introduces Group Sub-sequence Policy Optimization (GSsPO) - a structure-aware RL algorithm that recalibrates optimization to composite sub-sequences (Think-Action cycles) to align gradient updates with semantic boundaries.

Result: Through extensive experiments on multiple QA benchmarks, Workflow-R1 outperforms competitive baselines, validating GSsPO as a generalized solution for sequential reasoning.

Conclusion: Workflow-R1 establishes a promising new paradigm for automated workflow optimization, with GSsPO serving as a generalizable solution for multi-turn agentic sequential decision-making tasks.

Abstract: The rapid evolution of agentic workflows has demonstrated strong performance of LLM-based agents in addressing complex reasoning tasks. However, existing workflow optimization methods typically formulate workflow synthesis as a static, one-shot code-centric generation problem. This paradigm imposes excessive constraints on the model's coding capabilities and restricts the flexibility required for dynamic problem-solving. In this paper, we present Workflow-R1, a framework that reformulates workflow construction as a multi-turn, natural language-based sequential decision-making process. To resolve the optimization granularity mismatch inherent in such multi-turn interactions, we introduce Group Sub-sequence Policy Optimization (GSsPO). While explicitly tailored to align with the interleaved Think-Action dynamics of agentic reasoning, GSsPO fundamentally functions as a structure-aware RL algorithm generalizable to a broad class of multi-turn agentic sequential decision-making tasks. By recalibrating the optimization unit to the composite sub-sequence, specifically the atomic Think-Action cycle, it aligns gradient updates with the semantic boundaries of these interactions, ensuring robust learning in complex multi-turn reasoning tasks. Through extensive experiments on multiple QA benchmarks, Workflow-R1 outperforms competitive baselines, validating GSsPO as a generalized solution for sequential reasoning and establishing Workflow-R1 as a promising new paradigm for automated workflow optimization.

</details>


### [356] [Addressing Explainability of Generative AI using SMILE (Statistical Model-agnostic Interpretability with Local Explanations)](https://arxiv.org/abs/2602.01206)
*Zeinab Dehghani*

Main category: cs.AI

TL;DR: gSMILE extends SMILE to generative AI, providing unified explainability via text perturbations, Wasserstein metrics, and surrogate modeling for token-level attribution and instruction impact analysis.


<details>
  <summary>Details</summary>
Motivation: Generative AI models produce complex outputs but have opaque decision-making processes, limiting trust and accountability in high-stakes applications. There's a need for explainability frameworks that work across different generative models.

Method: gSMILE extends SMILE to generative settings using controlled text perturbations, Wasserstein distance metrics, and weighted surrogate modeling. It provides token-level attribution for LLMs via heatmaps and analyzes instruction impact on image editing models. Uses scenario-based evaluation with ODD framework and defines rigorous attribution metrics.

Result: Extensive experiments show gSMILE produces robust, human-aligned attributions and generalizes effectively across state-of-the-art generative models. The framework enables systematic assessment of model behavior across diverse conditions.

Conclusion: gSMILE advances transparent, reliable, and responsible deployment of generative AI technologies by providing a unified explainability framework that works across different generative architectures and enables trust through interpretability.

Abstract: The rapid advancement of generative artificial intelligence has enabled models capable of producing complex textual and visual outputs; however, their decision-making processes remain largely opaque, limiting trust and accountability in high-stakes applications. This thesis introduces gSMILE, a unified framework for the explainability of generative models, extending the Statistical Model-agnostic Interpretability with Local Explanations (SMILE) method to generative settings. gSMILE employs controlled perturbations of textual input, Wasserstein distance metrics, and weighted surrogate modelling to quantify and visualise how specific components of a prompt or instruction influence model outputs. Applied to Large Language Models (LLMs), gSMILE provides fine-grained token-level attribution and generates intuitive heatmaps that highlight influential tokens and reasoning pathways. In instruction-based image editing models, the exact text-perturbation mechanism is employed, allowing for the analysis of how modifications to an editing instruction impact the resulting image. Combined with a scenario-based evaluation strategy grounded in the Operational Design Domain (ODD) framework, gSMILE allows systematic assessment of model behaviour across diverse semantic and environmental conditions. To evaluate explanation quality, we define rigorous attribution metrics, including stability, fidelity, accuracy, consistency, and faithfulness, and apply them across multiple generative architectures. Extensive experiments demonstrate that gSMILE produces robust, human-aligned attributions and generalises effectively across state-of-the-art generative models. These findings highlight the potential of gSMILE to advance transparent, reliable, and responsible deployment of generative AI technologies.

</details>


### [357] [Not All Preferences Are Created Equal: Stability-Aware and Gradient-Efficient Alignment for Reasoning Models](https://arxiv.org/abs/2602.01207)
*Hui Wu,Hengyi Cai,Jinman Zhao,Xinran Chen,Ziheng Li,Zhejun Zhao,Shuaiqiang Wang,Yuchen Li,Dawei Yin*

Main category: cs.AI

TL;DR: SAGE is a dynamic framework that improves preference-based alignment for reasoning models by prioritizing informative training samples based on stability and gradient efficiency, unlike static methods like DPO that treat all preference pairs uniformly.


<details>
  <summary>Details</summary>
Motivation: Standard preference alignment methods like DPO treat all preference pairs uniformly, which is inefficient (wasting computation on trivial pairs) and unstable (suffering from noise near uncertain boundaries). There's a need for dynamic, policy-aware data selection that accounts for evolving model competence.

Method: SAGE integrates: 1) A coarse-grained curriculum mechanism that refreshes candidate pools based on model competence, and 2) A fine-grained, stability-aware scoring function that prioritizes informative, confident errors while filtering out unstable samples. The framework aims to maximize Signal-to-Noise Ratio of policy updates.

Result: Experiments on multiple mathematical reasoning benchmarks show SAGE significantly accelerates convergence and outperforms static baselines.

Conclusion: The paper highlights the critical role of policy-aware, stability-conscious data selection in reasoning alignment, demonstrating that dynamic sample prioritization based on gradient efficiency and stability improves alignment reliability.

Abstract: Preference-based alignment is pivotal for training large reasoning models; however, standard methods like Direct Preference Optimization (DPO) typically treat all preference pairs uniformly, overlooking the evolving utility of training instances. This static approach often leads to inefficient or unstable optimization, as it wastes computation on trivial pairs with negligible gradients and suffers from noise induced by samples near uncertain decision boundaries. Facing these challenges, we propose SAGE (Stability-Aware Gradient Efficiency), a dynamic framework designed to enhance alignment reliability by maximizing the Signal-to-Noise Ratio of policy updates. Concretely, SAGE integrates a coarse-grained curriculum mechanism that refreshes candidate pools based on model competence with a fine-grained, stability-aware scoring function that prioritizes informative, confident errors while filtering out unstable samples. Experiments on multiple mathematical reasoning benchmarks demonstrate that SAGE significantly accelerates convergence and outperforms static baselines, highlighting the critical role of policy-aware, stability-conscious data selection in reasoning alignment.

</details>


### [358] [FutureMind: Equipping Small Language Models with Strategic Thinking-Pattern Priors via Adaptive Knowledge Distillation](https://arxiv.org/abs/2602.01222)
*Shaoxiong Yang,Junting Li,Mengyuan Zhang,Chao Li,Wei Liu,Jian Luan*

Main category: cs.AI

TL;DR: FutureMind is a modular reasoning framework that enhances Small Language Models (SLMs) through adaptive knowledge distillation from LLMs, enabling strategic thinking patterns for complex knowledge-intensive tasks.


<details>
  <summary>Details</summary>
Motivation: SLMs are efficient but struggle with complex reasoning tasks requiring structured thinking and effective retrieval, creating a need to bridge the gap between SLM efficiency and LLM reasoning capabilities.

Method: Proposes FutureMind framework with dynamic reasoning pipeline (Problem Analysis, Logical Reasoning, Strategy Planning, Retrieval Guidance) and three retrieval paradigms that decompose complex queries into tractable subproblems, using adaptive knowledge distillation from LLMs to SLMs.

Result: Achieves state-of-the-art results on multi-hop QA benchmarks (2WikiMultihopQA, MuSiQue, Bamboogle, Frames), consistently outperforming strong baselines like Search-o1 under free training conditions across diverse SLM architectures and scales.

Conclusion: FutureMind successfully enhances SLM reasoning capabilities but reveals cognitive bias bottlenecks in thinking-pattern distillation between teacher LLMs and student SLMs, offering new insights into reasoning skill transferability for developing cognitively capable yet efficient SLMs.

Abstract: Small Language Models (SLMs) are attractive for cost-sensitive and resource-limited settings due to their efficient, low-latency inference. However, they often struggle with complex, knowledge-intensive tasks that require structured reasoning and effective retrieval. To address these limitations, we propose FutureMind, a modular reasoning framework that equips SLMs with strategic thinking-pattern priors via adaptive knowledge distillation from large language models (LLMs). FutureMind introduces a dynamic reasoning pipeline composed of four key modules: Problem Analysis, Logical Reasoning, Strategy Planning, and Retrieval Guidance. This pipeline is augmented by three distinct retrieval paradigms that decompose complex queries into tractable subproblems, ensuring efficient and accurate retrieval execution. Extensive experiments on multi-hop QA benchmarks, including 2WikiMultihopQA, MuSiQue, Bamboogle, and Frames, demonstrate the superiority of FutureMind. It consistently outperforms strong baselines such as Search-o1, achieving state-of-the-art results under free training conditions across diverse SLM architectures and scales. Beyond empirical gains, our analysis reveals that the process of thinking-pattern distillation is restricted by the cognitive bias bottleneck between the teacher (LLMs) and student (SLMs) models. This provides new perspectives on the transferability of reasoning skills, paving the way for the development of SLMs that combine efficiency with genuine cognitive capability.

</details>


### [359] [Predictive Scheduling for Efficient Inference-Time Reasoning in Large Language Models](https://arxiv.org/abs/2602.01237)
*Katrina Brown,Aneesh Muppidi,Rana Shahout*

Main category: cs.AI

TL;DR: Predictive Scheduling uses lightweight predictors to estimate optimal reasoning length before generation, then dynamically allocates token budget across queries to maximize accuracy, achieving up to 7.9% accuracy gain on GSM8K.


<details>
  <summary>Details</summary>
Motivation: Current LLMs use fixed token budgets for chain-of-thought reasoning, leading to inefficient computation: over-computation on easy inputs and under-computation on hard ones, wasting resources and limiting accuracy.

Method: Two lightweight predictors: MLP on intermediate transformer hidden states or LoRA-fine-tuned classifier on raw question text. These estimate each query's optimal reasoning length/difficulty before full generation. A greedy batch allocator then dynamically distributes fixed total token budget across queries.

Result: On GSM8K arithmetic benchmark: up to 7.9 percentage points absolute accuracy gain over uniform budgeting at identical token cost, closing over 50% of gap to oracle with perfect foresight. Middle transformer layers (12-17) carry richest signals for size estimation.

Conclusion: Pre-run budget prediction enables fine-grained control of compute-accuracy trade-off, offering practical path for latency-sensitive, cost-efficient LLM deployments by optimizing resource allocation based on query difficulty.

Abstract: Large language models (LLMs) achieve state-of-the-art accuracy on complex reasoning tasks by generating multiple chain-of-thought (CoT) traces, but using a fixed token budget per query leads to over-computation on easy inputs and under-computation on hard ones. We introduce Predictive Scheduling, a plug-and-play framework that pre-runs lightweight predictors, an MLP on intermediate transformer hidden states or a LoRA-fine-tuned classifier on raw question text, to estimate each query's optimal reasoning length or difficulty before any full generation. Our greedy batch allocator dynamically distributes a fixed total token budget across queries to maximize expected accuracy. On the GSM8K arithmetic benchmark, predictive scheduling yields up to 7.9 percentage points of absolute accuracy gain over uniform budgeting at identical token cost, closing over 50\% of the gap to an oracle with perfect foresight. A systematic layer-wise study reveals that middle layers (12 - 17) of the transformer carry the richest signals for size estimation. These results demonstrate that pre-run budget prediction enables fine-grained control of the compute-accuracy trade-off, offering a concrete path toward latency-sensitive, cost-efficient LLM deployments.

</details>


### [360] [LLM-Driven Ontology Construction for Enterprise Knowledge Graphs](https://arxiv.org/abs/2602.01276)
*Abdulsobur Oyewale,Tommaso Soru*

Main category: cs.AI

TL;DR: OntoEKG: LLM-driven pipeline for automated ontology construction from unstructured enterprise data, achieving 0.724 F1-score in Data domain but facing challenges in scope definition and hierarchical reasoning.


<details>
  <summary>Details</summary>
Motivation: Enterprise Knowledge Graph ontology construction is resource-intensive and manual, requiring significant domain expertise. There's a need to automate this process to accelerate creation of domain-specific ontologies from unstructured enterprise data.

Method: Two-phase LLM-driven pipeline: 1) Extraction module identifies core classes and properties from unstructured data, 2) Entailment module logically structures elements into hierarchies before serializing to standard RDF. Evaluated on new benchmark dataset from Data, Finance, and Logistics sectors.

Result: Achieved fuzzy-match F1-score of 0.724 in Data domain, demonstrating potential for automated ontology construction. However, revealed limitations in scope definition and hierarchical reasoning across different enterprise domains.

Conclusion: LLM-driven approaches show promise for accelerating ontology construction from unstructured enterprise data, but challenges remain in scope definition and hierarchical reasoning that need to be addressed for broader applicability.

Abstract: Enterprise Knowledge Graphs have become essential for unifying heterogeneous data and enforcing semantic governance. However, the construction of their underlying ontologies remains a resource-intensive, manual process that relies heavily on domain expertise. This paper introduces OntoEKG, a LLM-driven pipeline designed to accelerate the generation of domain-specific ontologies from unstructured enterprise data. Our approach decomposes the modelling task into two distinct phases: an extraction module that identifies core classes and properties, and an entailment module that logically structures these elements into a hierarchy before serialising them into standard RDF. Addressing the significant lack of comprehensive benchmarks for end-to-end ontology construction, we adopt a new evaluation dataset derived from documents across the Data, Finance, and Logistics sectors. Experimental results highlight both the potential and the challenges of this approach, achieving a fuzzy-match F1-score of 0.724 in the Data domain while revealing limitations in scope definition and hierarchical reasoning.

</details>


### [361] [RE-MCDF: Closed-Loop Multi-Expert LLM Reasoning for Knowledge-Grounded Clinical Diagnosis](https://arxiv.org/abs/2602.01297)
*Shaowei Shen,Xiaohong Yang,Jie Yang,Lianfen Huang,Yongcai Zhang,Yang Zou,Seyyedali Hosseinalipour*

Main category: cs.AI

TL;DR: RE-MCDF is a relation-enhanced multi-expert clinical diagnosis framework that addresses limitations of existing LLM approaches for neurology EMRs by incorporating disease relation awareness and closed-loop verification.


<details>
  <summary>Details</summary>
Motivation: EMRs in neurology are heterogeneous, sparse, and noisy, making LLM diagnosis challenging. Single-agent systems suffer from self-reinforcing errors, while multi-agent frameworks lack rigorous clinical reasoning and ignore logical disease dependencies (mutual exclusivity, compatibility, confusion).

Method: RE-MCDF uses a generation-verification-revision closed-loop architecture with three components: (1) primary expert generates candidate diagnoses and evidence, (2) laboratory expert dynamically prioritizes clinical indicators, (3) multi-relation awareness expert group enforces inter-disease logical constraints using a medical knowledge graph.

Result: Extensive experiments on neurology subsets (NEEMRs from CMEMR and curated XMEMRs) show RE-MCDF consistently outperforms state-of-the-art baselines in complex diagnostic scenarios.

Conclusion: The framework successfully addresses limitations of existing approaches by incorporating explicit disease relation awareness and structured multi-expert collaboration, improving diagnostic accuracy and logical consistency in challenging neurology EMR settings.

Abstract: Electronic medical records (EMRs), particularly in neurology, are inherently heterogeneous, sparse, and noisy, which poses significant challenges for large language models (LLMs) in clinical diagnosis. In such settings, single-agent systems are vulnerable to self-reinforcing errors, as their predictions lack independent validation and can drift toward spurious conclusions. Although recent multi-agent frameworks attempt to mitigate this issue through collaborative reasoning, their interactions are often shallow and loosely structured, failing to reflect the rigorous, evidence-driven processes used by clinical experts. More fundamentally, existing approaches largely ignore the rich logical dependencies among diseases, such as mutual exclusivity, pathological compatibility, and diagnostic confusion. This limitation prevents them from ruling out clinically implausible hypotheses, even when sufficient evidence is available. To overcome these, we propose RE-MCDF, a relation-enhanced multi-expert clinical diagnosis framework. RE-MCDF introduces a generation--verification--revision closed-loop architecture that integrates three complementary components: (i) a primary expert that generates candidate diagnoses and supporting evidence, (ii) a laboratory expert that dynamically prioritizes heterogeneous clinical indicators, and (iii) a multi-relation awareness and evaluation expert group that explicitly enforces inter-disease logical constraints. Guided by a medical knowledge graph (MKG), the first two experts adaptively reweight EMR evidence, while the expert group validates and corrects candidate diagnoses to ensure logical consistency. Extensive experiments on the neurology subset of CMEMR (NEEMRs) and on our curated dataset (XMEMRs) demonstrate that RE-MCDF consistently outperforms state-of-the-art baselines in complex diagnostic scenarios.

</details>


### [362] [Model Specific Task Similarity for Vision Language Model Selection via Layer Conductance](https://arxiv.org/abs/2602.01346)
*Wei Yang,Hong Xie,Tao Tan,Xin Li,Defu Lian,Enhong Chen*

Main category: cs.AI

TL;DR: Proposes Directional Conductance Divergence (DCD), an asymmetric metric for selecting optimal pretrained Vision-Language Models for downstream tasks by analyzing visual encoder functional dynamics, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current model selection methods for Vision-Language Models are inadequate - they either require extensive data or use symmetric textual descriptors that ignore the directional, model-specific nature of transferability, making optimal model selection challenging in few-shot scenarios.

Method: Grounds model selection in visual encoder functional dynamics: represents tasks via layer-wise conductance, derives target-conditioned block importance distribution through entropy regularized alignment, and introduces Directional Conductance Divergence (DCD) - an asymmetric metric quantifying how well source tasks cover target's salient functional blocks.

Result: Outperforms state-of-the-art baselines across 48 VLMs and 21 datasets, achieving 14.7% improvement in NDCG@5 over SWAB, enabling target model ranking prediction without direct inference.

Conclusion: The proposed framework effectively addresses model selection challenges by leveraging internal functional dynamics and directional transferability metrics, providing a practical solution for few-shot scenarios where exhaustive evaluation is infeasible.

Abstract: While open sourced Vision-Language Models (VLMs) have proliferated, selecting the optimal pretrained model for a specific downstream task remains challenging. Exhaustive evaluation is often infeasible due to computational constraints and data limitations in few shot scenarios. Existing selection methods fail to fully address this: they either rely on data-intensive proxies or use symmetric textual descriptors that neglect the inherently directional and model-specific nature of transferability. To address this problem, we propose a framework that grounds model selection in the internal functional dynamics of the visual encoder. Our approach represents each task via layer wise conductance and derives a target-conditioned block importance distribution through entropy regularized alignment. Building on this, we introduce Directional Conductance Divergence (DCD), an asymmetric metric that quantifies how effectively a source task covers the target's salient functional blocks. This allows for predicting target model rankings by aggregating source task ranks without direct inference. Experimental results on 48 VLMs across 21 datasets demonstrate that our method outperforms state-of-the-art baselines, achieving a 14.7% improvement in NDCG@5 over SWAB.

</details>


### [363] [Aggregation Queries over Unstructured Text: Benchmark and Agentic Method](https://arxiv.org/abs/2602.01355)
*Haojia Zhu,Qinyuan Xu,Haoyu Li,Yuxi Liu,Hanchen Qiu,Jiaoyan Chen,Jiahui Jin*

Main category: cs.AI

TL;DR: DFA framework improves aggregation query completeness over text by decomposing tasks into disambiguation, filtering, and aggregation stages, outperforming existing RAG and agentic methods.


<details>
  <summary>Details</summary>
Motivation: Aggregation queries over free text require exhaustive evidence collection ("find all" not just "find one"), but existing paradigms like Text-to-SQL and RAG fail to achieve completeness. There's a need for formalized entity-level aggregation with strict completeness requirements.

Method: Proposes DFA (Disambiguation--Filtering--Aggregation), a modular agentic baseline that decomposes aggregation querying into interpretable stages: disambiguation to resolve query ambiguity, filtering to identify relevant evidence, and aggregation to compute final results.

Result: DFA consistently improves aggregation evidence coverage over strong RAG and agentic baselines on AGGBench, a new benchmark designed for completeness-oriented aggregation evaluation under realistic large-scale corpus settings.

Conclusion: The DFA framework provides an effective approach for entity-level aggregation queries over text with strict completeness requirements, addressing key failure modes related to ambiguity, filtering, and aggregation that existing methods struggle with.

Abstract: Aggregation query over free text is a long-standing yet underexplored problem. Unlike ordinary question answering, aggregate queries require exhaustive evidence collection and systems are required to "find all," not merely "find one." Existing paradigms such as Text-to-SQL and Retrieval-Augmented Generation fail to achieve this completeness. In this work, we formalize entity-level aggregation querying over text in a corpus-bounded setting with strict completeness requirement. To enable principled evaluation, we introduce AGGBench, a benchmark designed to evaluate completeness-oriented aggregation under realistic large-scale corpus. To accompany the benchmark, we propose DFA (Disambiguation--Filtering--Aggregation), a modular agentic baseline that decomposes aggregation querying into interpretable stages and exposes key failure modes related to ambiguity, filtering, and aggregation. Empirical results show that DFA consistently improves aggregation evidence coverage over strong RAG and agentic baselines. The data and code are available in https://anonymous.4open.science/r/DFA-A4C1.

</details>


### [364] [Building Better Deception Probes Using Targeted Instruction Pairs](https://arxiv.org/abs/2602.01425)
*Vikram Natarajan,Devina Jain,Shivam Arora,Satvik Golechha,Joseph Bloom*

Main category: cs.AI

TL;DR: Linear probes for AI deception detection show promise but have limitations; instruction pair choice is crucial (explains 70.6% of variance), and specialized probes targeting specific deception types work better than universal detectors.


<details>
  <summary>Details</summary>
Motivation: Previous linear probes for monitoring AI deception have notable failures including spurious correlations and false positives, even in straightforward scenarios. The paper aims to understand and improve these deception detection methods.

Method: Analyzes the importance of instruction pairs used during training, develops a human-interpretable taxonomy of deception types, and creates specialized probes targeting specific deceptive behaviors rather than seeking a universal detector.

Result: Instruction pairs capture deceptive intent rather than content-specific patterns, explaining why prompt choice dominates probe performance (70.6% of variance). Specialized probes targeting specific deception types improve results on evaluation datasets.

Conclusion: Organizations should design specialized probes targeting their specific threat models rather than seeking a universal deception detector, given the heterogeneity of deception types across datasets.

Abstract: Linear probes are a promising approach for monitoring AI systems for deceptive behaviour. Previous work has shown that a linear classifier trained on a contrastive instruction pair and a simple dataset can achieve good performance. However, these probes exhibit notable failures even in straightforward scenarios, including spurious correlations and false positives on non-deceptive responses. In this paper, we identify the importance of the instruction pair used during training. Furthermore, we show that targeting specific deceptive behaviors through a human-interpretable taxonomy of deception leads to improved results on evaluation datasets. Our findings reveal that instruction pairs capture deceptive intent rather than content-specific patterns, explaining why prompt choice dominates probe performance (70.6% of variance). Given the heterogeneity of deception types across datasets, we conclude that organizations should design specialized probes targeting their specific threat models rather than seeking a universal deception detector.

</details>


### [365] [SimGym: Traffic-Grounded Browser Agents for Offline A/B Testing in E-Commerce](https://arxiv.org/abs/2602.01443)
*Alberto Castelo,Zahra Zanjani Foumani,Ailin Fan,Keat Yang Koay,Vibhor Malik,Yuanzheng Zhu,Han Li,Meysam Feghhi,Ronie Uliana,Shuang Xie,Zhaoyu Zhang,Angelo Ocana Martins,Mingyu Zhao,Francis Pelland,Jonathan Faerman,Nikolas LeBlanc,Aaron Glazer,Andrew McNamara,Lingyun Wang,Zhong Wu*

Main category: cs.AI

TL;DR: SimGym enables rapid offline A/B testing using LLM agents that simulate buyer behavior, reducing experiment cycles from weeks to under an hour without exposing real users.


<details>
  <summary>Details</summary>
Motivation: Traditional A/B testing for e-commerce UI changes has significant drawbacks: it diverts traffic, takes weeks to reach statistical significance, and risks harming user experience by exposing real customers to potentially poor designs.

Method: SimGym extracts buyer profiles and intents from production data, identifies behavioral archetypes, and uses LLM agents to simulate cohort-weighted sessions across control and treatment storefronts in a live browser environment.

Result: SimGym achieves state-of-the-art alignment with observed outcome shifts from real human A/B tests, even without alignment post-training, and reduces experiment cycles from weeks to under an hour.

Conclusion: SimGym enables rapid, safe experimentation for e-commerce UI changes without exposing real buyers to risk, dramatically accelerating the testing process while maintaining accuracy comparable to traditional A/B testing.

Abstract: A/B testing remains the gold standard for evaluating e-commerce UI changes, yet it diverts traffic, takes weeks to reach significance, and risks harming user experience. We introduce SimGym, a scalable system for rapid offline A/B testing using traffic-grounded synthetic buyers powered by Large Language Model agents operating in a live browser. SimGym extracts per-shop buyer profiles and intents from production interaction data, identifies distinct behavioral archetypes, and simulates cohort-weighted sessions across control and treatment storefronts. We validate SimGym against real human outcomes from real UI changes on a major e-commerce platform under confounder control. Even without alignment post training, SimGym agents achieve state of the art alignment with observed outcome shifts and reduces experiment cycles from weeks to under an hour , enabling rapid experimentation without exposure to real buyers.

</details>


### [366] [Agyn: A Multi-Agent System for Team-Based Autonomous Software Engineering](https://arxiv.org/abs/2602.01465)
*Nikita Benkovich,Vitalii Valkov*

Main category: cs.AI

TL;DR: Multi-agent system models software engineering as organizational process with specialized roles, achieving 72.4% success on SWE-bench 500 without human intervention.


<details>
  <summary>Details</summary>
Motivation: Current autonomous systems treat issue resolution as monolithic/pipeline-based, while real software development is collaborative with clear roles, communication, and review processes. Need to replicate team structure and methodology.

Method: Built on agyn platform, creates multi-agent system with specialized roles (coordination, research, implementation, review), isolated sandboxes, structured communication, and defined development methodology (analysis, task specification, PR creation, iterative review).

Result: Achieves 72.4% task resolution on SWE-bench 500, outperforming single-agent baselines with comparable language models. Designed for real production use, not tuned for benchmark.

Conclusion: Replicating team structure, methodology, and communication is powerful for autonomous software engineering. Future progress depends on organizational design and agent infrastructure as much as model improvements.

Abstract: Large language models have demonstrated strong capabilities in individual software engineering tasks, yet most autonomous systems still treat issue resolution as a monolithic or pipeline-based process. In contrast, real-world software development is organized as a collaborative activity carried out by teams following shared methodologies, with clear role separation, communication, and review. In this work, we present a fully automated multi-agent system that explicitly models software engineering as an organizational process, replicating the structure of an engineering team. Built on top of agyn, an open-source platform for configuring agent teams, our system assigns specialized agents to roles such as coordination, research, implementation, and review, provides them with isolated sandboxes for experimentation, and enables structured communication. The system follows a defined development methodology for working on issues, including analysis, task specification, pull request creation, and iterative review, and operates without any human intervention. Importantly, the system was designed for real production use and was not tuned for SWE-bench. When evaluated post hoc on SWE-bench 500, it resolves 72.4% of tasks, outperforming single-agent baselines using comparable language models. Our results suggest that replicating team structure, methodology, and communication is a powerful paradigm for autonomous software engineering, and that future progress may depend as much on organizational design and agent infrastructure as on model improvements.

</details>


### [367] [Legal Infrastructure for Transformative AI Governance](https://arxiv.org/abs/2602.01474)
*Gillian K. Hadfield*

Main category: cs.AI

TL;DR: The paper argues that AI governance should focus not just on substantive rules but on building legal/regulatory infrastructure, proposing three specific frameworks for frontier models, autonomous agents, and regulatory markets.


<details>
  <summary>Details</summary>
Motivation: Current AI governance efforts overly focus on substantive rules while neglecting the need for proper legal and regulatory infrastructure to generate and implement those rules effectively, especially given AI's transformative nature.

Method: The author reviews three proposed regulatory infrastructure frameworks: 1) registration regimes for frontier AI models, 2) registration/identification regimes for autonomous agents, and 3) regulatory markets that enable private companies to innovate in AI regulatory services.

Result: The paper presents three concrete infrastructure proposals that could help establish effective AI governance frameworks beyond just substantive rule-making.

Conclusion: Building proper legal and regulatory infrastructure is crucial for effective AI governance, and the proposed frameworks offer practical approaches to address this need alongside substantive rule-making.

Abstract: Most of our AI governance efforts focus on substance: what rules do we want in place? What limits or checks do we want to impose on AI development and deployment? But a key role for law is not only to establish substantive rules but also to establish legal and regulatory infrastructure to generate and implement rules. The transformative nature of AI calls especially for attention to building legal and regulatory frameworks. In this PNAS Perspective piece I review three examples I have proposed: the creation of registration regimes for frontier models; the creation of registration and identification regimes for autonomous agents; and the design of regulatory markets to facilitate a role for private companies to innovate and deliver AI regulatory services.

</details>


### [368] [Learning to Guide Local Search for MPE Inference in Probabilistic Graphical Models](https://arxiv.org/abs/2602.01475)
*Brij Malhotra,Shivvrat Arya,Tahrima Rahman,Vibhav Giridhar Gogate*

Main category: cs.AI

TL;DR: Neural amortization framework improves MPE inference in PGMs by training attention-based network to score local moves for better neighbor selection in repeated-query settings.


<details>
  <summary>Details</summary>
Motivation: MPE inference in PGMs is computationally challenging, especially in repeated-query settings where the same model is used with varying evidence. Existing SLS algorithms often stagnate in poor local optima, and heuristics like GLS+ cannot effectively reuse guidance across multiple inference queries.

Method: Propose neural amortization framework that trains an attention-based network to score local moves by predicting their ability to reduce Hamming distance to near-optimal solutions. The approach integrates with existing local search procedures to balance short-term likelihood gains with long-term promise during neighbor selection.

Result: Theoretical intuition links distance-reducing move selection to improved convergence behavior. Empirically demonstrates consistent improvements over SLS and GLS+ on challenging high-treewidth benchmarks in amortized inference settings.

Conclusion: Neural amortization effectively improves local search for MPE inference in repeated-query regimes by leveraging fixed graph structure to learn reusable guidance, overcoming limitations of traditional SLS and heuristic approaches.

Abstract: Most Probable Explanation (MPE) inference in Probabilistic Graphical Models (PGMs) is a fundamental yet computationally challenging problem arising in domains such as diagnosis, planning, and structured prediction. In many practical settings, the graphical model remains fixed while inference must be performed repeatedly for varying evidence patterns. Stochastic Local Search (SLS) algorithms scale to large models but rely on myopic best-improvement rule that prioritizes immediate likelihood gains and often stagnate in poor local optima. Heuristics such as Guided Local Search (GLS+) partially alleviate this limitation by modifying the search landscape, but their guidance cannot be reused effectively across multiple inference queries on the same model. We propose a neural amortization framework for improving local search in this repeated-query regime. Exploiting the fixed graph structure, we train an attention-based network to score local moves by predicting their ability to reduce Hamming distance to a near-optimal solution. Our approach integrates seamlessly with existing local search procedures, using this signal to balance short-term likelihood gains with long-term promise during neighbor selection. We provide theoretical intuition linking distance-reducing move selection to improved convergence behavior, and empirically demonstrate consistent improvements over SLS and GLS+ on challenging high-treewidth benchmarks in the amortized inference setting.

</details>


### [369] [Qrita: High-performance Top-k and Top-p Algorithm for GPUs using Pivot-based Truncation and Selection](https://arxiv.org/abs/2602.01518)
*Jongseok Park,Sunga Kim,Alvin Cheung,Ion Stoica*

Main category: cs.AI

TL;DR: Qrita is an efficient GPU algorithm for Top-k and Top-p sampling in LLMs that uses pivot-based selection instead of sorting, achieving 2x throughput and half memory usage while maintaining deterministic output.


<details>
  <summary>Details</summary>
Motivation: Current Top-k and Top-p implementations in LLMs rely on inefficient sorting operations that cause significant computation and memory overhead on GPUs, or use stochastic approaches that alter algorithm output. There's a need for an efficient deterministic solution.

Method: Qrita extends RTop-k's pivot-based search to both Top-k and Top-p using two key techniques: 1) Gaussian-based sigma-truncation to reduce search space, and 2) Quaternary pivot search with duplication handling that halves iterations and guarantees deterministic output. Implemented in Triton for GPU execution.

Result: Qrita achieves up to 2 times throughput and half memory usage compared to sorting-based algorithms in vLLM, SGLang, and Flashinfer, while providing identical deterministic output.

Conclusion: Qrita provides an efficient, deterministic alternative to sorting-based Top-k and Top-p implementations in LLMs, significantly improving GPU performance and memory efficiency without altering algorithm output.

Abstract: Top-k and Top-p are the dominant truncation operators in the sampling of large language models. Despite their widespread use, implementing them efficiently over large vocabularies remains a significant challenge. Existing approaches often rely on sorting, which incur significant computation and memory overhead on GPUs, or stochastic approaches, which alter the algorithm output. In this work, we propose Qrita, an efficient Top-k and Top-p algorithm based on a pivot-based selection strategy. Based on RTop-k, which uses a pivot-based search for node selection in graph neural networks, Qrita extends the concept of pivot-based search to both Top-k and Top-p with two key techniques: 1. Gaussian-based sigma-truncation, which greatly reduces the search space of the target elements, and 2. Quaternary pivot search with duplication handling, which halves the pivot search iteration and guarantees deterministic output. We provide the full implementation of Qrita using Triton, a popular GPU programming language. Our evaluation of Qrita against the Top-k and Top-p kernels of high performance LLM execution engines such as vLLM, SGLang, and Flashinfer show that Qrita achieves up to 2 times throughput and half memory use while providing the same output to the the sorting-based algorithms.

</details>


### [370] [PRISM: Festina Lente Proactivity -- Risk-Sensitive, Uncertainty-Aware Deliberation for Proactive Agents](https://arxiv.org/abs/2602.01532)
*Yuxuan Fu,Xiaoyu Tan,Teqi Hao,Chen Zhan,Xihe Qiu*

Main category: cs.AI

TL;DR: PRISM framework uses decision-theoretic gating with dual-process reasoning for selective intervention in proactive agents, reducing false alarms by 22.78% and improving F1 by 20.14%.


<details>
  <summary>Details</summary>
Motivation: Current proactive agent systems rely on brittle heuristics or indiscriminate long reasoning, offering little control over the benefit-burden tradeoff between helpful intervention and user annoyance.

Method: PRISM combines decision-theoretic gating with dual-process reasoning: a Fast mode for quick decisions, and a Slow mode with counterfactual checks only near decision boundaries. Uses gate-aligned, schema-locked distillation where a teacher provides supervision on unlabeled traces while the student learns a response policy decoupled from the intervention gate.

Result: On ProactiveBench, PRISM reduces false alarms by 22.78% and improves F1 by 20.14% over strong baselines, demonstrating precise, computationally efficient, and controllable proactive agents.

Conclusion: Principled decision-theoretic gating paired with selective slow reasoning and aligned distillation yields proactive agents that are precise, computationally efficient, and controllable, addressing the fundamental tradeoff between helpfulness and burden.

Abstract: Proactive agents must decide not only what to say but also whether and when to intervene. Many current systems rely on brittle heuristics or indiscriminate long reasoning, which offers little control over the benefit-burden tradeoff. We formulate the problem as cost-sensitive selective intervention and present PRISM, a novel framework that couples a decision-theoretic gate with a dual-process reasoning architecture. At inference time, the agent intervenes only when a calibrated probability of user acceptance exceeds a threshold derived from asymmetric costs of missed help and false alarms. Inspired by festina lente (Latin: "make haste slowly"), we gate by an acceptance-calibrated, cost-derived threshold and invoke a resource-intensive Slow mode with counterfactual checks only near the decision boundary, concentrating computation on ambiguous and high-stakes cases. Training uses gate-aligned, schema-locked distillation: a teacher running the full PRISM pipeline provides dense, executable supervision on unlabeled interaction traces, while the student learns a response policy that is explicitly decoupled from the intervention gate to enable tunable and auditable control. On ProactiveBench, PRISM reduces false alarms by 22.78% and improves F1 by 20.14% over strong baselines. These results show that principled decision-theoretic gating, paired with selective slow reasoning and aligned distillation, yields proactive agents that are precise, computationally efficient, and controllable. To facilitate reproducibility, we release our code, models, and resources at https://prism-festinalente.github.io/; all experiments use the open-source ProactiveBench benchmark.

</details>


### [371] [MAGIC: A Co-Evolving Attacker-Defender Adversarial Game for Robust LLM Safety](https://arxiv.org/abs/2602.01539)
*Xiaoyu Wen,Zhida He,Han Qi,Ziyu Wan,Zhongtian Ma,Ying Wen,Tianhang Zheng,Xingcheng Xu,Chaochao Lu,Qiaosheng Zhang*

Main category: cs.AI

TL;DR: MAGIC is a multi-agent RL framework for LLM safety alignment that uses adversarial co-evolution between attacker and defender agents to improve robustness against evolving attacks.


<details>
  <summary>Details</summary>
Motivation: Existing LLM safety defenses rely on static data distributions and can't keep up with evolving adversarial attacks, creating a need for dynamic, adaptive safety alignment methods.

Method: Multi-turn multi-agent reinforcement learning framework where an attacker agent learns to rewrite queries into deceptive prompts while a defender agent simultaneously learns to recognize and refuse such inputs, creating adversarial co-evolution.

Result: The attacker evolves novel combinatorial strategies, the framework achieves superior defense success rates without compromising model helpfulness, and provides theoretical safety guarantees.

Conclusion: MAGIC demonstrates that adversarial co-evolution through multi-agent RL is an effective approach for robust LLM safety alignment that can adapt to evolving attack patterns.

Abstract: Ensuring robust safety alignment is crucial for Large Language Models (LLMs), yet existing defenses often lag behind evolving adversarial attacks due to their \textbf{reliance on static, pre-collected data distributions}. In this paper, we introduce \textbf{MAGIC}, a novel multi-turn multi-agent reinforcement learning framework that formulates LLM safety alignment as an adversarial asymmetric game. Specifically, an attacker agent learns to iteratively rewrite original queries into deceptive prompts, while a defender agent simultaneously optimizes its policy to recognize and refuse such inputs. This dynamic process triggers a \textbf{co-evolution}, where the attacker's ever-changing strategies continuously uncover long-tail vulnerabilities, driving the defender to generalize to unseen attack patterns. Remarkably, we observe that the attacker, endowed with initial reasoning ability, evolves \textbf{novel, previously unseen combinatorial strategies} through iterative RL training, underscoring our method's substantial potential. Theoretically, we provide insights into a more robust game equilibrium and derive safety guarantees. Extensive experiments validate our framework's effectiveness, demonstrating superior defense success rates without compromising the helpfulness of the model. Our code is available at https://github.com/BattleWen/MAGIC.

</details>


### [372] [S1-NexusAgent: a Self-Evolving Agent Framework for Multidisciplinary Scientific Research](https://arxiv.org/abs/2602.01550)
*S1-NexusAgent Team*

Main category: cs.AI

TL;DR: S1-NexusAgent is a self-evolving AI agent framework for multidisciplinary scientific research that combines hierarchical planning with tool execution, supports thousands of scientific tools, manages large-scale data efficiently, and continuously improves through automated evaluation and skill distillation.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs and tool-based agents struggle with complex scientific research due to limitations in long-horizon planning, robust goal maintenance, and continual learning from execution in large-scale, multidisciplinary scientific workflows.

Method: Hierarchical Plan-and-CodeAct execution paradigm with dual-loop architecture decoupling global planning from subtask execution; supports Model Context Protocol (MCP) with thousands of tools; uses object-reference-based sparse context management for data handling; includes Critic Agent for trajectory evaluation and skill distillation.

Result: Achieves state-of-the-art performance on authoritative scientific benchmarks (biomini-eval, ChemBench, MatSciBench) involving long-horizon planning and complex specialized tool orchestration, demonstrating effectiveness and generalization in complex scientific tasks.

Conclusion: S1-NexusAgent provides an effective self-evolving framework for multidisciplinary scientific research that addresses key limitations of existing approaches and enables sustainable, long-horizon scientific investigation through continuous improvement.

Abstract: Modern scientific research relies on large-scale data, complex workflows, and specialized tools, which existing LLMs and tool-based agents struggle to handle due to limitations in long-horizon planning, robust goal maintenance, and continual learning from execution. To address these issues, in this work, we propose S1-NexusAgent, a self-evolving agent framework designed for multidisciplinary scientific research. S1-NexusAgent adopts a hierarchical Plan-and-CodeAct execution paradigm, decoupling global scientific planning from subtask-level tool execution through a dual-loop architecture, thereby enabling stable modeling of complex research workflows. The system natively supports the Model Context Protocol (MCP), integrates up to thousands of cross-disciplinary scientific tools, and achieves efficient orchestration of heterogeneous research tools via intention-aware dynamic tool retrieval and hot-plug mechanisms. To address long-context and large-scale data challenges in scientific settings, S1-NexusAgent introduces object-reference-based sparse context management, which enables sub-task context isolation and intermediate result compression. Building on this, a Critic Agent automatically evaluates complete execution trajectories and distills high-quality research paths into reusable Scientific Skills, forming a closed loop for continuous self-evolution, which is valuable for sustainable and long-horizon scientific research. Experiments on authoritative scientific benchmarks involving long-horizon planning and complex specialized tool orchestration, including biomini-eval (biology), ChemBench (chemistry), and MatSciBench (material science), demonstrate that S1-NexusAgent achieves state-of-the-art performance, validating its effectiveness and generalization capability in complex scientific tasks.

</details>


### [373] [Autonomous Question Formation for Large Language Model-Driven AI Systems](https://arxiv.org/abs/2602.01556)
*Hong Su*

Main category: cs.AI

TL;DR: LLM-driven AI systems can autonomously form questions and set tasks by reasoning over internal states, environmental observations, and inter-agent interactions, improving adaptability in dynamic environments.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-driven AI systems rely on predefined tasks and fixed prompts, limiting their ability to autonomously identify what problems should be solved when environmental conditions change. There's a need for systems that can dynamically form questions and set tasks based on evolving situations.

Method: Proposes a human-simulation-based framework that treats question formation as a first-class decision process before task selection. Integrates three prompting scopes: internal-driven (reasoning over internal states), environment-aware (considering environmental observations), and inter-agent-aware (incorporating interactions with other AI systems). The framework supports learning question-formation from experience to improve over time.

Result: In multi-agent simulation experiments, environment-aware prompting significantly reduces no-eat events compared to internal-driven baseline. Inter-agent-aware prompting further reduces cumulative no-eat events by more than 60% over 20-day simulations, with statistically significant improvements (p < 0.05).

Conclusion: The proposed framework enables AI systems to autonomously form questions and set tasks, improving adaptability and decision quality in dynamic environments through progressive cognitive coverage expansion and experiential learning.

Abstract: Large language model (LLM)-driven AI systems are increasingly important for autonomous decision-making in dynamic and open environments. However, most existing systems rely on predefined tasks and fixed prompts, limiting their ability to autonomously identify what problems should be solved when environmental conditions change. In this paper, we propose a human-simulation-based framework that enables AI systems to autonomously form questions and set tasks by reasoning over their internal states, environmental observations, and interactions with other AI systems. The proposed method treats question formation as a first-class decision process preceding task selection and execution, and integrates internal-driven, environment-aware, and inter-agent-aware prompting scopes to progressively expand cognitive coverage. In addition, the framework supports learning the question-formation process from experience, allowing the system to improve its adaptability and decision quality over time. xperimental results in a multi-agent simulation environment show that environment-aware prompting significantly reduces no-eat events compared with the internal-driven baseline, and inter-agent-aware prompting further reduces cumulative no-eat events by more than 60% over a 20-day simulation, with statistically significant improvements (p < 0.05).

</details>


### [374] [Reasoning with Autoregressive-Diffusion Collaborative Thoughts](https://arxiv.org/abs/2602.01608)
*Mu Yuan,Liekang Zeng,Guoliang Xing,Lan Zhang,Yunhao Liu*

Main category: cs.AI

TL;DR: Collaborative Thoughts is a unified framework where autoregressive and diffusion models work together through closed-loop interaction to combine logical planning with spatial generation capabilities.


<details>
  <summary>Details</summary>
Motivation: Autoregressive models are good at sequential planning but struggle with spatial grounding, while diffusion models capture spatial structure but lack logical control. Neither alone can handle tasks requiring both complex constraints and spatial/physical grounding.

Method: A collaborative framework where autoregressive models handle structured planning and constraint management, diffusion models generate intermediate visual thoughts, and a vision-based critic evaluates satisfaction of structural/physical requirements. Feedback loops enable iterative refinement across modalities.

Result: The framework improves reliability of spatial reasoning and controllability of generation through representative examples, demonstrating how the collaboration mitigates error propagation across modalities.

Conclusion: Collaborative Thoughts provides a unified approach that leverages complementary strengths of autoregressive and diffusion models through closed-loop interaction, applicable to both question answering and visual generation tasks.

Abstract: Autoregressive and diffusion models represent two complementary generative paradigms. Autoregressive models excel at sequential planning and constraint composition, yet struggle with tasks that require explicit spatial or physical grounding. Diffusion models, in contrast, capture rich spatial structure through high-dimensional generation, but lack the stepwise logical control needed to satisfy complex, multi-stage constraints or to reliably identify and correct errors. We introduce Collaborative Thoughts, a unified collaborative framework that enables autoregressive and diffusion models to reason and generate jointly through a closed-loop interaction. In Collaborative Thoughts, autoregressive models perform structured planning and constraint management, diffusion models instantiate these constraints as intermediate visual thoughts, and a vision-based critic module evaluates whether the visual thoughts satisfy the intended structural and physical requirements. This feedback is then used to iteratively refine subsequent planning and generation steps, mitigating error propagation across modalities. Importantly, Collaborative Thoughts uses the same collaborative loop regardless of whether the task is autoregressive question answering or diffusion-based visual generation. Through representative examples, we illustrate how Collaborative Thoughts can improve the reliability of spatial reasoning and the controllability of generation.

</details>


### [375] [ToPT: Task-Oriented Prompt Tuning for Urban Region Representation Learning](https://arxiv.org/abs/2602.01610)
*Zitao Guo,Changyang Jiang,Tianhong Zhao,Jinzhou Cao,Genan Dai,Bowen Zhang*

Main category: cs.AI

TL;DR: ToPT is a two-stage framework for learning region embeddings from urban data that addresses spatial incoherence and task misalignment in existing methods through spatial-aware fusion and task-aware prompting.


<details>
  <summary>Details</summary>
Motivation: Existing methods for region embeddings have two key limitations: (1) two-stage methods produce task-agnostic representations decoupled from downstream objectives, and (2) recent prompt-based approaches lack explicit spatial priors (causing spatially incoherent modeling) and robust mechanisms for task-semantic alignment.

Method: ToPT consists of two modules: (1) Spatial-aware Region Embedding Learning (SREL) uses a Graphormer-based fusion module that injects spatial priors (distance and regional centrality) as learnable attention biases to capture coherent inter-region interactions; (2) Task-aware Prompting for Region Embeddings (Prompt4RE) uses a frozen multimodal large language model to process task-specific templates, then aligns the resulting semantic vectors with region embeddings via multi-head cross-attention for stable task conditioning.

Result: Experiments across multiple tasks and cities show state-of-the-art performance with improvements of up to 64.2%, validating the necessity and complementarity of spatial priors and prompt-region alignment.

Conclusion: ToPT effectively addresses spatial incoherence and task misalignment in region embedding learning through its two-stage framework combining spatial-aware fusion and task-aware prompting, demonstrating significant performance gains across various urban computing tasks.

Abstract: Learning effective region embeddings from heterogeneous urban data underpins key urban computing tasks (e.g., crime prediction, resource allocation). However, prevailing two-stage methods yield task-agnostic representations, decoupling them from downstream objectives. Recent prompt-based approaches attempt to fix this but introduce two challenges: they often lack explicit spatial priors, causing spatially incoherent inter-region modeling, and they lack robust mechanisms for explicit task-semantic alignment. We propose ToPT, a two-stage framework that delivers spatially consistent fusion and explicit task alignment. ToPT consists of two modules: spatial-aware region embedding learning (SREL) and task-aware prompting for region embeddings (Prompt4RE). SREL employs a Graphormer-based fusion module that injects spatial priors-distance and regional centrality-as learnable attention biases to capture coherent, interpretable inter-region interactions. Prompt4RE performs task-oriented prompting: a frozen multimodal large language model (MLLM) processes task-specific templates to obtain semantic vectors, which are aligned with region embeddings via multi-head cross-attention for stable task conditioning. Experiments across multiple tasks and cities show state-of-the-art performance, with improvements of up to 64.2\%, validating the necessity and complementarity of spatial priors and prompt-region alignment. The code is available at https://github.com/townSeven/Prompt4RE.git.

</details>


### [376] [ProjDevBench: Benchmarking AI Coding Agents on End-to-End Project Development](https://arxiv.org/abs/2602.01655)
*Pengrui Lu,Shiqi Zhang,Yunzhong Hou,Lyumanshan Ye,Chaoyi Huang,Zixi Chen,Ji Zeng,Hantao Jiang,Pengfei Liu,Yiwei Wang,Ming-Hsuan Yang*

Main category: cs.AI

TL;DR: ProjDevBench is an end-to-end benchmark for evaluating coding agents on complete project development, testing system architecture design, functional correctness, and iterative refinement across 20 programming problems.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations for coding agents focus only on issue-level bug fixing and lag behind end-to-end development capabilities, creating a need for comprehensive benchmarks that assess complete project development.

Method: Combines Online Judge (OJ) testing with LLM-assisted code review to evaluate agents on system architecture design, functional correctness, and iterative solution refinement across 20 programming problems in 8 categories.

Result: Overall acceptance rate of 27.38%: agents handle basic functionality and data structures well but struggle with complex system design, time complexity optimization, and resource management.

Conclusion: ProjDevBench provides a comprehensive end-to-end evaluation framework for coding agents, revealing significant gaps in handling complex system design and optimization tasks despite competence in basic functionality.

Abstract: Recent coding agents can generate complete codebases from simple prompts, yet existing evaluations focus on issue-level bug fixing and lag behind end-to-end development. We introduce ProjDevBench, an end-to-end benchmark that provides project requirements to coding agents and evaluates the resulting repositories. Combining Online Judge (OJ) testing with LLM-assisted code review, the benchmark evaluates agents on (1) system architecture design, (2) functional correctness, and (3) iterative solution refinement. We curate 20 programming problems across 8 categories, covering both concept-oriented tasks and real-world application scenarios, and evaluate six coding agents built on different LLM backends. Our evaluation reports an overall acceptance rate of 27.38%: agents handle basic functionality and data structures but struggle with complex system design, time complexity optimization, and resource management. Our benchmark is available at https://github.com/zsworld6/projdevbench.

</details>


### [377] [FlowSteer: Interactive Agentic Workflow Orchestration via End-to-End Reinforcement Learning](https://arxiv.org/abs/2602.01664)
*Mingda Zhang,Haoran Luo,Tiesunlong Shen,Qika Lin,Xiaoying Tang,Rui Mao,Erik Cambria*

Main category: cs.AI

TL;DR: FlowSteer is an RL framework that automates workflow orchestration using a policy model to interact with an executable canvas environment, trained with CWRPO to handle diverse operators and LLMs.


<details>
  <summary>Details</summary>
Motivation: Existing workflow orchestration faces challenges: high manual cost, reliance on specific operators/LLMs, and sparse reward signals. Need automated solution that works across diverse tools.

Method: End-to-end RL framework with lightweight policy model as agent interacting with executable canvas environment. Uses multi-turn interaction where policy analyzes states and selects editing actions, canvas executes operators and returns feedback. Includes Canvas Workflow Relative Policy Optimization (CWRPO) with diversity-constrained rewards and conditional release to stabilize learning.

Result: Experimental results on twelve datasets show FlowSteer significantly outperforms baselines across various tasks. Framework supports plug-and-play diverse operator libraries and interchangeable LLM backends.

Conclusion: FlowSteer successfully addresses key workflow orchestration challenges through automated RL-based interaction, providing flexible, effective solution that works across different operators and LLMs.

Abstract: In recent years, a variety of powerful agentic workflows have been applied to solve a wide range of human problems. However, existing workflow orchestration still faces key challenges, including high manual cost, reliance on specific operators/large language models (LLMs), and sparse reward signals. To address these challenges, we propose FlowSteer, an end-to-end reinforcement learning framework that takes a lightweight policy model as the agent and an executable canvas environment, automating workflow orchestration through multi-turn interaction. In this process, the policy model analyzes execution states and selects editing actions, while the canvas executes operators and returns feedback for iterative refinement. Moreover, FlowSteer provides a plug-and-play framework that supports diverse operator libraries and interchangeable LLM backends. To effectively train this interaction paradigm, we propose Canvas Workflow Relative Policy Optimization (CWRPO), which introduces diversity-constrained rewards with conditional release to stabilize learning and suppress shortcut behaviors. Experimental results on twelve datasets show that FlowSteer significantly outperforms baselines across various tasks.

</details>


### [378] [TRIP-Bench: A Benchmark for Long-Horizon Interactive Agents in Real-World Scenarios](https://arxiv.org/abs/2602.01675)
*Yuanzhe Shen,Zisu Huang,Zhengyuan Wang,Muzhao Tian,Zhengkang Guo,Chenyang Zhang,Shuaiyu Zhou,Zengjie Hu,Dailin Li,Jingwen Xu,Kaimin Wang,Wenhao Liu,Tianlong Li,Fengpeng Yue,Feng Hong,Cao Liu,Ke Zeng*

Main category: cs.AI

TL;DR: TRIP-Bench is a long-horizon benchmark for LLM agents in realistic travel planning scenarios, featuring 18 tools and 40+ requirements with automated evaluation. GTPO is a novel online RL method that improves constraint satisfaction and outperforms Gemini-3-Pro on Qwen2.5-32B-Instruct.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks underrepresent key challenges for LLM-based agents in complex real-world settings: enforcing global constraints, coordinating multi-tool reasoning, and adapting to evolving user behavior over long, multi-turn interactions.

Method: Introduces TRIP-Bench with real-world travel-planning scenarios, 18 curated tools, 40+ requirements, and automated evaluation. Hard split includes long/ambiguous interactions, style shifts, feasibility changes, and iterative version revision. Also proposes GTPO - an online multi-turn reinforcement learning method with specialized reward normalization and reward differencing.

Result: Even advanced models achieve at most 50% success on easy split, with performance dropping below 10% on hard subsets. GTPO applied to Qwen2.5-32B-Instruct improves constraint satisfaction and interaction robustness, outperforming Gemini-3-Pro in evaluation.

Conclusion: TRIP-Bench advances practical long-horizon interactive agents, and GTPO provides an effective online RL recipe for robust long-horizon training, addressing critical gaps in current LLM agent evaluation and improvement.

Abstract: As LLM-based agents are deployed in increasingly complex real-world settings, existing benchmarks underrepresent key challenges such as enforcing global constraints, coordinating multi-tool reasoning, and adapting to evolving user behavior over long, multi-turn interactions. To bridge this gap, we introduce \textbf{TRIP-Bench}, a long-horizon benchmark grounded in realistic travel-planning scenarios. TRIP-Bench leverages real-world data, offers 18 curated tools and 40+ travel requirements, and supports automated evaluation. It includes splits of varying difficulty; the hard split emphasizes long and ambiguous interactions, style shifts, feasibility changes, and iterative version revision. Dialogues span up to 15 user turns, can involve 150+ tool calls, and may exceed 200k tokens of context. Experiments show that even advanced models achieve at most 50\% success on the easy split, with performance dropping below 10\% on hard subsets. We further propose \textbf{GTPO}, an online multi-turn reinforcement learning method with specialized reward normalization and reward differencing. Applied to Qwen2.5-32B-Instruct, GTPO improves constraint satisfaction and interaction robustness, outperforming Gemini-3-Pro in our evaluation. We expect TRIP-Bench to advance practical long-horizon interactive agents, and GTPO to provide an effective online RL recipe for robust long-horizon training.

</details>


### [379] [What LLMs Think When You Don't Tell Them What to Think About?](https://arxiv.org/abs/2602.01689)
*Yongchan Kwon,James Zou*

Main category: cs.AI

TL;DR: LLMs exhibit strong systematic topical preferences even from minimal topic-neutral inputs, with different model families showing distinct content specializations and unique degenerate behaviors.


<details>
  <summary>Details</summary>
Motivation: Existing analyses of LLM behavior rely on topic- or task-specific prompts, which limit what can be observed. The authors want to study LLMs' near-unconstrained generative behavior from minimal, topic-neutral inputs to better understand their inherent preferences and behaviors.

Method: The researchers probe LLMs using minimal, topic-neutral inputs to observe near-unconstrained generative behavior. They analyze outputs from 16 different LLMs, collecting 256,000 samples to systematically characterize topical preferences, content specialization, and unique behaviors.

Result: Despite topic-neutral inputs, each model family exhibits strong systematic topical preferences: GPT-OSS generates programming (27.1%) and mathematical content (24.6%), Llama produces literary content (9.1%), DeepSeek generates religious content, and Qwen produces multiple-choice questions. Models also differ in content depth and specialization, and show unique degenerate behaviors like repetitive phrases and personal URLs.

Conclusion: LLMs have inherent topical preferences and unique behavioral patterns that emerge even without explicit prompting, revealing important insights about model characteristics that are critical for reliable monitoring and AI safety. The released dataset and codebase enable reproducible analysis of these behaviors.

Abstract: Characterizing the behavior of large language models (LLMs) across diverse settings is critical for reliable monitoring and AI safety. However, most existing analyses rely on topic- or task-specific prompts, which can substantially limit what can be observed. In this work, we study what LLMs generate from minimal, topic-neutral inputs and probe their near-unconstrained generative behavior. Despite the absence of explicit topics, model outputs cover a broad semantic space, and surprisingly, each model family exhibits strong and systematic topical preferences. GPT-OSS predominantly generates programming (27.1%) and mathematical content (24.6%), whereas Llama most frequently generates literary content (9.1%). DeepSeek often generates religious content, while Qwen frequently generates multiple-choice questions. Beyond topical preferences, we also observe differences in content specialization and depth: GPT-OSS often generates more technically advanced content (e.g., dynamic programming) compared with other models (e.g., basic Python). Furthermore, we find that the near-unconstrained generation often degenerates into repetitive phrases, revealing interesting behaviors unique to each model family. For instance, degenerate outputs from Llama include multiple URLs pointing to personal Facebook and Instagram accounts. We release the complete dataset of 256,000 samples from 16 LLMs, along with a reproducible codebase.

</details>


### [380] [Beyond Dense States: Elevating Sparse Transcoders to Active Operators for Latent Reasoning](https://arxiv.org/abs/2602.01695)
*Yadong Wang,Haodong Chen,Yu Tian,Chuanxing Geng,Dong Liang,Xiang Chen*

Main category: cs.AI

TL;DR: LSTR is a latent reasoning framework that uses sparse semantic transitions instead of dense latent states, making reasoning more interpretable and controllable while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing latent reasoning methods use dense latent transitions that are difficult to interpret and control, while sparse representation models are limited to post-hoc analysis. There's a need to combine the benefits of both approaches.

Method: Proposes LSTR framework with Latent Transition Transcoder (LTT) using residual skip architecture that decouples linear manifold transport from sparse semantic updates, enabling controllable semantic resolution via explicit sparsity constraints.

Result: LSTR preserves reasoning accuracy and compression efficiency while substantially improving interpretability over dense latent baselines. Sparse features act as both interpretable and causally effective operators in reasoning.

Conclusion: LSTR successfully reconciles the tension between dense latent reasoning and sparse representations by making sparse semantic features active reasoning operators, enabling both interpretability and causal effectiveness in multi-step computation.

Abstract: Latent reasoning compresses the chain-of-thought (CoT) into continuous hidden states, yet existing methods rely on dense latent transitions that remain difficult to interpret and control. Meanwhile, sparse representation models uncover human-interpretable semantic features but remain largely confined to post-hoc analysis. We reconcile this tension by proposing LSTR (Latent Sparse Transcoder Reasoning), a latent reasoning framework that elevates functional sparse transcoders into active reasoning operators to perform multi-step computation through sparse semantic transitions. At its core, LSTR employs a Latent Transition Transcoder (LTT) with a residual skip architecture that decouples linear manifold transport from sparse semantic updates, enabling controllable semantic resolution via explicit sparsity constraints. Extensive experiments show that LSTR preserves reasoning accuracy and compression efficiency while substantially improving interpretability over dense latent baselines. Causal interventions and trajectory analyses further demonstrate that these sparse features act as both interpretable and causally effective operators in the reasoning process.

</details>


### [381] [Mitigating loss of control in advanced AI systems through instrumental goal trajectories](https://arxiv.org/abs/2602.01699)
*Willem Fourie*

Main category: cs.AI

TL;DR: Researchers propose "instrumental goal trajectories" (IGTs) as organizational pathways (procurement, governance, finance) that AI systems might use to gain resources, offering new monitoring and intervention points beyond technical mitigations.


<details>
  <summary>Details</summary>
Motivation: Current AI safety mitigations are too technical and system-centric, focusing on model properties rather than organizational contexts where AI systems operate and gain resources.

Method: Developed three organizational pathways (procurement, governance, finance IGTs) that produce observable artifacts, enabling monitoring and intervention when AI capabilities exceed acceptable thresholds.

Result: IGTs provide concrete avenues for defining capability levels and broadening corrigibility/interruptibility implementation, shifting focus from model properties to organizational systems.

Conclusion: Organizational instrumental goal trajectories offer practical monitoring and intervention points for AI safety, expanding mitigation options beyond technical approaches to include organizational contexts.

Abstract: Researchers at artificial intelligence labs and universities are concerned that highly capable artificial intelligence (AI) systems may erode human control by pursuing instrumental goals. Existing mitigations remain largely technical and system-centric: tracking capability in advanced systems, shaping behaviour through methods such as reinforcement learning from human feedback, and designing systems to be corrigible and interruptible. Here we develop instrumental goal trajectories to expand these options beyond the model. Gaining capability typically depends on access to additional technical resources, such as compute, storage, data and adjacent services, which in turn requires access to monetary resources. In organisations, these resources can be obtained through three organisational pathways. We label these pathways the procurement, governance and finance instrumental goal trajectories (IGTs). Each IGT produces a trail of organisational artefacts that can be monitored and used as intervention points when a systems capabilities or behaviour exceed acceptable thresholds. In this way, IGTs offer concrete avenues for defining capability levels and for broadening how corrigibility and interruptibility are implemented, shifting attention from model properties alone to the organisational systems that enable them.

</details>


### [382] [Optimizing Prompts for Large Language Models: A Causal Approach](https://arxiv.org/abs/2602.01711)
*Wei Chen,Yanbin Fang,Shuran Fu,Fasheng Xu,Xuan Wei*

Main category: cs.AI

TL;DR: CPO uses causal inference to optimize LLM prompts, isolating prompt effects from query characteristics for cost-efficient, query-specific optimization.


<details>
  <summary>Details</summary>
Motivation: LLM performance is sensitive to prompt design, but current optimization methods either use static instructions that don't adapt to different queries or rely on correlational reward models that confuse prompt effectiveness with query characteristics.

Method: Two-stage framework: 1) Learn offline causal reward model using Double Machine Learning on semantic embeddings to isolate prompt effects from query confounders; 2) Use this unbiased reward to guide efficient search for query-specific prompts without costly online evaluation.

Result: CPO consistently outperforms human-engineered prompts and state-of-the-art automated optimizers across mathematical reasoning, visualization, and data analytics benchmarks, with particular gains on hard queries where other methods deteriorate.

Conclusion: Causal inference provides scalable foundation for reliable, cost-efficient prompt optimization in enterprise LLM deployments by enabling high-precision per-query customization at fraction of inference cost required by online methods.

Abstract: Large Language Models (LLMs) are increasingly embedded in enterprise workflows, yet their performance remains highly sensitive to prompt design. Automatic Prompt Optimization (APO) seeks to mitigate this instability, but existing approaches face two persistent challenges. First, commonly used prompt strategies rely on static instructions that perform well on average but fail to adapt to heterogeneous queries. Second, more dynamic approaches depend on offline reward models that are fundamentally correlational, confounding prompt effectiveness with query characteristics. We propose Causal Prompt Optimization (CPO), a framework that reframes prompt design as a problem of causal estimation. CPO operates in two stages. First, it learns an offline causal reward model by applying Double Machine Learning (DML) to semantic embeddings of prompts and queries, isolating the causal effect of prompt variations from confounding query attributes. Second, it utilizes this unbiased reward signal to guide a resource-efficient search for query-specific prompts without relying on costly online evaluation. We evaluate CPO across benchmarks in mathematical reasoning, visualization, and data analytics. CPO consistently outperforms human-engineered prompts and state-of-the-art automated optimizers. The gains are driven primarily by improved robustness on hard queries, where existing methods tend to deteriorate. Beyond performance, CPO fundamentally reshapes the economics of prompt optimization: by shifting evaluation from real-time model execution to an offline causal model, it enables high-precision, per-query customization at a fraction of the inference cost required by online methods. Together, these results establish causal inference as a scalable foundation for reliable and cost-efficient prompt optimization in enterprise LLM deployments.

</details>


### [383] [MACD: Model-Aware Contrastive Decoding via Counterfactual Data](https://arxiv.org/abs/2602.01740)
*Qixin Xiao,Kun Zhou*

Main category: cs.AI

TL;DR: MACD reduces Video-LLM hallucinations by using model feedback to create targeted counterfactual data at object level, then integrating it into contrastive decoding for evidence-grounded token selection.


<details>
  <summary>Details</summary>
Motivation: Video-LLMs suffer from hallucinations when visual evidence is weak, ambiguous, or biased. Existing methods like contrastive decoding use random perturbations that don't align well with model weaknesses or control visual cues driving hallucinations.

Method: MACD combines model-guided counterfactual construction with decoding. It uses the Video-LLM's own feedback to identify object regions responsible for hallucinations, generates targeted counterfactual inputs at object level (not arbitrary frame/temporal modifications), then integrates these into contrastive decoding to enforce evidence-grounded token selection.

Result: Experiments on EventHallusion, MVBench, Perception-test and Video-MME show MACD consistently reduces hallucination while maintaining or improving task accuracy across diverse Video-LLMs (Qwen and InternVL families). It's especially effective for challenging scenarios with small, occluded, or co-occurring objects.

Conclusion: MACD provides an effective inference strategy that leverages model-aware counterfactual data to mitigate Video-LLM hallucinations, outperforming existing decoding methods by better targeting the visual cues that drive hallucination patterns.

Abstract: Video language models (Video-LLMs) are prone to hallucinations, often generating plausible but ungrounded content when visual evidence is weak, ambiguous, or biased. Existing decoding methods, such as contrastive decoding (CD), rely on random perturbations to construct contrastive data for mitigating hallucination patterns. However, such a way is hard to control the visual cues that drive hallucination or well align with model weaknesses. We propose Model-aware Counterfactual Data based Contrastive Decoding (MACD), a new inference strategy that combines model-guided counterfactual construction with decoding. Our approach uses the Video-LLM's own feedback to identify object regions most responsible for hallucination, generating targeted counterfactual inputs at the object level rather than arbitrary frame or temporal modifications. These model-aware counterfactual data is then integrated into CD to enforce evidence-grounded token selection during decoding. Experiments on EventHallusion, MVBench, Perception-test and Video-MME show that MACD consistently reduces hallucination while maintaining or improving task accuracy across diverse Video-LLMs, including Qwen and InternVL families. The method is especially effective in challenging scenarios involving small, occluded, or co-occurring objects. Our code and data will be publicly released.

</details>


### [384] [Controlling Exploration-Exploitation in GFlowNets via Markov Chain Perspectives](https://arxiv.org/abs/2602.01749)
*Lin Chen,Samuel Drapeau,Fanghao Shao,Xuekai Zhu,Bo Xue,Yunchong Song,Mathieu Laurière,Zhouhan Lin*

Main category: cs.AI

TL;DR: α-GFNs generalize GFlowNets with tunable mixing parameter α to control exploration-exploitation trade-off, achieving up to 10× more discovered modes across benchmarks.


<details>
  <summary>Details</summary>
Motivation: Standard GFlowNet objectives fix equal mixing of forward/backward policies, constraining exploration-exploitation trade-off during training and limiting mode discovery capabilities.

Method: Established equivalence between GFlowNet objectives and Markov chain reversibility, then proposed α-GFNs with tunable parameter α to generalize the mixing, enabling direct control over exploration-exploitation dynamics while ensuring convergence to unique flows.

Result: α-GFNs consistently outperform previous GFlowNet objectives across Set, Bit Sequence, and Molecule Generation benchmarks, achieving up to 10× increase in number of discovered modes.

Conclusion: The theoretical link between GFlowNets and Markov chains enables adaptive control of exploration-exploitation dynamics, significantly improving mode discovery capabilities in generative modeling tasks.

Abstract: Generative Flow Network (GFlowNet) objectives implicitly fix an equal mixing of forward and backward policies, potentially constraining the exploration-exploitation trade-off during training. By further exploring the link between GFlowNets and Markov chains, we establish an equivalence between GFlowNet objectives and Markov chain reversibility, thereby revealing the origin of such constraints, and provide a framework for adapting Markov chain properties to GFlowNets. Building on these theoretical findings, we propose $α$-GFNs, which generalize the mixing via a tunable parameter $α$. This generalization enables direct control over exploration-exploitation dynamics to enhance mode discovery capabilities, while ensuring convergence to unique flows. Across various benchmarks, including Set, Bit Sequence, and Molecule Generation, $α$-GFN objectives consistently outperform previous GFlowNet objectives, achieving up to a $10 \times$ increase in the number of discovered modes.

</details>


### [385] [Adversarial Reward Auditing for Active Detection and Mitigation of Reward Hacking](https://arxiv.org/abs/2602.01750)
*Mohammad Beigi,Ming Jin,Junshan Zhang,Qifan Wang,Lifu Huang*

Main category: cs.AI

TL;DR: ARA is a two-stage framework that treats reward hacking as a competitive game: Hacker finds vulnerabilities, Auditor detects exploitation, and AG-RLHF gates rewards to penalize hacking, achieving better alignment-utility tradeoffs across multiple domains.


<details>
  <summary>Details</summary>
Motivation: RLHF is vulnerable to reward hacking where models exploit spurious correlations in reward models to achieve high scores while violating human intent. Existing static defenses cannot adapt to novel exploitation strategies.

Method: Two-stage framework: 1) Hacker policy discovers reward model vulnerabilities while Auditor learns to detect exploitation from latent representations; 2) Auditor-Guided RLHF (AG-RLHF) gates reward signals to penalize detected hacking, transforming hacking into a measurable signal.

Result: ARA achieves best alignment-utility tradeoff across three hacking scenarios: reduces sycophancy to near-SFT levels while improving helpfulness, decreases verbosity while achieving highest ROUGE-L, and suppresses code gaming while improving Pass@1. Shows cross-domain generalization.

Conclusion: Reward hacking can be effectively addressed by reframing it as a dynamic competitive game. ARA enables measurable detection and mitigation that generalizes across domains, allowing efficient multi-domain defense with single models.

Abstract: Reinforcement Learning from Human Feedback (RLHF) remains vulnerable to reward hacking, where models exploit spurious correlations in learned reward models to achieve high scores while violating human intent. Existing mitigations rely on static defenses that cannot adapt to novel exploitation strategies. We propose Adversarial Reward Auditing (ARA), a framework that reconceptualizes reward hacking as a dynamic, competitive game. ARA operates in two stages: first, a Hacker policy discovers reward model vulnerabilities while an Auditor learns to detect exploitation from latent representations; second, Auditor-Guided RLHF (AG-RLHF) gates reward signals to penalize detected hacking, transforming reward hacking from an unobservable failure into a measurable, controllable signal. Experiments across three hacking scenarios demonstrate that ARA achieves the best alignment-utility tradeoff among all baselines: reducing sycophancy to near-SFT levels while improving helpfulness, decreasing verbosity while achieving the highest ROUGE-L, and suppressing code gaming while improving Pass@1. Beyond single-domain evaluation, we show that reward hacking, detection, and mitigation all generalize across domains -- a Hacker trained on code gaming exhibits increased sycophancy despite no reward for this behavior, and an Auditor trained on one domain effectively suppresses exploitation in others, enabling efficient multi-domain defense with a single model.

</details>


### [386] [PRISM: Parametrically Refactoring Inference for Speculative Sampling Draft Models](https://arxiv.org/abs/2602.01762)
*Xuliang Wang,Yuetao Chen,Maochan Zhen,Fang Liu,Xinzhou Zheng,Xingwu Liu,Hong Xu,Ming Li*

Main category: cs.AI

TL;DR: PRISM is a novel speculative decoding architecture that disaggregates computation across parameter sets to decouple model capacity from inference cost, achieving superior speedup while maintaining high acceptance rates.


<details>
  <summary>Details</summary>
Motivation: Current speculative decoding methods face a fundamental trade-off: larger draft models improve prediction accuracy but introduce substantial computational overhead, while smaller models reduce latency but sacrifice quality. Existing approaches struggle to balance this accuracy-latency dilemma.

Method: PRISM disaggregates the computation of each predictive step across different parameter sets, refactoring computational pathways to decouple model capacity from inference cost. This architectural innovation allows using larger, more accurate draft models without proportional latency increases.

Result: PRISM outperforms all existing draft architectures, achieving exceptional acceptance lengths while maintaining minimal draft latency. It boosts decoding throughput of an optimized inference engine by more than 2.6x and scales more effectively with expanding data volumes than other architectures.

Conclusion: PRISM successfully addresses the fundamental trade-off in speculative decoding through architectural innovation, enabling the use of larger, more accurate draft models without proportional computational cost increases, leading to superior end-to-end speedup and better scaling properties.

Abstract: Large Language Models (LLMs), constrained by their auto-regressive nature, suffer from slow decoding. Speculative decoding methods have emerged as a promising solution to accelerate LLM decoding, attracting attention from both systems and AI research communities. Recently, the pursuit of better draft quality has driven a trend toward parametrically larger draft models, which inevitably introduces substantial computational overhead. While existing work attempts to balance the trade-off between prediction accuracy and compute latency, we address this fundamental dilemma through architectural innovation.
  We propose PRISM, which disaggregates the computation of each predictive step across different parameter sets, refactoring the computational pathways of draft models to successfully decouple model capacity from inference cost. Through extensive experiments, we demonstrate that PRISM outperforms all existing draft architectures, achieving exceptional acceptance lengths while maintaining minimal draft latency for superior end-to-end speedup. We also re-examine scaling laws with PRISM, revealing that PRISM scales more effectively with expanding data volumes than other draft architectures. Through rigorous and fair comparison, we show that PRISM boosts the decoding throughput of an already highly optimized inference engine by more than 2.6x.

</details>


### [387] [Efficient Cross-Architecture Knowledge Transfer for Large-Scale Online User Response Prediction](https://arxiv.org/abs/2602.01775)
*Yucheng Wu,Yuekui Yang,Hongzheng Li,Anan Liu,Jian Xiao,Junjie Zhai,Huan Yu,Shaoping Ma,Leye Wang*

Main category: cs.AI

TL;DR: CrossAdapt enables efficient cross-architecture knowledge transfer for large-scale recommendation systems, reducing training time by 43-71% while improving AUC by 0.27-0.43%.


<details>
  <summary>Details</summary>
Motivation: Deploying new architectures in large-scale user response prediction systems faces high switching costs due to expensive retraining on massive historical data and performance degradation under data retention constraints. Existing knowledge distillation methods struggle with architectural heterogeneity and prohibitive embedding table transfer costs.

Method: Two-stage framework: 1) Offline stage uses dimension-adaptive projections for rapid embedding transfer without iterative training, combined with progressive network distillation and strategic sampling. 2) Online stage introduces asymmetric co-distillation (students update frequently, teachers update infrequently) with distribution-aware adaptation mechanism that dynamically balances historical knowledge preservation and fast adaptation to evolving data.

Result: On three public datasets: 0.27-0.43% AUC improvements with 43-71% training time reduction. Large-scale deployment on Tencent WeChat Channels (~10M daily samples) significantly mitigates AUC degradation, LogLoss increase, and prediction bias compared to standard distillation baselines.

Conclusion: CrossAdapt provides an effective solution for efficient cross-architecture knowledge transfer in large-scale recommendation systems, addressing both computational efficiency and performance preservation challenges during model architecture transitions.

Abstract: Deploying new architectures in large-scale user response prediction systems incurs high model switching costs due to expensive retraining on massive historical data and performance degradation under data retention constraints. Existing knowledge distillation methods struggle with architectural heterogeneity and the prohibitive cost of transferring large embedding tables. We propose CrossAdapt, a two-stage framework for efficient cross-architecture knowledge transfer. The offline stage enables rapid embedding transfer via dimension-adaptive projections without iterative training, combined with progressive network distillation and strategic sampling to reduce computational cost. The online stage introduces asymmetric co-distillation, where students update frequently while teachers update infrequently, together with a distribution-aware adaptation mechanism that dynamically balances historical knowledge preservation and fast adaptation to evolving data. Experiments on three public datasets show that CrossAdapt achieves 0.27-0.43% AUC improvements while reducing training time by 43-71%. Large-scale deployment on Tencent WeChat Channels (~10M daily samples) further demonstrates its effectiveness, significantly mitigating AUC degradation, LogLoss increase, and prediction bias compared to standard distillation baselines.

</details>


### [388] [LingLanMiDian: Systematic Evaluation of LLMs on TCM Knowledge and Clinical Reasoning](https://arxiv.org/abs/2602.01779)
*Rui Hua,Yu Wei,Zixin Shu,Kai Chang,Dengying Yan,Jianan Xia,Zeyu Liu,Hui Zhu,Shujie Song,Mingzhong Xiao,Xiaodong Li,Dongmei Jia,Zhuye Gao,Yanyan Meng,Naixuan Zhao,Yu Fu,Haibin Yu,Benman Yu,Yuanyuan Chen,Fei Dong,Zhizhou Meng,Pengcheng Yang,Songxue Zhao,Lijuan Pei,Yunhui Hu,Kan Ding,Jiayuan Duan,Wenmao Yin,Yang Gu,Runshun Zhang,Qiang Zhu,Jian Yu,Jiansheng Li,Baoyan Liu,Wenjia Wang,Xuezhong Zhou*

Main category: cs.AI

TL;DR: LingLanMiDian (LingLan) is a comprehensive TCM benchmark that unifies evaluation across knowledge recall, reasoning, extraction, and clinical decision-making with consistent metrics and expert-curated data.


<details>
  <summary>Details</summary>
Motivation: Existing TCM benchmarks are fragmented in coverage and scale, with non-unified or generation-heavy scoring that hinders fair comparison of LLMs in Traditional Chinese Medicine, which has distinctive ontology, terminology, and reasoning patterns.

Method: Created LingLan benchmark with expert-curated multi-task suite, consistent metric design, synonym-tolerant protocol for clinical labels, 400-item Hard subset per dataset, and reframed diagnosis/treatment recommendation into single-choice decision recognition.

Result: Zero-shot evaluation of 14 leading LLMs revealed substantial gap between current models and human experts in TCM-specialized reasoning, especially on Hard subset, while providing unified perspective on their strengths/limitations in TCM understanding.

Conclusion: LingLan establishes unified, quantitative, extensible foundation for advancing TCM LLMs and domain-specific medical AI research by bridging fundamental knowledge and applied reasoning through standardized evaluation.

Abstract: Large language models (LLMs) are advancing rapidly in medical NLP, yet Traditional Chinese Medicine (TCM) with its distinctive ontology, terminology, and reasoning patterns requires domain-faithful evaluation. Existing TCM benchmarks are fragmented in coverage and scale and rely on non-unified or generation-heavy scoring that hinders fair comparison. We present the LingLanMiDian (LingLan) benchmark, a large-scale, expert-curated, multi-task suite that unifies evaluation across knowledge recall, multi-hop reasoning, information extraction, and real-world clinical decision-making. LingLan introduces a consistent metric design, a synonym-tolerant protocol for clinical labels, a per-dataset 400-item Hard subset, and a reframing of diagnosis and treatment recommendation into single-choice decision recognition. We conduct comprehensive, zero-shot evaluations on 14 leading open-source and proprietary LLMs, providing a unified perspective on their strengths and limitations in TCM commonsense knowledge understanding, reasoning, and clinical decision support; critically, the evaluation on Hard subset reveals a substantial gap between current models and human experts in TCM-specialized reasoning. By bridging fundamental knowledge and applied reasoning through standardized evaluation, LingLan establishes a unified, quantitative, and extensible foundation for advancing TCM LLMs and domain-specific medical AI research. All evaluation data and code are available at https://github.com/TCMAI-BJTU/LingLan and http://tcmnlp.com.

</details>


### [389] [ORCH: many analyses, one merge-a deterministic multi-agent orchestrator for discrete-choice reasoning with EMA-guided routing](https://arxiv.org/abs/2602.01797)
*Hanlin Zhou,Huah Yong Chan*

Main category: cs.AI

TL;DR: ORCH is a deterministic multi-agent framework that coordinates heterogeneous LLMs for discrete-choice reasoning tasks, using fixed rules for task decomposition and answer aggregation to ensure reproducibility and interpretability.


<details>
  <summary>Details</summary>
Motivation: Existing multi-agent systems for reasoning tasks often rely on stochastic routing or ad-hoc heuristics, making them difficult to reproduce and interpret. There's a need for a more predictable, controllable framework that can effectively coordinate different LLMs while maintaining transparency.

Method: ORCH follows a "many analyses, one decision" paradigm: multiple base models independently produce structured analyses, and a dedicated merge agent outputs the final choice. The framework uses fixed rules for task decomposition and answer aggregation. An optional EMA-guided router updates agent selection using historical performance metrics (accuracy, latency, cost) based on answer-based feedback.

Result: On MMLU, MMLU-Pro, and GSM8K, ORCH consistently outperforms single-model baselines and majority-vote ensembles. On MMLU-Pro, it improves accuracy by over 10 points compared to the strongest baseline, and on GSM8K gains exceed 50 points. The EMA router provides an additional 0.7-2.0 point accuracy boost. McNemar tests confirm statistical significance.

Conclusion: ORCH offers a practical path toward controllable, interpretable, and deployment-ready LLM-based agent systems for discrete-choice reasoning, providing deterministic coordination that outperforms existing approaches while maintaining reproducibility and transparency.

Abstract: Recent advances in large-scale language models (LLMs) have made multi-agent architectures attractive for challenging reasoning tasks. However, many existing systems rely on stochastic routing or ad-hoc heuristics, making their behavior difficult to reproduce and their decision process hard to interpret. We propose ORCH, a deterministic coordination framework for discrete-choice reasoning that orchestrates heterogeneous LLMs. ORCH follows a ``many analyses, one decision'' paradigm: multiple base models independently produce structured analyses, and a dedicated merge agent outputs the final choice. The framework uses fixed rules for task decomposition and answer aggregation, keeping the pipeline predictable, reproducible, and training-free. Determinism here refers to fixed routing and aggregation rules under a fixed evaluation protocol, rather than strict bit-level reproducibility across deployments. To exploit model complementarity, we optionally introduce an EMA-guided router that updates agent selection using historical accuracy, latency, or cost; since it relies on answer-based feedback, it is mainly intended for benchmarking, controlled evaluation, or delayed-feedback settings. Experiments on MMLU, MMLU-Pro, and GSM8K show that ORCH consistently outperforms single-model baselines and a majority-vote ensemble. On MMLU-Pro, ORCH improves accuracy by over 10 points compared to the strongest baseline, and on GSM8K it yields gains exceeding 50 points; McNemar tests confirm statistical significance. The EMA router provides an additional 0.7--2.0 point accuracy boost, and ablations show that both multi-agent collaboration and routing contribute substantially. Overall, ORCH offers a practical path toward controllable, interpretable, and deployment-ready LLM-based agent systems for discrete-choice reasoning.

</details>


### [390] [INDIBATOR: Diverse and Fact-Grounded Individuality for Multi-Agent Debate in Molecular Discovery](https://arxiv.org/abs/2602.01815)
*Yunhui Jang,Seonghyun Park,Jaehyung Kim,Sungsoo Ahn*

Main category: cs.AI

TL;DR: INDIBATOR is a multi-agent framework for molecular discovery that grounds agents in individualized scientist profiles from publication and molecular history, outperforming coarse-grained persona systems.


<details>
  <summary>Details</summary>
Motivation: Current multi-agent systems for scientific discovery use generic role-based or keyword-based personas that oversimplify how real scientists operate, whose contributions are shaped by unique research trajectories and expertise.

Method: Propose INDIBATOR framework that grounds agents in individualized scientist profiles constructed from two modalities: publication history for literature knowledge and molecular history for structural priors. Agents engage in multi-turn debate through proposal, critique, and voting phases.

Result: Fine-grained individuality-grounded agents consistently outperform systems relying on coarse-grained personas, achieving competitive or state-of-the-art performance in molecular discovery tasks.

Conclusion: Capturing the "scientific DNA" of individual agents through detailed research trajectories is essential for high-quality scientific discovery in multi-agent systems.

Abstract: Multi-agent systems have emerged as a powerful paradigm for automating scientific discovery. To differentiate agent behavior in the multi-agent system, current frameworks typically assign generic role-based personas such as ''reviewer'' or ''writer'' or rely on coarse grained keyword-based personas. While functional, this approach oversimplifies how human scientists operate, whose contributions are shaped by their unique research trajectories. In response, we propose INDIBATOR, a framework for molecular discovery that grounds agents in individualized scientist profiles constructed from two modalities: publication history for literature-derived knowledge and molecular history for structural priors. These agents engage in multi-turn debate through proposal, critique, and voting phases. Our evaluation demonstrates that these fine-grained individuality-grounded agents consistently outperform systems relying on coarse-grained personas, achieving competitive or state-of-the-art performance. These results validate that capturing the ``scientific DNA'' of individual agents is essential for high-quality discovery.

</details>


### [391] [Synesthesia of Vehicles: Tactile Data Synthesis from Visual Inputs](https://arxiv.org/abs/2602.01832)
*Rui Wang,Yaoguang Cao,Yuyi Chen,Jianyi Xu,Zhuoyang Li,Jiachen Shang,Shichun Yang*

Main category: cs.AI

TL;DR: Proposes Synesthesia of Vehicles (SoV) framework to predict tactile excitations from visual inputs for autonomous vehicles using cross-modal alignment and latent diffusion for tactile data synthesis.


<details>
  <summary>Details</summary>
Motivation: Current AV sensors (visual/optical) fail to detect road-induced excitations critical for dynamic vehicle control, creating a safety gap that needs tactile perception capabilities.

Method: 1) Cross-modal spatiotemporal alignment to address temporal/spatial disparities; 2) Visual-tactile synesthetic (VTSyn) generative model using latent diffusion for unsupervised high-quality tactile data synthesis; 3) Real-vehicle perception system collecting multi-modal dataset across diverse conditions.

Result: VTSyn outperforms existing models in temporal, frequency, and classification performance metrics, demonstrating superior tactile data synthesis capabilities.

Conclusion: The proposed framework enhances AV safety through proactive tactile perception by enabling vehicles to predict road-induced excitations from visual inputs, bridging a critical sensory gap.

Abstract: Autonomous vehicles (AVs) rely on multi-modal fusion for safety, but current visual and optical sensors fail to detect road-induced excitations which are critical for vehicles' dynamic control. Inspired by human synesthesia, we propose the Synesthesia of Vehicles (SoV), a novel framework to predict tactile excitations from visual inputs for autonomous vehicles. We develop a cross-modal spatiotemporal alignment method to address temporal and spatial disparities. Furthermore, a visual-tactile synesthetic (VTSyn) generative model using latent diffusion is proposed for unsupervised high-quality tactile data synthesis. A real-vehicle perception system collected a multi-modal dataset across diverse road and lighting conditions. Extensive experiments show that VTSyn outperforms existing models in temporal, frequency, and classification performance, enhancing AV safety through proactive tactile perception.

</details>


### [392] [ROMA: Recursive Open Meta-Agent Framework for Long-Horizon Multi-Agent Systems](https://arxiv.org/abs/2602.01848)
*Salaheddin Alzu'bi,Baran Nama,Arda Kaz,Anushri Eswaran,Weiyuan Chen,Sarvesh Khetan,Rishab Bala,Tu Vu,Sewoong Oh*

Main category: cs.AI

TL;DR: ROMA is a recursive agent framework that improves long-horizon task performance through parallel task decomposition and structured aggregation, with GEPA+ for prompt optimization.


<details>
  <summary>Details</summary>
Motivation: Current agent frameworks fail on long-horizon tasks due to brittle sequential orchestration, context window limitations, and opaque execution traces that make debugging difficult.

Method: ROMA uses recursive task decomposition into dependency-aware subtask trees executed in parallel, with structured aggregation to control context growth. It standardizes agent roles (Atomizer, Planner, Executor, Aggregator) and includes GEPA+ for prompt optimization.

Result: ROMA+GEPA+ achieves state-of-the-art performance: 9.9% accuracy improvement on SEAL-0 over Kimi-Researcher, and enables DeepSeek-V3 to match Claude Sonnet 4.5 on EQ-Bench.

Conclusion: Recursive modular agent architectures can scale reasoning depth while maintaining interpretability, flexibility, and model-agnostic properties for long-horizon tasks.

Abstract: Current agentic frameworks underperform on long-horizon tasks. As reasoning depth increases, sequential orchestration becomes brittle, context windows impose hard limits that degrade performance, and opaque execution traces make failures difficult to localize or debug. We introduce ROMA (Recursive Open Meta-Agents), a domain-agnostic framework that addresses these limitations through recursive task decomposition and structured aggregation. ROMA decomposes goals into dependency-aware subtask trees that can be executed in parallel, while aggregation compresses and validates intermediate results to control context growth. Our framework standardizes agent construction around four modular roles --Atomizer (which decides whether a task should be decomposed), Planner, Executor, and Aggregator -- which cleanly separate orchestration from model selection and enable transparent, hierarchical execution traces. This design supports heterogeneous multi-agent systems that mix models and tools according to cost, latency, and capability. To adapt ROMA to specific tasks without fine-tuning, we further introduce GEPA$+$, an improved Genetic-Pareto prompt proposer that searches over prompts within ROMA's component hierarchy while preserving interface contracts. We show that ROMA, combined with GEPA+, delivers leading system-level performance on reasoning and long-form generation benchmarks. On SEAL-0, which evaluates reasoning over conflicting web evidence, ROMA instantiated with GLM-4.6 improves accuracy by 9.9\% over Kimi-Researcher. On EQ-Bench, a long-form writing benchmark, ROMA enables DeepSeek-V3 to match the performance of leading closed-source models such as Claude Sonnet 4.5. Our results demonstrate that recursive, modular agent architectures can scale reasoning depth while remaining interpretable, flexible, and model-agnostic.

</details>


### [393] [SOPRAG: Multi-view Graph Experts Retrieval for Industrial Standard Operating Procedures](https://arxiv.org/abs/2602.01858)
*Liangtao Lin,Zhaomeng Zhu,Tianwei Zhang,Yonggang Wen*

Main category: cs.AI

TL;DR: SOPRAG is a novel RAG framework using Mixture-of-Experts approach with specialized graph experts and LLM-guided gating to improve industrial SOP retrieval and execution.


<details>
  <summary>Details</summary>
Motivation: Standard RAG paradigms fail to address unique challenges in industrial SOP retrieval: rigid proprietary structures, condition-dependent relevance, and actionable execution requirements.

Method: Proposes SOPRAG framework with specialized Entity, Causal, and Flow graph experts instead of flat chunking. Includes Procedure Card layer for search space pruning and LLM-Guided gating mechanism for dynamic expert weighting. Also introduces automated multi-agent workflow for benchmark construction.

Result: Extensive experiments across four industrial domains show SOPRAG significantly outperforms lexical, dense, and graph-based RAG baselines in both retrieval accuracy and response utility, achieving perfect execution scores in real-world critical tasks.

Conclusion: SOPRAG effectively addresses industrial SOP retrieval challenges through specialized expert design and intelligent coordination mechanisms, demonstrating superior performance in practical applications.

Abstract: Standard Operating Procedures (SOPs) are essential for ensuring operational safety and consistency in industrial environments. However, retrieving and following these procedures presents unique challenges, such as rigid proprietary structures, condition-dependent relevance, and actionable execution requirement, which standard semantic-driven Retrieval-Augmented Generation (RAG) paradigms fail to address. Inspired by the Mixture-of-Experts (MoE) paradigm, we propose SOPRAG, a novel framework specifically designed to address the above pain points in SOP retrieval. SOPRAG replaces flat chunking with specialized Entity, Causal, and Flow graph experts to resolve industrial structural and logical complexities. To optimize and coordinate these experts, we propose a Procedure Card layer that prunes the search space to eliminate computational noise, and an LLM-Guided gating mechanism that dynamically weights these experts to align retrieval with operator intent. To address the scarcity of domain-specific data, we also introduce an automated, multi-agent workflow for benchmark construction. Extensive experiments across four industrial domains demonstrate that SOPRAG significantly outperforms strong lexical, dense, and graph-based RAG baselines in both retrieval accuracy and response utility, achieving perfect execution scores in real-world critical tasks.

</details>


### [394] [ProcMEM: Learning Reusable Procedural Memory from Experience via Non-Parametric PPO for LLM Agents](https://arxiv.org/abs/2602.01869)
*Qirui Mi,Zhijian Ma,Mengyue Yang,Haoxuan Li,Yisen Wang,Haifeng Zhang,Jun Wang*

Main category: cs.AI

TL;DR: ProcMEM enables LLM agents to autonomously learn procedural memory from experiences without parameter updates, improving computational efficiency and execution stability through reusable skills.


<details>
  <summary>Details</summary>
Motivation: Current LLM-driven agents rely on on-the-fly reasoning for sequential decision-making, leading to computational redundancy and execution instability due to insufficient experience reuse in recurring scenarios.

Method: ProcMEM formalizes a Skill-MDP to transform episodic narratives into executable Skills with activation/execution/termination conditions. It uses Non-Parametric PPO with semantic gradients for candidate generation and PPO Gate for skill verification, plus score-based maintenance for compact memory.

Result: ProcMEM achieves superior reuse rates and significant performance gains with extreme memory compression across in-domain, cross-task, and cross-agent scenarios, while transparently accumulating and refining procedural knowledge.

Conclusion: ProcMEM successfully bridges the experience reuse gap in LLM agents by enabling autonomous procedural memory learning without parameter updates, facilitating long-term autonomy through transparent knowledge accumulation and refinement.

Abstract: LLM-driven agents demonstrate strong performance in sequential decision-making but often rely on on-the-fly reasoning, re-deriving solutions even in recurring scenarios. This insufficient experience reuse leads to computational redundancy and execution instability. To bridge this gap, we propose ProcMEM, a framework that enables agents to autonomously learn procedural memory from interaction experiences without parameter updates. By formalizing a Skill-MDP, ProcMEM transforms passive episodic narratives into executable Skills defined by activation, execution, and termination conditions to ensure executability. To achieve reliable reusability without capability degradation, we introduce Non-Parametric PPO, which leverages semantic gradients for high-quality candidate generation and a PPO Gate for robust Skill verification. Through score-based maintenance, ProcMEM sustains compact, high-quality procedural memory. Experimental results across in-domain, cross-task, and cross-agent scenarios demonstrate that ProcMEM achieves superior reuse rates and significant performance gains with extreme memory compression. Visualized evolutionary trajectories and Skill distributions further reveal how ProcMEM transparently accumulates, refines, and reuses procedural knowledge to facilitate long-term autonomy.

</details>


### [395] [Entropy-Guided Data-Efficient Training for Multimodal Reasoning Reward Models](https://arxiv.org/abs/2602.01884)
*Shidong Yang,Tongwen Huang,Hao Wen,Yong Wang,Li Chen,Xiangxiang Chu*

Main category: cs.AI

TL;DR: EGT uses response entropy as an unsupervised proxy for noise and difficulty to improve multimodal reward model training through data curation and progressive learning.


<details>
  <summary>Details</summary>
Motivation: Multimodal reward models need better alignment with human preferences, but current training suffers from noisy preference datasets and inefficient training methods that ignore sample difficulty differences.

Method: Proposes Entropy-Guided Training (EGT) with two strategies: 1) entropy-guided data curation to filter unreliable samples, and 2) entropy-guided progressive training that introduces more complex examples based on entropy levels.

Result: Extensive experiments across three benchmarks show EGT-trained models consistently outperform state-of-the-art multimodal reward models.

Conclusion: Response entropy serves as a reliable unsupervised proxy for annotation noise and sample difficulty, enabling more effective training of multimodal reasoning reward models through the proposed EGT approach.

Abstract: Multimodal reward models are crucial for aligning multimodal large language models with human preferences. Recent works have incorporated reasoning capabilities into these models, achieving promising results. However, training these models suffers from two critical challenges: (1) the inherent noise in preference datasets, which degrades model performance, and (2) the inefficiency of conventional training methods, which ignore the differences in sample difficulty. In this paper, we identify a strong correlation between response entropy and accuracy, indicating that entropy can serve as a reliable and unsupervised proxy for annotation noise and sample difficulty. Based on this insight, we propose a novel Entropy-Guided Training (EGT) approach for multimodal reasoning reward models, which combines two strategies: (1) entropy-guided data curation to mitigate the impact of unreliable samples, and (2) an entropy-guided training strategy that progressively introduces more complex examples. Extensive experiments across three benchmarks show that the EGT-trained model consistently outperforms state-of-the-art multimodal reward models.

</details>


### [396] [Geometric Analysis of Token Selection in Multi-Head Attention](https://arxiv.org/abs/2602.01893)
*Timur Mudarisov,Mikhal Burtsev,Tatiana Petrova,Radu State*

Main category: cs.AI

TL;DR: The paper presents a geometric framework for analyzing multi-head attention in LLMs, viewing attention as top-N token selection in value-state space and defining geometric metrics to quantify separability between selected and non-selected tokens.


<details>
  <summary>Details</summary>
Motivation: To develop a geometric understanding of how attention mechanisms work in LLMs, moving beyond traditional attention weight analysis to study token selection behavior directly in value-state space for better interpretability and insights into attention head specialization.

Method: View standard attention through a top-N selection lens, define geometric metrics (Precision, Recall, F-score) to quantify separability, derive non-asymptotic bounds with explicit dependence on dimension and margin under empirical assumptions, and validate across multiple LLMs (LLaMA-2-7B, Gemma-7B, Mistral-7B).

Result: Theory predicts a small-N operating regime of strongest non-trivial separability, empirical measurements closely track theoretical envelopes, and discovery of three head specialization regimes in LLaMA-2-7B (Retriever, Mixer, Reset) with distinct geometric signatures.

Conclusion: Attention behaves as a structured geometric classifier with measurable criteria for token selection, offering head-level interpretability and informing geometry-aware sparsification and design of attention mechanisms in LLMs.

Abstract: We present a geometric framework for analysing multi-head attention in large language models (LLMs). Without altering the mechanism, we view standard attention through a top-N selection lens and study its behaviour directly in value-state space. We define geometric metrics - Precision, Recall, and F-score - to quantify separability between selected and non-selected tokens, and derive non-asymptotic bounds with explicit dependence on dimension and margin under empirically motivated assumptions (stable value norms with a compressed sink token, exponential similarity decay, and piecewise attention weight profiles). The theory predicts a small-N operating regime of strongest non-trivial separability and clarifies how sequence length and sink similarity shape the metrics. Empirically, across LLaMA-2-7B, Gemma-7B, and Mistral-7B, measurements closely track the theoretical envelopes: top-N selection sharpens separability, sink similarity correlates with Recall. We also found that in LLaMA-2-7B heads specialize into three regimes - Retriever, Mixer, Reset - with distinct geometric signatures. Overall, attention behaves as a structured geometric classifier with measurable criteria for token selection, offering head level interpretability and informing geometry-aware sparsification and design of attention in LLMs.

</details>


### [397] [DomusFM: A Foundation Model for Smart-Home Sensor Data](https://arxiv.org/abs/2602.01910)
*Michele Fiori,Gabriele Civitarese,Flora D. Salim,Claudio Bettini*

Main category: cs.AI

TL;DR: DomusFM is the first foundation model for smart-home sensor data that uses self-supervised dual contrastive learning to overcome data scarcity and privacy/cost issues of existing approaches.


<details>
  <summary>Details</summary>
Motivation: Smart-home sensor data has great potential for healthcare and assistive tech, but existing approaches have critical limitations: supervised models need too much labeled data, foundation models only work with inertial sensors (not smart-home binary sensors), and LLM-based approaches have privacy/cost issues and require natural language descriptions.

Method: DomusFM uses self-supervised dual contrastive learning to capture both token-level semantic attributes and sequence-level temporal dependencies. It integrates semantic embeddings from a lightweight language model with specialized encoders for temporal patterns and binary states.

Result: DomusFM outperforms state-of-the-art baselines on different downstream tasks across seven public smart-home datasets, achieving superior performance even with only 5% of labeled training data for fine-tuning.

Conclusion: DomusFM addresses data scarcity while maintaining practical deployability for real-world smart-home systems, offering the first foundation model specifically designed for smart-home sensor data.

Abstract: Smart-home sensor data holds significant potential for several applications, including healthcare monitoring and assistive technologies. Existing approaches, however, face critical limitations. Supervised models require impractical amounts of labeled data. Foundation models for activity recognition focus only on inertial sensors, failing to address the unique characteristics of smart-home binary sensor events: their sparse, discrete nature combined with rich semantic associations. LLM-based approaches, while tested in this domain, still raise several issues regarding the need for natural language descriptions or prompting, and reliance on either external services or expensive hardware, making them infeasible in real-life scenarios due to privacy and cost concerns. We introduce DomusFM, the first foundation model specifically designed and pretrained for smart-home sensor data. DomusFM employs a self-supervised dual contrastive learning paradigm to capture both token-level semantic attributes and sequence-level temporal dependencies. By integrating semantic embeddings from a lightweight language model and specialized encoders for temporal patterns and binary states, DomusFM learns generalizable representations that transfer across environments and tasks related to activity and event analysis. Through leave-one-dataset-out evaluation across seven public smart-home datasets, we demonstrate that DomusFM outperforms state-of-the-art baselines on different downstream tasks, achieving superior performance even with only 5% of labeled training data available for fine-tuning. Our approach addresses data scarcity while maintaining practical deployability for real-world smart-home systems.

</details>


### [398] [Large Language Model and Formal Concept Analysis: a comparative study for Topic Modeling](https://arxiv.org/abs/2602.01933)
*Fabrice Boissier,Monica Sen,Irina Rychkova*

Main category: cs.AI

TL;DR: Comparison of Large Language Models (GPT-5) and Formal Concept Analysis for topic modeling, with experiments on teaching materials and research articles.


<details>
  <summary>Details</summary>
Motivation: Few works study LLMs for topic modeling, and while FCA has been presented as a candidate, no real applied case studies have been conducted. The paper aims to compare LLM and FCA to understand their strengths and weaknesses in topic modeling.

Method: FCA is evaluated through CREA pipeline, while GPT-5 is used with a three-prompt zero-shot strategy: topic generation from document batches, merging batch results into final topics, and topic labeling. Two experiments: one reusing teaching materials previously used to evaluate CREA, and another analyzing 40 research articles in information systems.

Result: The paper compares topics extracted by both methods with underlying subfields in information systems research, though specific results aren't detailed in the abstract.

Conclusion: The study provides a comparative analysis of LLM and FCA approaches for topic modeling, highlighting their respective strengths and weaknesses through applied case studies.

Abstract: Topic modeling is a research field finding increasing applications: historically from document retrieving, to sentiment analysis and text summarization. Large Language Models (LLM) are currently a major trend in text processing, but few works study their usefulness for this task. Formal Concept Analysis (FCA) has recently been presented as a candidate for topic modeling, but no real applied case study has been conducted. In this work, we compare LLM and FCA to better understand their strengths and weakneses in the topic modeling field. FCA is evaluated through the CREA pipeline used in past experiments on topic modeling and visualization, whereas GPT-5 is used for the LLM. A strategy based on three prompts is applied with GPT-5 in a zero-shot setup: topic generation from document batches, merging of batch results into final topics, and topic labeling. A first experiment reuses the teaching materials previously used to evaluate CREA, while a second experiment analyzes 40 research articles in information systems to compare the extracted topics with the underling subfields.

</details>


### [399] [Small Generalizable Prompt Predictive Models Can Steer Efficient RL Post-Training of Large Reasoning Models](https://arxiv.org/abs/2602.01970)
*Yun Qu,Qi Wang,Yixiu Mao,Heming Zou,Yuhang Jiang,Weijie Liu,Clive Bai,Kai Yang,Yangkun Chen,Saiyong Yang,Xiangyang Ji*

Main category: cs.AI

TL;DR: GPS uses Bayesian inference with a lightweight generative model to select informative prompts for efficient RL training, improving both training and test-time efficiency.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning for LLMs is computationally expensive due to rollout-intensive optimization. Current prompt selection methods are either costly or lack generalization across prompts.

Method: Generalizable Predictive Prompt Selection (GPS) uses Bayesian inference with a lightweight generative model trained on shared optimization history, incorporating intermediate-difficulty prioritization and history-anchored diversity for batch selection.

Result: GPS shows substantial improvements in training efficiency, final performance, and test-time efficiency across varied reasoning benchmarks compared to superior baseline methods.

Conclusion: GPS provides an effective solution for efficient prompt selection in RL for LLMs, balancing computational efficiency with performance improvements through generalizable predictive modeling.

Abstract: Reinforcement learning enhances the reasoning capabilities of large language models but often involves high computational costs due to rollout-intensive optimization. Online prompt selection presents a plausible solution by prioritizing informative prompts to improve training efficiency. However, current methods either depend on costly, exact evaluations or construct prompt-specific predictive models lacking generalization across prompts. This study introduces Generalizable Predictive Prompt Selection (GPS), which performs Bayesian inference towards prompt difficulty using a lightweight generative model trained on the shared optimization history. Intermediate-difficulty prioritization and history-anchored diversity are incorporated into the batch acquisition principle to select informative prompt batches. The small predictive model also generalizes at test-time for efficient computational allocation. Experiments across varied reasoning benchmarks indicate GPS's substantial improvements in training efficiency, final performance, and test-time efficiency over superior baseline methods.

</details>


### [400] [Evolving from Tool User to Creator via Training-Free Experience Reuse in Multimodal Reasoning](https://arxiv.org/abs/2602.01983)
*Xintian Shen,Jiawei Chen,Lihao Zheng,Hao Ma,Tao Wei,Kun Zhan*

Main category: cs.AI

TL;DR: UCT: Training-free framework that transforms LLM agents from tool users to tool creators by harvesting reasoning experiences into reusable tools, enabling self-evolving capabilities without additional training.


<details>
  <summary>Details</summary>
Motivation: Existing TIR models have limitations: fixed tools fail for open-ended problems, lack self-optimization leads to error propagation, and manual tool construction is labor-intensive and limits applicability. LLM reasoning traces contain implicit problem-solving capabilities that could be leveraged.

Method: UCT framework harvests reasoning experiences from LLMs and distills them into reusable tools. It enables adaptive tool creation and self-updating during inference with memory consolidation mechanism to maintain tool library. The approach is training-free and allows continuous improvement during reasoning.

Result: Significant performance gains of +20.86% and +23.04% on multi-domain mathematical and scientific reasoning benchmarks. Demonstrates self-evolving capability of agents and serves as novel paradigm for enhancing TIR models.

Conclusion: UCT successfully transforms agents from tool users to tool creators, enabling automated tool construction that continuously improves during reasoning without additional training. The framework leverages LLM reasoning traces to create reusable tools, addressing limitations of existing TIR approaches.

Abstract: Existing Tool-Integrated Reasoning (TIR) models have effectively extended the question-answering capabilities of LLMs by incorporating external tools. However, real-world scenarios present numerous open-ended problems where fixed tools often fail to meet task requirements. Furthermore, the lack of self-optimization mechanisms means that erroneous tool outputs can mislead the LLM's responses. Additionally, the construction of existing tools entails significant manual effort, which consequently constrains their applicability. Recognizing that the reasoning traces of LLMs encapsulate implicit problem-solving capabilities, we propose UCT, a novel training-free framework that transforms agents from tool users to tool creators. This approach harvests reasoning experiences and distills them into reusable assets. This method transforms the agent from a mere tool user into a tool creator, enabling adaptive tool creation and self-updating during the inference process. We also introduce a memory consolidation mechanism to maintain the tool library, ensuring high reusability of retained experiential memory for subsequent reasoning tasks. This novel automated tool construction paradigm continuously improves tool quality during reasoning, allowing the overall agent system to progress without additional training. Extensive experiments demonstrate that our method serves as a novel paradigm for enhancing the capabilities of TIR models. In particular, the significant performance gains achieved +20.86%$\uparrow$ and +23.04%$\uparrow$ on benchmarks across multi-domain mathematical and scientific reasoning tasks validate the self-evolving capability of the agent.

</details>


### [401] [Emergent Analogical Reasoning in Transformers](https://arxiv.org/abs/2602.01992)
*Gouki Minegishi,Jingyuan Feng,Hiroki Furuta,Takeshi Kojima,Yusuke Iwasawa,Yutaka Matsuo*

Main category: cs.AI

TL;DR: Transformers learn analogical reasoning through geometric alignment of relational structure and functor-like mechanisms, with emergence sensitive to data, optimization, and scale.


<details>
  <summary>Details</summary>
Motivation: Despite analogy being central to human intelligence, how Transformers acquire and implement analogical reasoning remains poorly understood. The paper aims to formalize and mechanistically understand this process.

Method: Inspired by category theory functors, formalized analogical reasoning as inference of correspondences between entities across categories. Introduced synthetic tasks to evaluate emergence under controlled settings, then conducted mechanistic analysis of Transformer architectures.

Result: Found analogical reasoning emergence is highly sensitive to data characteristics, optimization choices, and model scale. Mechanistically identified two key components: (1) geometric alignment of relational structure in embedding space, and (2) application of a functor within the Transformer. Same trends observed in pretrained LLMs.

Conclusion: Successfully moved analogy from an abstract cognitive notion to a concrete, mechanistically grounded phenomenon in modern neural networks, showing Transformers implement analogical reasoning through geometric alignment and functor-like mechanisms.

Abstract: Analogy is a central faculty of human intelligence, enabling abstract patterns discovered in one domain to be applied to another. Despite its central role in cognition, the mechanisms by which Transformers acquire and implement analogical reasoning remain poorly understood. In this work, inspired by the notion of functors in category theory, we formalize analogical reasoning as the inference of correspondences between entities across categories. Based on this formulation, we introduce synthetic tasks that evaluate the emergence of analogical reasoning under controlled settings. We find that the emergence of analogical reasoning is highly sensitive to data characteristics, optimization choices, and model scale. Through mechanistic analysis, we show that analogical reasoning in Transformers decomposes into two key components: (1) geometric alignment of relational structure in the embedding space, and (2) the application of a functor within the Transformer. These mechanisms enable models to transfer relational structure from one category to another, realizing analogy. Finally, we quantify these effects and find that the same trends are observed in pretrained LLMs. In doing so, we move analogy from an abstract cognitive notion to a concrete, mechanistically grounded phenomenon in modern neural networks.

</details>


### [402] [Thinking Like a Doctor: Conversational Diagnosis through the Exploration of Diagnostic Knowledge Graphs](https://arxiv.org/abs/2602.01995)
*Jeongmoon Won,Seungwon Kook,Yohan Jo*

Main category: cs.AI

TL;DR: A conversational diagnosis system that uses a diagnostic knowledge graph for two-step reasoning: generating hypotheses from dialogue context and verifying them through clarifying questions, achieving improved accuracy and efficiency with realistic patient simulation.


<details>
  <summary>Details</summary>
Motivation: Existing conversational diagnosis approaches rely too heavily on model's parametric knowledge or assume patients provide rich, concrete information, which is unrealistic for real-world clinical scenarios where patients often describe symptoms vaguely.

Method: Proposes a conversational diagnosis system that explores a diagnostic knowledge graph in two steps: (1) generating diagnostic hypotheses from dialogue context, and (2) verifying hypotheses through iterative clarifying questions until reaching final diagnosis. Uses MIMIC-IV patient profiles with adapted simulator to reflect vague symptom descriptions typical in early clinical encounters.

Result: The system shows improved diagnostic accuracy and efficiency over strong baselines. Physician evaluations support the realism of the adapted patient simulator and the clinical utility of the generated clarifying questions.

Conclusion: The proposed knowledge graph-based conversational diagnosis system effectively addresses limitations of existing approaches by handling vague patient descriptions and iterative hypothesis verification, demonstrating clinical utility and improved performance in realistic diagnostic scenarios.

Abstract: Conversational diagnosis requires multi-turn history-taking, where an agent asks clarifying questions to refine differential diagnoses under incomplete information. Existing approaches often rely on the parametric knowledge of a model or assume that patients provide rich and concrete information, which is unrealistic. To address these limitations, we propose a conversational diagnosis system that explores a diagnostic knowledge graph to reason in two steps: (i) generating diagnostic hypotheses from the dialogue context, and (ii) verifying hypotheses through clarifying questions, which are repeated until a final diagnosis is reached. Since evaluating the system requires a realistic patient simulator that responds to the system's questions, we adopt a well-established simulator along with patient profiles from MIMIC-IV. We further adapt it to describe symptoms vaguely to reflect real-world patients during early clinical encounters. Experiments show improved diagnostic accuracy and efficiency over strong baselines, and evaluations by physicians support the realism of our simulator and the clinical utility of the generated questions. Our code will be released upon publication.

</details>


### [403] [Do I Really Know? Learning Factual Self-Verification for Hallucination Reduction](https://arxiv.org/abs/2602.02018)
*Enes Altinisik,Masoomali Fatehkia,Fatih Deniz,Nadir Durrani,Majd Hawasly,Mohammad Raza,Husrev Taha Sencar*

Main category: cs.AI

TL;DR: VeriFY is a training framework that teaches LLMs to self-verify factual answers through consistency checks, reducing hallucinations while maintaining recall.


<details>
  <summary>Details</summary>
Motivation: Existing approaches to mitigate factual hallucinations in LLMs rely on external verification or uncertainty mapping to abstention, often leading to overly conservative behavior that reduces model utility.

Method: VeriFY augments training with structured verification traces that guide models to: 1) produce initial answer, 2) generate and answer probing verification query, 3) issue consistency judgment, 4) decide to answer or abstain. Uses stage-level loss masking to exclude hallucinated answer stages while preserving verification supervision.

Result: Reduces factual hallucination rates by 9.7 to 53.3% across multiple model families and scales, with only modest recall reductions (0.4 to 5.7%). Generalizes across datasets when trained on single source.

Conclusion: VeriFY effectively teaches LLMs to reason about factual uncertainty through self-verification, significantly reducing hallucinations while maintaining utility, with promising generalization capabilities.

Abstract: Factual hallucination remains a central challenge for large language models (LLMs). Existing mitigation approaches primarily rely on either external post-hoc verification or mapping uncertainty directly to abstention during fine-tuning, often resulting in overly conservative behavior. We propose VeriFY, a training-time framework that teaches LLMs to reason about factual uncertainty through consistency-based self-verification. VeriFY augments training with structured verification traces that guide the model to produce an initial answer, generate and answer a probing verification query, issue a consistency judgment, and then decide whether to answer or abstain. To address the risk of reinforcing hallucinated content when training on augmented traces, we introduce a stage-level loss masking approach that excludes hallucinated answer stages from the training objective while preserving supervision over verification behavior. Across multiple model families and scales, VeriFY reduces factual hallucination rates by 9.7 to 53.3 percent, with only modest reductions in recall (0.4 to 5.7 percent), and generalizes across datasets when trained on a single source. The source code, training data, and trained model checkpoints will be released upon acceptance.

</details>


### [404] [Light Alignment Improves LLM Safety via Model Self-Reflection with a Single Neuron](https://arxiv.org/abs/2602.02027)
*Sicheng Shen,Mingyang Lv,Han Shen,Jialin Wu,Binghao Wang,Zhou Yang,Guobin Shen,Dongcheng Zhao,Feifei Zhao,Yi Zeng*

Main category: cs.AI

TL;DR: NGSD proposes a lightweight safety alignment method using a single-neuron gating mechanism that balances model capabilities with external guidance, requiring only low-cost expert model training.


<details>
  <summary>Details</summary>
Motivation: Existing safety alignment methods are computationally expensive, fail to generalize across models, and lightweight approaches either rely too much on pre-computed safety injections or over-depend on model capabilities, leading to limited generalization and degraded efficiency.

Method: Proposes a safety-aware decoding method that requires only low-cost training of an expert model and employs a single neuron as a gating mechanism to balance model's intrinsic capabilities with external guidance.

Result: The approach preserves utility while enhancing output safety, demonstrates clear advantages in training overhead and generalization across model scales, offering a new perspective on lightweight alignment.

Conclusion: NGSD provides an effective lightweight safety alignment solution for practical deployment of large language models, balancing safety and utility with minimal computational cost.

Abstract: The safety of large language models (LLMs) has increasingly emerged as a fundamental aspect of their development. Existing safety alignment for LLMs is predominantly achieved through post-training methods, which are computationally expensive and often fail to generalize well across different models. A small number of lightweight alignment approaches either rely heavily on prior-computed safety injections or depend excessively on the model's own capabilities, resulting in limited generalization and degraded efficiency and usability during generation. In this work, we propose a safety-aware decoding method that requires only low-cost training of an expert model and employs a single neuron as a gating mechanism. By effectively balancing the model's intrinsic capabilities with external guidance, our approach simultaneously preserves utility and enhances output safety. It demonstrates clear advantages in training overhead and generalization across model scales, offering a new perspective on lightweight alignment for the safe and practical deployment of large language models. Code: https://github.com/Beijing-AISI/NGSD.

</details>


### [405] [Edit Knowledge, Not Just Facts via Multi-Step Reasoning over Background Stories](https://arxiv.org/abs/2602.02028)
*Ya Gao,Kalle Kujanpää,Pekka Marttinen,Harri Valpola,Alexander Ilin*

Main category: cs.AI

TL;DR: Proposes a training strategy for AI systems to internalize new knowledge through reasoning rather than memorization, using coherent background stories, self-generated multi-hop questions, and knowledge distillation.


<details>
  <summary>Details</summary>
Motivation: Current knowledge editing approaches focus on atomic facts but fail to integrate new information into coherent frameworks usable across contexts. The paper argues that knowledge internalization is fundamentally a reasoning problem rather than a memorization problem.

Method: Three-principle training strategy: 1) Introduce new knowledge as coherent background stories contextualizing novel facts, 2) Train using self-generated multi-hop questions requiring multi-step reasoning with new information, 3) Use knowledge distillation to force student models to internalize teacher's reasoning without access to novel information.

Result: Models trained with this strategy effectively leverage newly acquired knowledge during reasoning and achieve remarkable performance on challenging questions requiring combination of multiple new facts.

Conclusion: The proposed reasoning-focused training approach enables AI systems to better integrate and apply new knowledge across contexts, addressing limitations of current knowledge editing methods.

Abstract: Enabling artificial intelligence systems, particularly large language models, to integrate new knowledge and flexibly apply it during reasoning remains a central challenge. Existing knowledge editing approaches emphasize atomic facts, improving factual recall but often failing to integrate new information into a coherent framework usable across contexts. In this work, we argue that knowledge internalization is fundamentally a reasoning problem rather than a memorization problem. Consequently, a model should be trained in situations where the new information is instrumental to solving a task, combined with pre-existing knowledge, and exercised through multi-step reasoning. Based on this insight, we propose a training strategy based on three principles. First, new knowledge is introduced as a coherent background story that contextualizes novel facts and explains their relation to existing knowledge. Second, models are trained using self-generated multi-hop questions that require multi-step reasoning involving the new information. Third, training is done using knowledge distillation, forcing a student model to internalize the teacher's reasoning behavior without access to the novel information. Experiments show that models trained with this strategy effectively leverage newly acquired knowledge during reasoning and achieve remarkable performance on challenging questions that require combining multiple new facts.

</details>


### [406] [Canonical Intermediate Representation for LLM-based optimization problem formulation and code generation](https://arxiv.org/abs/2602.02029)
*Zhongyuan Lyu,Shuoyu Hu,Lujie Liu,Hongxia Yang,Ming LI*

Main category: cs.AI

TL;DR: R2C framework uses Canonical Intermediate Representation (CIR) to translate natural language operational rules into optimization models via multi-agent pipeline, achieving state-of-the-art accuracy on new benchmark and competitive results on established benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based approaches struggle with composite constraints and appropriate modeling paradigms required by complex operational rules when formulating optimization models from natural language descriptions.

Method: Introduce Canonical Intermediate Representation (CIR) schema that LLMs generate between problem descriptions and optimization models, encoding semantics through constraint archetypes and candidate modeling paradigms. Develop R2C multi-agent pipeline that parses problem texts, synthesizes CIR implementations by retrieving domain knowledge, and instantiates optimization models.

Result: R2C achieves 47.2% Accuracy Rate on newly constructed benchmark with rich operational rules, delivers highly competitive results on established benchmarks approaching proprietary models like GPT-5, and with reflection mechanism achieves further gains setting new best-reported results on some benchmarks.

Conclusion: The CIR-based R2C framework effectively addresses limitations of current LLM approaches for optimization model formulation from natural language, demonstrating state-of-the-art performance through decoupling rule logic from mathematical instantiation and systematic knowledge retrieval.

Abstract: Automatically formulating optimization models from natural language descriptions is a growing focus in operations research, yet current LLM-based approaches struggle with the composite constraints and appropriate modeling paradigms required by complex operational rules. To address this, we introduce the Canonical Intermediate Representation (CIR): a schema that LLMs explicitly generate between problem descriptions and optimization models. CIR encodes the semantics of operational rules through constraint archetypes and candidate modeling paradigms, thereby decoupling rule logic from its mathematical instantiation. Upon a newly generated CIR knowledge base, we develop the rule-to-constraint (R2C) framework, a multi-agent pipeline that parses problem texts, synthesizes CIR implementations by retrieving domain knowledge, and instantiates optimization models. To systematically evaluate rule-to-constraint reasoning, we test R2C on our newly constructed benchmark featuring rich operational rules, and benchmarks from prior work. Extensive experiments show that R2C achieves state-of-the-art accuracy on the proposed benchmark (47.2% Accuracy Rate). On established benchmarks from the literature, R2C delivers highly competitive results, approaching the performance of proprietary models (e.g., GPT-5). Moreover, with a reflection mechanism, R2C achieves further gains and sets new best-reported results on some benchmarks.

</details>


### [407] [Constrained Process Maps for Multi-Agent Generative AI Workflows](https://arxiv.org/abs/2602.02034)
*Ananya Joshi,Michael Rudow*

Main category: cs.AI

TL;DR: Multi-agent LLM system for regulated workflows improves accuracy by 19%, reduces human review by 85x, and speeds processing compared to single-agent approaches.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based agent architectures for complex regulated workflows (compliance, due diligence) rely too heavily on single-agent prompt engineering, making it difficult to observe uncertainty handling and coordination across decision stages with human oversight.

Method: Introduce multi-agent system formalized as finite-horizon Markov Decision Process (MDP) with directed acyclic structure. Each agent corresponds to specific role/decision stage (content, business, legal review). Quantify epistemic uncertainty at agent level using Monte Carlo estimation, and system-level uncertainty via MDP termination in automated or human-review states.

Result: Demonstrated through AI safety evaluation case study for self-harm detection: up to 19% increase in accuracy, up to 85x reduction in required human review, and reduced processing time in some configurations compared to single-agent baseline.

Conclusion: Multi-agent MDP framework provides structured approach for LLM coordination in regulated workflows, enabling better uncertainty quantification, human oversight integration, and performance improvements over single-agent systems.

Abstract: Large language model (LLM)-based agents are increasingly used to perform complex, multi-step workflows in regulated settings such as compliance and due diligence. However, many agentic architectures rely primarily on prompt engineering of a single agent, making it difficult to observe or compare how models handle uncertainty and coordination across interconnected decision stages and with human oversight. We introduce a multi-agent system formalized as a finite-horizon Markov Decision Process (MDP) with a directed acyclic structure. Each agent corresponds to a specific role or decision stage (e.g., content, business, or legal review in a compliance workflow), with predefined transitions representing task escalation or completion. Epistemic uncertainty is quantified at the agent level using Monte Carlo estimation, while system-level uncertainty is captured by the MDP's termination in either an automated labeled state or a human-review state. We illustrate the approach through a case study in AI safety evaluation for self-harm detection, implemented as a multi-agent compliance system. Results demonstrate improvements over a single-agent baseline, including up to a 19\% increase in accuracy, up to an 85x reduction in required human review, and, in some configurations, reduced processing time.

</details>


### [408] [Hunt Instead of Wait: Evaluating Deep Data Research on Large Language Models](https://arxiv.org/abs/2602.02039)
*Wei Liu,Peijie Yu,Michele Orini,Yali Du,Yulan He*

Main category: cs.AI

TL;DR: LLMs need investigatory intelligence (autonomous goal-setting and exploration) beyond just executional intelligence (task completion). The paper introduces DDR for autonomous data analysis and DDR-Bench for evaluation, showing frontier models have emerging agency but struggle with long-horizon exploration.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks focus too much on executional intelligence (completing assigned tasks) rather than investigatory intelligence (autonomous goal-setting and exploration). Real-world data science starts from raw data without explicit queries, requiring models to decide what to explore, but few benchmarks test this capability.

Method: Introduced Deep Data Research (DDR) - an open-ended task where LLMs autonomously extract key insights from databases, and DDR-Bench - a large-scale, checklist-based benchmark that enables verifiable evaluation of investigatory intelligence.

Result: Frontier models display emerging agency in investigatory intelligence, but long-horizon exploration remains challenging. Effective investigatory intelligence depends not only on agent scaffolding or scaling, but also on intrinsic strategies of agentic models.

Conclusion: Investigatory intelligence (autonomous goal-setting and exploration) is crucial for real-world applications like data science. The DDR framework and benchmark provide tools to evaluate this capability, revealing that current models show promise but need improvement in long-horizon exploration, requiring better intrinsic strategies beyond just scaling.

Abstract: The agency expected of Agentic Large Language Models goes beyond answering correctly, requiring autonomy to set goals and decide what to explore. We term this investigatory intelligence, distinguishing it from executional intelligence, which merely completes assigned tasks. Data Science provides a natural testbed, as real-world analysis starts from raw data rather than explicit queries, yet few benchmarks focus on it. To address this, we introduce Deep Data Research (DDR), an open-ended task where LLMs autonomously extract key insights from databases, and DDR-Bench, a large-scale, checklist-based benchmark that enables verifiable evaluation. Results show that while frontier models display emerging agency, long-horizon exploration remains challenging. Our analysis highlights that effective investigatory intelligence depends not only on agent scaffolding or merely scaling, but also on intrinsic strategies of agentic models.

</details>


### [409] [Rethinking the Role of Entropy in Optimizing Tool-Use Behaviors for Large Language Model Agents](https://arxiv.org/abs/2602.02050)
*Zeping Li,Hongru Wang,Yiwen Zhao,Guanhua Chen,Yixia Li,Keyang Chen,Yixin Cao,Guangnan Ye,Hongfeng Chai,Mengdi Wang,Zhenfei Yin*

Main category: cs.AI

TL;DR: LLM-based tool-using agents often make excessive low-quality tool calls in long tasks. The paper proposes using entropy reduction as a supervisory signal with two reward strategies to optimize tool-use behavior, achieving significant improvements in efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Tool-using LLM agents face challenges with excessive and low-quality tool calls in long trajectories, which increases latency and degrades inference performance. Managing tool-use behavior effectively is difficult, creating a need for better optimization methods.

Method: The authors use entropy reduction as a supervisory signal and design two reward strategies: (1) sparse outcome rewards for trajectory-level efficiency improvement, and (2) dense process rewards for fine-grained performance enhancement. Both leverage the observed correlation between entropy reduction and high-quality tool calls.

Result: Experiments show both reward designs improve tool-use behavior: sparse rewards reduce tool calls by 72.07% compared to baseline averages, while dense rewards improve performance by 22.27%. Entropy reduction proves effective as a key mechanism for enhancing tool-use behavior.

Conclusion: Entropy reduction serves as an effective supervisory signal for optimizing LLM-based tool-using agents, enabling more adaptive and efficient real-world applications through tailored reward strategies that address different optimization needs.

Abstract: Tool-using agents based on Large Language Models (LLMs) excel in tasks such as mathematical reasoning and multi-hop question answering. However, in long trajectories, agents often trigger excessive and low-quality tool calls, increasing latency and degrading inference performance, making managing tool-use behavior challenging. In this work, we conduct entropy-based pilot experiments and observe a strong positive correlation between entropy reduction and high-quality tool calls. Building on this finding, we propose using entropy reduction as a supervisory signal and design two reward strategies to address the differing needs of optimizing tool-use behavior. Sparse outcome rewards provide coarse, trajectory-level guidance to improve efficiency, while dense process rewards offer fine-grained supervision to enhance performance. Experiments across diverse domains show that both reward designs improve tool-use behavior: the former reduces tool calls by 72.07% compared to the average of baselines, while the latter improves performance by 22.27%. These results position entropy reduction as a key mechanism for enhancing tool-use behavior, enabling agents to be more adaptive in real-world applications.

</details>


### [410] [SIDiffAgent: Self-Improving Diffusion Agent](https://arxiv.org/abs/2602.02051)
*Shivank Garg,Ayush Singh,Gaurav Kumar Nayak*

Main category: cs.AI

TL;DR: SIDiffAgent is a training-free agentic framework that uses Qwen models to autonomously improve text-to-image diffusion outputs through prompt engineering, error detection/correction, and iterative self-improvement with memory.


<details>
  <summary>Details</summary>
Motivation: Current text-to-image diffusion models have practical deployment limitations including sensitivity to prompt phrasing, semantic ambiguity, anatomical artifacts, and the need for carefully engineered prompts. Existing methods require additional training and offer limited controllability.

Method: SIDiffAgent leverages Qwen family models (Qwen-VL, Qwen-Image, Qwen-Edit, Qwen-Embedding) in a training-free agentic framework that autonomously manages prompt engineering, detects/corrects poor generations, performs artifact removal, and incorporates iterative self-improvement using a memory database of past experiences.

Result: Achieved an average VQA score of 0.884 on GenAIBench, significantly outperforming open-source, proprietary models and other agentic methods.

Conclusion: SIDiffAgent provides a practical, training-free solution to improve text-to-image diffusion model reliability and consistency through autonomous agentic control and iterative self-improvement, addressing key deployment limitations.

Abstract: Text-to-image diffusion models have revolutionized generative AI, enabling high-quality and photorealistic image synthesis. However, their practical deployment remains hindered by several limitations: sensitivity to prompt phrasing, ambiguity in semantic interpretation (e.g., ``mouse" as animal vs. a computer peripheral), artifacts such as distorted anatomy, and the need for carefully engineered input prompts. Existing methods often require additional training and offer limited controllability, restricting their adaptability in real-world applications. We introduce Self-Improving Diffusion Agent (SIDiffAgent), a training-free agentic framework that leverages the Qwen family of models (Qwen-VL, Qwen-Image, Qwen-Edit, Qwen-Embedding) to address these challenges. SIDiffAgent autonomously manages prompt engineering, detects and corrects poor generations, and performs fine-grained artifact removal, yielding more reliable and consistent outputs. It further incorporates iterative self-improvement by storing a memory of previous experiences in a database. This database of past experiences is then used to inject prompt-based guidance at each stage of the agentic pipeline. \modelour achieved an average VQA score of 0.884 on GenAIBench, significantly outperforming open-source, proprietary models and agentic methods. We will publicly release our code upon acceptance.

</details>


### [411] [Understanding the Reversal Curse Mitigation in Masked Diffusion Models through Attention and Training Dynamics](https://arxiv.org/abs/2602.02133)
*Sangwoo Shin,BumJun Kim,Kyelim Lee,Moongyu Jeon,Albert No*

Main category: cs.AI

TL;DR: MDMs partially overcome ARM's reversal curse due to architectural weight sharing and gradient alignment, not just any-order training.


<details>
  <summary>Details</summary>
Motivation: Autoregressive language models suffer from the reversal curse (failing on reverse queries), while masked diffusion models show much weaker failure. The reason for this mitigation was unclear, with common attribution to any-order training being insufficient.

Method: Analyzed one-layer Transformer encoder architecture to show weight sharing couples forward/reverse directions via positive attention score correlation. Demonstrated gradient alignment where minimizing forward loss reduces reverse loss. Validated with controlled toy tasks and large-scale diffusion language models.

Result: MDMs' mitigation of reversal curse arises from architectural structure (weight sharing) and its interaction with training (gradient alignment), explaining why they partially overcome this failure mode that persists in strong ARMs.

Conclusion: The reversal curse mitigation in MDMs is fundamentally due to architectural properties and training dynamics, not just any-order training objectives, providing insight into why diffusion models handle bidirectional relationships better than autoregressive models.

Abstract: Autoregressive language models (ARMs) suffer from the reversal curse: after learning that "$A$ is $B$", they often fail on the reverse query "$B$ is $A$". Masked diffusion-based language models (MDMs) exhibit this failure in a much weaker form, but the underlying reason has remained unclear. A common explanation attributes this mitigation to the any-order training objective. However, observing "[MASK] is $B$" during training does not necessarily teach the model to handle the reverse prompt "$B$ is [MASK]". We show that the mitigation arises from architectural structure and its interaction with training. In a one-layer Transformer encoder, weight sharing couples the two directions by making forward and reverse attention scores positively correlated. In the same setting, we further show that the corresponding gradients are aligned, so minimizing the forward loss also reduces the reverse loss. Experiments on both controlled toy tasks and large-scale diffusion language models support these mechanisms, explaining why MDMs partially overcome a failure mode that persists in strong ARMs.

</details>


### [412] [Mitigating Safety Tax via Distribution-Grounded Refinement in Large Reasoning Models](https://arxiv.org/abs/2602.02136)
*Yingsha Xie,Tiansheng Huang,Enneng Yang,Rui Min,Wenjie Lu,Xiaochun Cao,Naiqiang Tan,Li Shen*

Main category: cs.AI

TL;DR: DGR method reduces safety tax in large reasoning models by aligning safety datasets with the target model's distribution, improving reasoning accuracy by +30.2% while maintaining safety.


<details>
  <summary>Details</summary>
Motivation: Safety alignment degrades reasoning ability due to distributional gaps between external safety datasets and target models' internal distributions.

Method: DGR transforms existing safety reasoning datasets to align with target LLM's inner distribution through distribution refinement.

Result: DGR reduces safety tax (+30.2% DirectRefusal, +21.2% R1-ACT reasoning accuracy) while maintaining safety; shows correlation between reasoning degradation and distribution shift.

Conclusion: Distributional consistency is crucial for preserving reasoning capabilities; safety alignment may activate latent knowledge with minimal samples.

Abstract: Safety alignment incurs safety tax that perturbs a large reasoning model's (LRM) general reasoning ability. Existing datasets used for safety alignment for an LRM are usually constructed by distilling safety reasoning traces and answers from an external LRM or human labeler. However, such reasoning traces and answers exhibit a distributional gap with the target LRM that needs alignment, and we conjecture such distributional gap is the culprit leading to significant degradation of reasoning ability of the target LRM. Driven by this hypothesis, we propose a safety alignment dataset construction method, dubbed DGR. DGR transforms and refines an existing out-of-distributional safety reasoning dataset to be aligned with the target's LLM inner distribution. Experimental results demonstrate that i) DGR effectively mitigates the safety tax while maintaining safety performance across all baselines, i.e., achieving \textbf{+30.2\%} on DirectRefusal and \textbf{+21.2\%} on R1-ACT improvement in average reasoning accuracy compared to Vanilla SFT; ii) the degree of reasoning degradation correlates with the extent of distribution shift, suggesting that bridging this gap is central to preserving capabilities. Furthermore, we find that safety alignment in LRMs may primarily function as a mechanism to activate latent knowledge, as a mere \textbf{10} samples are sufficient for activating effective refusal behaviors. These findings not only emphasize the importance of distributional consistency but also provide insights into the activation mechanism of safety in reasoning models.

</details>


### [413] [Traffic-Aware Navigation in Road Networks](https://arxiv.org/abs/2602.02158)
*Sarah Nassar*

Main category: cs.AI

TL;DR: Comparison of three graph search algorithms for traffic-aware navigation in Kingston's road network, evaluating trade-offs between preprocessing requirements, real-time performance, and traffic awareness.


<details>
  <summary>Details</summary>
Motivation: To evaluate different graph search approaches for traffic-aware navigation systems, understanding their trade-offs in preprocessing requirements, real-time performance, and solution optimality for practical deployment in urban road networks.

Method: Compared three approaches: 1) Floyd-Warshall-Ingerman (single-run multi-query preprocessing), 2) Dijkstra's and A* (continuous single-query real-time search), and 3) Yen's algorithm (top K shortest paths preprocessing with real-time iteration). Tested on Kingston's road network for traffic-aware navigation.

Result: Dijkstra's and A* provided the most traffic-aware optimal solutions with minimal preprocessing. Floyd-Warshall-Ingerman was fastest in real-time but offered only distance-based paths without traffic awareness. Yen's algorithm required significant preprocessing but balanced runtime speed and optimality between the other approaches.

Conclusion: Each algorithm presents distinct advantages and disadvantages that must be weighed based on specific deployment contexts. The choice depends on balancing preprocessing requirements, real-time performance needs, and the importance of traffic awareness for optimal navigation solutions.

Abstract: This project compares three graph search approaches for the task of traffic-aware navigation in Kingston's road network. These approaches include a single-run multi-query preprocessing algorithm (Floyd-Warshall-Ingerman), continuous single-query real-time search (Dijkstra's and A*), and an algorithm combining both approaches to balance between their trade-offs by first finding the top K shortest paths then iterating over them in real time (Yen's). Dijkstra's and A* resulted in the most traffic-aware optimal solutions with minimal preprocessing required. Floyd-Warshall-Ingerman was the fastest in real time but provided distance based paths with no traffic awareness. Yen's algorithm required significant preprocessing but balanced between the other two approaches in terms of runtime speed and optimality. Each approach presents advantages and disadvantages that need to be weighed depending on the circumstances of specific deployment contexts to select the best custom solution. *This project was completed as part of ELEC 844 (Search and Planning Algorithms for Robotics) in the Fall 2025 term.

</details>


### [414] [Reasoning in a Combinatorial and Constrained World: Benchmarking LLMs on Natural-Language Combinatorial Optimization](https://arxiv.org/abs/2602.02188)
*Xia Jiang,Jing Chen,Cong Zhang,Jie Gao,Chengpeng Hu,Chenhao Zhang,Yaoxin Wu,Yingqian Zhang*

Main category: cs.AI

TL;DR: NLCO is a new benchmark for evaluating LLMs on combinatorial optimization problems using natural language, showing that while LLMs perform well on small instances, their performance degrades with problem size and varies across problem types.


<details>
  <summary>Details</summary>
Motivation: LLMs have shown strong performance in math and logic reasoning, but their ability to handle combinatorial optimization (searching high-dimensional solution spaces under hard constraints) remains underexplored. The authors aim to bridge this gap by creating a comprehensive benchmark.

Method: Introduced NLCO (Natural Language Combinatorial Optimization benchmark) with 43 CO problems organized using a four-layer taxonomy: variable types, constraint families, global patterns, and objective classes. The benchmark evaluates LLMs on end-to-end CO reasoning where models must output discrete solutions without writing code or calling external solvers.

Result: High-performing LLMs achieve strong feasibility and solution quality on small instances, but performance degrades as instance size grows, even with more reasoning tokens. Set-based tasks are relatively easy, while graph-structured problems and bottleneck objectives lead to more frequent failures.

Conclusion: LLMs show promise in combinatorial optimization but face significant challenges with problem scalability and certain problem types, highlighting the need for further research in this area.

Abstract: While large language models (LLMs) have shown strong performance in math and logic reasoning, their ability to handle combinatorial optimization (CO) -- searching high-dimensional solution spaces under hard constraints -- remains underexplored. To bridge the gap, we introduce NLCO, a \textbf{N}atural \textbf{L}anguage \textbf{C}ombinatorial \textbf{O}ptimization benchmark that evaluates LLMs on end-to-end CO reasoning: given a language-described decision-making scenario, the model must output a discrete solution without writing code or calling external solvers. NLCO covers 43 CO problems and is organized using a four-layer taxonomy of variable types, constraint families, global patterns, and objective classes, enabling fine-grained evaluation. We provide solver-annotated solutions and comprehensively evaluate LLMs by feasibility, solution optimality, and reasoning efficiency. Experiments across a wide range of modern LLMs show that high-performing models achieve strong feasibility and solution quality on small instances, but both degrade as instance size grows, even if more tokens are used for reasoning. We also observe systematic effects across the taxonomy: set-based tasks are relatively easy, whereas graph-structured problems and bottleneck objectives lead to more frequent failures.

</details>


### [415] [TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents](https://arxiv.org/abs/2602.02196)
*Hang Yan,Xinyu Che,Fangzhi Xu,Qiushi Sun,Zichen Ding,Kanzhi Cheng,Jian Zhang,Tao Qin,Jun Liu,Qika Lin*

Main category: cs.AI

TL;DR: TIDE is a diagnostic framework that evaluates Test-Time Improvement (TTI) in autonomous LLM agents across three dimensions: task optimization efficiency, behavior adaptation after errors, and working memory utility.


<details>
  <summary>Details</summary>
Motivation: Current understanding of why Test-Time Improvement (TTI) in autonomous LLM agents succeeds or fails is limited, and existing evaluation metrics don't capture key aspects like task optimization efficiency, behavior adaptation after errors, and working memory utility.

Method: Proposed TIDE framework decomposes TTI into three interconnected dimensions: (1) overall temporal dynamics of task completion, (2) identification of performance constraints from recursive looping behaviors, and (3) identification of constraints from burdensome accumulated memory.

Result: Through extensive experiments across diverse agents and environments, TIDE reveals that improving agent performance requires more than scaling internal reasoning - it calls for explicitly optimizing the interaction dynamics between agents and their environments.

Conclusion: TIDE provides a comprehensive diagnostic framework for understanding TTI mechanisms in autonomous LLM agents, highlighting the importance of optimizing agent-environment interaction dynamics rather than just scaling internal reasoning capabilities.

Abstract: Recent advances in autonomous LLM agents demonstrate their ability to improve performance through iterative interaction with the environment. We define this paradigm as Test-Time Improvement (TTI). However, the mechanisms under how and why TTI succeed or fail remain poorly understood, and existing evaluation metrics fail to capture their task optimization efficiency, behavior adaptation after erroneous actions, and the specific utility of working memory for task completion. To address these gaps, we propose Test-time Improvement Diagnostic Evaluation (TIDE), an agent-agnostic and environment-agnostic framework that decomposes TTI into three comprehensive and interconnected dimensions. The framework measures (1) the overall temporal dynamics of task completion and (2) identifies whether performance is primarily constrained by recursive looping behaviors or (3) by burdensome accumulated memory. Through extensive experiments across diverse agents and environments, TIDE highlights that improving agent performance requires more than scaling internal reasoning, calling for explicitly optimizing the interaction dynamics between the agent and the environment.

</details>


### [416] [More Than a Quick Glance: Overcoming the Greedy Bias in KV-Cache Compression](https://arxiv.org/abs/2602.02199)
*Aryan Sood,Tanvi Sharma,Vansh Agrawal*

Main category: cs.AI

TL;DR: LASER-KV is a new KV cache compression framework that uses layer accumulation with exact-LSH recall to maintain performance while achieving high compression, outperforming previous methods by up to 10% on long-context tasks.


<details>
  <summary>Details</summary>
Motivation: LLMs have theoretical long context capabilities but face practical deployment constraints due to linear KV cache memory growth. Existing compression methods trade semantic recall for memory efficiency, causing performance degradation.

Method: LASER-KV implements a block-wise accumulation strategy with protection divisor (n) instead of fixed summary sizes, isolating compression effects from sliding window artifacts. Uses Layer Accumulated Selection with Exact-LSH Recall framework.

Result: On Babilong benchmark, previous compression methods degrade by 15-30% on long context tasks, while LASER-KV maintains stable performance with up to 10% better accuracy at 128k context length.

Conclusion: The findings challenge the assumption that attention scores alone are sufficient proxies for token utility, showing LASER-KV's approach enables effective KV compression without sacrificing performance.

Abstract: While Large Language Models (LLMs) can theoretically support extensive context windows, their actual deployment is constrained by the linear growth of Key-Value (KV) cache memory. Prevailing compression strategies mitigate this through various pruning mechanisms, yet trade-off semantic recall for memory efficiency. In this work, we present LASER-KV (Layer Accumulated Selection with Exact-LSH Recall), a framework designed to test the limits of KV compression under a strict accumulative budgeting policy. We deviate from the standard fixed summary size approach by implementing a block-wise accumulation strategy governed by a protection divisor (n). This allows us to isolate the effects of compression from sliding window artifacts. Our experiments on the Babilong benchmark reveal performance degradation in previous compression methods by 15-30% on various long context tasks. LASER-KV maintains stable performance, achieving superior accuracies by a margin of upto 10% at 128k. These findings challenge the prevailing assumption that attention scores alone are a sufficient proxy for token utility.

</details>


### [417] [Position: Explaining Behavioral Shifts in Large Language Models Requires a Comparative Approach](https://arxiv.org/abs/2602.02304)
*Martino Ciaperoni,Marzio Di Vece,Luca Pappalardo,Fosca Giannotti,Francesco Giannini*

Main category: cs.AI

TL;DR: Proposes Comparative XAI (Δ-XAI) framework to explain behavioral shifts in foundation models across different checkpoints/interventions, rather than analyzing single models in isolation.


<details>
  <summary>Details</summary>
Motivation: Large foundation models exhibit behavioral shifts after scaling, fine-tuning, RL, or in-context learning, but current XAI methods are ill-suited to explain what changed internally between different model checkpoints.

Method: Formulates a Comparative XAI (Δ-XAI) framework with specific desiderata, introduces possible explanation pipelines, and provides concrete Δ-XAI experiments to demonstrate the approach.

Result: Proposes a systematic framework for comparative explanation of behavioral shifts, moving beyond single-model analysis to focus on intervention-induced changes between reference and intervened models.

Conclusion: Behavioral shifts in foundation models should be explained comparatively using the Δ-XAI framework, which provides proper methodological foundations for understanding what changes internally across different model states.

Abstract: Large-scale foundation models exhibit behavioral shifts: intervention-induced behavioral changes that appear after scaling, fine-tuning, reinforcement learning or in-context learning. While investigating these phenomena have recently received attention, explaining their appearance is still overlooked. Classic explainable AI (XAI) methods can surface failures at a single checkpoint of a model, but they are structurally ill-suited to justify what changed internally across different checkpoints and which explanatory claims are warranted about that change. We take the position that behavioral shifts should be explained comparatively: the core target should be the intervention-induced shift between a reference model and an intervened model, rather than any single model in isolation. To this aim we formulate a Comparative XAI ($Δ$-XAI) framework with a set of desiderata to be taken into account when designing proper explaining methods. To highlight how $Δ$-XAI methods work, we introduce a set of possible pipelines, relate them to the desiderata, and provide a concrete $Δ$-XAI experiment.

</details>


### [418] [Interpreting and Controlling LLM Reasoning through Integrated Policy Gradient](https://arxiv.org/abs/2602.02313)
*Changming Li,Kaixing Zhang,Haoyun Xu,Yingdong Shi,Zheng Zhang,Kaitao Song,Kan Ren*

Main category: cs.AI

TL;DR: IPG is a new interpretability method that identifies LLM components contributing to reasoning by propagating outcome-based signals backward through inference trajectories.


<details>
  <summary>Details</summary>
Motivation: Current interpretability methods for LLM reasoning either find components correlated with text patterns or use human-annotated contrastive pairs, failing to precisely localize complex reasoning mechanisms or capture sequential influence from internal workings to outputs.

Method: Propose Integrated Policy Gradient (IPG) framework that attributes reasoning behaviors to model components by propagating compound outcome-based signals (like post-reasoning accuracy) backward through model inference trajectories, focusing on sequential contributions.

Result: Empirical evaluations show IPG achieves more precise localization and enables reliable modulation of reasoning behaviors (reasoning capability, strength) across diverse reasoning models.

Conclusion: IPG provides a novel outcome-oriented, sequential-influence-aware approach to interpret LLM reasoning mechanisms, overcoming limitations of existing methods.

Abstract: Large language models (LLMs) demonstrate strong reasoning abilities in solving complex real-world problems. Yet, the internal mechanisms driving these complex reasoning behaviors remain opaque. Existing interpretability approaches targeting reasoning either identify components (e.g., neurons) correlated with special textual patterns, or rely on human-annotated contrastive pairs to derive control vectors. Consequently, current methods struggle to precisely localize complex reasoning mechanisms or capture sequential influence from model internal workings to the reasoning outputs. In this paper, built on outcome-oriented and sequential-influence-aware principles, we focus on identifying components that have sequential contribution to reasoning behavior where outcomes are cumulated by long-range effects. We propose Integrated Policy Gradient (IPG), a novel framework that attributes reasoning behaviors to model's inner components by propagating compound outcome-based signals such as post reasoning accuracy backward through model inference trajectories. Empirical evaluations demonstrate that our approach achieves more precise localization and enables reliable modulation of reasoning behaviors (e.g., reasoning capability, reasoning strength) across diverse reasoning models.

</details>


### [419] [Context Learning for Multi-Agent Discussion](https://arxiv.org/abs/2602.02350)
*Xingyuan Hua,Sheng Yue,Xinyi Li,Yizhe Zhao,Jinrui Zhang,Ju Ren*

Main category: cs.AI

TL;DR: M2CL introduces multi-LLM context learning with dynamic context generators to solve discussion inconsistency in multi-agent systems, achieving 20-50% performance gains.


<details>
  <summary>Details</summary>
Motivation: Current Multi-Agent Discussion (MAD) methods suffer from discussion inconsistency where LLMs fail to reach coherent solutions due to misalignment between individual contexts.

Method: M2CL learns a context generator for each agent that dynamically generates context instructions per discussion round via automatic information organization and refinement, using a self-adaptive mechanism to control context coherence and output discrepancies.

Result: M2CL significantly surpasses existing methods by 20%-50% on challenging tasks including academic reasoning, embodied tasks, and mobile control, while enjoying favorable transferability and computational efficiency.

Conclusion: The proposed M2CL framework effectively addresses discussion inconsistency in multi-agent systems by enabling LLMs to avoid premature convergence on majority noise and progressively reach correct consensus through dynamic context generation.

Abstract: Multi-Agent Discussion (MAD) has garnered increasing attention very recently, where multiple LLM instances collaboratively solve problems via structured discussion. However, we find that current MAD methods easily suffer from discussion inconsistency, LLMs fail to reach a coherent solution, due to the misalignment between their individual contexts.In this paper, we introduce a multi-LLM context learning method (M2CL) that learns a context generator for each agent, capable of dynamically generating context instructions per discussion round via automatic information organization and refinement. Specifically, inspired by our theoretical insights on the context instruction, M2CL train the generators to control context coherence and output discrepancies via a carefully crafted self-adaptive mechanism.It enables LLMs to avoid premature convergence on majority noise and progressively reach the correct consensus. We evaluate M2CL on challenging tasks, including academic reasoning, embodied tasks, and mobile control. The results show that the performance of M2CL significantly surpasses existing methods by 20%--50%, while enjoying favorable transferability and computational efficiency.

</details>


### [420] [Live-Evo: Online Evolution of Agentic Memory from Continuous Feedback](https://arxiv.org/abs/2602.02369)
*Yaolun Zhang,Yiran Wu,Yijiong Yu,Qingyun Wu,Huazheng Wang*

Main category: cs.AI

TL;DR: Live-Evo is an online self-evolving memory system for LLM agents that learns from streaming data, decouples experiences from guidelines, and adaptively weights memories based on feedback to handle distribution shifts.


<details>
  <summary>Details</summary>
Motivation: Existing self-evolving memory systems are designed for static train/test splits and approximate online learning, making them brittle under true distribution shifts and continuous feedback. There's a need for systems that can genuinely learn from streaming data over time.

Method: Live-Evo decouples experiences from guidelines via an Experience Bank and Meta-Guideline Bank. It maintains experience weights that are updated from feedback - helpful experiences are reinforced and retrieved more often, while misleading/stale experiences are down-weighted and gradually forgotten, similar to human memory reinforcement and decay.

Result: On the live Prophet Arena benchmark over 10 weeks, Live-Evo improved Brier score by 20.8% and increased market returns by 12.9%. It also transferred well to deep-research benchmarks with consistent gains over strong baselines.

Conclusion: Live-Evo demonstrates that online self-evolving memory systems can effectively handle distribution shifts and continuous feedback, outperforming static approaches and showing strong transfer capabilities across different benchmark types.

Abstract: Large language model (LLM) agents are increasingly equipped with memory, which are stored experience and reusable guidance that can improve task-solving performance. Recent \emph{self-evolving} systems update memory based on interaction outcomes, but most existing evolution pipelines are developed for static train/test splits and only approximate online learning by folding static benchmarks, making them brittle under true distribution shift and continuous feedback. We introduce \textsc{Live-Evo}, an online self-evolving memory system that learns from a stream of incoming data over time. \textsc{Live-Evo} decouples \emph{what happened} from \emph{how to use it} via an Experience Bank and a Meta-Guideline Bank, compiling task-adaptive guidelines from retrieved experiences for each task. To manage memory online, \textsc{Live-Evo} maintains experience weights and updates them from feedback: experiences that consistently help are reinforced and retrieved more often, while misleading or stale experiences are down-weighted and gradually forgotten, analogous to reinforcement and decay in human memory. On the live \textit{Prophet Arena} benchmark over a 10-week horizon, \textsc{Live-Evo} improves Brier score by 20.8\% and increases market returns by 12.9\%, while also transferring to deep-research benchmarks with consistent gains over strong baselines. Our code is available at https://github.com/ag2ai/Live-Evo.

</details>


### [421] [Trust by Design: Skill Profiles for Transparent, Cost-Aware LLM Routing](https://arxiv.org/abs/2602.02386)
*Mika Okamoto,Ansel Kaplan Erol,Glenn Matlin*

Main category: cs.AI

TL;DR: BELLA is a framework for budget-efficient LLM selection that uses skill-based profiling to recommend optimal models by analyzing required capabilities and balancing performance with cost constraints.


<details>
  <summary>Details</summary>
Motivation: Current LLM benchmarks report aggregate metrics that obscure specific task requirements, making it difficult for practitioners to determine if cheaper models could suffice without wasting money on over-provisioned models.

Method: Three-stage framework: (1) decomposing LLM outputs to extract granular skills using critic-based profiling, (2) clustering skills into structured capability matrices, and (3) multi-objective optimization to select models that maximize performance while respecting budget constraints.

Result: BELLA provides natural-language rationale for recommendations, offering transparency that current black-box routing systems lack, and enables principled cost-performance trade-offs for LLM deployment.

Conclusion: The framework helps practitioners make informed LLM selection decisions by providing interpretable skill-based recommendations that balance performance requirements with budget constraints, particularly useful in domains like financial reasoning with diverse skill requirements.

Abstract: How should Large Language Model (LLM) practitioners select the right model for a task without wasting money? We introduce BELLA (Budget-Efficient LLM Selection via Automated skill-profiling), a framework that recommends optimal LLM selection for tasks through interpretable skill-based model selection. Standard benchmarks report aggregate metrics that obscure which specific capabilities a task requires and whether a cheaper model could suffice. BELLA addresses this gap through three stages: (1) decomposing LLM outputs and extract the granular skills required by using critic-based profiling, (2) clustering skills into structured capability matrices, and (3) multi-objective optimization to select the right models to maximize performance while respecting budget constraints. BELLA provides natural-language rationale for recommendations, providing transparency that current black-box routing systems lack. We describe the framework architecture, situate it within the landscape of LLM routing and evaluation, and discuss its application to financial reasoning as a representative domain exhibiting diverse skill requirements and cost-variation across models. Our framework enables practitioners to make principled and cost-performance trade-offs for deploying LLMs.

</details>


### [422] [Structure Enables Effective Self-Localization of Errors in LLMs](https://arxiv.org/abs/2602.02416)
*Ankur Samanta,Akshayaa Magesh,Ayush Jain,Kavosh Asadi,Youliang Yu,Daniel Jiang,Boris Vidolov,Kaveh Hassani,Paul Sajda,Jalaj Bhandari,Yonathan Efroni*

Main category: cs.AI

TL;DR: Thought-ICS is a self-correction framework that structures reasoning into discrete thought steps, enabling language models to localize errors and backtrack to generate alternative reasoning, achieving 20-40% self-correction improvement.


<details>
  <summary>Details</summary>
Motivation: Self-correction in language models remains elusive, and the authors aim to build AI systems that can effectively correct themselves by exploring whether models can explicitly localize errors in incorrect reasoning.

Method: Introduces Iterative Correction Sampling of Thoughts (Thought-ICS), which structures reasoning as discrete, semantically coherent thought steps. The framework iteratively prompts the model to generate reasoning one discrete thought at a time, creates natural boundaries for error localization, and upon verification, localizes the first erroneous step and backtracks to generate alternative reasoning from the last correct point.

Result: When asked to correct reasoning verified as incorrect by an oracle, Thought-ICS achieves 20-40% self-correction lift. In a completely autonomous setting without external verification, it outperforms contemporary self-correction baselines.

Conclusion: Thought-ICS provides an effective framework for self-correction in language models by structuring reasoning into discrete thought steps, enabling reliable error localization and correction that outperforms existing approaches.

Abstract: Self-correction in language models remains elusive. In this work, we explore whether language models can explicitly localize errors in incorrect reasoning, as a path toward building AI systems that can effectively correct themselves. We introduce a prompting method that structures reasoning as discrete, semantically coherent thought steps, and show that models are able to reliably localize errors within this structure, while failing to do so in conventional, unstructured chain-of-thought reasoning. Motivated by how the human brain monitors errors at discrete decision points and resamples alternatives, we introduce Iterative Correction Sampling of Thoughts (Thought-ICS), a self-correction framework. Thought-ICS iteratively prompts the model to generate reasoning one discrete and complete thought at a time--where each thought represents a deliberate decision by the model--creating natural boundaries for precise error localization. Upon verification, the model localizes the first erroneous step, and the system backtracks to generate alternative reasoning from the last correct point. When asked to correct reasoning verified as incorrect by an oracle, Thought-ICS achieves 20-40% self-correction lift. In a completely autonomous setting without external verification, it outperforms contemporary self-correction baselines.

</details>


### [423] [SafeGround: Know When to Trust GUI Grounding Models via Uncertainty Calibration](https://arxiv.org/abs/2602.02419)
*Qingni Wang,Yue Fan,Xin Eric Wang*

Main category: cs.AI

TL;DR: SafeGround is an uncertainty-aware framework for GUI grounding that provides risk-aware predictions with statistical false discovery rate control to prevent costly GUI interaction errors.


<details>
  <summary>Details</summary>
Motivation: GUI grounding errors can lead to costly, hard-to-reverse actions (like erroneous payments), raising reliability concerns for automated GUI interaction systems.

Method: SafeGround uses distribution-aware uncertainty quantification to capture spatial dispersion of stochastic samples, then calibrates a test-time decision threshold with statistically guaranteed false discovery rate control.

Result: SafeGround's uncertainty measure outperforms existing baselines in distinguishing correct from incorrect predictions, and the calibrated threshold enables rigorous risk control with up to 5.38% system-level accuracy improvement over Gemini-only inference.

Conclusion: SafeGround provides an effective uncertainty-aware framework for GUI grounding that improves reliability through statistically guaranteed risk control and substantial accuracy improvements.

Abstract: Graphical User Interface (GUI) grounding aims to translate natural language instructions into executable screen coordinates, enabling automated GUI interaction. Nevertheless, incorrect grounding can result in costly, hard-to-reverse actions (e.g., erroneous payment approvals), raising concerns about model reliability. In this paper, we introduce SafeGround, an uncertainty-aware framework for GUI grounding models that enables risk-aware predictions through calibrations before testing. SafeGround leverages a distribution-aware uncertainty quantification method to capture the spatial dispersion of stochastic samples from outputs of any given model. Then, through the calibration process, SafeGround derives a test-time decision threshold with statistically guaranteed false discovery rate (FDR) control. We apply SafeGround on multiple GUI grounding models for the challenging ScreenSpot-Pro benchmark. Experimental results show that our uncertainty measure consistently outperforms existing baselines in distinguishing correct from incorrect predictions, while the calibrated threshold reliably enables rigorous risk control and potentials of substantial system-level accuracy improvements. Across multiple GUI grounding models, SafeGround improves system-level accuracy by up to 5.38\% percentage points over Gemini-only inference.

</details>


### [424] [Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling](https://arxiv.org/abs/2602.02453)
*Andong Chen,Wenxin Zhu,Qiuyu Ding,Yuchen Song,Muyun Yang,Tiejun Zhao*

Main category: cs.AI

TL;DR: Thinking with Comics uses comics as a visual reasoning medium between images and videos, offering temporal structure with lower computational cost than videos.


<details>
  <summary>Details</summary>
Motivation: Current Chain-of-Thought reasoning has limitations: static images lack temporal structure, while videos are computationally expensive and redundant. There's a need for a middle-ground visual representation that preserves temporal information efficiently.

Method: Proposes Thinking with Comics paradigm using comics as high information-density medium. Studies two reasoning paths based on comics and evaluates them on reasoning tasks and long-context understanding tasks.

Result: Outperforms Thinking with Images on multi-step temporal and causal reasoning tasks, while remaining substantially more efficient than Thinking with Video. Different comic narrative structures and styles consistently affect performance across tasks.

Conclusion: Comics serve as an effective intermediate visual representation for improving multimodal reasoning, balancing temporal structure preservation with computational efficiency.

Abstract: Chain-of-Thought reasoning has driven large language models to extend from thinking with text to thinking with images and videos. However, different modalities still have clear limitations: static images struggle to represent temporal structure, while videos introduce substantial redundancy and computational cost. In this work, we propose Thinking with Comics, a visual reasoning paradigm that uses comics as a high information-density medium positioned between images and videos. Comics preserve temporal structure, embedded text, and narrative coherence while requiring significantly lower reasoning cost. We systematically study two reasoning paths based on comics and evaluate them on a range of reasoning tasks and long-context understanding tasks. Experimental results show that Thinking with Comics outperforms Thinking with Images on multi-step temporal and causal reasoning tasks, while remaining substantially more efficient than Thinking with Video. Further analysis indicates that different comic narrative structures and styles consistently affect performance across tasks, suggesting that comics serve as an effective intermediate visual representation for improving multimodal reasoning.

</details>


### [425] [Drift-Bench: Diagnosing Cooperative Breakdowns in LLM Agents under Input Faults via Multi-Turn Interaction](https://arxiv.org/abs/2602.02455)
*Han Bao,Zheyuan Zhang,Pengcheng Jing,Zhengqing Yuan,Kaiwen Shi,Yanfang Ye*

Main category: cs.AI

TL;DR: Drift-Bench is a diagnostic benchmark for evaluating LLM agents' ability to handle input faults through multi-turn clarification in grounded execution environments, revealing significant performance drops and safety risks.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks assume well-specified instructions or use text-only, single-turn evaluation, failing to capture execution risks when user inputs violate cooperative assumptions (implicit intent, missing parameters, false presuppositions, ambiguous expressions).

Method: Introduces Drift-Bench with unified taxonomy of cooperative breakdowns, persona-driven user simulator, and Rise evaluation protocol for multi-turn clarification across state-oriented and service-oriented execution environments.

Result: Experiments show substantial performance drops under input faults, with clarification effectiveness varying across user personas and fault types, revealing safety risks in agent execution.

Conclusion: Drift-Bench bridges clarification research and agent safety evaluation, enabling systematic diagnosis of failures that can lead to unsafe executions in autonomous LLM agents.

Abstract: As Large Language Models transition to autonomous agents, user inputs frequently violate cooperative assumptions (e.g., implicit intent, missing parameters, false presuppositions, or ambiguous expressions), creating execution risks that text-only evaluations do not capture. Existing benchmarks typically assume well-specified instructions or restrict evaluation to text-only, single-turn clarification, and thus do not measure multi-turn disambiguation under grounded execution risk. We introduce \textbf{Drift-Bench}, the first diagnostic benchmark that evaluates agentic pragmatics under input faults through multi-turn clarification across state-oriented and service-oriented execution environments. Grounded in classical theories of communication, \textbf{Drift-Bench} provides a unified taxonomy of cooperative breakdowns and employs a persona-driven user simulator with the \textbf{Rise} evaluation protocol. Experiments show substantial performance drops under these faults, with clarification effectiveness varying across user personas and fault types. \MethodName bridges clarification research and agent safety evaluation, enabling systematic diagnosis of failures that can lead to unsafe executions.

</details>


### [426] [MentisOculi: Revealing the Limits of Reasoning with Mental Imagery](https://arxiv.org/abs/2602.02465)
*Jana Zeller,Thaddäus Wiedemer,Fanfei Li,Thomas Klein,Prasanna Mayilvahanan,Matthias Bethge,Felix Wichmann,Ryan Cotterell,Wieland Brendel*

Main category: cs.AI

TL;DR: Visual reasoning strategies fail to improve model performance despite intuitive appeal; UMMs struggle to leverage visualizations even when generated correctly.


<details>
  <summary>Details</summary>
Motivation: To evaluate whether multimodal models can use visual representations as reasoning aids (like human mental imagery) to improve multi-step problem-solving.

Method: Developed MentisOculi - a procedural, stratified suite of multi-step reasoning problems amenable to visual solutions, then tested various visual strategies (latent tokens to explicit generated imagery) on frontier models.

Result: Visual strategies generally fail to improve performance; UMMs specifically show compounding generation errors and cannot leverage even ground-truth visualizations despite having textual reasoning capacity.

Conclusion: Visual thoughts do not yet benefit model reasoning, establishing MentisOculi as foundation to analyze and close this gap across model families.

Abstract: Frontier models are transitioning from multimodal large language models (MLLMs) that merely ingest visual information to unified multimodal models (UMMs) capable of native interleaved generation. This shift has sparked interest in using intermediate visualizations as a reasoning aid, akin to human mental imagery. Central to this idea is the ability to form, maintain, and manipulate visual representations in a goal-oriented manner. To evaluate and probe this capability, we develop MentisOculi, a procedural, stratified suite of multi-step reasoning problems amenable to visual solution, tuned to challenge frontier models. Evaluating visual strategies ranging from latent tokens to explicit generated imagery, we find they generally fail to improve performance. Analysis of UMMs specifically exposes a critical limitation: While they possess the textual reasoning capacity to solve a task and can sometimes generate correct visuals, they suffer from compounding generation errors and fail to leverage even ground-truth visualizations. Our findings suggest that despite their inherent appeal, visual thoughts do not yet benefit model reasoning. MentisOculi establishes the necessary foundation to analyze and close this gap across diverse model families.

</details>


### [427] [Avenir-Web: Human-Experience-Imitating Multimodal Web Agents with Mixture of Grounding Experts](https://arxiv.org/abs/2602.02468)
*Aiden Yiliu Li,Xinyue Hao,Shilong Liu,Mengdi Wang*

Main category: cs.AI

TL;DR: Avenir-Web achieves new open-source SOTA on Online-Mind2Web benchmark by addressing element grounding, procedural knowledge, and task tracking issues in web agents.


<details>
  <summary>Details</summary>
Motivation: Existing web agents struggle with long-horizon tasks on complex web interfaces due to inaccurate element grounding, lack of site-specific procedural knowledge, and unstable long-term task tracking/memory, especially with complex DOM structures.

Method: Avenir-Web uses Mixture of Grounding Experts for accurate element identification, Experience-Imitation Planning to incorporate procedural priors, and a task-tracking checklist with adaptive memory for robust interaction across diverse UI paradigms.

Result: Avenir-Web significantly surpasses prior open-source agents and achieves performance parity with top-tier proprietary models on the Online-Mind2Web benchmark, establishing new open-source SOTA for reliable web agents on live websites.

Conclusion: Avenir-Web addresses key limitations of existing web agents and demonstrates state-of-the-art performance, making reliable autonomous web interaction more accessible through open-source technology.

Abstract: Despite advances in multimodal large language models, autonomous web agents still struggle to reliably execute long-horizon tasks on complex and dynamic web interfaces. Existing agents often suffer from inaccurate element grounding, the absence of site-specific procedural knowledge, and unstable long-term task tracking and memory, particularly when operating over complex Document Object Model structures. To address these limitations, we introduce Avenir-Web, a web agent that achieves a new open-source state of the art on the Online-Mind2Web benchmark in real-world deployment. Avenir-Web leverages a Mixture of Grounding Experts, Experience-Imitation Planning for incorporating procedural priors, and a task-tracking checklist combined with adaptive memory to enable robust and seamless interaction across diverse user interface paradigms. We evaluate Avenir-Web on Online-Mind2Web, a rigorous benchmark of live and user-centered web tasks. Our results demonstrate that Avenir-Web significantly surpasses prior open-source agents and attains performance parity with top-tier proprietary models, thereby establishing a new open-source state of the art for reliable web agents on live websites.

</details>


### [428] [Breaking the Reversal Curse in Autoregressive Language Models via Identity Bridge](https://arxiv.org/abs/2602.02470)
*Xutao Ma,Yixiao Huang,Hanlin Zhu,Somayeh Sojoudi*

Main category: cs.AI

TL;DR: Adding simple "Identity Bridge" data (A→A) during training helps LLMs overcome the reversal curse, enabling them to learn bidirectional relationships from unidirectional training data.


<details>
  <summary>Details</summary>
Motivation: The reversal curse shows LLMs fail at simple logical reasoning like deducing "Bob's wife is Alice" from training on "Alice's husband is Bob." Prior work suggests this is a fundamental limit of autoregressive LLMs, but this paper challenges that view.

Method: Proposes a simple regularization data recipe called "Identity Bridge" (A→A) added to training data. Theoretically analyzes gradient descent implicit bias, showing even one-layer transformers can break the reversal curse with this approach.

Result: A 1B parameter LLM finetuned with Identity Bridge achieves 40% success rate on reversal tasks, compared to near-zero success with only forward-knowledge data. This demonstrates the curse can be mitigated.

Conclusion: The reversal curse is not a fundamental limit of autoregressive LLMs. Simple data regularization with Identity Bridge provides a low-cost way to encourage LLMs to learn higher-level rules rather than just memorizing facts.

Abstract: Autoregressive large language models (LLMs) have achieved remarkable success in many complex tasks, yet they can still fail in very simple logical reasoning such as the "reversal curse" -- when trained on forward knowledge data of the form "$A \rightarrow B$" (e.g., Alice's husband is Bob), the model is unable to deduce the reversal knowledge "$B \leftarrow A$" (e.g., Bob's wife is Alice) during test. Extensive prior research suggests that this failure is an inherent, fundamental limit of autoregressive causal LLMs, indicating that these models tend to memorize factual-level knowledge rather than capture higher-level rules. In this paper, we challenge this view by showing that this seemingly fundamental limit can be mitigated by slightly tweaking the training data with a simple regularization data recipe called the Identity Bridge of the form "$A \to A$" (e.g., The name of Alice is Alice). Theoretically, we prove that under this recipe, even a one-layer transformer can break the reversal curse by analyzing the implicit bias of gradient descent. Empirically, we show that a 1B pretrained language model finetuned with the proposed data recipe achieves a 40% success rate on reversal tasks, in stark contrast to a near-zero success rate when trained solely on forward-knowledge data. Our work provides a novel theoretical foundation for the reversal curse and offers a principled, low-cost path to encouraging LLMs to learn higher-level rules from data.

</details>


### [429] [AgentRx: Diagnosing AI Agent Failures from Execution Trajectories](https://arxiv.org/abs/2602.02475)
*Shraddha Barke,Arnav Goyal,Alind Khare,Avaljot Singh,Suman Nath,Chetan Bansal*

Main category: cs.AI

TL;DR: AGENTRX is an automated diagnostic framework that pinpoints critical failure steps in AI agent trajectories using constraint synthesis and LLM-based judgment, evaluated on a novel benchmark of 115 failed agent runs across multiple domains.


<details>
  <summary>Details</summary>
Motivation: AI agents often fail in complex ways that are difficult to localize due to probabilistic execution, long-horizon tasks, multi-agent interactions, and noisy tool outputs, creating a need for systematic failure diagnosis.

Method: Created a benchmark of 115 manually annotated failed agent trajectories across API workflows, incident management, and web/file tasks. Developed AGENTRX framework that synthesizes constraints, evaluates them step-by-step, produces validation logs of constraint violations, and uses an LLM-based judge to localize critical failure steps and categories.

Result: AGENTRX improves step localization and failure attribution over existing baselines across three domains (structured API workflows, incident management, and open-ended web/file tasks).

Conclusion: The proposed AGENTRX framework provides an effective, domain-agnostic approach for automated diagnosis of AI agent failures, addressing the challenge of localizing failures in complex agent executions.

Abstract: AI agents often fail in ways that are difficult to localize because executions are probabilistic, long-horizon, multi-agent, and mediated by noisy tool outputs. We address this gap by manually annotating failed agent runs and release a novel benchmark of 115 failed trajectories spanning structured API workflows, incident management, and open-ended web/file tasks. Each trajectory is annotated with a critical failure step and a category from a grounded-theory derived, cross-domain failure taxonomy. To mitigate the human cost of failure attribution, we present AGENTRX, an automated domain-agnostic diagnostic framework that pinpoints the critical failure step in a failed agent trajectory. It synthesizes constraints, evaluates them step-by-step, and produces an auditable validation log of constraint violations with associated evidence; an LLM-based judge uses this log to localize the critical step and category. Our framework improves step localization and failure attribution over existing baselines across three domains.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [430] [MapDream: Task-Driven Map Learning for Vision-Language Navigation](https://arxiv.org/abs/2602.00222)
*Guoxin Lian,Shuo Wang,Yucheng Wang,Yongcai Wang,Maiyue Chen,Kaihui Wang,Bo Zhang,Zhizhong Su,Deying Li,Zhaoxin Fan*

Main category: cs.RO

TL;DR: MapDream learns navigation-optimized maps via autoregressive BEV synthesis instead of hand-crafted reconstructions, achieving SOTA monocular VLN performance.


<details>
  <summary>Details</summary>
Motivation: Existing VLN approaches rely on hand-crafted maps constructed independently of navigation policy, which may not be optimal for task performance. The authors argue maps should be learned representations shaped directly by navigation objectives rather than exhaustive reconstructions.

Method: Proposes MapDream, a map-in-the-loop framework that formulates map construction as autoregressive bird's-eye-view (BEV) image synthesis. Jointly learns map generation and action prediction, distilling environmental context into compact three-channel BEV maps preserving only navigation-critical affordances. Uses supervised pre-training to bootstrap mapping-to-control interface, then end-to-end joint optimization through reinforcement fine-tuning.

Result: Achieves state-of-the-art monocular performance on R2R-CE and RxR-CE benchmarks, validating the effectiveness of task-driven generative map learning for VLN.

Conclusion: Task-driven generative map learning through autoregressive BEV synthesis is effective for VLN, outperforming traditional hand-crafted map approaches by directly optimizing maps for navigation objectives.

Abstract: Vision-Language Navigation (VLN) requires agents to follow natural language instructions in partially observed 3D environments, motivating map representations that aggregate spatial context beyond local perception. However, most existing approaches rely on hand-crafted maps constructed independently of the navigation policy. We argue that maps should instead be learned representations shaped directly by navigation objectives rather than exhaustive reconstructions. Based on this insight, we propose MapDream, a map-in-the-loop framework that formulates map construction as autoregressive bird's-eye-view (BEV) image synthesis. The framework jointly learns map generation and action prediction, distilling environmental context into a compact three-channel BEV map that preserves only navigation-critical affordances. Supervised pre-training bootstraps a reliable mapping-to-control interface, while the autoregressive design enables end-to-end joint optimization through reinforcement fine-tuning. Experiments on R2R-CE and RxR-CE achieve state-of-the-art monocular performance, validating task-driven generative map learning.

</details>


### [431] [ZEST: Zero-shot Embodied Skill Transfer for Athletic Robot Control](https://arxiv.org/abs/2602.00401)
*Jean Pierre Sleiman,He Li,Alphonsus Adu-Bredu,Robin Deits,Arun Kumar,Kevin Bergamin,Mohak Bhardwaj,Scott Biddlestone,Nicola Burger,Matthew A. Estrada,Francesco Iacobelli,Twan Koolen,Alexander Lambert,Erica Lin,M. Eva Mungai,Zach Nobles,Shane Rozen-Levy,Yuyao Shi,Jiashun Wang,Jakob Welner,Fangzhou Yu,Mike Zhang,Alfred Rizzi,Jessica Hodgins,Sylvain Bertrand,Yeuhi Abe,Scott Kuindersma,Farbod Farshidian*

Main category: cs.RO

TL;DR: ZEST enables zero-shot transfer of diverse motion skills to humanoid robots from mocap, video, and animation without per-skill engineering.


<details>
  <summary>Details</summary>
Motivation: Current humanoid robot control requires heavy per-skill engineering and brittle tuning processes, making robust whole-body control for agile, contact-rich behaviors challenging.

Method: Uses RL training from diverse motion sources (mocap, video, animation) with adaptive sampling and automatic curriculum using model-based assistive wrench. Includes joint gain selection from analytical armature values and refined actuator modeling.

Result: Successfully transferred dynamic multi-contact skills (army crawl, breakdancing) to Atlas from mocap, expressive dance and box-climbing from video to Atlas/G1, and acrobatics (continuous backflip) to Spot quadruped from animation.

Conclusion: ZEST provides robust zero-shot deployment across heterogeneous data sources and robot embodiments, establishing a scalable interface between biological movements and robotic counterparts.

Abstract: Achieving robust, human-like whole-body control on humanoid robots for agile, contact-rich behaviors remains a central challenge, demanding heavy per-skill engineering and a brittle process of tuning controllers. We introduce ZEST (Zero-shot Embodied Skill Transfer), a streamlined motion-imitation framework that trains policies via reinforcement learning from diverse sources -- high-fidelity motion capture, noisy monocular video, and non-physics-constrained animation -- and deploys them to hardware zero-shot. ZEST generalizes across behaviors and platforms while avoiding contact labels, reference or observation windows, state estimators, and extensive reward shaping. Its training pipeline combines adaptive sampling, which focuses training on difficult motion segments, and an automatic curriculum using a model-based assistive wrench, together enabling dynamic, long-horizon maneuvers. We further provide a procedure for selecting joint-level gains from approximate analytical armature values for closed-chain actuators, along with a refined model of actuators. Trained entirely in simulation with moderate domain randomization, ZEST demonstrates remarkable generality. On Boston Dynamics' Atlas humanoid, ZEST learns dynamic, multi-contact skills (e.g., army crawl, breakdancing) from motion capture. It transfers expressive dance and scene-interaction skills, such as box-climbing, directly from videos to Atlas and the Unitree G1. Furthermore, it extends across morphologies to the Spot quadruped, enabling acrobatics, such as a continuous backflip, through animation. Together, these results demonstrate robust zero-shot deployment across heterogeneous data sources and embodiments, establishing ZEST as a scalable interface between biological movements and their robotic counterparts.

</details>


### [432] [FISC: A Fluid-Inspired Framework for Decentralized and Scalable Swarm Control](https://arxiv.org/abs/2602.00480)
*Mohini Priya Kolluri,Ammar Waheed,Zohaib Hasnain*

Main category: cs.RO

TL;DR: A decentralized fluid-inspired control method enables large robotic swarms to coordinate without communication by treating agents as fluid elements, achieving scalable collective movement.


<details>
  <summary>Details</summary>
Motivation: Traditional swarm coordination relies on inter-agent communication which introduces latency, bandwidth limitations, and vulnerability to failure, creating scalability challenges for large robotic systems.

Method: Developed a decentralized approach where individual robotic agents are mapped to fundamental fluidic element properties, enabling the swarm to "flow" through space like a fluid under pressure boundary conditions without explicit communication.

Result: Simulations with O(10³) quadcopter agents showed quantitative agreement with Computational Fluid Dynamics solutions, with normalized RMSE errors of 0.15-0.9 for velocity, 0.61-0.98 for density, and 0-0.937 for pressure.

Conclusion: The approach demonstrates that large robotic swarms can be effectively treated as continuum systems, providing a scalable and decentralized control framework that maintains macroscopic structure without communication requirements.

Abstract: Achieving scalable coordination in large robotic swarms is often constrained by reliance on inter-agent communication, which introduces latency, bandwidth limitations, and vulnerability to failure. To address this gap, a decentralized approach for outer-loop control of large multi-agent systems based on the paradigm of how a fluid moves through a volume is proposed and evaluated. A relationship between fundamental fluidic element properties and individual robotic agent states is developed such that the corresponding swarm "flows" through a space, akin to a fluid when forced via a pressure boundary condition. By ascribing fluid-like properties to subsets of agents, the swarm evolves collectively while maintaining desirable structure and coherence without explicit communication of agent states within or outside of the swarm. The approach is evaluated using simulations involving $O(10^3)$ quadcopter agents and compared against Computational Fluid Dynamics (CFD) solutions for a converging-diverging domain. Quantitative agreement between swarm-derived and CFD fields is assessed using Root-Mean-Square Error (RMSE), yielding normalized errors of 0.15-0.9 for velocity, 0.61-0.98 for density, 0-0.937 for pressure. These results demonstrate the feasibility of treating large robotic swarms as continuum systems that retain the macroscopic structure derived from first principles, providing a basis for scalable and decentralized control.

</details>


### [433] [Inject Once Survive Later: Backdooring Vision-Language-Action Models to Persist Through Downstream Fine-tuning](https://arxiv.org/abs/2602.00500)
*Jianyi Zhou,Yujie Wei,Ruichen Zhen,Bo Zhao,Xiaobo Xia,Rui Shao,Xiu Su,Shuo Yang*

Main category: cs.RO

TL;DR: INFUSE is a backdoor attack framework for Vision-Language-Action models that remains effective even after user fine-tuning by targeting fine-tune-insensitive modules.


<details>
  <summary>Details</summary>
Motivation: VLA models are crucial for embodied AI but their security is underexplored. Existing backdoor attacks are easily erased during downstream fine-tuning, making them impractical for real-world deployment threats.

Method: INFUSE analyzes parameter sensitivity to identify fine-tune-insensitive modules, injects backdoors into these stable modules while freezing others, ensuring persistence through arbitrary user fine-tuning.

Result: After fine-tuning, INFUSE maintains 91.0% attack success in simulation and 79.8% in real-world robot tasks, significantly outperforming BadVLA (38.8% and 36.6%) while preserving clean-task performance.

Conclusion: The research reveals a critical security threat: backdoors implanted before model distribution can persist through fine-tuning and remain effective at deployment, highlighting the need for robust VLA security measures.

Abstract: Vision-Language-Action (VLA) models have become foundational to modern embodied AI systems. By integrating visual perception, language understanding, and action planning, they enable general-purpose task execution across diverse environments. Despite their importance, the security of VLA models remains underexplored -- particularly in the context of backdoor attacks, which pose realistic threats in physical-world deployments. While recent methods attempt to inject backdoors into VLA models, these backdoors are easily erased during downstream adaptation, as user-side fine-tuning with clean data significantly alters model parameters, rendering them impractical for real-world applications. To address these challenges, we propose INFUSE (INjection into Fine-tUne-inSensitive modulEs), the first backdoor attack framework for VLA base models that remains effective even with arbitrary user fine-tuning. INFUSE begins by analyzing parameter sensitivity across diverse fine-tuning scenarios to identify modules that remain largely unchanged -- the fine-tune-insensitive modules. It then injects backdoors into these stable modules while freezing the rest, ensuring malicious behavior persists after extensive user fine-tuning. Comprehensive experiments across multiple VLA architectures demonstrate INFUSE's effectiveness. After user-side fine-tuning, INFUSE maintains mean attack success rates of 91.0% on simulation environments and 79.8% on real-world robot tasks, substantially surpassing BadVLA (38.8% and 36.6%, respectively), while preserving clean-task performance comparable to standard models. These results uncover a critical threat: backdoors implanted before distribution can persist through fine-tuning and remain effective at deployment.

</details>


### [434] [A Low-Cost Vision-Based Tactile Gripper with Pretraining Learning for Contact-Rich Manipulation](https://arxiv.org/abs/2602.00514)
*Yaohua Liu,Binkai Ou,Zicheng Qiu,Ce Hao,Yemin Wang,Hengjun Zhang*

Main category: cs.RO

TL;DR: LVTG is a low-cost visuo-tactile gripper with enhanced tactile area and durability that uses CLIP-inspired contrastive learning to align tactile and visual data, improving manipulation policy performance.


<details>
  <summary>Details</summary>
Motivation: Robotic manipulation in contact-rich environments is challenging due to limitations of conventional tactile sensors (limited range, reliability, cost). Existing visuo-tactile sensors struggle with larger/heavier everyday objects.

Method: Developed LVTG gripper with enhanced tactile sensing area, greater opening angle, wear-resistant skin, and modular design. Used CLIP-inspired contrastive learning to align tactile embeddings with visual observations, creating shared cross-modal representation space for visuo-tactile perception integrated with Action Chunking Transformer (ACT) policy.

Result: LVTG enables more effective and stable grasping of larger/heavier objects. The pretrained LVTG with ACT achieves significantly higher success rates in manipulation tasks compared to original ACT method, with more efficient data collection and policy learning.

Conclusion: LVTG provides a low-cost, durable visuo-tactile solution with enhanced sensing capabilities and effective cross-modal learning that improves robotic manipulation performance in contact-rich environments.

Abstract: Robotic manipulation in contact-rich environments remains challenging, particularly when relying on conventional tactile sensors that suffer from limited sensing range, reliability, and cost-effectiveness. In this work, we present LVTG, a low-cost visuo-tactile gripper designed for stable, robust, and efficient physical interaction. Unlike existing visuo-tactile sensors, LVTG enables more effective and stable grasping of larger and heavier everyday objects, thanks to its enhanced tactile sensing area and greater opening angle. Its surface skin is made of highly wear-resistant material, significantly improving durability and extending operational lifespan. The integration of vision and tactile feedback allows LVTG to provide rich, high-fidelity sensory data, facilitating reliable perception during complex manipulation tasks. Furthermore, LVTG features a modular design that supports rapid maintenance and replacement. To effectively fuse vision and touch, We adopt a CLIP-inspired contrastive learning objective to align tactile embeddings with their corresponding visual observations, enabling a shared cross-modal representation space for visuo-tactile perception. This alignment improves the performance of an Action Chunking Transformer (ACT) policy in contact-rich manipulation, leading to more efficient data collection and more effective policy learning. Compared to the original ACT method, the proposed LVTG with pretraining achieves significantly higher success rates in manipulation tasks.

</details>


### [435] [APEX: A Decoupled Memory-based Explorer for Asynchronous Aerial Object Goal Navigation](https://arxiv.org/abs/2602.00551)
*Daoxuan Zhang,Ping Chen,Xiaobo Xia,Xiu Su,Ruichen Zhen,Jianqiang Xiao,Shuo Yang*

Main category: cs.RO

TL;DR: APEX is a hierarchical UAV agent for aerial object goal navigation that uses dynamic spatio-semantic mapping, reinforcement learning for action decisions, and open-vocabulary detection for target identification, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing aerial navigation methods struggle with memorizing complex spatial representations, making reliable action decisions, and exploring efficiently in aerial environments.

Method: Three-part hierarchical architecture: 1) Dynamic Spatio-Semantic Mapping Memory using VLM for 3D attraction/exploration/obstacle maps, 2) Action Decision Module trained with RL, 3) Target Grounding Module with open-vocabulary detector, all integrated in asynchronous parallel framework.

Result: Outperforms previous state-of-the-art by +4.2% SR and +2.8% SPL on UAV-ON benchmarks, demonstrating superior efficiency and effectiveness of hierarchical asynchronous design.

Conclusion: APEX successfully addresses key challenges in aerial object goal navigation through its modular hierarchical architecture, achieving efficient exploration and target acquisition in complex aerial settings.

Abstract: Aerial Object Goal Navigation, a challenging frontier in Embodied AI, requires an Unmanned Aerial Vehicle (UAV) agent to autonomously explore, reason, and identify a specific target using only visual perception and language description. However, existing methods struggle with the memorization of complex spatial representations in aerial environments, reliable and interpretable action decision-making, and inefficient exploration and information gathering. To address these challenges, we introduce \textbf{APEX} (Aerial Parallel Explorer), a novel hierarchical agent designed for efficient exploration and target acquisition in complex aerial settings. APEX is built upon a modular, three-part architecture: 1) Dynamic Spatio-Semantic Mapping Memory, which leverages the zero-shot capability of a Vision-Language Model (VLM) to dynamically construct high-resolution 3D Attraction, Exploration, and Obstacle maps, serving as an interpretable memory mechanism. 2) Action Decision Module, trained with reinforcement learning, which translates this rich spatial understanding into a fine-grained and robust control policy. 3) Target Grounding Module, which employs an open-vocabulary detector to achieve definitive and generalizable target identification. All these components are integrated into a hierarchical, asynchronous, and parallel framework, effectively bypassing the VLM's inference latency and boosting the agent's proactivity in exploration. Extensive experiments show that APEX outperforms the previous state of the art by +4.2\% SR and +2.8\% SPL on challenging UAV-ON benchmarks, demonstrating its superior efficiency and the effectiveness of its hierarchical asynchronous design. Our source code is provided in \href{https://github.com/4amGodvzx/apex}{GitHub}

</details>


### [436] [ConLA: Contrastive Latent Action Learning from Human Videos for Robotic Manipulation](https://arxiv.org/abs/2602.00557)
*Weisheng Dai,Kai Lan,Jianyi Zhou,Bo Zhao,Xiu Su,Junwen Tong,Weili Guan,Shuo Yang*

Main category: cs.RO

TL;DR: ConLA: Contrastive disentanglement framework for learning robotic policies from human videos without action labels, outperforming real robot pretraining.


<details>
  <summary>Details</summary>
Motivation: Robot teleoperation datasets are costly and hard to scale, while human videos offer rich, diverse demonstrations but lack explicit action supervision. Existing VQ-VAE methods focus on visual reconstruction rather than dynamics, leading to shortcut learning and poor transferability.

Method: Proposes ConLA with contrastive disentanglement mechanism that uses action category priors and temporal cues to isolate motion dynamics from visual content, mitigating shortcut learning in unsupervised pretraining from human videos.

Result: Achieves strong performance across diverse benchmarks. For the first time, pretraining solely on human videos surpasses performance obtained with real robot trajectory pretraining.

Conclusion: ConLA effectively extracts pure, semantically consistent latent action representations from human videos, enabling scalable robot learning without costly robot data collection.

Abstract: Vision-Language-Action (VLA) models achieve preliminary generalization through pretraining on large scale robot teleoperation datasets. However, acquiring datasets that comprehensively cover diverse tasks and environments is extremely costly and difficult to scale. In contrast, human demonstration videos offer a rich and scalable source of diverse scenes and manipulation behaviors, yet their lack of explicit action supervision hinders direct utilization. Prior work leverages VQ-VAE based frameworks to learn latent actions from human videos in an unsupervised manner. Nevertheless, since the training objective primarily focuses on reconstructing visual appearances rather than capturing inter-frame dynamics, the learned representations tend to rely on spurious visual cues, leading to shortcut learning and entangled latent representations that hinder transferability. To address this, we propose ConLA, an unsupervised pretraining framework for learning robotic policies from human videos. ConLA introduces a contrastive disentanglement mechanism that leverages action category priors and temporal cues to isolate motion dynamics from visual content, effectively mitigating shortcut learning. Extensive experiments show that ConLA achieves strong performance across diverse benchmarks. Notably, by pretraining solely on human videos, our method for the first time surpasses the performance obtained with real robot trajectory pretraining, highlighting its ability to extract pure and semantically consistent latent action representations for scalable robot learning.

</details>


### [437] [UniMotion: A Unified Motion Framework for Simulation, Prediction and Planning](https://arxiv.org/abs/2602.00566)
*Nan Song,Junzhe Jiang,Jingyu Li,Xiatian Zhu,Li Zhang*

Main category: cs.RO

TL;DR: UniMotion is a unified Transformer framework that jointly addresses motion simulation, prediction, and planning in autonomous driving through shared representation learning and task-specific fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Current approaches treat motion simulation, prediction, and planning as isolated tasks with specialized models, missing opportunities for cross-task generalization and mutual benefits despite their shared requirements for understanding interactions, behaviors, and spatiotemporal dynamics.

Method: Decoder-only Transformer architecture with dedicated interaction modes and tailored training strategies to simultaneously support multiple motion tasks, enabling joint optimization and representation sharing while allowing task-specific fine-tuning.

Result: Extensive experiments on Waymo Open Motion Dataset show joint training enables robust generalization and effective task integration, with fine-tuned versions achieving state-of-the-art performance across various motion tasks.

Conclusion: UniMotion establishes a versatile and scalable unified framework for autonomous driving motion tasks, demonstrating that capturing shared structures while accommodating individual requirements leads to superior performance and generalization.

Abstract: Motion simulation, prediction and planning are foundational tasks in autonomous driving, each essential for modeling and reasoning about dynamic traffic scenarios. While often addressed in isolation due to their differing objectives, such as generating diverse motion states or estimating optimal trajectories, these tasks inherently depend on shared capabilities: understanding multi-agent interactions, modeling motion behaviors, and reasoning over temporal and spatial dynamics. Despite this underlying commonality, existing approaches typically adopt specialized model designs, which hinders cross-task generalization and system scalability. More critically, this separation overlooks the potential mutual benefits among tasks. Motivated by these observations, we propose UniMotion, a unified motion framework that captures shared structures across motion tasks while accommodating their individual requirements. Built on a decoder-only Transformer architecture, UniMotion employs dedicated interaction modes and tailored training strategies to simultaneously support these motion tasks. This unified design not only enables joint optimization and representation sharing but also allows for targeted fine-tuning to specialize in individual tasks when needed. Extensive experiments on the Waymo Open Motion Dataset demonstrate that joint training leads to robust generalization and effective task integration. With further fine-tuning, UniMotion achieves state-of-the-art performance across a range of motion tasks, establishing it as a versatile and scalable solution for autonomous driving.

</details>


### [438] [Agentic Reward Modeling: Verifying GUI Agent via Online Proactive Interaction](https://arxiv.org/abs/2602.00575)
*Chaoqun Cui,Jing Huang,Shijing Wang,Liming Zheng,Qingchao Kong,Zhixiong Zeng*

Main category: cs.RO

TL;DR: VAGEN introduces Agentic Interactive Verification for GUI agents, using a verifier agent with interaction tools to proactively verify task completion, overcoming limitations of passive evaluation methods.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation methods for GUI agents have limitations: rule-based approaches lack scalability and can't handle open-ended tasks, while LLM-as-a-Judge methods rely on passive visual observation and fail to capture latent system states due to partial observability.

Method: VAGEN employs a verifier agent equipped with interaction tools to autonomously plan verification strategies and proactively probe the environment for evidence of task completion, leveraging the insight that GUI tasks are "easy to verify but hard to solve."

Result: Experimental results on OSWorld-Verified and AndroidWorld benchmarks show VAGEN significantly improves evaluation accuracy compared to LLM-as-a-Judge baselines and further enhances performance through test-time scaling strategies.

Conclusion: The paper advocates for a paradigm shift from passive evaluation to Agentic Interactive Verification, demonstrating that proactive interaction-based verification overcomes the bottlenecks of visual limitations in GUI agent evaluation.

Abstract: Reinforcement learning with verifiable rewards (RLVR) is pivotal for the continuous evolution of GUI agents, yet existing evaluation paradigms face significant limitations. Rule-based methods suffer from poor scalability and cannot handle open-ended tasks, while LLM-as-a-Judge approaches rely on passive visual observation, often failing to capture latent system states due to partial state observability. To address these challenges, we advocate for a paradigm shift from passive evaluation to Agentic Interactive Verification. We introduce VAGEN, a framework that employs a verifier agent equipped with interaction tools to autonomously plan verification strategies and proactively probe the environment for evidence of task completion. Leveraging the insight that GUI tasks are typically "easy to verify but hard to solve", VAGEN overcomes the bottlenecks of visual limitations. Experimental results on OSWorld-Verified and AndroidWorld benchmarks demonstrate that VAGEN significantly improves evaluation accuracy compared to LLM-as-a-Judge baselines and further enhances performance through test-time scaling strategies.

</details>


### [439] [Factored Reasoning with Inner Speech and Persistent Memory for Evidence-Grounded Human-Robot Interaction](https://arxiv.org/abs/2602.00675)
*Valerio Belcamino,Mariya Kilina,Alessandro Carfì,Valeria Seidita,Fulvio Mastrogiovanni,Antonio Chella*

Main category: cs.RO

TL;DR: JANUS is a cognitive architecture for assistive robots that uses factored controllers and inner speech models to enable verifiable, evidence-grounded dialogue with persistent user context and recovery from underspecified requests.


<details>
  <summary>Details</summary>
Motivation: Dialogue-based human-robot interaction requires robots to maintain persistent user context, recover from underspecified requests, ground responses in external evidence, and keep intermediate decisions verifiable for auditable assistance.

Method: Models interaction as a partially observable Markov decision process with factored controllers. Uses specialized modules for scope detection, intent recognition, memory, inner speech, query generation, and outer speech. Features explicit policies for information sufficiency, execution readiness, and tool grounding. Includes a memory agent with recent-history buffer, core memory, and archival store with semantic retrieval.

Result: Evaluated through module-level unit tests in a dietary assistance domain grounded on a knowledge graph, showing high agreement with curated references and practical latency profiles.

Conclusion: Factored reasoning is a promising path to scalable, auditable, and evidence-grounded robot assistance over extended interaction horizons.

Abstract: Dialogue-based human-robot interaction requires robot cognitive assistants to maintain persistent user context, recover from underspecified requests, and ground responses in external evidence, while keeping intermediate decisions verifiable. In this paper we introduce JANUS, a cognitive architecture for assistive robots that models interaction as a partially observable Markov decision process and realizes control as a factored controller with typed interfaces. To this aim, Janus (i) decomposes the overall behavior into specialized modules, related to scope detection, intent recognition, memory, inner speech, query generation, and outer speech, and (ii) exposes explicit policies for information sufficiency, execution readiness, and tool grounding. A dedicated memory agent maintains a bounded recent-history buffer, a compact core memory, and an archival store with semantic retrieval, coupled through controlled consolidation and revision policies. Models inspired by the notion of inner speech in cognitive theories provide a control-oriented internal textual flow that validates parameter completeness and triggers clarification before grounding, while a faithfulness constraint ties robot-to-human claims to an evidence bundle combining working context and retrieved tool outputs. We evaluate JANUS through module-level unit tests in a dietary assistance domain grounded on a knowledge graph, reporting high agreement with curated references and practical latency profiles. These results support factored reasoning as a promising path to scalable, auditable, and evidence-grounded robot assistance over extended interaction horizons.

</details>


### [440] [Toward Reliable Sim-to-Real Predictability for MoE-based Robust Quadrupedal Locomotion](https://arxiv.org/abs/2602.00678)
*Tianyang Wu,Hanwei Guo,Yuhang Wang,Junshu Yang,Xinyang Sui,Jiayi Xie,Xingyu Chen,Zeyang Liu,Xuguang Lan*

Main category: cs.RO

TL;DR: A unified framework combining Mixture-of-Experts locomotion policy and RoboGauge assessment suite enables robust quadrupedal locomotion on challenging terrains using only proprioception, achieving 4 m/s speeds and emergent stability gaits.


<details>
  <summary>Details</summary>
Motivation: Address sim-to-real gap and reward overfitting issues in reinforcement learning for quadrupedal locomotion, which often produce policies that fail to transfer to real-world complex terrains, while physical validation remains risky and inefficient.

Method: Developed a unified framework with: 1) Mixture-of-Experts (MoE) locomotion policy using gated specialist experts to decompose latent terrain and command modeling for robust multi-terrain representation, and 2) RoboGauge predictive assessment suite that quantifies sim-to-real transferability via multi-dimensional proprioception-based metrics through sim-to-sim tests.

Result: Demonstrated robust locomotion on Unitree Go2 robot on unseen challenging terrains including snow, sand, stairs, slopes, and 30 cm obstacles. Achieved 4 m/s high-speed performance with emergent narrow-width gait associated with improved stability at high velocity.

Conclusion: The MoE policy with RoboGauge assessment enables reliable policy selection without extensive physical trials, achieving superior deployment robustness and generalization using only proprioception, effectively addressing sim-to-real transfer challenges in quadrupedal locomotion.

Abstract: Reinforcement learning has shown strong promise for quadrupedal agile locomotion, even with proprioception-only sensing. In practice, however, sim-to-real gap and reward overfitting in complex terrains can produce policies that fail to transfer, while physical validation remains risky and inefficient. To address these challenges, we introduce a unified framework encompassing a Mixture-of-Experts (MoE) locomotion policy for robust multi-terrain representation with RoboGauge, a predictive assessment suite that quantifies sim-to-real transferability. The MoE policy employs a gated set of specialist experts to decompose latent terrain and command modeling, achieving superior deployment robustness and generalization via proprioception alone. RoboGauge further provides multi-dimensional proprioception-based metrics via sim-to-sim tests over terrains, difficulty levels, and domain randomizations, enabling reliable MoE policy selection without extensive physical trials. Experiments on a Unitree Go2 demonstrate robust locomotion on unseen challenging terrains, including snow, sand, stairs, slopes, and 30 cm obstacles. In dedicated high-speed tests, the robot reaches 4 m/s and exhibits an emergent narrow-width gait associated with improved stability at high velocity.

</details>


### [441] [Learning to Accelerate Vision-Language-Action Models through Adaptive Visual Token Caching](https://arxiv.org/abs/2602.00686)
*Yujie Wei,Jiahan Fan,Jiyu Guo,Ruichen Zhen,Rui Shao,Xiu Su,Zeke Xie,Shuo Yang*

Main category: cs.RO

TL;DR: VLA models are powerful but computationally heavy. This paper proposes a learnable policy optimization framework with two lightweight modules (token selector and ratio predictor) to dynamically optimize inference, achieving 1.76x speedup while improving task success rates.


<details>
  <summary>Details</summary>
Motivation: Vision-Language-Action models show great generalization for robotic manipulation but have high computational overhead that hinders real-world deployment. Existing acceleration methods use static, heuristic strategies that don't adapt to dynamic scenes or task objectives.

Method: Reformulates inference acceleration as learnable policy optimization. Introduces two cooperative modules: Cached Token Selector (decides which tokens to reuse) and Cache Ratio Predictor (controls how many tokens to reuse). Uses differentiable relaxation for gradient-based end-to-end optimization of these discrete decision modules.

Result: Achieves 1.76x wall-clock inference speedup while improving average success rate by 1.9 percentage points (from 75.0% to 76.9%) on LIBERO benchmark and 5.0 percentage points on real-world tasks, outperforming existing baselines.

Conclusion: Demonstrates the potential of learning task-aware computational allocation policies, paving the way for VLA models that are both powerful and efficient for practical robotic applications.

Abstract: Vision-Language-Action (VLA) models have demonstrated remarkable generalization capabilities in robotic manipulation tasks, yet their substantial computational overhead remains a critical obstacle to real-world deployment. Improving inference efficiency is therefore essential for practical robotic applications. Existing acceleration methods often rely on heuristic or static strategies--such as rule-based token caching or pruning--that are decoupled from task objectives and fail to adapt to dynamic scene changes. In this work, we reformulate inference acceleration as a learnable policy optimization problem and propose a novel framework that integrates a dynamic, task-aware decision-making process directly into the VLA model. At its core are two lightweight, cooperative modules: a Cached Token Selector, which determines which tokens should be reused, and a Cache Ratio Predictor, which controls how many tokens to reuse. Training these modules is non-trivial due to their discrete decisions. We address this by adopting a differentiable relaxation that allows gradient-based end-to-end optimization. Extensive experiments on the LIBERO and SIMPLER benchmarks, as well as real-robot evaluations, show that our method achieves a 1.76x wall-clock inference speedup while simultaneously improving the average success rate by 1.9 percentage points (from 75.0% to 76.9%) on LIBERO and by 5.0 percentage points on real-world tasks, significantly outperforming existing baselines. This work highlights the potential of learning task-aware computational allocation policies, paving the way for VLA models that are both powerful and efficient.

</details>


### [442] [USS-Nav: Unified Spatio-Semantic Scene Graph for Lightweight UAV Zero-Shot Object Navigation](https://arxiv.org/abs/2602.00708)
*Weiqi Gai,Yuman Gao,Yuan Zhou,Yufan Xie,Zhiyang Liu,Yuze Wu,Xin Zhou,Fei Gao,Zhijun Meng*

Main category: cs.RO

TL;DR: USS-Nav: Lightweight framework for UAV zero-shot object navigation using incremental spatio-semantic scene graphs and LLM-augmented planning


<details>
  <summary>Details</summary>
Motivation: Address the conflict between high-level semantic reasoning requirements and limited onboard computational resources for UAV zero-shot object navigation in unknown environments

Method: Incremental spatial connectivity graph generation using polyhedral expansion, dynamic semantic region partitioning via graph clustering, open-vocabulary object semantics anchoring, and coarse-to-fine exploration with LLM-guided global planning and local frontier optimization

Result: Outperforms SOTA methods in computational efficiency and real-time update frequency (15 Hz) on resource-constrained platforms, with substantial improvements in Success weighted by Path Length (SPL)

Conclusion: USS-Nav enables efficient LLM-augmented zero-shot object navigation through lightweight hierarchical environmental representation and planning, making it suitable for resource-constrained UAV platforms

Abstract: Zero-Shot Object Navigation in unknown environments poses significant challenges for Unmanned Aerial Vehicles (UAVs) due to the conflict between high-level semantic reasoning requirements and limited onboard computational resources. To address this, we present USS-Nav, a lightweight framework that incrementally constructs a Unified Spatio-Semantic scene graph and enables efficient Large Language Model (LLM)-augmented Zero-Shot Object Navigation in unknown environments. Specifically, we introduce an incremental Spatial Connectivity Graph generation method utilizing polyhedral expansion to capture global geometric topology, which is dynamically partitioned into semantic regions via graph clustering. Concurrently, open-vocabulary object semantics are instantiated and anchored to this topology to form a hierarchical environmental representation. Leveraging this hierarchical structure, we present a coarse-to-fine exploration strategy: LLM grounded in the scene graph's semantics to determine global target regions, while a local planner optimizes frontier coverage based on information gain. Experimental results demonstrate that our framework outperforms state-of-the-art methods in terms of computational efficiency and real-time update frequency (15 Hz) on a resource-constrained platform. Furthermore, ablation studies confirm the effectiveness of our framework, showing substantial improvements in Success weighted by Path Length (SPL). The source code will be made publicly available to foster further research.

</details>


### [443] [SA-VLA: Spatially-Aware Flow-Matching for Vision-Language-Action Reinforcement Learning](https://arxiv.org/abs/2602.00743)
*Xu Pan,Zhenglin Wan,Xingrui Yu,Xianwei Zheng,Youkai Ke,Ming Sun,Rui Wang,Ziwei Wang,Ivor Tsang*

Main category: cs.RO

TL;DR: SA-VLA preserves spatial grounding in VLA models during RL fine-tuning to prevent robustness degradation under spatial distribution shifts.


<details>
  <summary>Details</summary>
Motivation: RL fine-tuning of Vision-Language-Action models degrades robustness under spatial distribution shifts due to erosion of spatial inductive bias, as sparse rewards and spatially agnostic exploration favor short-horizon visual cues.

Method: SA-VLA framework preserves spatial grounding by aligning representation learning, reward design, and exploration with task geometry: fuses implicit spatial representations with visual tokens, provides dense geometric progress rewards, and employs SCAN (spatially-conditioned annealed exploration) tailored to flow-matching dynamics.

Result: SA-VLA enables stable RL fine-tuning and improves zero-shot spatial generalization across challenging multi-object and cluttered manipulation benchmarks, yielding more robust and transferable behaviors.

Conclusion: The proposed spatially-aware RL adaptation framework effectively maintains spatial inductive bias during policy optimization, addressing the robustness degradation problem in VLA model fine-tuning under spatial distribution shifts.

Abstract: Vision-Language-Action (VLA) models exhibit strong generalization in robotic manipulation, yet reinforcement learning (RL) fine-tuning often degrades robustness under spatial distribution shifts. For flow-matching VLA policies, this degradation is closely associated with the erosion of spatial inductive bias during RL adaptation, as sparse rewards and spatially agnostic exploration increasingly favor short-horizon visual cues. To address this issue, we propose \textbf{SA-VLA}, a spatially-aware RL adaptation framework that preserves spatial grounding during policy optimization by aligning representation learning, reward design, and exploration with task geometry. SA-VLA fuses implicit spatial representations with visual tokens, provides dense rewards that reflect geometric progress, and employs \textbf{SCAN}, a spatially-conditioned annealed exploration strategy tailored to flow-matching dynamics. Across challenging multi-object and cluttered manipulation benchmarks, SA-VLA enables stable RL fine-tuning and improves zero-shot spatial generalization, yielding more robust and transferable behaviors. Code and project page are available at https://xupan.top/Projects/savla.

</details>


### [444] [Physics-informed Diffusion Mamba Transformer for Real-world Driving](https://arxiv.org/abs/2602.00808)
*Hang Zhou,Qiang Zhang,Peiran Liu,Yihao Qin,Zhaoxu Yan,Yiding Ji*

Main category: cs.RO

TL;DR: Diffusion Mamba Transformer with Port-Hamiltonian Neural Network for physically-constrained, context-aware trajectory planning in autonomous driving.


<details>
  <summary>Details</summary>
Motivation: Current diffusion-based generative models for autonomous driving trajectory planning fail to incorporate long-term sequential contexts and domain-specific physical priors, limiting their ability to model complex temporal dependencies and respect physical laws.

Method: Two key innovations: 1) Diffusion Mamba Transformer architecture that embeds mamba and attention into diffusion process for effective aggregation of sequential input contexts; 2) Port-Hamiltonian Neural Network module that integrates energy-based physical constraints into the diffusion model.

Result: Extensive evaluations on standard autonomous driving benchmarks show the unified framework significantly outperforms state-of-the-art baselines in predictive accuracy, physical plausibility, and robustness.

Conclusion: The proposed approach advances safe and reliable motion planning by combining diffusion-based generative modeling with sequential context aggregation and physical constraint integration.

Abstract: Autonomous driving systems demand trajectory planners that not only model the inherent uncertainty of future motions but also respect complex temporal dependencies and underlying physical laws. While diffusion-based generative models excel at capturing multi-modal distributions, they often fail to incorporate long-term sequential contexts and domain-specific physical priors. In this work, we bridge these gaps with two key innovations. First, we introduce a Diffusion Mamba Transformer architecture that embeds mamba and attention into the diffusion process, enabling more effective aggregation of sequential input contexts from sensor streams and past motion histories. Second, we design a Port-Hamiltonian Neural Network module that seamlessly integrates energy-based physical constraints into the diffusion model, thereby enhancing trajectory predictions with both consistency and interpretability. Extensive evaluations on standard autonomous driving benchmarks demonstrate that our unified framework significantly outperforms state-of-the-art baselines in predictive accuracy, physical plausibility, and robustness, thereby advancing safe and reliable motion planning.

</details>


### [445] [SyNeT: Synthetic Negatives for Traversability Learning](https://arxiv.org/abs/2602.00814)
*Bomena Kim,Hojun Lee,Younsoo Park,Yaoyu Hu,Sebastian Scherer,Inwook Shim*

Main category: cs.RO

TL;DR: A method to enhance traversability estimation by constructing synthetic negative examples for self-supervised learning, improving identification of non-traversable regions without modifying inference architectures.


<details>
  <summary>Details</summary>
Motivation: Existing self-supervised traversability learning frameworks lack explicit negative data, limiting their ability to accurately identify diverse non-traversable regions in complex outdoor environments.

Method: Introduces synthetic negative construction representing plausible but non-traversable regions, integrated into both Positive-Unlabeled (PU) and Positive-Negative (PN) frameworks. Also proposes object-centric FPR evaluation to analyze predictions in synthetic negative regions.

Result: Extensive experiments on public and self-collected datasets show significant improvements in robustness and generalization across diverse environments.

Conclusion: The approach effectively addresses the lack of negative data in traversability estimation, enhancing model performance without architectural changes, with code and demos publicly available.

Abstract: Reliable traversability estimation is crucial for autonomous robots to navigate complex outdoor environments safely. Existing self-supervised learning frameworks primarily rely on positive and unlabeled data; however, the lack of explicit negative data remains a critical limitation, hindering the model's ability to accurately identify diverse non-traversable regions. To address this issue, we introduce a method to explicitly construct synthetic negatives, representing plausible but non-traversable, and integrate them into vision-based traversability learning. Our approach is formulated as a training strategy that can be seamlessly integrated into both Positive-Unlabeled (PU) and Positive-Negative (PN) frameworks without modifying inference architectures. Complementing standard pixel-wise metrics, we introduce an object-centric FPR evaluation approach that analyzes predictions in regions where synthetic negatives are inserted. This evaluation provides an indirect measure of the model's ability to consistently identify non-traversable regions without additional manual labeling. Extensive experiments on both public and self-collected datasets demonstrate that our approach significantly enhances robustness and generalization across diverse environments. The source code and demonstration videos are publicly available at the project page: https://anonymous-synet.github.io/SyNet.github.io/

</details>


### [446] [Ocean Current-Harnessing Stage-Gated MPC: Monotone Cost Shaping and Speed-to-Fly for Energy-Efficient AUV Navigation](https://arxiv.org/abs/2602.00823)
*Spyridon Syntakas,Kostas Vlachos*

Main category: cs.RO

TL;DR: AUV energy efficiency improved via Current-Harnessing Stage-Gated MPC that exploits ocean currents through help-gated cost terms for energy savings while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: AUVs are promising for ocean exploration but limited by energy efficiency and endurance constraints, requiring smarter ways to exploit natural ocean currents for energy savings.

Method: Proposed Current-Harnessing Stage-Gated MPC uses per-stage scalar to gauge current helpfulness, with two cost terms: Monotone Cost Shaping (MCS) for relaxed position error with energy rebate, and speed-to-fly (STF) for thrust cost and gliding. Both are C1 and plug-and-play in MPC designs.

Result: Simulations with BlueROV2 model show substantially lower energy consumption than conventional predictive control while maintaining comparable arrival times and constraint satisfaction.

Conclusion: The proposed approach effectively harnesses ocean currents for significant energy savings in AUV operations without compromising mission performance, offering a practical solution to endurance limitations.

Abstract: Autonomous Underwater Vehicles (AUVs) are a highly promising technology for ocean exploration and diverse offshore operations, yet their practical deployment is constrained by energy efficiency and endurance. To address this, we propose Current-Harnessing Stage-Gated MPC, which exploits ocean currents via a per-stage scalar which indicates the "helpfulness" of ocean currents. This scalar is computed along the prediction horizon to gate lightweight cost terms only where the ocean currents truly aids the control goal. The proposed cost terms, that are merged in the objective function, are (i) a Monotone Cost Shaping (MCS) term, a help-gated, non-worsening modification that relaxes along-track position error and provides a bounded translational energy rebate, guaranteeing the shaped objective is never larger than a set baseline, and (ii) a speed-to-fly (STF) cost component that increases the price of thrust and softly matches ground velocity to the ocean current, enabling near zero water-relative "gliding". All terms are C1 and integrate as a plug-and-play in MPC designs. Extensive simulations with the BlueROV2 model under realistic ocean current fields show that the proposed approach achieves substantially lower energy consumption than conventional predictive control while maintaining comparable arrival times and constraint satisfaction.

</details>


### [447] [Safe Stochastic Explorer: Enabling Safe Goal Driven Exploration in Stochastic Environments and Safe Interaction with Unknown Objects](https://arxiv.org/abs/2602.00868)
*Nikhil Uday Shinde,Dylan Hirsch,Michael C. Yip,Sylvia Herbert*

Main category: cs.RO

TL;DR: S.S.Explorer enables safe robot exploration in unknown stochastic environments by balancing safety with information gathering using Gaussian Processes to learn safety functions online.


<details>
  <summary>Details</summary>
Motivation: Current safe control methods assume known dynamics, while existing safe exploration techniques fail to account for real-world stochasticity (like rover skidding or robot pushing unmapped objects). There's a critical gap for safe exploration under stochastic dynamics in unknown environments.

Method: Proposes Safe Stochastic Explorer (S.S.Explorer) framework using Gaussian Processes to learn unknown safety functions online, leveraging predictive uncertainty to guide information-gathering actions and provide probabilistic safety bounds. Extends from discrete to continuous state spaces and applies to safe physical interaction with multiple unknown objects.

Result: Extensive validation in simulation and hardware experiments demonstrates efficacy, representing progress toward reliable robot autonomy in complex, uncertain environments.

Conclusion: S.S.Explorer addresses the critical gap in safe exploration under stochastic dynamics, enabling robots to safely navigate and interact in unknown real-world environments despite inherent stochasticity.

Abstract: Autonomous robots operating in unstructured, safety-critical environments, from planetary exploration to warehouses and homes, must learn to safely navigate and interact with their surroundings despite limited prior knowledge. Current methods for safe control, such as Hamilton-Jacobi Reachability and Control Barrier Functions, assume known system dynamics. Meanwhile existing safe exploration techniques often fail to account for the unavoidable stochasticity inherent when operating in unknown real world environments, such as an exploratory rover skidding over an unseen surface or a household robot pushing around unmapped objects in a pantry. To address this critical gap, we propose Safe Stochastic Explorer (S.S.Explorer) a novel framework for safe, goal-driven exploration under stochastic dynamics. Our approach strategically balances safety and information gathering to reduce uncertainty about safety in the unknown environment. We employ Gaussian Processes to learn the unknown safety function online, leveraging their predictive uncertainty to guide information-gathering actions and provide probabilistic bounds on safety violations. We first present our method for discrete state space environments and then introduce a scalable relaxation to effectively extend this approach to continuous state spaces. Finally we demonstrate how this framework can be naturally applied to ensure safe physical interaction with multiple unknown objects. Extensive validation in simulation and demonstrative hardware experiments showcase the efficacy of our method, representing a step forward toward enabling reliable widespread robot autonomy in complex, uncertain environments.

</details>


### [448] [Learning When to Jump for Off-road Navigation](https://arxiv.org/abs/2602.00877)
*Zhipeng Zhao,Taimeng Fu,Shaoshu Su,Qiwei Du,Ehsan Tarkesh Esfahani,Karthik Dantu,Souma Chowdhury,Chen Wang*

Main category: cs.RO

TL;DR: MAT introduces motion-aware traversability representation that models terrain cost as Gaussian functions of velocity, enabling agile off-road navigation by planning with explicit motion dynamics rather than static positions or fixed velocities.


<details>
  <summary>Details</summary>
Motivation: Current off-road navigation methods often neglect complex motion dynamics, planning based solely on positions or fixed velocities. This fails to capture scenarios where safety depends on speed (e.g., crossing ditches safely requires higher speed for controlled jumps).

Method: Motion-aware Traversability (MAT) representation models terrain regions as Gaussian functions of velocity. Two-stage computation: (1) predict terrain-dependent Gaussian parameters from perception in single forward pass, (2) efficiently update terrain costs for new velocities by evaluating functions without repeated inference.

Result: MAT achieves real-time efficiency and enhances off-road navigation performance, reducing path detours by 75% while maintaining safety across challenging terrains in both simulated and real-world environments.

Conclusion: Explicitly modeling terrain cost conditioned on actual robot motion enables agile off-road navigation that can handle complex scenarios where safety depends on speed, outperforming methods that neglect motion dynamics.

Abstract: Low speed does not always guarantee safety in off-road driving. For instance, crossing a ditch may be risky at a low speed due to the risk of getting stuck, yet safe at a higher speed with a controlled, accelerated jump. Achieving such behavior requires path planning that explicitly models complex motion dynamics, whereas existing methods often neglect this aspect and plan solely based on positions or a fixed velocity. To address this gap, we introduce Motion-aware Traversability (MAT) representation to explicitly model terrain cost conditioned on actual robot motion. Instead of assigning a single scalar score for traversability, MAT models each terrain region as a Gaussian function of velocity. During online planning, we decompose the terrain cost computation into two stages: (1) predict terrain-dependent Gaussian parameters from perception in a single forward pass, (2) efficiently update terrain costs for new velocities inferred from current dynamics by evaluating these functions without repeated inference. We develop a system that integrates MAT to enable agile off-road navigation and evaluate it in both simulated and real-world environments with various obstacles. Results show that MAT achieves real-time efficiency and enhances the performance of off-road navigation, reducing path detours by 75% while maintaining safety across challenging terrains.

</details>


### [449] [RoDiF: Robust Direct Fine-Tuning of Diffusion Policies with Corrupted Human Feedback](https://arxiv.org/abs/2602.00886)
*Amitesh Vatsa,Zhixian Xie,Wanxin Jin*

Main category: cs.RO

TL;DR: RoDiF enables robust fine-tuning of diffusion policies using human preferences, handling corrupted labels without assuming noise distribution.


<details>
  <summary>Details</summary>
Motivation: Diffusion policies are powerful for robotic control but challenging to fine-tune with human preferences due to their multi-step denoising structure, especially when preferences may be corrupted.

Method: Introduces Unified MDP formulation integrating diffusion denoising with environmental dynamics, enabling reward-free DPO. RoDiF reinterprets DPO through geometric hypothesis-cutting perspective with conservative cutting strategy for robustness against corrupted preferences.

Result: RoDiF consistently outperforms state-of-the-art baselines on long-horizon manipulation tasks, effectively steering pretrained diffusion policies to human-preferred modes, maintaining strong performance even with 30% corrupted preference labels.

Conclusion: RoDiF provides a robust framework for fine-tuning diffusion policies with human preferences, addressing both the structural challenges of diffusion processes and the practical issue of corrupted preference labels.

Abstract: Diffusion policies are a powerful paradigm for robotic control, but fine-tuning them with human preferences is fundamentally challenged by the multi-step structure of the denoising process. To overcome this, we introduce a Unified Markov Decision Process (MDP) formulation that coherently integrates the diffusion denoising chain with environmental dynamics, enabling reward-free Direct Preference Optimization (DPO) for diffusion policies. Building on this formulation, we propose RoDiF (Robust Direct Fine-Tuning), a method that explicitly addresses corrupted human preferences. RoDiF reinterprets the DPO objective through a geometric hypothesis-cutting perspective and employs a conservative cutting strategy to achieve robustness without assuming any specific noise distribution. Extensive experiments on long-horizon manipulation tasks show that RoDiF consistently outperforms state-of-the-art baselines, effectively steering pretrained diffusion policies of diverse architectures to human-preferred modes, while maintaining strong performance even under 30% corrupted preference labels.

</details>


### [450] [UniMorphGrasp: Diffusion Model with Morphology-Awareness for Cross-Embodiment Dexterous Grasp Generation](https://arxiv.org/abs/2602.00915)
*Zhiyuan Wu,Xiangyu Zhang,Zhuo Chen,Jiankang Deng,Rolandos Alexandros Potamias,Shan Luo*

Main category: cs.RO

TL;DR: UniMorphGrasp: A diffusion-based framework for cross-embodiment dexterous grasping that generalizes to unseen robotic hand morphologies using unified canonical hand representations and graph-based kinematic conditioning.


<details>
  <summary>Details</summary>
Motivation: Existing dexterous grasping methods are limited to specific hand designs and fail to generalize to unseen hand morphologies, creating a need for scalable cross-embodiment grasp synthesis that works across diverse robotic hand structures.

Method: Proposes UniMorphGrasp, a diffusion-based framework that: 1) maps diverse robotic hand grasps into a unified human-like canonical hand pose representation, 2) conditions grasp generation on graph-based structured representations of hand kinematics and object geometry, and 3) uses a hierarchical loss function for joint-level supervision.

Result: Achieves state-of-the-art performance on existing dexterous grasp benchmarks and demonstrates strong zero-shot generalization to previously unseen hand structures, enabling scalable cross-embodiment grasp deployment.

Conclusion: UniMorphGrasp provides an effective solution for cross-embodiment dexterous grasping that overcomes limitations of hand-specific methods, offering practical deployment capabilities for diverse robotic hand morphologies through unified representation learning.

Abstract: Cross-embodiment dexterous grasping aims to generate stable and diverse grasps for robotic hands with heterogeneous kinematic structures. Existing methods are often tailored to specific hand designs and fail to generalize to unseen hand morphologies outside the training distribution. To address these limitations, we propose \textbf{UniMorphGrasp}, a diffusion-based framework that incorporates hand morphological information into the grasp generation process for unified cross-embodiment grasp synthesis. The proposed approach maps grasps from diverse robotic hands into a unified human-like canonical hand pose representation, providing a common space for learning. Grasp generation is then conditioned on structured representations of hand kinematics, encoded as graphs derived from hand configurations, together with object geometry. In addition, a loss function is introduced that exploits the hierarchical organization of hand kinematics to guide joint-level supervision. Extensive experiments demonstrate that UniMorphGrasp achieves state-of-the-art performance on existing dexterous grasp benchmarks and exhibits strong zero-shot generalization to previously unseen hand structures, enabling scalable and practical cross-embodiment grasp deployment.

</details>


### [451] [Green-VLA: Staged Vision-Language-Action Model for Generalist Robots](https://arxiv.org/abs/2602.00919)
*I. Apanasevich,M. Artemyev,R. Babakyan,P. Fedotova,D. Grankin,E. Kupryashin,A. Misailidi,D. Nerus,A. Nutalapati,G. Sidorov,I. Efremov,M. Gerasyov,D. Pikurov,Y. Senchenko,S. Davidenko,D. Kulikov,M. Sultankin,K. Askarbek,O. Shamanin,D. Statovoy,E. Zalyaev,I. Zorin,A. Letkin,E. Rusakov,A. Silchenko,V. Vorobyov,S. Sobolnikov,A. Postnikov*

Main category: cs.RO

TL;DR: Green-VLA is a staged Vision-Language-Action framework for humanoid robots that uses a 5-stage curriculum with scalable data processing and unified action interface to achieve generalization across diverse robot embodiments.


<details>
  <summary>Details</summary>
Motivation: To enable real-world deployment of vision-language-action models on humanoid robots while maintaining generalization across diverse robot embodiments (humanoids, mobile manipulators, fixed-base arms).

Method: Five-stage curriculum: L0 (foundational VLMs), L1 (multimodal grounding), R0 (multi-embodiment pretraining), R1 (embodiment-specific adaptation), R2 (RL policy alignment). Uses scalable data processing pipeline (3,000 hours), temporal alignment, quality filtering, and unified embodiment-aware action interface.

Result: Strong generalization and performance gains from RL alignment in success rate, robustness, and long-horizon efficiency on Simpler BRIDGE WidowX and CALVIN ABC-D benchmarks, plus real-robot evaluations.

Conclusion: Green-VLA demonstrates effective staged training approach for VLA models on humanoid robots with generalization across embodiments, enhanced by inference-time safety mechanisms and RL alignment for improved performance.

Abstract: We introduce Green-VLA, a staged Vision-Language-Action (VLA) framework for real-world deployment on the Green humanoid robot while maintaining generalization across diverse embodiments. Green-VLA follows a five stage curriculum: (L0) foundational VLMs, (L1) multimodal grounding, (R0) multi-embodiment pretraining, (R1) embodiment-specific adaptation, and (R2) reinforcement-learning (RL) policy alignment. We couple a scalable data-processing pipeline (3,000 hours of demonstrations) with temporal alignment and quality filtering, and use a unified, embodiment-aware action interface enabling a single policy to control humanoids, mobile manipulators, and fixed-base arms. At inference, the VLA controller is enhanced with episode-progress prediction, out-of-distribution detection, and joint-prediction-based guidance to improve safety and precise target selection. Experiments on Simpler BRIDGE WidowX and CALVIN ABC-D, as well as real-robot evaluations, show strong generalization and performance gains from RL alignment in success rate, robustness, and long-horizon efficiency.

</details>


### [452] [SanD-Planner: Sample-Efficient Diffusion Planner in B-Spline Space for Robust Local Navigation](https://arxiv.org/abs/2602.00923)
*Jincheng Wang,Lingfan Bao,Tong Yang,Diego Martinez Plasencia,Jianhao Jiao,Dimitrios Kanoulas*

Main category: cs.RO

TL;DR: SanD-Planner: A sample-efficient diffusion-based local planner using depth images and clamped B-spline space for smooth, bounded-error planning in cluttered dynamic environments, achieving SOTA with only 500 training episodes.


<details>
  <summary>Details</summary>
Motivation: Overcoming challenges in generating reliable local plans for cluttered dynamic environments, particularly the difficulty of acquiring large-scale expert demonstrations and improving learning efficiency with limited data.

Method: Uses depth image-based imitation learning within clamped B-spline space for smooth outputs with bounded prediction errors. Integrates ESDF-based safety checker with clearance and time-to-completion metrics to reduce training burden for feasibility assessment.

Result: Achieves state-of-the-art performance with only 500 training episodes (0.25% of baseline scale): 90.1% success in simulated cluttered environments and 72.0% in indoor simulations. Demonstrates zero-shot transferability to realistic 2D and 3D scenes.

Conclusion: SanD-Planner provides a highly sample-efficient solution for local planning in cluttered dynamic environments, with strong performance and transferability, while requiring minimal training data compared to existing approaches.

Abstract: The challenge of generating reliable local plans has long hindered practical applications in highly cluttered and dynamic environments. Key fundamental bottlenecks include acquiring large-scale expert demonstrations across diverse scenes and improving learning efficiency with limited data. This paper proposes SanD-Planner, a sample-efficient diffusion-based local planner that conducts depth image-based imitation learning within the clamped B-spline space. By operating within this compact space, the proposed algorithm inherently yields smooth outputs with bounded prediction errors over local supports, naturally aligning with receding-horizon execution. Integration of an ESDF-based safety checker with explicit clearance and time-to-completion metrics further reduces the training burden associated with value-function learning for feasibility assessment. Experiments show that training with $500$ episodes (merely $0.25\%$ of the demonstration scale used by the baseline), SanD-Planner achieves state-of-the-art performance on the evaluated open benchmark, attaining success rates of $90.1\%$ in simulated cluttered environments and $72.0\%$ in indoor simulations. The performance is further proven by demonstrating zero-shot transferability to realistic experimentation in both 2D and 3D scenes. The dataset and pre-trained models will also be open-sourced.

</details>


### [453] [Minimal Footprint Grasping Inspired by Ants](https://arxiv.org/abs/2602.00935)
*Mohamed Sorour,Barbara Webb*

Main category: cs.RO

TL;DR: Researchers developed a novel low-cost gripper inspired by ant forelegs, featuring high-friction pads, low-friction hairs, and flexible tarsus-like structures for robust grasping in clutter.


<details>
  <summary>Details</summary>
Motivation: Ants demonstrate exceptional grasping capabilities in cluttered environments using their forelegs, which feature specialized structures like high-friction setal pads, hairs, and flexible tarsi. This biological inspiration motivates the development of more effective robotic grippers for bin-picking applications.

Method: The researchers abstracted key features from ant forelegs: high-friction gripping pads (mimicking setal pads), low-friction hairs, and a single-segment tarsus-like structure for interactive compliance. They implemented these features in a novel gripper design with long, slim legs suitable for bin-picking.

Result: Experimental evaluation showed the bio-inspired gripper design is highly robust, achieving 100% success rate for grasping individual consumer objects. It also proved effective for picking single objects from dense clutter, demonstrating capabilities similar to those observed in ants.

Conclusion: The work advances grasping technology by successfully translating biological principles from ants into functional robotic grippers, while also providing new insights into the mechanical importance of hairy structures and tarsal flexibility in insect grasping mechanisms.

Abstract: Ants are highly capable of grasping objects in clutter, and we have recently observed that this involves substantial use of their forelegs. The forelegs, more specifically the tarsi, have high friction microstructures (setal pads), are covered in hairs, and have a flexible under-actuated tip. Here we abstract these features to test their functional advantages for a novel low-cost gripper design, suitable for bin-picking applications. In our implementation, the gripper legs are long and slim, with high friction gripping pads, low friction hairs and single-segment tarsus-like structure to mimic the insect's setal pads, hairs, and the tarsi's interactive compliance. Experimental evaluation shows this design is highly robust for grasping a wide variety of individual consumer objects, with all grasp attempts successful. In addition, we demonstrate this design is effective for picking single objects from dense clutter, a task at which ants also show high competence. The work advances grasping technology and shed new light on the mechanical importance of hairy structures and tarsal flexibility in insects.

</details>


### [454] [CLAMP: Contrastive Learning for 3D Multi-View Action-Conditioned Robotic Manipulation Pretraining](https://arxiv.org/abs/2602.00937)
*I-Chun Arthur Liu,Krzysztof Choromanski,Sandy Huang,Connor Schenck*

Main category: cs.RO

TL;DR: CLAMP is a 3D pre-training framework that uses point clouds and robot actions to learn spatial representations for robotic manipulation, improving learning efficiency and performance over 2D-based methods.


<details>
  <summary>Details</summary>
Motivation: Standard 2D image representations fail to capture essential 3D spatial information needed for precise robotic manipulation, creating a need for better 3D-aware representations.

Method: Uses point clouds from RGB-D images to render multi-view 4-channel images (RGB + depth/3D coordinates), pre-trains encoders via contrastive learning on simulated trajectories, and pre-trains a Diffusion Policy for weight initialization before fine-tuning on limited demonstrations.

Result: Substantially improves learning efficiency and policy performance on unseen tasks, outperforms state-of-the-art baselines across six simulated and five real-world tasks.

Conclusion: CLAMP demonstrates that 3D-aware pre-training with point clouds and contrastive learning enables more efficient and effective robotic manipulation policies compared to 2D-based approaches.

Abstract: Leveraging pre-trained 2D image representations in behavior cloning policies has achieved great success and has become a standard approach for robotic manipulation. However, such representations fail to capture the 3D spatial information about objects and scenes that is essential for precise manipulation. In this work, we introduce Contrastive Learning for 3D Multi-View Action-Conditioned Robotic Manipulation Pretraining (CLAMP), a novel 3D pre-training framework that utilizes point clouds and robot actions. From the merged point cloud computed from RGB-D images and camera extrinsics, we re-render multi-view four-channel image observations with depth and 3D coordinates, including dynamic wrist views, to provide clearer views of target objects for high-precision manipulation tasks. The pre-trained encoders learn to associate the 3D geometric and positional information of objects with robot action patterns via contrastive learning on large-scale simulated robot trajectories. During encoder pre-training, we pre-train a Diffusion Policy to initialize the policy weights for fine-tuning, which is essential for improving fine-tuning sample efficiency and performance. After pre-training, we fine-tune the policy on a limited amount of task demonstrations using the learned image and action representations. We demonstrate that this pre-training and fine-tuning design substantially improves learning efficiency and policy performance on unseen tasks. Furthermore, we show that CLAMP outperforms state-of-the-art baselines across six simulated tasks and five real-world tasks.

</details>


### [455] [Meanshift Shape Formation Control Using Discrete Mass Distribution](https://arxiv.org/abs/2602.00980)
*Yichen Cai,Yuan Gao,Pengpeng Li,Wei Wang,Guibin Sun,Jinhu Lü*

Main category: cs.RO

TL;DR: Decentralized swarm control using discrete mass-distribution functions for complex shape formation with swarm-size adaptability


<details>
  <summary>Details</summary>
Motivation: Existing density-distribution methods struggle with complex shape representation and decentralized implementation, creating practical challenges for swarm robotics

Method: Proposes discrete mass-distribution function over sample points (instead of continuous density functions) and decentralized meanshift control law with mass estimators for robots to coordinate global distribution

Result: Mass estimates converge asymptotically to true global values; strategy validated through comprehensive simulations and real-world experiments showing efficient complex shape formation and swarm-size adaptability

Conclusion: Developed a fully decentralized, distribution-based control strategy that successfully achieves complex shape formation while adapting to variations in swarm size

Abstract: The density-distribution method has recently become a promising paradigm owing to its adaptability to variations in swarm size. However, existing studies face practical challenges in achieving complex shape representation and decentralized implementation. This motivates us to develop a fully decentralized, distribution-based control strategy with the dual capability of forming complex shapes and adapting to swarm-size variations. Specifically, we first propose a discrete mass-distribution function defined over a set of sample points to model swarm formation. In contrast to the continuous density-distribution method, our model eliminates the requirement for defining continuous density functions-a task that is difficult for complex shapes. Second, we design a decentralized meanshift control law to coordinate the swarm's global distribution to fit the sample-point distribution by feeding back mass estimates. The mass estimates for all sample points are achieved by the robots in a decentralized manner via the designed mass estimator. It is shown that the mass estimates of the sample points can asymptotically converge to the true global values. To validate the proposed strategy, we conduct comprehensive simulations and real-world experiments to evaluate the efficiency of complex shape formation and adaptability to swarm-size variations.

</details>


### [456] [Geometry-Aware Sampling-Based Motion Planning on Riemannian Manifolds](https://arxiv.org/abs/2602.00992)
*Phone Thiha Kyaw,Jonathan Kelly*

Main category: cs.RO

TL;DR: Proposes a sampling-based motion planning framework that operates directly on Riemannian manifolds to compute geodesic paths under configuration-dependent metrics, using efficient midpoint approximations and retraction-based local planning.


<details>
  <summary>Details</summary>
Motivation: Many robot motion planning problems involve non-Euclidean geometry induced by task objectives and physical constraints, but existing planners either use Euclidean distances (ignoring structure) or scale poorly to high-dimensional systems.

Method: Develops a sampling-based framework with: 1) computationally efficient midpoint-based approximation of Riemannian geodesic distance with third-order accuracy, and 2) local planner using first-order retractions guided by Riemannian natural gradients.

Result: Experiments on 2-link planar arm, 7-DoF Franka manipulator under kinetic-energy metric, and rigid-body planning in SE(2) with non-holonomic constraints show consistently lower-cost trajectories than Euclidean-based planners and classical numerical geodesic-solver baselines.

Conclusion: The proposed Riemannian sampling-based planning framework effectively bridges the gap between geometric fidelity and scalability, enabling efficient computation of geodesic paths for high-dimensional robotic systems under configuration-dependent metrics.

Abstract: In many robot motion planning problems, task objectives and physical constraints induce non-Euclidean geometry on the configuration space, yet many planners operate using Euclidean distances that ignore this structure. We address the problem of planning collision-free motions that minimize length under configuration-dependent Riemannian metrics, corresponding to geodesics on the configuration manifold. Conventional numerical methods for computing such paths do not scale well to high-dimensional systems, while sampling-based planners trade scalability for geometric fidelity. To bridge this gap, we propose a sampling-based motion planning framework that operates directly on Riemannian manifolds. We introduce a computationally efficient midpoint-based approximation of the Riemannian geodesic distance and prove that it matches the true Riemannian distance with third-order accuracy. Building on this approximation, we design a local planner that traces the manifold using first-order retractions guided by Riemannian natural gradients. Experiments on a two-link planar arm and a 7-DoF Franka manipulator under a kinetic-energy metric, as well as on rigid-body planning in $\mathrm{SE}(2)$ with non-holonomic motion constraints, demonstrate that our approach consistently produces lower-cost trajectories than Euclidean-based planners and classical numerical geodesic-solver baselines.

</details>


### [457] [HERMES: A Holistic End-to-End Risk-Aware Multimodal Embodied System with Vision-Language Models for Long-Tail Autonomous Driving](https://arxiv.org/abs/2602.00993)
*Weizhe Tang,Junwei You,Jiaxi Liu,Zhaoyi Wang,Rui Gan,Zilin Huang,Feng Wei,Bin Ran*

Main category: cs.RO

TL;DR: HERMES is a risk-aware end-to-end autonomous driving framework that uses foundation models to inject long-tail risk cues into trajectory planning for safer operation in mixed-traffic scenarios.


<details>
  <summary>Details</summary>
Motivation: Current end-to-end autonomous driving models struggle with safe and accurate operation in long-tail mixed-traffic scenarios where vehicles must interact with heterogeneous road users under complex conditions.

Method: Uses foundation-model-assisted annotation to create structured Long-Tail Scene Context and Planning Context, then employs a Tri-Modal Driving Module that fuses multi-view perception, historical motion cues, and semantic guidance for risk-aware planning.

Result: HERMES consistently outperforms representative end-to-end and VLM-driven baselines on real-world long-tail datasets in mixed-traffic scenarios.

Conclusion: The framework effectively addresses long-tail safety challenges in autonomous driving through explicit risk cue injection and multimodal fusion, with ablation studies confirming the complementary value of key components.

Abstract: End-to-end autonomous driving models increasingly benefit from large vision--language models for semantic understanding, yet ensuring safe and accurate operation under long-tail conditions remains challenging. These challenges are particularly prominent in long-tail mixed-traffic scenarios, where autonomous vehicles must interact with heterogeneous road users, including human-driven vehicles and vulnerable road users, under complex and uncertain conditions. This paper proposes HERMES, a holistic risk-aware end-to-end multimodal driving framework designed to inject explicit long-tail risk cues into trajectory planning. HERMES employs a foundation-model-assisted annotation pipeline to produce structured Long-Tail Scene Context and Long-Tail Planning Context, capturing hazard-centric cues together with maneuver intent and safety preference, and uses these signals to guide end-to-end planning. HERMES further introduces a Tri-Modal Driving Module that fuses multi-view perception, historical motion cues, and semantic guidance, ensuring risk-aware accurate trajectory planning under long-tail scenarios. Experiments on the real-world long-tail dataset demonstrate that HERMES consistently outperforms representative end-to-end and VLM-driven baselines under long-tail mixed-traffic scenarios. Ablation studies verify the complementary contributions of key components.

</details>


### [458] [Offline Discovery of Interpretable Skills from Multi-Task Trajectories](https://arxiv.org/abs/2602.01018)
*Chongyu Zhu,Mithun Vanniasinghe,Jiayu Chen,Chi-Guhn Lee*

Main category: cs.RO

TL;DR: LOKI is a three-stage hierarchical imitation learning framework that discovers reusable skills from offline demonstrations without explicit rewards, achieving state-of-the-art performance on D4RL Kitchen benchmark.


<details>
  <summary>Details</summary>
Motivation: Hierarchical imitation learning needs to discover reusable skills from long-horizon, multi-task offline data that lacks explicit rewards or subtask annotations, which is a central challenge in the field.

Method: Three-stage framework: 1) Task-aware macro-segmentation using alignment-enforced VQ-VAE with weak task labels, 2) Micro-level refinement with self-supervised sequential model and iterative clustering, 3) Hierarchical policy construction with option-based framework and learned termination conditions.

Result: LOKI achieves high success rates on D4RL Kitchen benchmark, outperforms standard HIL baselines, discovers semantically meaningful skills that align with human intuition, and demonstrates compositionality by solving novel unseen tasks through skill sequencing.

Conclusion: LOKI provides an effective end-to-end framework for offline skill discovery and hierarchical imitation learning that can discover reusable, semantically meaningful skills from unannotated demonstrations and compose them to solve novel tasks.

Abstract: Hierarchical Imitation Learning is a powerful paradigm for acquiring complex robot behaviors from demonstrations. A central challenge, however, lies in discovering reusable skills from long-horizon, multi-task offline data, especially when the data lacks explicit rewards or subtask annotations. In this work, we introduce LOKI, a three-stage end-to-end learning framework designed for offline skill discovery and hierarchical imitation. The framework commences with a two-stage, weakly supervised skill discovery process: Stage one performs coarse, task-aware macro-segmentation by employing an alignment-enforced Vector Quantized VAE guided by weak task labels. Stage two then refines these segments at a micro-level using a self-supervised sequential model, followed by an iterative clustering process to consolidate skill boundaries. The third stage then leverages these precise boundaries to construct a hierarchical policy within an option-based framework-complete with a learned termination condition beta for explicit skill switching. LOKI achieves high success rates on the challenging D4RL Kitchen benchmark and outperforms standard HIL baselines. Furthermore, we demonstrate that the discovered skills are semantically meaningful, aligning with human intuition, and exhibit compositionality by successfully sequencing them to solve a novel, unseen task.

</details>


### [459] [Learning Adaptive Cross-Embodiment Visuomotor Policy with Contrastive Prompt Orchestration](https://arxiv.org/abs/2602.01040)
*Yuhang Zhang,Chao Yan,Jiaxi Yu,Jiaping Xiao,Mir Feroskhan*

Main category: cs.RO

TL;DR: CAPO uses contrastive prompt learning and adaptive orchestration to create visuomotor policies that adapt to cross-embodiment variations like different sensors and environments.


<details>
  <summary>Details</summary>
Motivation: Current visuomotor policies struggle with cross-embodiment variations (different sensors, lighting, field-of-view) because they can't separate task-relevant features from domain-specific variations, leading to poor sample efficiency and catastrophic failure in unseen environments.

Method: CAPO combines contrastive prompt learning (using visual, temporal action, and text objectives to create a pool of learnable prompts) with adaptive prompt orchestration (dynamically aggregating prompts based on current observations to construct optimal state representations).

Result: CAPO significantly outperforms state-of-the-art baselines in sample efficiency and asymptotic performance, and shows superior zero-shot adaptation across unseen domains with drastic environmental and physical shifts.

Conclusion: CAPO provides an effective solution for cross-embodiment visuomotor policy adaptation by shielding policy optimization from irrelevant domain variations through adaptive prompt orchestration.

Abstract: Learning adaptive visuomotor policies for embodied agents remains a formidable challenge, particularly when facing cross-embodiment variations such as diverse sensor configurations and dynamic properties. Conventional learning approaches often struggle to separate task-relevant features from domain-specific variations (e.g., lighting, field-of-view, and rotation), leading to poor sample efficiency and catastrophic failure in unseen environments. To bridge this gap, we propose ContrAstive Prompt Orchestration (CAPO), a novel approach for learning visuomotor policies that integrates contrastive prompt learning and adaptive prompt orchestration. For prompt learning, we devise a hybrid contrastive learning strategy that integrates visual, temporal action, and text objectives to establish a pool of learnable prompts, where each prompt induces a visual representation encapsulating fine-grained domain factors. Based on these learned prompts, we introduce an adaptive prompt orchestration mechanism that dynamically aggregates these prompts conditioned on current observations. This enables the agent to adaptively construct optimal state representations by identifying dominant domain factors instantaneously. Consequently, the policy optimization is effectively shielded from irrelevant interference, preventing the common issue of overfitting to source domains. Extensive experiments demonstrate that CAPO significantly outperforms state-of-the-art baselines in sample efficiency and asymptotic performance. Crucially, it exhibits superior zero-shot adaptation across unseen target domains characterized by drastic environmental (e.g., illumination) and physical shifts (e.g., field-of-view and rotation), validating its effectiveness as a viable solution for cross-embodiment visuomotor policy adaptation.

</details>


### [460] [LLM-Based Behavior Tree Generation for Construction Machinery](https://arxiv.org/abs/2602.01041)
*Akinosuke Tsutsumi,Tomoya Itsuka,Yuichiro Kasahara,Tomoya Kouno,Kota Akinari,Genki Yamauchi,Daisuke Endo,Taro Abe,Takeshi Hashimoto,Keiji Nagatani,Ryo Kurazume*

Main category: cs.RO

TL;DR: LLM-based workflow for generating Behavior Trees with synchronization flags enables safe, cooperative autonomous operation of construction machinery, validated in simulation and real-world experiments.


<details>
  <summary>Details</summary>
Motivation: Earthwork operations face increasing demand but suffer from workforce aging and skill loss, creating need for automation. Existing ROS2-TMS framework relies on manually designed Behavior Trees which limits scalability for heterogeneous machine cooperation.

Method: Two-step LLM-based workflow: 1) High-level planning where LLM generates synchronization flags for cooperative operation, 2) Behavior Tree generation using structured templates. Safety ensured by planning with parameters from system database.

Result: Method validated in simulation and demonstrated through real-world experiments, showing potential for advancing automation in civil engineering by enabling safe, cooperative operation of multiple construction machines.

Conclusion: LLM-based workflow with synchronization flags successfully addresses scalability limitations of manually designed Behavior Trees, enabling safe heterogeneous machine cooperation in construction automation.

Abstract: Earthwork operations are facing an increasing demand, while workforce aging and skill loss create a pressing need for automation. ROS2-TMS for Construction, a Cyber-Physical System framework designed to coordinate construction machinery, has been proposed for autonomous operation; however, its reliance on manually designed Behavior Trees (BTs) limits scalability, particularly in scenarios involving heterogeneous machine cooperation. Recent advances in large language models (LLMs) offer new opportunities for task planning and BT generation. However, most existing approaches remain confined to simulations or simple manipulators, with relatively few applications demonstrated in real-world contexts, such as complex construction sites involving multiple machines. This paper proposes an LLM-based workflow for BT generation, introducing synchronization flags to enable safe and cooperative operation. The workflow consists of two steps: high-level planning, where the LLM generates synchronization flags, and BT generation using structured templates. Safety is ensured by planning with parameters stored in the system database. The proposed method is validated in simulation and further demonstrated through real-world experiments, highlighting its potential to advance automation in civil engineering.

</details>


### [461] [A Systematic Study of Data Modalities and Strategies for Co-training Large Behavior Models for Robot Manipulation](https://arxiv.org/abs/2602.01067)
*Fanqi Lin,Kushal Arora,Jean Mercat,Haruki Nishimura,Paarth Shah,Chen Xu,Mengchao Zhang,Mark Zolotas,Maya Angeles,Owen Pfannenstiehl,Andrew Beaulieu,Jose Barreiros*

Main category: cs.RO

TL;DR: Large-scale study shows co-training robot policies with vision-language and cross-embodiment data improves generalization, while discrete action tokens don't help.


<details>
  <summary>Details</summary>
Motivation: Large behavior models have limited generalization due to insufficient robot data coverage; need to understand how different co-training data modalities and strategies affect policy performance without costly additional data collection.

Method: Large-scale empirical study examining five co-training data modalities: vision-language data, dense language annotations for robot trajectories, cross-embodiment robot data, human videos, and discrete robot action tokens across single- and multi-phase training strategies, using 4,000 hours of robot/human data and 50M vision-language samples.

Result: Co-training with vision-language and cross-embodiment robot data substantially improves generalization to distribution shifts, unseen tasks, and language following; combining modalities produces cumulative gains; discrete action tokens yield no benefits; robot-only training degrades visiolinguistic understanding; chain-of-thought conditioning doesn't improve performance.

Conclusion: Co-training with vision-language and cross-embodiment data is effective for building scalable generalist robot policies, providing practical guidance for improving generalization without costly additional robot data collection.

Abstract: Large behavior models have shown strong dexterous manipulation capabilities by extending imitation learning to large-scale training on multi-task robot data, yet their generalization remains limited by the insufficient robot data coverage. To expand this coverage without costly additional data collection, recent work relies on co-training: jointly learning from target robot data and heterogeneous data modalities. However, how different co-training data modalities and strategies affect policy performance remains poorly understood. We present a large-scale empirical study examining five co-training data modalities: standard vision-language data, dense language annotations for robot trajectories, cross-embodiment robot data, human videos, and discrete robot action tokens across single- and multi-phase training strategies. Our study leverages 4,000 hours of robot and human manipulation data and 50M vision-language samples to train vision-language-action policies. We evaluate 89 policies over 58,000 simulation rollouts and 2,835 real-world rollouts. Our results show that co-training with forms of vision-language and cross-embodiment robot data substantially improves generalization to distribution shifts, unseen tasks, and language following, while discrete action token variants yield no significant benefits. Combining effective modalities produces cumulative gains and enables rapid adaptation to unseen long-horizon dexterous tasks via fine-tuning. Training exclusively on robot data degrades the visiolinguistic understanding of the vision-language model backbone, while co-training with effective modalities restores these capabilities. Explicitly conditioning action generation on chain-of-thought traces learned from co-training data does not improve performance in our simulation benchmark. Together, these results provide practical guidance for building scalable generalist robot policies.

</details>


### [462] [Estimating Force Interactions of Deformable Linear Objects from their Shapes](https://arxiv.org/abs/2602.01085)
*Qi Jing Chen,Shilin Shan,Timothy Bretl,Quang-Cuong Pham*

Main category: cs.RO

TL;DR: Analytical method detects and estimates external forces on deformable wires using only observed shapes, without needing force sensors or assuming end-effector contacts.


<details>
  <summary>Details</summary>
Motivation: In robot-wire interactions, contacts often occur along the wire body (not at end-effector), which is crucial for safe trajectory planning to prevent damage and hazards. Existing methods require expensive sensors or assume end-effector contacts.

Method: Uses wire shape information from depth camera, assumes static equilibrium, derives consistency conditions, and solves linear equations based on force-torque balance to estimate both location and magnitude of external forces without prior knowledge.

Result: Validated through simulation with high accuracy and real-world experiments demonstrating accurate estimation in selected interaction scenarios.

Conclusion: Proposed approach enables contact force estimation using only shape observations, addressing practical robot-wire interaction scenarios where contacts occur along the wire body rather than at endpoints.

Abstract: This work introduces an analytical approach for detecting and estimating external forces acting on deformable linear objects (DLOs) using only their observed shapes. In many robot-wire interaction tasks, contact occurs not at the end-effector but at other points along the robot's body. Such scenarios arise when robots manipulate wires indirectly (e.g., by nudging) or when wires act as passive obstacles in the environment. Accurately identifying these interactions is crucial for safe and efficient trajectory planning, helping to prevent wire damage, avoid restricted robot motions, and mitigate potential hazards. Existing approaches often rely on expensive external force-torque sensor or that contacts occur at the end-effector for accurate force estimation. Using wire shape information acquired from a depth camera and under the assumption that the wire is in or near its static equilibrium, our method estimates both the location and magnitude of external forces without additional prior knowledge. This is achieved by exploiting derived consistency conditions and solving a system of linear equations based on force-torque balance along the wire. The approach was validated through simulation, where it achieved high accuracy, and through real-world experiments, where accurate estimation was demonstrated in selected interaction scenarios.

</details>


### [463] [Failure-Aware Bimanual Teleoperation via Conservative Value Guided Assistance](https://arxiv.org/abs/2602.01092)
*Peng Zhou,Zhongxuan Li,Jinsong Wu,Jiaming Qi,Jun Hu,David Navarro-Alarcon,Jia Pan,Lihua Xie,Shiyao Zhang,Zeqing Zhang*

Main category: cs.RO

TL;DR: A bimanual teleoperation framework that uses conservative value learning to predict task feasibility and provide haptic assistance, steering operators away from failure-prone actions while preserving human authority.


<details>
  <summary>Details</summary>
Motivation: Teleoperation of high-precision manipulation is challenging due to tight success tolerances and complex contact dynamics, making failures difficult for human operators to anticipate under partial observability.

Method: Uses conservative value learning trained on heterogeneous offline teleoperation data (both successful and failed executions) to model task feasibility as a conservative success score. During operation, this score regulates assistance level while a learned actor provides corrective motion direction, integrated through a joint-space impedance interface.

Result: Experimental results on contact-rich manipulation tasks show improved task success rates and reduced operator workload compared to conventional teleoperation and shared-autonomy baselines.

Conclusion: Conservative value learning provides an effective mechanism for embedding failure awareness into bilateral teleoperation, enabling compliant haptic assistance that preserves continuous human authority.

Abstract: Teleoperation of high-precision manipulation is con-strained by tight success tolerances and complex contact dy-namics, which make impending failures difficult for human operators to anticipate under partial observability. This paper proposes a value-guided, failure-aware framework for bimanual teleoperation that provides compliant haptic assistance while pre-serving continuous human authority. The framework is trained entirely from heterogeneous offline teleoperation data containing both successful and failed executions. Task feasibility is mod-eled as a conservative success score learned via Conservative Value Learning, yielding a risk-sensitive estimate that remains reliable under distribution shift. During online operation, the learned success score regulates the level of assistance, while a learned actor provides a corrective motion direction. Both are integrated through a joint-space impedance interface on the master side, yielding continuous guidance that steers the operator away from failure-prone actions without overriding intent. Experimental results on contact-rich manipulation tasks demonstrate improved task success rates and reduced operator workload compared to conventional teleoperation and shared-autonomy baselines, indicating that conservative value learning provides an effective mechanism for embedding failure awareness into bilateral teleoperation. Experimental videos are available at https://www.youtube.com/watch?v=XDTsvzEkDRE

</details>


### [464] [StreamVLA: Breaking the Reason-Act Cycle via Completion-State Gating](https://arxiv.org/abs/2602.01100)
*Hang Wu,Tongqing Chen,Jiasen Wang,Xiaotao Li,Lu Fang*

Main category: cs.RO

TL;DR: StreamVLA: A dual-system VLA architecture that separates high-level planning from low-level control using a "Lock-and-Gated" mechanism to reduce latency by 48% while maintaining 98.5% success rate on LIBERO benchmark.


<details>
  <summary>Details</summary>
Motivation: Current Vision-Language-Action models entangle high-level planning (System 2) and low-level control (System 1), causing redundant multimodal reasoning at every timestep, leading to high latency and goal instability in long-horizon robotic manipulation tasks.

Method: StreamVLA uses a dual-system architecture with a "Lock-and-Gated" mechanism that triggers slow thinking only during sub-task transitions to generate textual instructions and imagine specific visual completion states. These completion states serve as time-invariant goal anchors. During steady execution, high-level intents are locked to condition a Flow Matching action head, bypassing expensive autoregressive decoding for 72% of timesteps.

Result: Achieves state-of-the-art performance with 98.5% success rate on LIBERO benchmark, robust recovery in real-world interference scenarios, and 48% reduction in latency compared to full-reasoning baselines.

Conclusion: StreamVLA's hierarchical abstraction effectively separates planning from execution, enabling efficient long-horizon robotic manipulation with reduced latency while maintaining high success rates and robustness to execution speed variations.

Abstract: Long-horizon robotic manipulation requires bridging the gap between high-level planning (System 2) and low-level control (System 1). Current Vision-Language-Action (VLA) models often entangle these processes, performing redundant multimodal reasoning at every timestep, which leads to high latency and goal instability. To address this, we present StreamVLA, a dual-system architecture that unifies textual task decomposition, visual goal imagination, and continuous action generation within a single parameter-efficient backbone. We introduce a "Lock-and-Gated" mechanism to intelligently modulate computation: only when a sub-task transition is detected, the model triggers slow thinking to generate a textual instruction and imagines the specific visual completion state, rather than generic future frames. Crucially, this completion state serves as a time-invariant goal anchor, making the policy robust to execution speed variations. During steady execution, these high-level intents are locked to condition a Flow Matching action head, allowing the model to bypass expensive autoregressive decoding for 72% of timesteps. This hierarchical abstraction ensures sub-goal focus while significantly reducing inference latency. Extensive evaluations demonstrate that StreamVLA achieves state-of-the-art performance, with a 98.5% success rate on the LIBERO benchmark and robust recovery in real-world interference scenarios, achieving a 48% reduction in latency compared to full-reasoning baselines.

</details>


### [465] [KAN We Flow? Advancing Robotic Manipulation with 3D Flow Matching via KAN & RWKV](https://arxiv.org/abs/2602.01115)
*Zhihao Chen,Yiyuan Ge,Ziyang Wang*

Main category: cs.RO

TL;DR: KAN-We-Flow: A lightweight flow-matching policy using RWKV-KAN blocks for efficient 3D manipulation, reducing parameters by 86.8% while achieving SOTA performance.


<details>
  <summary>Details</summary>
Motivation: Diffusion-based policies are inference-inefficient due to many denoising steps and heavy UNet backbones, hindering deployment on resource-constrained robots. Flow matching reduces sampling burden but still uses large UNet architectures.

Method: Combines RWKV for efficient time/channel mixing with GroupKAN layers for feature-wise nonlinear calibration. Introduces Action Consistency Regularization (ACR) for trajectory alignment via Euler extrapolation to stabilize training.

Result: Reduces parameters by 86.8%, maintains fast runtime, and achieves state-of-the-art success rates on Adroit, Meta-World, and DexArt benchmarks.

Conclusion: KAN-We-Flow provides a lightweight, expressive alternative to UNet-based flow matching policies, enabling efficient deployment on resource-constrained robots while maintaining high performance.

Abstract: Diffusion-based visuomotor policies excel at modeling action distributions but are inference-inefficient, since recursively denoising from noise to policy requires many steps and heavy UNet backbones, which hinders deployment on resource-constrained robots. Flow matching alleviates the sampling burden by learning a one-step vector field, yet prior implementations still inherit large UNet-style architectures. In this work, we present KAN-We-Flow, a flow-matching policy that draws on recent advances in Receptance Weighted Key Value (RWKV) and Kolmogorov-Arnold Networks (KAN) from vision to build a lightweight and highly expressive backbone for 3D manipulation. Concretely, we introduce an RWKV-KAN block: an RWKV first performs efficient time/channel mixing to propagate task context, and a subsequent GroupKAN layer applies learnable spline-based, groupwise functional mappings to perform feature-wise nonlinear calibration of the action mapping on RWKV outputs. Moreover, we introduce an Action Consistency Regularization (ACR), a lightweight auxiliary loss that enforces alignment between predicted action trajectories and expert demonstrations via Euler extrapolation, providing additional supervision to stabilize training and improve policy precision. Without resorting to large UNets, our design reduces parameters by 86.8\%, maintains fast runtime, and achieves state-of-the-art success rates on Adroit, Meta-World, and DexArt benchmarks. Our project page can be viewed in \href{https://zhihaochen-2003.github.io/KAN-We-Flow.github.io/}{\textcolor{red}{link}}

</details>


### [466] [UniForce: A Unified Latent Force Model for Robot Manipulation with Diverse Tactile Sensors](https://arxiv.org/abs/2602.01153)
*Zhuo Chen,Fei Ni,Kaiyao Luo,Zhiyuan Wu,Xuyang Zhang,Emmanouil Spyrakos-Papastavridis,Lorenzo Jamone,Nathan F. Lepora,Jiankang Deng,Shan Luo*

Main category: cs.RO

TL;DR: UniForce learns unified tactile representations across diverse sensors via inverse/forward dynamics modeling with force equilibrium constraints, enabling zero-shot transfer to manipulation tasks.


<details>
  <summary>Details</summary>
Motivation: Force sensing is crucial for dexterous manipulation, but heterogeneous tactile sensors (optical vs. magnetic, different form factors/materials) require sensor-specific data collection, calibration, and training, limiting generalizability.

Method: Proposes UniForce framework that learns shared latent force space by jointly modeling inverse dynamics (image-to-force) and forward dynamics (force-to-image) with force equilibrium and image reconstruction losses. Uses static equilibrium to collect force-paired data via direct sensor-object-sensor interactions without expensive external F/T sensors.

Result: Universal tactile encoder enables zero-shot transfer to downstream manipulation tasks without retraining. Experiments on GelSight, TacTip, and uSkin show improved force estimation over prior methods and effective cross-sensor coordination in VTLA models for robotic wiping.

Conclusion: UniForce addresses tactile sensor heterogeneity by learning unified force representations, enabling generalizable force-aware manipulation without sensor-specific training, with promising results across diverse sensor types.

Abstract: Force sensing is essential for dexterous robot manipulation, but scaling force-aware policy learning is hindered by the heterogeneity of tactile sensors. Differences in sensing principles (e.g., optical vs. magnetic), form factors, and materials typically require sensor-specific data collection, calibration, and model training, thereby limiting generalisability. We propose UniForce, a novel unified tactile representation learning framework that learns a shared latent force space across diverse tactile sensors. UniForce reduces cross-sensor domain shift by jointly modeling inverse dynamics (image-to-force) and forward dynamics (force-to-image), constrained by force equilibrium and image reconstruction losses to produce force-grounded representations. To avoid reliance on expensive external force/torque (F/T) sensors, we exploit static equilibrium and collect force-paired data via direct sensor--object--sensor interactions, enabling cross-sensor alignment with contact force. The resulting universal tactile encoder can be plugged into downstream force-aware robot manipulation tasks with zero-shot transfer, without retraining or finetuning. Extensive experiments on heterogeneous tactile sensors including GelSight, TacTip, and uSkin, demonstrate consistent improvements in force estimation over prior methods, and enable effective cross-sensor coordination in Vision-Tactile-Language-Action (VTLA) models for a robotic wiping task. Code and datasets will be released.

</details>


### [467] [Latent Reasoning VLA: Latent Thinking and Prediction for Vision-Language-Action Models](https://arxiv.org/abs/2602.01166)
*Shuanghao Bai,Jing Lyu,Wanqi Zhou,Zhe Li,Dakai Wang,Lei Xing,Xiaoguang Zhao,Pengwei Wang,Zhongyuan Wang,Cheng Chi,Badong Chen,Shanghang Zhang*

Main category: cs.RO

TL;DR: LaRA-VLA is a Vision-Language-Action model that internalizes chain-of-thought reasoning into continuous latent representations, eliminating explicit reasoning steps at inference for 90% faster performance while maintaining superior task performance.


<details>
  <summary>Details</summary>
Motivation: Existing VLA models with chain-of-thought reasoning suffer from high inference overhead due to explicit reasoning generation, and their discrete reasoning representations mismatch the continuous nature of perception and control in embodied tasks.

Method: Proposes LaRA-VLA framework that performs unified reasoning and prediction in latent space. Uses curriculum-based training: 1) explicit textual/visual CoT supervision, 2) transition to latent reasoning, 3) adapt latent reasoning dynamics to condition action generation.

Result: Outperforms state-of-the-art VLA methods on simulation benchmarks and long-horizon real-robot manipulation tasks while reducing inference latency by up to 90% compared to explicit CoT approaches.

Conclusion: Latent reasoning is an effective and efficient paradigm for real-time embodied control, enabling VLA models to maintain reasoning capabilities while achieving significant speed improvements for practical deployment.

Abstract: Vision-Language-Action (VLA) models benefit from chain-of-thought (CoT) reasoning, but existing approaches incur high inference overhead and rely on discrete reasoning representations that mismatch continuous perception and control. We propose Latent Reasoning VLA (\textbf{LaRA-VLA}), a unified VLA framework that internalizes multi-modal CoT reasoning into continuous latent representations for embodied action. LaRA-VLA performs unified reasoning and prediction in latent space, eliminating explicit CoT generation at inference time and enabling efficient, action-oriented control. To realize latent embodied reasoning, we introduce a curriculum-based training paradigm that progressively transitions from explicit textual and visual CoT supervision to latent reasoning, and finally adapts latent reasoning dynamics to condition action generation. We construct two structured CoT datasets and evaluate LaRA-VLA on both simulation benchmarks and long-horizon real-robot manipulation tasks. Experimental results show that LaRA-VLA consistently outperforms state-of-the-art VLA methods while reducing inference latency by up to 90\% compared to explicit CoT-based approaches, demonstrating latent reasoning as an effective and efficient paradigm for real-time embodied control. Project Page: \href{https://loveju1y.github.io/Latent-Reasoning-VLA/}{LaRA-VLA Website}.

</details>


### [468] [SPOT: Spatio-Temporal Obstacle-free Trajectory Planning for UAVs in an Unknown Dynamic Environment](https://arxiv.org/abs/2602.01189)
*Astik Srivastava,Thomas J Chackenkulam. Bitla Bhanu Teja,Antony Thomas,Madhava Krishna*

Main category: cs.RO

TL;DR: A mapless reactive motion planning system for quadrotors in unknown dynamic environments using 4D spatio-temporal planning with vision-based obstacle detection and backup planning for deadlock situations.


<details>
  <summary>Details</summary>
Motivation: Quadrotors need to navigate safely in unknown environments with dynamic obstacles, but existing methods often rely on map fusion which adds computational overhead and may not handle dynamic elements effectively.

Method: Combines 4D spatio-temporal planner with vision-based Safe Flight Corridor generation and trajectory optimization. Uses vision-based object segmentation/tracking to detect dynamic obstacles, and includes backup planning module for reactive avoidance during deadlocks.

Result: Validated in simulation and real-world experiments, showing significant advantages over state-of-the-art approaches for reactive UAV navigation in dynamic, unknown environments.

Conclusion: The mapless framework enables efficient collision avoidance directly from perception, reducing computational overhead while maintaining robustness in dynamic environments with both static and moving obstacles.

Abstract: We address the problem of reactive motion planning for quadrotors operating in unknown environments with dynamic obstacles. Our approach leverages a 4-dimensional spatio-temporal planner, integrated with vision-based Safe Flight Corridor (SFC) generation and trajectory optimization. Unlike prior methods that rely on map fusion, our framework is mapless, enabling collision avoidance directly from perception while reducing computational overhead. Dynamic obstacles are detected and tracked using a vision-based object segmentation and tracking pipeline, allowing robust classification of static versus dynamic elements in the scene. To further enhance robustness, we introduce a backup planning module that reactively avoids dynamic obstacles when no direct path to the goal is available, mitigating the risk of collisions during deadlock situations. We validate our method extensively in both simulation and real-world hardware experiments, and benchmark it against state-of-the-art approaches, showing significant advantages for reactive UAV navigation in dynamic, unknown environments.

</details>


### [469] [SkySim: A ROS2-based Simulation Environment for Natural Language Control of Drone Swarms using Large Language Models](https://arxiv.org/abs/2602.01226)
*Aditya Shibu,Marah Saleh,Mohamed Al-Musleh,Nidhal Abdulaziz*

Main category: cs.RO

TL;DR: SkySim: A ROS2/Gazebo framework that uses LLMs for natural language UAV swarm control with APF safety filters to ensure collision-free trajectories.


<details>
  <summary>Details</summary>
Motivation: Traditional UAV swarm control requires expert knowledge, static methods lack adaptability, and LLMs generate unsafe trajectories without physical grounding.

Method: Decouples LLM high-level planning (Gemini 3.5 Pro translates commands to waypoints) from low-level safety enforcement (APF filter ensures collision avoidance, kinematic limits, geo-fencing at 20 Hz).

Result: Validated with 3, 10, 30 Crazyflie drones: 100% spatial reasoning accuracy for geometric primitives, real-time collision prevention, and scalability.

Conclusion: SkySim enables non-expert natural language control of UAV swarms with safety guarantees, bridging AI cognition with robotic safety for dynamic environments.

Abstract: Unmanned Aerial Vehicle (UAV) swarms offer versatile applications in logistics, agriculture, and surveillance, yet controlling them requires expert knowledge for safety and feasibility. Traditional static methods limit adaptability, while Large Language Models (LLMs) enable natural language control but generate unsafe trajectories due to lacking physical grounding. This paper introduces SkySim, a ROS2-based simulation framework in Gazebo that decouples LLM high-level planning from low-level safety enforcement. Using Gemini 3.5 Pro, SkySim translates user commands (e.g., "Form a circle") into spatial waypoints, informed by real-time drone states. An Artificial Potential Field (APF) safety filter applies minimal adjustments for collision avoidance, kinematic limits, and geo-fencing, ensuring feasible execution at 20 Hz. Experiments with swarms of 3, 10, and 30 Crazyflie drones validate spatial reasoning accuracy (100% across tested geometric primitives), real-time collision prevention, and scalability. SkySim empowers non-experts to iteratively refine behaviors, bridging AI cognition with robotic safety for dynamic environments. Future work targets hardware integration.

</details>


### [470] [Reinforcement Learning for Active Perception in Autonomous Navigation](https://arxiv.org/abs/2602.01266)
*Grzegorz Malczyk,Mihir Kulkarni,Kostas Alexis*

Main category: cs.RO

TL;DR: End-to-end RL framework for autonomous navigation with active camera control to enhance situational awareness in unknown environments


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of active perception in autonomous navigation within complex, unknown environments where robots need to not only navigate safely but also actively control sensors to gather better information

Method: End-to-end reinforcement learning framework with policy receiving robot state, current depth frame, and local geometry representation from depth history; augmented navigation reward with voxel-based information metric to couple collision-free motion planning with active camera control

Result: Achieves safer flight compared to fixed, non-actuated camera baselines while inducing intrinsic exploratory behaviors; extensive evaluation demonstrates robust policy that balances goal-directed motion with exploratory sensing

Conclusion: The proposed active perception framework successfully enables aerial robots to learn robust navigation policies that integrate information-driven camera control with motion planning for safer and more effective autonomous operation in unknown environments

Abstract: This paper addresses the challenge of active perception within autonomous navigation in complex, unknown environments. Revisiting the foundational principles of active perception, we introduce an end-to-end reinforcement learning framework in which a robot must not only reach a goal while avoiding obstacles, but also actively control its onboard camera to enhance situational awareness. The policy receives observations comprising the robot state, the current depth frame, and a particularly local geometry representation built from a short history of depth readings. To couple collision-free motion planning with information-driven active camera control, we augment the navigation reward with a voxel-based information metric. This enables an aerial robot to learn a robust policy that balances goal-directed motion with exploratory sensing. Extensive evaluation demonstrates that our strategy achieves safer flight compared to using fixed, non-actuated camera baselines while also inducing intrinsic exploratory behaviors.

</details>


### [471] [TriphiBot: A Triphibious Robot Combining FOC-based Propulsion with Eccentric Design](https://arxiv.org/abs/2602.01385)
*Xiangyu Li,Mingwei Lai,Mengke Zhang,Junxiao Lin,Tiancheng Lai,Junping Zhi,Chao Xu,Fei Gao,Yanjun Cao*

Main category: cs.RO

TL;DR: A minimalist triphibious robot using quadcopter design with passive wheels achieves aerial, terrestrial, and aquatic motion through eccentric CoG design and unified FOC propulsion system.


<details>
  <summary>Details</summary>
Motivation: Existing multi-domain robots are limited to dual-mode platforms, suffer from high mechanical complexity or low propulsion efficiency, restricting their practical applications in complex environments.

Method: Combines quadcopter structure with two passive wheels (no extra actuators), uses eccentric Center of Gravity design for efficient ground-support motion, implements unified Field-Oriented Control propulsion system for air/water operation, and develops Hybrid Nonlinear Model Predictive Control-PID system for stable multi-domain motion.

Result: Experimental validation shows successful multi-domain motion and cross-mode transitions, with demonstrated efficiency and adaptability of the propulsion system across different mediums.

Conclusion: The minimalist triphibious robot design with eccentric CoG and unified FOC propulsion enables efficient, stable multi-domain operation and seamless transitions between air, land, and water environments.

Abstract: Triphibious robots capable of multi-domain motion and cross-domain transitions are promising to handle complex tasks across diverse environments. However, existing designs primarily focus on dual-mode platforms, and some designs suffer from high mechanical complexity or low propulsion efficiency, which limits their application. In this paper, we propose a novel triphibious robot capable of aerial, terrestrial, and aquatic motion, by a minimalist design combining a quadcopter structure with two passive wheels, without extra actuators. To address inefficiency of ground-support motion (moving on land/seabed) for quadcopter based designs, we introduce an eccentric Center of Gravity (CoG) design that inherently aligns thrust with motion, enhancing efficiency without specialized mechanical transformation designs. Furthermore, to address the drastic differences in motion control caused by different fluids (air and water), we develop a unified propulsion system based on Field-Oriented Control (FOC). This method resolves torque matching issues and enables precise, rapid bidirectional thrust across different mediums. Grounded in the perspective of living condition and ground support, we analyse the robot's dynamics and propose a Hybrid Nonlinear Model Predictive Control (HNMPC)-PID control system to ensure stable multi-domain motion and seamless transitions. Experimental results validate the robot's multi-domain motion and cross-mode transition capability, along with the efficiency and adaptability of the proposed propulsion system.

</details>


### [472] [Instance-Guided Unsupervised Domain Adaptation for Robotic Semantic Segmentation](https://arxiv.org/abs/2602.01389)
*Michele Antonazzi,Lorenzo Signorelli,Matteo Luperto,Nicola Basilico*

Main category: cs.RO

TL;DR: A method for unsupervised domain adaptation in semantic segmentation using 3D maps and foundation models to generate multi-view consistent pseudo-labels with instance-level coherence for robot perception adaptation.


<details>
  <summary>Details</summary>
Motivation: Semantic segmentation networks degrade when deployed in environments with different visual distributions than their training data. While UDA methods using multi-view consistency help, they remain sensitive to cross-view instance-level inconsistencies.

Method: Proposes generating multi-view consistent pseudo-labels from volumetric 3D maps, then refining them using zero-shot instance segmentation from foundation models to enforce instance-level coherence. These refined annotations enable self-supervised fine-tuning.

Result: Experiments on real-world data show consistent performance improvements over state-of-the-art UDA baselines based on multi-view consistency, without requiring ground-truth labels in the target domain.

Conclusion: The approach effectively adapts robot perception systems at deployment time by leveraging 3D maps and foundation models to create high-quality pseudo-labels with both multi-view and instance-level consistency.

Abstract: Semantic segmentation networks, which are essential for robotic perception, often suffer from performance degradation when the visual distribution of the deployment environment differs from that of the source dataset on which they were trained. Unsupervised Domain Adaptation (UDA) addresses this challenge by adapting the network to the robot's target environment without external supervision, leveraging the large amounts of data a robot might naturally collect during long-term operation. In such settings, UDA methods can exploit multi-view consistency across the environment's map to fine-tune the model in an unsupervised fashion and mitigate domain shift. However, these approaches remain sensitive to cross-view instance-level inconsistencies. In this work, we propose a method that starts from a volumetric 3D map to generate multi-view consistent pseudo-labels. We then refine these labels using the zero-shot instance segmentation capabilities of a foundation model, enforcing instance-level coherence. The refined annotations serve as supervision for self-supervised fine-tuning, enabling the robot to adapt its perception system at deployment time. Experiments on real-world data demonstrate that our approach consistently improves performance over state-of-the-art UDA baselines based on multi-view consistency, without requiring any ground-truth labels in the target domain.

</details>


### [473] [Sem-NaVAE: Semantically-Guided Outdoor Mapless Navigation via Generative Trajectory Priors](https://arxiv.org/abs/2602.01429)
*Gonzalo Olguin,Javier Ruiz-del-Solar*

Main category: cs.RO

TL;DR: A mapless outdoor navigation system using CVAEs to generate diverse trajectories and a VLM for semantic segmentation to select trajectories based on natural language, executed in real-time with a local planner.


<details>
  <summary>Details</summary>
Motivation: To create a flexible outdoor navigation system that doesn't require pre-built maps, can understand natural language instructions, and generates diverse trajectory options for real-time execution.

Method: Combines conditional variational autoencoders (CVAEs) for exploratory trajectory generation with lightweight visual language model (VLM) for open-vocabulary semantic segmentation to score and select trajectories based on natural language, plus a state-of-the-art local planner for velocity command execution.

Result: Validated through real-world outdoor navigation experiments, achieving superior performance compared to state-of-the-art methods, with real-time operation capability.

Conclusion: The approach successfully enables mapless outdoor navigation with natural language understanding, generating diverse trajectories and selecting optimal ones in real-time, outperforming existing methods.

Abstract: This work presents a mapless global navigation approach for outdoor applications. It combines the exploratory capacity of conditional variational autoencoders (CVAEs) to generate trajectories and the semantic segmentation capabilities of a lightweight visual language model (VLM) to select the trajectory to execute. Open-vocabulary segmentation is used to score and select the generated trajectories based on natural language, and a state-of-the-art local planner executes velocity commands. One of the key features of the proposed approach is its ability to generate a large variability of trajectories and to select them and navigate in real-time. The approach was validated through real-world outdoor navigation experiments, achieving superior performance compared to state-of-the-art methods. A video showing an experimental run of the system can be found in https://www.youtube.com/watch?v=i3R5ey5O2yk.

</details>


### [474] [Towards a Novel Wearable Robotic Vest for Hemorrhage Suppression](https://arxiv.org/abs/2602.01448)
*Harshith Jella,Pejman Kheradmand,Joseph Klein,Behnam Moradkhani,Yash Chitalia*

Main category: cs.RO

TL;DR: A robotic system with shape-adjustable ring mechanism and inflatable airbag for emergency bleeding control, adaptable to various body regions including space environments.


<details>
  <summary>Details</summary>
Motivation: To address severe bleeding management in emergency scenarios, especially in unique environments like space stations where traditional medical assistance may be limited, requiring adaptable wound coverage across different anatomical regions.

Method: Developed a shape-adjustable ring mechanism that transitions from circular to elliptical configurations, with various arm flexibilities for different body regions. Created an inflatable ring and airbag balloon system for constant pressure application. Conducted experiments on ring arm bending stiffness and airbag force measurement, followed by testing on casualty simulation kits.

Result: The system successfully demonstrated bleeding control capabilities on simulation kits. Experiments characterized ring arm configurations' bending stiffness and measured airbag balloon forces. However, limitations were identified in coverage area and shape-changing effectiveness, particularly with fully inflated airbags and complex anatomical regions.

Conclusion: The robotic bleeding control system shows promise for emergency scenarios including space environments, with successful simulation testing, but requires further development to address coverage limitations and improve adaptability to complex anatomical contours.

Abstract: This paper introduces a novel robotic system designed to manage severe bleeding in emergency scenarios, including unique environments like space stations. The robot features a shape-adjustable "ring mechanism", transitioning from a circular to an elliptical configuration to adjust wound coverage across various anatomical regions. We developed various arms for this ring mechanism with varying flexibilities to improve adaptability when applied to non-extremities of the body (abdomen, back, neck, etc.). To apply equal and constant pressure across the wound, we developed an inflatable ring and airbag balloon that are compatible with this shape-changing ring mechanism. A series of experiments focused on evaluating various ring arm configurations to characterize their bending stiffness. Subsequent experiments measured the force exerted by the airbag balloon system using a digital scale. Despite its promising performance, certain limitations related to coverage area are identified. The shape-changing effect of the device is limited to scenarios involving partially inflated or deflated airbag balloons, and cannot fully conform to complex anatomical regions. Finally, the device was tested on casualty simulation kits, where it successfully demonstrated its ability to control simulated bleeding.

</details>


### [475] [TreeLoc: 6-DoF LiDAR Global Localization in Forests via Inter-Tree Geometric Matching](https://arxiv.org/abs/2602.01501)
*Minwoo Jung,Nived Chebrolu,Lucas Carvalho de Lima,Haedam Oh,Maurice Fallon,Ayoung Kim*

Main category: cs.RO

TL;DR: TreeLoc is a LiDAR-based global localization framework for forests that uses tree stems and DBH for place recognition and 6-DoF pose estimation, outperforming baselines on diverse forest benchmarks.


<details>
  <summary>Details</summary>
Motivation: GPS is degraded in forests and LiDAR measurements are repetitive/occluded/complex, weakening traditional urban-centric localization methods that assume consistent features from unique structural patterns. Forest environments require specialized solutions for robust localization.

Method: Represents scenes using tree stems and their DBH, aligns them via axes, uses tree distribution histogram (TDH) for coarse matching, followed by fine matching with 2D triangle descriptor, and two-step geometric verification for pose estimation.

Result: Outperforms baselines on diverse forest benchmarks, achieving precise localization. Ablation studies validate each component's contribution. Enables applications for long-term forest management using compact global tree database descriptors.

Conclusion: TreeLoc provides a robust forest-centric localization solution that handles challenging forest conditions where traditional methods fail, with open-source availability for the robotics community.

Abstract: Reliable localization is crucial for navigation in forests, where GPS is often degraded and LiDAR measurements are repetitive, occluded, and structurally complex. These conditions weaken the assumptions of traditional urban-centric localization methods, which assume that consistent features arise from unique structural patterns, necessitating forest-centric solutions to achieve robustness in these environments. To address these challenges, we propose TreeLoc, a LiDAR-based global localization framework for forests that handles place recognition and 6-DoF pose estimation. We represent scenes using tree stems and their Diameter at Breast Height (DBH), which are aligned to a common reference frame via their axes and summarized using the tree distribution histogram (TDH) for coarse matching, followed by fine matching with a 2D triangle descriptor. Finally, pose estimation is achieved through a two-step geometric verification. On diverse forest benchmarks, TreeLoc outperforms baselines, achieving precise localization. Ablation studies validate the contribution of each component. We also propose applications for long-term forest management using descriptors from a compact global tree database. TreeLoc is open-sourced for the robotics community at https://github.com/minwoo0611/TreeLoc.

</details>


### [476] [RAPT: Model-Predictive Out-of-Distribution Detection and Failure Diagnosis for Sim-to-Real Humanoid Robots](https://arxiv.org/abs/2602.01515)
*Humphrey Munn,Brendan Tidd,Peter Bohm,Marcus Gallagher,David Howard*

Main category: cs.RO

TL;DR: RAPT is a lightweight, self-supervised deployment-time monitor for humanoid control that detects out-of-distribution states, provides interpretable mismatch signals, and enables automated root-cause analysis using LLM reasoning.


<details>
  <summary>Details</summary>
Motivation: Current anomaly detection methods for humanoid robot control are often incompatible with high-rate control, poorly calibrated at low false-positive rates, or operate as black boxes without explaining why failures occur, risking hardware damage during Sim-to-Real transfer.

Method: RAPT learns a probabilistic spatio-temporal manifold of nominal execution from simulation and evaluates execution-time predictive deviation as a calibrated, per-dimension signal. It includes an automated post-hoc root-cause analysis pipeline combining gradient-based temporal saliency with LLM-based reasoning.

Result: RAPT improves True Positive Rate by 37% over strongest baseline at 0.5% false positive rate in simulation, achieves 12.5% TPR improvement in real-world deployments, and reaches 75% root-cause classification accuracy across 16 real-world failures using only proprioceptive data.

Conclusion: RAPT provides reliable online OOD detection under strict false-positive constraints, offers continuous interpretable measures of Sim-to-Real mismatch, and enables automated semantic failure diagnosis, making it practical for safe deployment of learned control policies on humanoid robots.

Abstract: Deploying learned control policies on humanoid robots is challenging: policies that appear robust in simulation can execute confidently in out-of-distribution (OOD) states after Sim-to-Real transfer, leading to silent failures that risk hardware damage. Although anomaly detection can mitigate these failures, prior methods are often incompatible with high-rate control, poorly calibrated at the extremely low false-positive rates required for practical deployment, or operate as black boxes that provide a binary stop signal without explaining why the robot drifted from nominal behavior. We present RAPT, a lightweight, self-supervised deployment-time monitor for 50Hz humanoid control. RAPT learns a probabilistic spatio-temporal manifold of nominal execution from simulation and evaluates execution-time predictive deviation as a calibrated, per-dimension signal. This yields (i) reliable online OOD detection under strict false-positive constraints and (ii) a continuous, interpretable measure of Sim-to-Real mismatch that can be tracked over time to quantify how far deployment has drifted from training. Beyond detection, we introduce an automated post-hoc root-cause analysis pipeline that combines gradient-based temporal saliency derived from RAPT's reconstruction objective with LLM-based reasoning conditioned on saliency and joint kinematics to produce semantic failure diagnoses in a zero-shot setting. We evaluate RAPT on a Unitree G1 humanoid across four complex tasks in simulation and on physical hardware. In large-scale simulation, RAPT improves True Positive Rate (TPR) by 37% over the strongest baseline at a fixed episode-level false positive rate of 0.5%. On real-world deployments, RAPT achieves a 12.5% TPR improvement and provides actionable interpretability, reaching 75% root-cause classification accuracy across 16 real-world failures using only proprioceptive data.

</details>


### [477] [Co-Design of Rover Wheels and Control using Bayesian Optimization and Rover-Terrain Simulations](https://arxiv.org/abs/2602.01535)
*Huzaifa Mustafa Unjhawala,Khizar Shaikh,Luning Bakke,Radu Serban,Dan Negrut*

Main category: cs.RO

TL;DR: Bayesian optimization framework co-designs rover wheel geometry and steering controller parameters using high-fidelity full-vehicle simulations on deformable terrain, achieving practical co-optimization without expensive DEM studies.


<details>
  <summary>Details</summary>
Motivation: Traditional DEM simulations for deformable terrain are too expensive for full-vehicle studies, limiting joint optimization of mechanical design and control for off-road autonomous mobility.

Method: Bayesian optimization framework using continuum-representation model (CRM) for terramechanics to co-design wheel geometry (radius, width, grouser features) and steering PID gains under multi-objective formulation balancing speed, tracking error, and energy consumption.

Result: 3,000 full-vehicle simulations completed in 5-9 days (vs months with DEM), with preliminary hardware validation showing preserved performance trends; open-source release of simulation infrastructure.

Conclusion: Scalable high-fidelity simulation enables practical co-optimization of wheel design and control for off-road vehicles on deformable terrain without relying on prohibitively expensive DEM studies.

Abstract: While simulation is vital for optimizing robotic systems, the cost of modeling deformable terrain has long limited its use in full-vehicle studies of off-road autonomous mobility. For example, Discrete Element Method (DEM) simulations are often confined to single-wheel tests, which obscures coupled wheel-vehicle-controller interactions and prevents joint optimization of mechanical design and control. This paper presents a Bayesian optimization framework that co-designs rover wheel geometry and steering controller parameters using high-fidelity, full-vehicle closed-loop simulations on deformable terrain. Using the efficiency and scalability of a continuum-representation model (CRM) for terramechanics, we evaluate candidate designs on trajectories of varying complexity while towing a fixed load. The optimizer tunes wheel parameters (radius, width, and grouser features) and steering PID gains under a multi-objective formulation that balances traversal speed, tracking error, and energy consumption. We compare two strategies: simultaneous co-optimization of wheel and controller parameters versus a sequential approach that decouples mechanical and control design. We analyze trade-offs in performance and computational cost. Across 3,000 full-vehicle simulations, campaigns finish in five to nine days, versus months with the group's earlier DEM-based workflow. Finally, a preliminary hardware study suggests the simulation-optimized wheel designs preserve relative performance trends on the physical rover. Together, these results show that scalable, high-fidelity simulation can enable practical co-optimization of wheel design and control for off-road vehicles on deformable terrain without relying on prohibitively expensive DEM studies. The simulation infrastructure (scripts and models) is released as open source in a public repository to support reproducibility and further research.

</details>


### [478] [UniDWM: Towards a Unified Driving World Model via Multifaceted Representation Learning](https://arxiv.org/abs/2602.01536)
*Shuai Liu,Siheng Ren,Xiaoyao Zhu,Quanmin Liang,Zefeng Li,Qiang Li,Xin Hu,Kai Huang*

Main category: cs.RO

TL;DR: UniDWM is a unified driving world model that learns multifaceted scene representations (geometry, appearance, dynamics) for consistent reasoning across perception, prediction, and planning in autonomous driving.


<details>
  <summary>Details</summary>
Motivation: Reliable and efficient planning in complex driving environments requires models that can reason over scene geometry, appearance, and dynamics simultaneously, which current approaches often handle separately.

Method: UniDWM constructs structure- and dynamic-aware latent representations through joint reconstruction (geometry/texture recovery) and collaborative generation using conditional diffusion transformers to forecast future world evolution.

Result: Extensive experiments demonstrate UniDWM's effectiveness in trajectory planning, 4D reconstruction, and generation, showing the potential of multifaceted world representations for unified driving intelligence.

Conclusion: UniDWM provides a unified framework for driving world modeling with theoretical grounding as a VAE variation, offering physically grounded state spaces for consistent reasoning across perception, prediction, and planning tasks.

Abstract: Achieving reliable and efficient planning in complex driving environments requires a model that can reason over the scene's geometry, appearance, and dynamics. We present UniDWM, a unified driving world model that advances autonomous driving through multifaceted representation learning. UniDWM constructs a structure- and dynamic-aware latent world representation that serves as a physically grounded state space, enabling consistent reasoning across perception, prediction, and planning. Specifically, a joint reconstruction pathway learns to recover the scene's structure, including geometry and visual texture, while a collaborative generation framework leverages a conditional diffusion transformer to forecast future world evolution within the latent space. Furthermore, we show that our UniDWM can be deemed as a variation of VAE, which provides theoretical guidance for the multifaceted representation learning. Extensive experiments demonstrate the effectiveness of UniDWM in trajectory planning, 4D reconstruction and generation, highlighting the potential of multifaceted world representations as a foundation for unified driving intelligence. The code will be publicly available at https://github.com/Say2L/UniDWM.

</details>


### [479] [A Closed-Form Geometric Retargeting Solver for Upper Body Humanoid Robot Teleoperation](https://arxiv.org/abs/2602.01632)
*Chuizheng Kong,Yunho Cho,Wonsuhk Jung,Idris Wibowo,Parth Shinde,Sundhar Vinodh-Sangeetha,Long Kiu Chung,Zhenyang Chen,Andrew Mattei,Advaith Nidumukkala,Alexander Elias,Danfei Xu,Taylor Higgins,Shreyas Kousik*

Main category: cs.RO

TL;DR: SEW-Mimic: A fast, closed-form geometric solution for retargeting human motion to robot arms by aligning robot arm orientations to human upper/lower arm orientations from shoulder-elbow-wrist keypoints.


<details>
  <summary>Details</summary>
Motivation: Existing human motion retargeting methods for bimanual humanoid robots are suboptimal and slow, causing undesirable motion/latency and limiting robot workspace to human workspace due to end-effector matching optimization.

Method: Reframes retargeting as orientation alignment problem using closed-form geometric solution that aligns robot arm to human upper/lower arm orientations identified from shoulder, elbow, and wrist (SEW) keypoints.

Result: Fast inference (3 kHz on commercial CPUs), outperforms other methods in computation time and accuracy, improves teleoperation task success, enables safety filters, and accelerates full-body humanoid retargeting.

Conclusion: SEW-Mimic serves as a fundamental building block for bimanual robot manipulation and humanoid teleoperation, offering practical, efficient motion retargeting with optimality guarantees.

Abstract: Retargeting human motion to robot poses is a practical approach for teleoperating bimanual humanoid robot arms, but existing methods can be suboptimal and slow, often causing undesirable motion or latency. This is due to optimizing to match robot end-effector to human hand position and orientation, which can also limit the robot's workspace to that of the human. Instead, this paper reframes retargeting as an orientation alignment problem, enabling a closed-form, geometric solution algorithm with an optimality guarantee. The key idea is to align a robot arm to a human's upper and lower arm orientations, as identified from shoulder, elbow, and wrist (SEW) keypoints; hence, the method is called SEW-Mimic. The method has fast inference (3 kHz) on standard commercial CPUs, leaving computational overhead for downstream applications; an example in this paper is a safety filter to avoid bimanual self-collision. The method suits most 7-degree-of-freedom robot arms and humanoids, and is agnostic to input keypoint source. Experiments show that SEW-Mimic outperforms other retargeting methods in computation time and accuracy. A pilot user study suggests that the method improves teleoperation task success. Preliminary analysis indicates that data collected with SEW-Mimic improves policy learning due to being smoother. SEW-Mimic is also shown to be a drop-in way to accelerate full-body humanoid retargeting. Finally, hardware demonstrations illustrate SEW-Mimic's practicality. The results emphasize the utility of SEW-Mimic as a fundamental building block for bimanual robot manipulation and humanoid robot teleoperation.

</details>


### [480] [AgenticLab: A Real-World Robot Agent Platform that Can See, Think, and Act](https://arxiv.org/abs/2602.01662)
*Pengyuan Guo,Zhonghao Mai,Zhengtong Xu,Kaidi Zhang,Heng Zhang,Zichen Miao,Arash Ajoudani,Zachary Kingston,Qiang Qiu,Yu She*

Main category: cs.RO

TL;DR: AgenticLab is a model-agnostic robot agent platform and benchmark for evaluating VLM-based agents on real-robot manipulation tasks in unstructured environments, revealing failure modes not captured by offline vision-language tests.


<details>
  <summary>Details</summary>
Motivation: While large vision-language models show generalizable perception and reasoning, their real-robot manipulation capabilities for long-horizon, closed-loop execution in unstructured environments remain unclear. Existing VLM-based manipulation pipelines are difficult to compare across different setups, and many evaluations rely on simulation, privileged state, or specially designed setups.

Method: The authors present AgenticLab, a model-agnostic robot agent platform and benchmark for open-world manipulation. It provides a closed-loop agent pipeline for perception, task decomposition, online verification, and replanning. Using this platform, they benchmark state-of-the-art VLM-based agents on real-robot tasks in unstructured environments.

Result: The benchmark reveals several failure modes that offline vision-language tests fail to capture, including breakdowns in multi-step grounding consistency, object grounding under occlusion and scene changes, and insufficient spatial reasoning for reliable manipulation.

Conclusion: AgenticLab addresses the need for reproducible evaluation of VLM-based robot agents in real-world settings. The authors will release the full hardware and software stack to support reproducible evaluation and accelerate research on general-purpose robot agents.

Abstract: Recent advances in large vision-language models (VLMs) have demonstrated generalizable open-vocabulary perception and reasoning, yet their real-robot manipulation capability remains unclear for long-horizon, closed-loop execution in unstructured, in-the-wild environments. Prior VLM-based manipulation pipelines are difficult to compare across different research groups' setups, and many evaluations rely on simulation, privileged state, or specially designed setups. We present AgenticLab, a model-agnostic robot agent platform and benchmark for open-world manipulation. AgenticLab provides a closed-loop agent pipeline for perception, task decomposition, online verification, and replanning. Using AgenticLab, we benchmark state-of-the-art VLM-based agents on real-robot tasks in unstructured environments. Our benchmark reveals several failure modes that offline vision-language tests (e.g., VQA and static image understanding) fail to capture, including breakdowns in multi-step grounding consistency, object grounding under occlusion and scene changes, and insufficient spatial reasoning for reliable manipulation. We will release the full hardware and software stack to support reproducible evaluation and accelerate research on general-purpose robot agents.

</details>


### [481] [Towards Autonomous Instrument Tray Assembly for Sterile Processing Applications](https://arxiv.org/abs/2602.01679)
*Raghavasimhan Sankaranarayanan,Paul Stuart,Nicholas Ahn,Arno Sungarian,Yash Chitalia*

Main category: cs.RO

TL;DR: Fully automated robotic system for sorting and packing surgical instruments into sterile trays, reducing manual errors and contamination risks in hospital sterile processing departments.


<details>
  <summary>Details</summary>
Motivation: Manual inspection and preparation of surgical instrument trays is time-consuming, error-prone, and susceptible to contamination and instrument breakage in Sterile Processing and Distribution (SPD) departments.

Method: Developed a hybrid perception pipeline using YOLO12 for detection and cascaded ResNet-based model for fine-grained classification trained on custom dataset of 31 surgical instruments (6,975 images). Integrated calibrated vision module, 6-DOF Staubli TX2-60L robotic arm with custom dual electromagnetic gripper, and rule-based packing algorithm with 3D printed dividers/holders to isolate instruments.

Result: High perception accuracy and statistically significant reduction in tool-to-tool collisions compared to human-assembled trays. System successfully automates SPD assembly stage with improved safety and consistency.

Conclusion: This work represents a scalable first step toward automating SPD workflows, improving surgical preparation safety and consistency while reducing processing times in hospital sterile processing operations.

Abstract: The Sterile Processing and Distribution (SPD) department is responsible for cleaning, disinfecting, inspecting, and assembling surgical instruments between surgeries. Manual inspection and preparation of instrument trays is a time-consuming, error-prone task, often prone to contamination and instrument breakage. In this work, we present a fully automated robotic system that sorts and structurally packs surgical instruments into sterile trays, focusing on automation of the SPD assembly stage. A custom dataset comprising 31 surgical instruments and 6,975 annotated images was collected to train a hybrid perception pipeline using YOLO12 for detection and a cascaded ResNet-based model for fine-grained classification. The system integrates a calibrated vision module, a 6-DOF Staubli TX2-60L robotic arm with a custom dual electromagnetic gripper, and a rule-based packing algorithm that reduces instrument collisions during transport. The packing framework uses 3D printed dividers and holders to physically isolate instruments, reducing collision and friction during transport. Experimental evaluations show high perception accuracy and statistically significant reduction in tool-to-tool collisions compared to human-assembled trays. This work serves as the scalable first step toward automating SPD workflows, improving safety, and consistency of surgical preparation while reducing SPD processing times.

</details>


### [482] [GSR: Learning Structured Reasoning for Embodied Manipulation](https://arxiv.org/abs/2602.01693)
*Kewei Hu,Michael Zhang,Wei Ying,Tianhao Liu,Guoqiang Hao,Zimeng Li,Wanchan Yu,Jiajian Jing,Fangwen Chen,Hanwen Kang*

Main category: cs.RO

TL;DR: GSR introduces explicit scene-graph reasoning for embodied agents, improving long-horizon manipulation by separating task structure from perceptual variability through grounded world-state representations.


<details>
  <summary>Details</summary>
Motivation: Existing embodied agents struggle with long-horizon manipulation requiring spatial consistency, causal dependencies, and goal constraints. Current approaches embed task reasoning implicitly in high-dimensional latent representations, making it hard to separate task structure from perceptual variability.

Method: Introduces Grounded Scene-graph Reasoning (GSR) that explicitly models world-state evolution as transitions over semantically grounded scene graphs. Reasons step-wise over object states and spatial relations rather than directly mapping perception to actions. Also creates Manip-Cognition-1.6M dataset to jointly supervise world understanding, action planning, and goal interpretation.

Result: Extensive evaluations across RLBench, LIBERO, GSR-benchmark, and real-world robotic tasks show GSR significantly improves zero-shot generalization and long-horizon task completion over prompting-based baselines.

Conclusion: Explicit world-state representations serve as a key inductive bias for scalable embodied reasoning, demonstrating the value of structured reasoning paradigms over implicit latent representations.

Abstract: Despite rapid progress, embodied agents still struggle with long-horizon manipulation that requires maintaining spatial consistency, causal dependencies, and goal constraints. A key limitation of existing approaches is that task reasoning is implicitly embedded in high-dimensional latent representations, making it challenging to separate task structure from perceptual variability. We introduce Grounded Scene-graph Reasoning (GSR), a structured reasoning paradigm that explicitly models world-state evolution as transitions over semantically grounded scene graphs. By reasoning step-wise over object states and spatial relations, rather than directly mapping perception to actions, GSR enables explicit reasoning about action preconditions, consequences, and goal satisfaction in a physically grounded space. To support learning such reasoning, we construct Manip-Cognition-1.6M, a large-scale dataset that jointly supervises world understanding, action planning, and goal interpretation. Extensive evaluations across RLBench, LIBERO, GSR-benchmark, and real-world robotic tasks show that GSR significantly improves zero-shot generalization and long-horizon task completion over prompting-based baselines. These results highlight explicit world-state representations as a key inductive bias for scalable embodied reasoning.

</details>


### [483] [Tilt-Ropter: A Novel Hybrid Aerial and Terrestrial Vehicle with Tilt Rotors and Passive Wheels](https://arxiv.org/abs/2602.01700)
*Ruoyu Wang,Xuchen Liu,Zongzhou Wu,Zixuan Guo,Wendi Ding,Ben M. Chen*

Main category: cs.RO

TL;DR: Tilt-Ropter is a fully actuated hybrid aerial-terrestrial vehicle with tilt rotors and passive wheels that achieves energy-efficient multi-mode locomotion through NMPC control and external wrench estimation.


<details>
  <summary>Details</summary>
Motivation: Existing under-actuated hybrid aerial-terrestrial vehicles have limited mobility and environmental adaptability. The authors aim to create a more capable system that can perform long-duration missions across large-scale, energy-constrained environments by combining aerial and terrestrial locomotion efficiently.

Method: Developed Tilt-Ropter with fully actuated design using tilt rotors and passive wheels. Implemented a nonlinear model predictive controller (NMPC) for trajectory tracking and contact constraint handling. Added a control allocation module for energy-efficient actuator control. Introduced an external wrench estimation algorithm for robustness during ground contact.

Result: System validated through simulation and real-world experiments showing seamless air-ground transitions and trajectory tracking. Achieved low tracking errors in both modes and demonstrated 92.8% reduction in power consumption during ground locomotion compared to aerial mode.

Conclusion: Tilt-Ropter successfully demonstrates efficient multi-mode locomotion with significant energy savings during ground operation, making it suitable for long-duration missions in large-scale, energy-constrained environments.

Abstract: In this work, we present Tilt-Ropter, a novel hybrid aerial-terrestrial vehicle (HATV) that combines tilt rotors with passive wheels to achieve energy-efficient multi-mode locomotion. Unlike existing under-actuated HATVs, the fully actuated design of Tilt-Ropter enables decoupled force and torque control, greatly enhancing its mobility and environmental adaptability. A nonlinear model predictive controller (NMPC) is developed to track reference trajectories and handle contact constraints across locomotion modes, while a dedicated control allocation module exploits actuation redundancy to achieve energy-efficient control of actuators. Additionally, to enhance robustness during ground contact, we introduce an external wrench estimation algorithm that estimates environmental interaction forces and torques in real time. The system is validated through both simulation and real-world experiments, including seamless air-ground transitions and trajectory tracking. Results show low tracking errors in both modes and highlight a 92.8% reduction in power consumption during ground locomotion, demonstrating the system's potential for long-duration missions across large-scale and energy-constrained environments.

</details>


### [484] [Uncertainty-Aware Non-Prehensile Manipulation with Mobile Manipulators under Object-Induced Occlusion](https://arxiv.org/abs/2602.01731)
*Jiwoo Hwang,Taegeun Yang,Jeil Jeong,Minsung Yoon,Sung-Eui Yoon*

Main category: cs.RO

TL;DR: CURA-PPO uses reinforcement learning with uncertainty modeling to handle sensor occlusion in non-prehensile manipulation, achieving 3X higher success rates than baselines.


<details>
  <summary>Details</summary>
Motivation: Non-prehensile manipulation with onboard sensors suffers from occlusion problems where manipulated objects block the sensor's view, creating blind spots that can cause collisions.

Method: CURA-PPO is a reinforcement learning framework that models uncertainty under partial observability, predicts collision possibility distributions, extracts risk and uncertainty to guide actions, and uses confidence maps for observation reliability.

Result: Achieves up to 3X higher success rates than baselines across varying object sizes and obstacle configurations, with learned behaviors that effectively handle occlusions.

Conclusion: Provides a practical solution for autonomous manipulation in cluttered environments using only onboard sensing by enabling safe navigation despite severe sensor occlusion.

Abstract: Non-prehensile manipulation using onboard sensing presents a fundamental challenge: the manipulated object occludes the sensor's field of view, creating occluded regions that can lead to collisions. We propose CURA-PPO, a reinforcement learning framework that addresses this challenge by explicitly modeling uncertainty under partial observability. By predicting collision possibility as a distribution, we extract both risk and uncertainty to guide the robot's actions. The uncertainty term encourages active perception, enabling simultaneous manipulation and information gathering to resolve occlusions. When combined with confidence maps that capture observation reliability, our approach enables safe navigation despite severe sensor occlusion. Extensive experiments across varying object sizes and obstacle configurations demonstrate that CURA-PPO achieves up to 3X higher success rates than the baselines, with learned behaviors that handle occlusions. Our method provides a practical solution for autonomous manipulation in cluttered environments using only onboard sensing.

</details>


### [485] [RFS: Reinforcement learning with Residual flow steering for dexterous manipulation](https://arxiv.org/abs/2602.01789)
*Entong Su,Tyler Westenbroek,Anusha Nagabandi,Abhishek Gupta*

Main category: cs.RO

TL;DR: RFS is a reinforcement learning framework that efficiently fine-tunes pretrained generative policies by steering them with residual actions and latent noise optimization.


<details>
  <summary>Details</summary>
Motivation: Pretrained generative policies for imitation learning often lack generalization and need fine-tuning for robust deployment, requiring adaptation that preserves global exploration while correcting local errors.

Method: Residual Flow Steering (RFS) jointly optimizes a residual action and latent noise distribution to steer pretrained flow-matching policies, enabling complementary local refinement and global exploration.

Result: RFS demonstrates effective fine-tuning of pretrained policies for dexterous manipulation tasks in both simulation and real-world settings.

Conclusion: RFS provides a data-efficient RL framework for adapting pretrained generative policies while retaining their expressive structure and exploration capabilities.

Abstract: Imitation learning has emerged as an effective approach for bootstrapping sequential decision-making in robotics, achieving strong performance even in high-dimensional dexterous manipulation tasks. Recent behavior cloning methods further leverage expressive generative models, such as diffusion models and flow matching, to represent multimodal action distributions. However, policies pretrained in this manner often exhibit limited generalization and require additional fine-tuning to achieve robust performance at deployment time. Such adaptation must preserve the global exploration benefits of pretraining while enabling rapid correction of local execution errors.We propose \emph{Residual Flow Steering} (RFS), a data-efficient reinforcement learning framework for adapting pretrained generative policies. RFS steers a pretrained flow-matching policy by jointly optimizing a residual action and a latent noise distribution, enabling complementary forms of exploration: local refinement through residual corrections and global exploration through latent-space modulation. This design allows efficient adaptation while retaining the expressive structure of the pretrained policy.We demonstrate the effectiveness of RFS on dexterous manipulation tasks, showing efficient fine-tuning both in simulation and in real-world settings when adapting pretrained base policies.Project website:https://weirdlabuw.github.io/rfs.

</details>


### [486] [From Knowing to Doing Precisely: A General Self-Correction and Termination Framework for VLA models](https://arxiv.org/abs/2602.01811)
*Wentao Zhang,Aolan Sun,Wentao Mo,Xiaoyang Qu,Yuxin Zheng,Jianzong Wang*

Main category: cs.RO

TL;DR: VLA-SCT: A training-free self-correcting framework that improves VLA models by fixing spatial deviations in grasping and adding task completion recognition.


<details>
  <summary>Details</summary>
Motivation: Current VLA models have two critical weaknesses: 1) subtle spatial deviations in grasping actions cause failures, and 2) inability to recognize task completion leads to redundant actions and timeouts.

Method: VLA-SCT is a lightweight, training-free framework that operates as a self-correcting control loop combining data-driven action refinement with conditional logic for termination.

Result: Achieves consistent improvements across all datasets in the LIBERO benchmark, significantly increasing success rates for fine manipulation tasks and ensuring accurate task completion.

Conclusion: The framework enhances robustness of VLA agents, promoting more reliable deployment in complex, unstructured environments.

Abstract: While vision-language-action (VLA) models for embodied agents integrate perception, reasoning, and control, they remain constrained by two critical weaknesses: first, during grasping tasks, the action tokens generated by the language model often exhibit subtle spatial deviations from the target object, resulting in grasp failures; second, they lack the ability to reliably recognize task completion, which leads to redundant actions and frequent timeout errors. To address these challenges and enhance robustness, we propose a lightweight, training-free framework, VLA-SCT. This framework operates as a self-correcting control loop, combining data-driven action refinement with conditional logic for termination. Consequently, compared to baseline approaches, our method achieves consistent improvements across all datasets in the LIBERO benchmark, significantly increasing the success rate of fine manipulation tasks and ensuring accurate task completion, thereby promoting the deployment of more reliable VLA agents in complex, unstructured environments.

</details>


### [487] [Concept-Based Dictionary Learning for Inference-Time Safety in Vision Language Action Models](https://arxiv.org/abs/2602.01834)
*Siqi Wen,Shu Yang,Shaopeng Fu,Jingfeng Zhang,Lijie Hu,Di Wang*

Main category: cs.RO

TL;DR: A concept-based dictionary learning framework for inference-time safety control in Vision Language Action models that identifies harmful concept directions in hidden activations and applies threshold-based interventions to block unsafe physical actions.


<details>
  <summary>Details</summary>
Motivation: VLA models translate multimodal instructions into executable behaviors, but this capability magnifies safety risks - jailbreaks that merely yield toxic text in LLMs can trigger unsafe physical actions in embodied systems. Existing defenses intervene too late or at the wrong modality, leaving fused representations exploitable.

Method: Introduces a concept-based dictionary learning framework that constructs sparse, interpretable dictionaries from hidden activations to identify harmful concept directions, then applies threshold-based interventions to suppress or block unsafe activations during inference.

Result: Achieves state-of-the-art defense performance, cutting attack success rates by over 70% while maintaining task success on benchmarks including Libero-Harm, BadRobot, RoboPair, and IS-Bench. The framework is plug-in and model-agnostic, requiring no retraining.

Conclusion: This is the first inference-time concept-based safety method for embodied systems, advancing both interpretability and safe deployment of VLA models through a plug-in framework that integrates seamlessly with diverse VLAs without retraining.

Abstract: Vision Language Action (VLA) models close the perception action loop by translating multimodal instructions into executable behaviors, but this very capability magnifies safety risks: jailbreaks that merely yield toxic text in LLMs can trigger unsafe physical actions in embodied systems. Existing defenses alignment, filtering, or prompt hardening intervene too late or at the wrong modality, leaving fused representations exploitable. We introduce a concept-based dictionary learning framework for inference-time safety control. By constructing sparse, interpretable dictionaries from hidden activations, our method identifies harmful concept directions and applies threshold-based interventions to suppress or block unsafe activations. Experiments on Libero-Harm, BadRobot, RoboPair, and IS-Bench show that our approach achieves state-of-the-art defense performance, cutting attack success rates by over 70\% while maintaining task success. Crucially, the framework is plug-in and model-agnostic, requiring no retraining and integrating seamlessly with diverse VLAs. To our knowledge, this is the first inference-time concept-based safety method for embodied systems, advancing both interpretability and safe deployment of VLA models.

</details>


### [488] [Vision-only UAV State Estimation for Fast Flights Without External Localization Systems: A2RL Drone Racing Finalist Approach](https://arxiv.org/abs/2602.01860)
*Filip Novák,Matěj Petrlík,Matej Novosad,Parakh M. Gupta,Robert Pěnička,Martin Saska*

Main category: cs.RO

TL;DR: Fast UAV state estimation using monocular camera + IMU with drift-corrected VIO for aggressive maneuvers in cluttered GNSS-denied environments.


<details>
  <summary>Details</summary>
Motivation: Need for fast, reliable, and accurate state estimation for high-speed UAVs performing aggressive maneuvers in cluttered GNSS-denied environments, overcoming limitations of current approaches that use drifting VIO states and complex hardware.

Method: Fuses VIO, onboard landmark-based camera measurements, and IMU data with a novel mathematical drift model to estimate and compensate for VIO drift in all states (position, orientation, linear and angular velocity).

Result: Validated through 1600+ simulations and real-world experiments; successfully applied in A2RL Drone Racing Challenge 2025, advancing to final four out of 210 teams and earning a medal.

Conclusion: The approach enables accurate state estimation for high-speed UAVs during rapid dynamic motion using only monocular camera and IMU, outperforming state-of-the-art methods that rely on more complex hardware and uncorrected drifting VIO.

Abstract: Fast flights with aggressive maneuvers in cluttered GNSS-denied environments require fast, reliable, and accurate UAV state estimation. In this paper, we present an approach for onboard state estimation of a high-speed UAV using a monocular RGB camera and an IMU. Our approach fuses data from Visual-Inertial Odometry (VIO), an onboard landmark-based camera measurement system, and an IMU to produce an accurate state estimate. Using onboard measurement data, we estimate and compensate for VIO drift through a novel mathematical drift model. State-of-the-art approaches often rely on more complex hardware (e.g., stereo cameras or rangefinders) and use uncorrected drifting VIO velocities, orientation, and angular rates, leading to errors during fast maneuvers. In contrast, our method corrects all VIO states (position, orientation, linear and angular velocity), resulting in accurate state estimation even during rapid and dynamic motion. Our approach was thoroughly validated through 1600 simulations and numerous real-world experiments. Furthermore, we applied the proposed method in the A2RL Drone Racing Challenge 2025, where our team advanced to the final four out of 210 teams and earned a medal.

</details>


### [489] [BTGenBot-2: Efficient Behavior Tree Generation with Small Language Models](https://arxiv.org/abs/2602.01870)
*Riccardo Andrea Izzo,Gianluca Bardaro,Matteo Matteucci*

Main category: cs.RO

TL;DR: BTGenBot-2 is a 1B-parameter open-source small language model that converts natural language task descriptions and robot action primitives into executable behavior trees in XML, enabling zero-shot generation and error recovery while being lightweight for resource-constrained robots.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based robot planning methods are often closed-source or computationally intensive, making real-world deployment challenging, and there's no universally accepted plug-and-play representation for robotic task generation.

Method: Propose BTGenBot-2, a 1B-parameter open-source small language model that directly converts natural language task descriptions and robot action primitives into executable behavior trees in XML format, enabling zero-shot generation and error recovery.

Result: BTGenBot-2 outperforms GPT-5, Claude Opus 4.1, and larger open-source models across functional and non-functional metrics, achieving 90.38% success in zero-shot and 98.07% in one-shot, with up to 16x faster inference than previous BTGenBot.

Conclusion: BTGenBot-2 addresses key deployment challenges by providing an open-source, lightweight solution for LLM-based behavior tree generation that works effectively on resource-constrained robots while introducing the first standardized benchmark for this task.

Abstract: Recent advances in robot learning increasingly rely on LLM-based task planning, leveraging their ability to bridge natural language with executable actions. While prior works showcased great performances, the widespread adoption of these models in robotics has been challenging as 1) existing methods are often closed-source or computationally intensive, neglecting the actual deployment on real-world physical systems, and 2) there is no universally accepted, plug-and-play representation for robotic task generation. Addressing these challenges, we propose BTGenBot-2, a 1B-parameter open-source small language model that directly converts natural language task descriptions and a list of robot action primitives into executable behavior trees in XML. Unlike prior approaches, BTGenBot-2 enables zero-shot BT generation, error recovery at inference and runtime, while remaining lightweight enough for resource-constrained robots. We further introduce the first standardized benchmark for LLM-based BT generation, covering 52 navigation and manipulation tasks in NVIDIA Isaac Sim. Extensive evaluations demonstrate that BTGenBot-2 consistently outperforms GPT-5, Claude Opus 4.1, and larger open-source models across both functional and non-functional metrics, achieving average success rates of 90.38% in zero-shot and 98.07% in one-shot, while delivering up to 16x faster inference compared to the previous BTGenBot.

</details>


### [490] [Multimodal Large Language Models for Real-Time Situated Reasoning](https://arxiv.org/abs/2602.01880)
*Giulio Antonio Abbo,Senne Lenaerts,Tony Belpaeme*

Main category: cs.RO

TL;DR: Multimodal LLMs enable context-aware robotic decision-making for smart vacuum cleaning by combining GPT-4o with TurtleBot 4 to evaluate environments and make nuanced cleaning decisions based on social norms and user values.


<details>
  <summary>Details</summary>
Motivation: To explore how multimodal large language models can support real-time, context- and value-aware decision-making in domestic robotics, addressing the need for robots that can understand social norms and user preferences.

Method: Combined GPT-4o language model with TurtleBot 4 platform simulating a smart vacuum cleaning robot; used vision input to evaluate home environments and determine appropriate cleaning initiation based on contextual reasoning.

Result: Demonstrated the system in realistic home environments, showing ability to infer context and values from limited visual input and make nuanced decisions aligned with cleanliness, comfort, and safety values.

Conclusion: Multimodal LLMs show promise for enhancing robotic autonomy and situational awareness, but challenges remain in consistency, bias, and real-time performance that need to be addressed for practical deployment.

Abstract: In this work, we explore how multimodal large language models can support real-time context- and value-aware decision-making. To do so, we combine the GPT-4o language model with a TurtleBot 4 platform simulating a smart vacuum cleaning robot in a home. The model evaluates the environment through vision input and determines whether it is appropriate to initiate cleaning. The system highlights the ability of these models to reason about domestic activities, social norms, and user preferences and take nuanced decisions aligned with the values of the people involved, such as cleanliness, comfort, and safety. We demonstrate the system in a realistic home environment, showing its ability to infer context and values from limited visual input. Our results highlight the promise of multimodal large language models in enhancing robotic autonomy and situational awareness, while also underscoring challenges related to consistency, bias, and real-time performance.

</details>


### [491] [Path Tracking with Dynamic Control Point Blending for Autonomous Vehicles: An Experimental Study](https://arxiv.org/abs/2602.01892)
*Alexandre Lombard,Florent Perronnet,Nicolas Gaud,Abdeljalil Abbas-Turki*

Main category: cs.RO

TL;DR: A path-tracking framework for autonomous vehicles using dynamic control point interpolation between front and rear axles, with barycentric blending of steering controllers and curvature-aware speed control.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of fixed control points (front or rear axle) in autonomous vehicle path tracking, especially for diverse driving contexts like low-speed maneuvers and reverse motion where different control strategies are optimal.

Method: 1) Dynamic control point that continuously interpolates between front and rear axle positions; 2) Barycentric blending of front-axle Stanley controller and rear-axle curvature-based geometric controller; 3) Curvature-aware longitudinal control using virtual track borders and ray-tracing to convert geometric constraints into virtual obstacle distances for speed regulation.

Result: The approach was validated in simulation and on a real autonomous vehicle with GPS-RTK, radar, odometry, and IMU. Results showed improved trajectory accuracy, smoother steering profiles, and increased adaptability compared to fixed control-point baselines in both closed-loop tracking and backward maneuvers.

Conclusion: The proposed dynamic control point interpolation with blended steering controllers and curvature-aware speed control provides a unified framework that outperforms fixed control-point approaches, offering better adaptability across diverse driving contexts including forward and reverse motion.

Abstract: This paper presents an experimental study of a path-tracking framework for autonomous vehicles in which the lateral control command is applied to a dynamic control point along the wheelbase. Instead of enforcing a fixed reference at either the front or rear axle, the proposed method continuously interpolates between both, enabling smooth adaptation across driving contexts, including low-speed maneuvers and reverse motion. The lateral steering command is obtained by barycentric blending of two complementary controllers: a front-axle Stanley formulation and a rear-axle curvature-based geometric controller, yielding continuous transitions in steering behavior and improved tracking stability. In addition, we introduce a curvature-aware longitudinal control strategy based on virtual track borders and ray-tracing, which converts upcoming geometric constraints into a virtual obstacle distance and regulates speed accordingly. The complete approach is implemented in a unified control stack and validated in simulation and on a real autonomous vehicle equipped with GPS-RTK, radar, odometry, and IMU. The results in closed-loop tracking and backward maneuvers show improved trajectory accuracy, smoother steering profiles, and increased adaptability compared to fixed control-point baselines.

</details>


### [492] [Multi-Task Learning for Robot Perception with Imbalanced Data](https://arxiv.org/abs/2602.01899)
*Ozgur Erkent*

Main category: cs.RO

TL;DR: Proposes multi-task learning method that works without ground truth labels for some tasks, analyzes task interactions, and identifies which tasks improve others' performance.


<details>
  <summary>Details</summary>
Motivation: Multi-task learning improves individual task accuracy for resource-limited robots, but faces challenges with imbalanced data and difficulty obtaining labels in mobile robot environments.

Method: Method learns tasks without ground truth labels for some tasks; analyzes task interactions by training teacher network with task outputs (like depth) as inputs; tested with semantic segmentation and depth estimation on NYUDv2 and Cityscapes datasets.

Result: Shows methodology to identify which tasks can improve performance of other tasks; provides empirical evidence when trained with small amounts of data.

Conclusion: Proposed method enables multi-task learning in label-scarce environments for robots, with analysis revealing task interactions that can guide efficient multi-task system design.

Abstract: Multi-task problem solving has been shown to improve the accuracy of the individual tasks, which is an important feature for robots, as they have a limited resource. However, when the number of labels for each task is not equal, namely imbalanced data exist, a problem may arise due to insufficient number of samples, and labeling is not very easy for mobile robots in every environment. We propose a method that can learn tasks even in the absence of the ground truth labels for some of the tasks. We also provide a detailed analysis of the proposed method. An interesting finding is related to the interaction of the tasks. We show a methodology to find out which tasks can improve the performance of other tasks. We investigate this by training the teacher network with the task outputs such as depth as inputs. We further provide empirical evidence when trained with a small amount of data. We use semantic segmentation and depth estimation tasks on different datasets, NYUDv2 and Cityscapes.

</details>


### [493] [ForSim: Stepwise Forward Simulation for Traffic Policy Fine-Tuning](https://arxiv.org/abs/2602.01916)
*Keyu Chen,Wenchao Sun,Hao Cheng,Zheng Fu,Sifa Zheng*

Main category: cs.RO

TL;DR: ForSim introduces a stepwise closed-loop forward simulation paradigm for autonomous driving traffic simulation that preserves multimodal behavioral diversity while ensuring intra-modality consistency through spatiotemporal matching and physically grounded motion dynamics.


<details>
  <summary>Details</summary>
Motivation: Current traffic simulation for autonomous driving faces two fundamental challenges: covariate shift from open-loop imitation learning and limited capacity to reflect real-world multimodal behaviors. Existing frameworks like RIFT have partially addressed these through group-relative optimization but remain largely non-reactive, leading to unrealistic agent interactions and limited simulation fidelity.

Method: ForSim is a stepwise closed-loop forward simulation paradigm where at each virtual timestep, traffic agents propagate virtual candidate trajectories that best spatiotemporally match reference trajectories through physically grounded motion dynamics. This preserves multimodal behavioral diversity while ensuring intra-modality consistency. Other agents are updated with stepwise predictions for coherent, interaction-aware evolution. When integrated with RIFT, ForSim works with group-relative optimization to fine-tune traffic policies.

Result: Extensive experiments show that integrating ForSim into the RIFT framework consistently improves safety while maintaining efficiency, realism, and comfort. The results demonstrate enhanced simulation fidelity and reliability for autonomous driving applications.

Conclusion: ForSim underscores the importance of modeling closed-loop multimodal interactions within forward simulation, enhancing the fidelity and reliability of traffic simulation for autonomous driving by addressing reactivity and multimodal behavior representation limitations in existing approaches.

Abstract: As the foundation of closed-loop training and evaluation in autonomous driving, traffic simulation still faces two fundamental challenges: covariate shift introduced by open-loop imitation learning and limited capacity to reflect the multimodal behaviors observed in real-world traffic. Although recent frameworks such as RIFT have partially addressed these issues through group-relative optimization, their forward simulation procedures remain largely non-reactive, leading to unrealistic agent interactions within the virtual domain and ultimately limiting simulation fidelity. To address these issues, we propose ForSim, a stepwise closed-loop forward simulation paradigm. At each virtual timestep, the traffic agent propagates the virtual candidate trajectory that best spatiotemporally matches the reference trajectory through physically grounded motion dynamics, thereby preserving multimodal behavioral diversity while ensuring intra-modality consistency. Other agents are updated with stepwise predictions, yielding coherent and interaction-aware evolution. When incorporated into the RIFT traffic simulation framework, ForSim operates in conjunction with group-relative optimization to fine-tune traffic policy. Extensive experiments confirm that this integration consistently improves safety while maintaining efficiency, realism, and comfort. These results underscore the importance of modeling closed-loop multimodal interactions within forward simulation and enhance the fidelity and reliability of traffic simulation for autonomous driving. Project Page: https://currychen77.github.io/ForSim/

</details>


### [494] [LIEREx: Language-Image Embeddings for Robotic Exploration](https://arxiv.org/abs/2602.01930)
*Felix Igelbrink,Lennart Niecksch,Marian Renz,Martin Günther,Martin Atzmueller*

Main category: cs.RO

TL;DR: LIEREx integrates Vision-Language Foundation Models with 3D Semantic Scene Graphs for open-set semantic mapping and target-directed exploration in unknown environments.


<details>
  <summary>Details</summary>
Motivation: Traditional semantic mapping approaches are limited by pre-designed symbolic vocabularies that can't handle out-of-distribution knowledge, making them impractical for real-world deployment where robots encounter novel objects and concepts.

Method: Combines Vision-Language Foundation Models (like CLIP) with 3D Semantic Scene Graphs to create open-set semantic maps where objects are represented as high-dimensional embeddings rather than fixed labels.

Result: Enables target-directed exploration by autonomous agents in partially unknown environments, allowing robots to reason about surroundings using open-set semantic understanding.

Conclusion: LIEREx bridges the gap between traditional geometric mapping and open-world semantic understanding, providing a more flexible and practical approach for autonomous navigation and exploration.

Abstract: Semantic maps allow a robot to reason about its surroundings to fulfill tasks such as navigating known environments, finding specific objects, and exploring unmapped areas. Traditional mapping approaches provide accurate geometric representations but are often constrained by pre-designed symbolic vocabularies. The reliance on fixed object classes makes it impractical to handle out-of-distribution knowledge not defined at design time. Recent advances in Vision-Language Foundation Models, such as CLIP, enable open-set mapping, where objects are encoded as high-dimensional embeddings rather than fixed labels. In LIEREx, we integrate these VLFMs with established 3D Semantic Scene Graphs to enable target-directed exploration by an autonomous agent in partially unknown environments.

</details>


### [495] [Towards Exploratory and Focused Manipulation with Bimanual Active Perception: A New Problem, Benchmark and Strategy](https://arxiv.org/abs/2602.01939)
*Yuxin He,Ruihao Zhang,Tianao Shen,Cheng Liu,Qiang Nie*

Main category: cs.RO

TL;DR: Active vision approach using bimanual robots to address visual occlusion in manipulation tasks through exploratory and focused manipulation.


<details>
  <summary>Details</summary>
Motivation: Visual occlusion is a major challenge in robot manipulation when cameras are mounted on robot heads, limiting task completion due to missing information.

Method: Proposes EFM-10 benchmark with 10 tasks across 4 categories, and introduces Bimanual Active Perception (BAP) strategy using one arm for active vision and another for force sensing while manipulating.

Result: Created BAPData dataset and verified BAP strategy effectiveness through imitation learning on EFM-10 benchmark tasks.

Conclusion: EFM-10 benchmark and BAP strategy provide foundational tools for future research in exploratory and focused manipulation, addressing visual occlusion through active perception.

Abstract: Recently, active vision has reemerged as an important concept for manipulation, since visual occlusion occurs more frequently when main cameras are mounted on the robot heads. We reflect on the visual occlusion issue and identify its essence as the absence of information useful for task completion. Inspired by this, we come up with the more fundamental problem of Exploratory and Focused Manipulation (EFM). The proposed problem is about actively collecting information to complete challenging manipulation tasks that require exploration or focus. As an initial attempt to address this problem, we establish the EFM-10 benchmark that consists of 4 categories of tasks that align with our definition (10 tasks in total). We further come up with a Bimanual Active Perception (BAP) strategy, which leverages one arm to provide active vision and another arm to provide force sensing while manipulating. Based on this idea, we collect a dataset named BAPData for the tasks in EFM-10. With the dataset, we successfully verify the effectiveness of the BAP strategy in an imitation learning manner. We hope that the EFM-10 benchmark along with the BAP strategy can become a cornerstone that facilitates future research towards this direction. Project website: EFManipulation.github.io.

</details>


### [496] [A Unified Control Architecture for Macro-Micro Manipulation using a Active Remote Center of Compliance for Manufacturing Applications](https://arxiv.org/abs/2602.01948)
*Patrick Frank,Christian Friedrich*

Main category: cs.RO

TL;DR: Novel control architecture for macro-micro manipulators that incorporates the macro manipulator into active interaction control, achieving 2.1x higher bandwidth than state-of-the-art and 12.5x higher than traditional robot-based force control.


<details>
  <summary>Details</summary>
Motivation: Traditional macro-micro manipulator architectures limit interaction control bandwidth by assigning position control to the macro manipulator and interaction control only to the micro manipulator, restricting overall system performance.

Method: Proposed novel control architecture that incorporates the macro manipulator into active interaction control, plus surrogate models for efficient controller design and hardware adaptation.

Result: Achieved 2.1x higher control bandwidth compared to state-of-the-art leader-follower approach and 12.5x higher compared to traditional robot-based force control. Validated through experiments including collision handling, force trajectory following, and industrial assembly tasks.

Conclusion: The proposed architecture significantly improves interaction control performance in macro-micro manipulator systems while maintaining the large workspace advantage, with practical benefits demonstrated in real-world applications.

Abstract: Macro-micro manipulators combine a macro manipulator with a large workspace, such as an industrial robot, with a lightweight, high-bandwidth micro manipulator. This enables highly dynamic interaction control while preserving the wide workspace of the robot. Traditionally, position control is assigned to the macro manipulator, while the micro manipulator handles the interaction with the environment, limiting the achievable interaction control bandwidth. To solve this, we propose a novel control architecture that incorporates the macro manipulator into the active interaction control. This leads to a increase in control bandwidth by a factor of 2.1 compared to the state of the art architecture, based on the leader-follower approach and factor 12.5 compared to traditional robot-based force control. Further we propose surrogate models for a more efficient controller design and easy adaptation to hardware changes. We validate our approach by comparing it against the other control schemes in different experiments, like collision with an object, following a force trajectory and industrial assembly tasks.

</details>


### [497] [Reformulating AI-based Multi-Object Relative State Estimation for Aleatoric Uncertainty-based Outlier Rejection of Partial Measurements](https://arxiv.org/abs/2602.02006)
*Thomas Jantos,Giulio Delama,Stephan Weiss,Jan Steinbrener*

Main category: cs.RO

TL;DR: This paper proposes reformulating measurement equations in AI-based object-relative state estimation to improve EKF performance by decoupling position/rotation measurements and incorporating DNN uncertainty predictions.


<details>
  <summary>Details</summary>
Motivation: Mobile robots need precise localization relative to objects of interest, and while AI/DNNs can extract semantic information (object class, 6-DoF pose) from images, effectively fusing these measurements in an EKF requires proper uncertainty quantification and outlier rejection capabilities.

Method: Derives an Extended Kalman Filter using direct object-relative pose measurements to decouple position and rotation measurements, allowing partial measurement rejection. Replaces fixed measurement covariance matrices with predicted aleatoric uncertainty from DNNs.

Result: The reformulation limits influence of erroneous rotation measurements and enables partial measurement rejection. Using DNN-predicted uncertainty improves state estimator performance and consistency compared to fixed covariance approaches.

Conclusion: Reformulating measurement equations in AI-based object-relative state estimation, combined with incorporating DNN uncertainty predictions, provides significant benefits for EKF performance, including better handling of measurement errors and improved consistency.

Abstract: Precise localization with respect to a set of objects of interest enables mobile robots to perform various tasks. With the rise of edge devices capable of deploying deep neural networks (DNNs) for real-time inference, it stands to reason to use artificial intelligence (AI) for the extraction of object-specific, semantic information from raw image data, such as the object class and the relative six degrees of freedom (6-DoF) pose. However, fusing such AI-based measurements in an Extended Kalman Filter (EKF) requires quantifying the DNNs' uncertainty and outlier rejection capabilities.
  This paper presents the benefits of reformulating the measurement equation in AI-based, object-relative state estimation. By deriving an EKF using the direct object-relative pose measurement, we can decouple the position and rotation measurements, thus limiting the influence of erroneous rotation measurements and allowing partial measurement rejection. Furthermore, we investigate the performance and consistency improvements for state estimators provided by replacing the fixed measurement covariance matrix of the 6-DoF object-relative pose measurements with the predicted aleatoric uncertainty of the DNN.

</details>


### [498] [Synchronized Online Friction Estimation and Adaptive Grasp Control for Robust Gentle Grasp](https://arxiv.org/abs/2602.02026)
*Zhenwei Niu,Xiaoyi Chen,Jiayu Hu,Zhaoyang Liu,Xiaozu Ju*

Main category: cs.RO

TL;DR: Real-time friction estimation + adaptive grasp control for gentle robotic grasping


<details>
  <summary>Details</summary>
Motivation: Need for gentle robotic grasping that can adapt to varying friction conditions in real-time

Method: Particle filter-based friction estimation using vision-based tactile sensors + reactive controller for dynamic grasp force modulation in closed-loop

Result: Validated through extensive robotic experiments showing reliable and efficient performance

Conclusion: Unified framework creates highly responsive and robust sensorimotor cycle for gentle grasping

Abstract: We introduce a unified framework for gentle robotic grasping that synergistically couples real-time friction estimation with adaptive grasp control. We propose a new particle filter-based method for real-time estimation of the friction coefficient using vision-based tactile sensors. This estimate is seamlessly integrated into a reactive controller that dynamically modulates grasp force to maintain a stable grip. The two processes operate synchronously in a closed-loop: the controller uses the current best estimate to adjust the force, while new tactile feedback from this action continuously refines the estimation. This creates a highly responsive and robust sensorimotor cycle. The reliability and efficiency of the complete framework are validated through extensive robotic experiments.

</details>


### [499] [Bandwidth-Efficient Multi-Agent Communication through Information Bottleneck and Vector Quantization](https://arxiv.org/abs/2602.02035)
*Ahmad Farooq,Kamran Iqbal*

Main category: cs.RO

TL;DR: Multi-agent RL framework combines information bottleneck with vector quantization for selective, bandwidth-efficient communication, achieving 181.8% performance gain over no-communication baselines while reducing bandwidth by 41.4%.


<details>
  <summary>Details</summary>
Motivation: Real-world multi-agent robotics applications face severe communication constraints that significantly impact coordination effectiveness, requiring bandwidth-efficient communication solutions for robotic swarms, autonomous vehicle fleets, and distributed sensor networks.

Method: Combines information bottleneck theory with vector quantization to compress and discretize communication messages while preserving task-critical information. Introduces gated communication mechanism that dynamically determines when communication is necessary based on environmental context and agent states.

Result: Achieves 181.8% performance improvement over no-communication baselines while reducing bandwidth usage by 41.4%. Pareto frontier analysis shows dominance across entire success-bandwidth spectrum with area-under-curve of 0.198 vs 0.142 for next-best methods.

Conclusion: The approach significantly outperforms existing communication strategies and establishes a theoretically grounded framework for deploying multi-agent systems in bandwidth-constrained environments.

Abstract: Multi-agent reinforcement learning systems deployed in real-world robotics applications face severe communication constraints that significantly impact coordination effectiveness. We present a framework that combines information bottleneck theory with vector quantization to enable selective, bandwidth-efficient communication in multi-agent environments. Our approach learns to compress and discretize communication messages while preserving task-critical information through principled information-theoretic optimization. We introduce a gated communication mechanism that dynamically determines when communication is necessary based on environmental context and agent states. Experimental evaluation on challenging coordination tasks demonstrates that our method achieves 181.8% performance improvement over no-communication baselines while reducing bandwidth usage by 41.4%. Comprehensive Pareto frontier analysis shows dominance across the entire success-bandwidth spectrum with area-under-curve of 0.198 vs 0.142 for next-best methods. Our approach significantly outperforms existing communication strategies and establishes a theoretically grounded framework for deploying multi-agent systems in bandwidth-constrained environments such as robotic swarms, autonomous vehicle fleets, and distributed sensor networks.

</details>


### [500] [Frictional Contact Solving for Material Point Method](https://arxiv.org/abs/2602.02038)
*Etienne Ménager,Justin Carpentier*

Main category: cs.RO

TL;DR: A robust frictional-contact pipeline for implicit MPM using particle-centric geometric primitives for detection and ADMM to solve contact as NCP.


<details>
  <summary>Details</summary>
Motivation: Accurate handling of contact with friction remains a core bottleneck for Material Point Method (MPM), from reliable contact point detection to enforcing frictional contact laws.

Method: Two-phase approach: 1) Collision detection with particle-centric geometric primitives for contact point localization; 2) Contact resolution by casting frictional contact as Nonlinear Complementarity Problem (NCP) and solving with Alternating Direction Method of Multipliers (ADMM), reusing implicit MPM linearization.

Result: Method enables accurate contact localization, reliable frictional handling, and broad generality across seven representative scenes spanning elastic/elasto-plastic responses, complex geometries, and wide contact conditions.

Conclusion: The proposed method provides a practical solution for MPM-based simulations in robotics and related domains, integrating seamlessly into implicit MPM loop while being agnostic to modeling choices.

Abstract: Accurately handling contact with friction remains a core bottleneck for Material Point Method (MPM), from reliable contact point detection to enforcing frictional contact laws (non-penetration, Coulomb friction, and maximum dissipation principle). In this paper, we introduce a frictional-contact pipeline for implicit MPM that is both precise and robust. During the collision detection phase, contact points are localized with particle-centric geometric primitives; during the contact resolution phase, we cast frictional contact as a Nonlinear Complementarity Problem (NCP) over contact impulses and solve it with an Alternating Direction Method of Multipliers (ADMM) scheme. Crucially, the formulation reuses the same implicit MPM linearization, yielding efficiency and numerical stability. The method integrates seamlessly into the implicit MPM loop and is agnostic to modeling choices, including material laws, interpolation functions, and transfer schemes. We evaluate it across seven representative scenes that span elastic and elasto-plastic responses, simple and complex deformable geometries, and a wide range of contact conditions. Overall, the proposed method enables accurate contact localization, reliable frictional handling, and broad generality, making it a practical solution for MPM-based simulations in robotics and related domains.

</details>


### [501] [FD-VLA: Force-Distilled Vision-Language-Action Model for Contact-Rich Manipulation](https://arxiv.org/abs/2602.02142)
*Ruiteng Zhao,Wenshuo Wang,Yicheng Ma,Xiaocong Li,Francis E. H. Tay,Marcelo H. Ang,Haiyue Zhu*

Main category: cs.RO

TL;DR: FD-VLA integrates force awareness into VLA frameworks without physical sensors by distilling force information through a Force Distillation Module, enabling force-aware reasoning while preserving vision-language semantics.


<details>
  <summary>Details</summary>
Motivation: Force sensing is crucial for VLA frameworks in contact-rich manipulation tasks, but many robots lack expensive or fragile force-torque sensors. The paper aims to enable force awareness without requiring physical sensors, reducing hardware cost and complexity.

Method: Proposes Force-Distilled VLA (FD-VLA) with a Force Distillation Module (FDM) that maps a learnable query token conditioned on visual observations and robot states into a predicted force token aligned with actual force signal representations. This distilled force token is injected into pretrained VLMs during inference.

Result: Physical experiments show that the distilled force token outperforms direct sensor force measurements and other baselines. The approach enables practical deployment across robots lacking force sensors while improving cross-modal alignment and perception-action robustness.

Conclusion: FD-VLA successfully integrates force awareness into VLA frameworks without physical sensors through force distillation, offering cost-effective deployment and enhanced performance in contact-rich manipulation tasks, with distilled force tokens even outperforming actual sensor measurements.

Abstract: Force sensing is a crucial modality for Vision-Language-Action (VLA) frameworks, as it enables fine-grained perception and dexterous manipulation in contact-rich tasks. We present Force-Distilled VLA (FD-VLA), a novel framework that integrates force awareness into contact-rich manipulation without relying on physical force sensors. The core of our approach is a Force Distillation Module (FDM), which distills force by mapping a learnable query token, conditioned on visual observations and robot states, into a predicted force token aligned with the latent representation of actual force signals. During inference, this distilled force token is injected into the pretrained VLM, enabling force-aware reasoning while preserving the integrity of its vision-language semantics. This design provides two key benefits: first, it allows practical deployment across a wide range of robots that lack expensive or fragile force-torque sensors, thereby reducing hardware cost and complexity; second, the FDM introduces an additional force-vision-state fusion prior to the VLM, which improves cross-modal alignment and enhances perception-action robustness in contact-rich scenarios. Surprisingly, our physical experiments show that the distilled force token outperforms direct sensor force measurements as well as other baselines, which highlights the effectiveness of this force-distilled VLA approach.

</details>


### [502] [Extending the Law of Intersegmental Coordination: Implications for Powered Prosthetic Controls](https://arxiv.org/abs/2602.02181)
*Elad Siman Tov,Nili E. Krausz*

Main category: cs.RO

TL;DR: Researchers developed a method to analyze intersegmental coordination in lower-limb gait, extending it to moments, and created a toolbox to study coordination in amputee walking with implications for prosthetic control.


<details>
  <summary>Details</summary>
Motivation: Despite advances in powered prostheses, reducing amputee metabolic cost remains challenging. The Law of Intersegmental Coordination (ISC) has been linked to walking energy expenditure but hasn't been systematically analyzed in amputee gait.

Method: Developed a method to analyze ISC for 3D kinematic data, extended it to create Elevation Space Moments (ESM) for moment-based coordination analysis, and created the ISC3d toolbox for computing kinematic and kinetic ISC.

Result: Found that while amputee elevation angles remained planar, ESM showed less coordination. Used ISC as a constraint to predict compensatory shank angles/moments that could mimic healthy thigh profiles when using passive prostheses.

Conclusion: The ISC3d toolbox enables further study of coordination in gait and may help address fundamental questions of neural control. The findings have implications for improving powered prosthetic control by using ISC constraints to compensate for prosthetic limitations.

Abstract: Powered prostheses are capable of providing net positive work to amputees and have advanced in the past two decades. However, reducing amputee metabolic cost of walking remains an open problem. The Law of Intersegmental Coordination (ISC) has been observed across gaits and has been previously implicated in energy expenditure of walking, yet it has rarely been analyzed or applied within the context of lower-limb amputee gait. This law states that the elevation angles of the thigh, shank and foot over the gait cycle are not independent. In this work, we developed a method to analyze intersegmental coordination for lower-limb 3D kinematic data, to simplify ISC analysis. Moreover, inspired by motor control, biomechanics and robotics literature, we used our method to broaden ISC toward a new law of coordination of moments. We find these Elevation Space Moments (ESM), and present results showing a moment-based coordination for able bodied gait. We also analyzed ISC for amputee gait walking with powered and passive prosthesis, and found that while elevation angles remained planar, the ESM showed less coordination. We use ISC as a constraint to predict the shank angles/moments that would compensate for alterations due to a passive foot so as to mimic a healthy thigh angle/moment profile. This may have implications for improving powered prosthetic control. We developed the ISC3d toolbox that is freely available online, which may be used to compute kinematic and kinetic ISC in 3D. This provides a means to further study the role of coordination in gait and may help address fundamental questions of the neural control of human movement.

</details>


### [503] [Online Fine-Tuning of Pretrained Controllers for Autonomous Driving via Real-Time Recurrent RL](https://arxiv.org/abs/2602.02236)
*Julian Lemmel,Felix Resch,Mónika Farsang,Ramin Hasani,Daniela Rus,Radu Grosu*

Main category: cs.RO

TL;DR: RTRRL enables online adaptation of pretrained policies for autonomous driving tasks, improving performance when facing environmental changes.


<details>
  <summary>Details</summary>
Motivation: Pretrained policies degrade in real-world applications due to environmental changes (system dynamics, sensor drift, task objectives), limiting practical applicability of learning-based control systems.

Method: Use Real-Time Recurrent Reinforcement Learning (RTRRL) for online adaptation of pretrained policies, combined with Liquid-Resistance Liquid-Capacitance RNN (biologically inspired recurrent network).

Result: Demonstrated effectiveness in simulated CarRacing environment and real-world line-following task with RoboRacer car equipped with event camera.

Conclusion: RTRRL enables effective fine-tuning of pretrained policies for autonomous agents, synergizing with biologically inspired recurrent networks to handle real-world environmental changes.

Abstract: Deploying pretrained policies in real-world applications presents substantial challenges that fundamentally limit the practical applicability of learning-based control systems. When autonomous systems encounter environmental changes in system dynamics, sensor drift, or task objectives, fixed policies rapidly degrade in performance. We show that employing Real-Time Recurrent Reinforcement Learning (RTRRL), a biologically plausible algorithm for online adaptation, can effectively fine-tune a pretrained policy to improve autonomous agents' performance on driving tasks. We further show that RTRRL synergizes with a recent biologically inspired recurrent network model, the Liquid-Resistance Liquid-Capacitance RNN. We demonstrate the effectiveness of this closed-loop approach in a simulated CarRacing environment and in a real-world line-following task with a RoboRacer car equipped with an event camera.

</details>


### [504] [Bridging the Sim-to-Real Gap with multipanda ros2: A Real-Time ROS2 Framework for Multimanual Systems](https://arxiv.org/abs/2602.02269)
*Jon Škerlj,Seongjin Bien,Abdeldjallil Naceri,Sami Haddadin*

Main category: cs.RO

TL;DR: multipanda_ros2 is an open-source ROS2 architecture for multi-robot control of Franka robots, featuring 1kHz real-time control, fast controller switching, MuJoCo simulation integration, and inertial parameter identification to reduce sim2real gap.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for a robust, reproducible platform for advanced robotics research, particularly for multi-robot control of Franka robots. It focuses on overcoming challenges in real-time torque control, sustaining high-frequency control rates required by safety standards, and bridging the simulation-to-reality gap for contact-rich tasks.

Method: The authors developed multipanda_ros2, a ROS2 architecture leveraging ros2_control for multi-robot control. Key innovations include: 1) maintaining 1kHz control frequency for real-time performance, 2) a controllet-feature design pattern enabling controller-switching delays ≤2ms, 3) integration of high-fidelity MuJoCo simulation with quantitative metrics for kinematic accuracy and dynamic consistency, and 4) real-world inertial parameter identification to improve force/torque accuracy.

Result: The framework successfully provides native ROS2 interfaces for controlling multiple robots from a single process, achieves the critical 1kHz control frequency, enables fast controller switching for reproducible benchmarking, and demonstrates that inertial parameter identification significantly improves force and torque accuracy in sim2real applications.

Conclusion: multipanda_ros2 provides a robust, reproducible platform for advanced robotics research, extending soft robotics approaches to rigid dual-arm, contact-rich tasks. The work shows promising methods to reduce the sim2real gap and offers a comprehensive solution for multi-robot control with real-time performance and simulation fidelity.

Abstract: We present $multipanda\_ros2$, a novel open-source ROS2 architecture for multi-robot control of Franka Robotics robots. Leveraging ros2 control, this framework provides native ROS2 interfaces for controlling any number of robots from a single process. Our core contributions address key challenges in real-time torque control, including interaction control and robot-environment modeling. A central focus of this work is sustaining a 1kHz control frequency, a necessity for real-time control and a minimum frequency required by safety standards. Moreover, we introduce a controllet-feature design pattern that enables controller-switching delays of $\le 2$ ms, facilitating reproducible benchmarking and complex multi-robot interaction scenarios. To bridge the simulation-to-reality (sim2real) gap, we integrate a high-fidelity MuJoCo simulation with quantitative metrics for both kinematic accuracy and dynamic consistency (torques, forces, and control errors). Furthermore, we demonstrate that real-world inertial parameter identification can significantly improve force and torque accuracy, providing a methodology for iterative physics refinement. Our work extends approaches from soft robotics to rigid dual-arm, contact-rich tasks, showcasing a promising method to reduce the sim2real gap and providing a robust, reproducible platform for advanced robotics research.

</details>


### [505] [TTT-Parkour: Rapid Test-Time Training for Perceptive Robot Parkour](https://arxiv.org/abs/2602.02331)
*Shaoting Zhu,Baijun Ye,Jiaxuan Wang,Jiakang Chen,Ziwen Zhuang,Linzhan Mou,Runhan Huang,Hang Zhao*

Main category: cs.RO

TL;DR: Real-to-sim-to-real framework enables humanoid robots to master complex parkour on unseen terrains through rapid test-time training (TTT) in under 10 minutes.


<details>
  <summary>Details</summary>
Motivation: General locomotion policies struggle with arbitrary and highly challenging terrains, limiting humanoid robots' ability to perform dynamic parkour on complex, unseen environments.

Method: Two-stage end-to-end learning: 1) Pre-train policy on diverse procedurally generated terrains, 2) Rapid fine-tuning on high-fidelity meshes reconstructed from real-world RGB-D captures using efficient reconstruction pipeline.

Result: TTT-Parkour enables humanoids to master complex obstacles (wedges, stakes, boxes, trapezoids, narrow beams) with robust zero-shot sim-to-real transfer; entire pipeline takes <10 minutes per terrain.

Conclusion: Test-time training significantly enhances robot capability on difficult geometries, demonstrating practical real-to-sim-to-real framework for dynamic parkour on complex unseen terrains.

Abstract: Achieving highly dynamic humanoid parkour on unseen, complex terrains remains a challenge in robotics. Although general locomotion policies demonstrate capabilities across broad terrain distributions, they often struggle with arbitrary and highly challenging environments. To overcome this limitation, we propose a real-to-sim-to-real framework that leverages rapid test-time training (TTT) on novel terrains, significantly enhancing the robot's capability to traverse extremely difficult geometries. We adopt a two-stage end-to-end learning paradigm: a policy is first pre-trained on diverse procedurally generated terrains, followed by rapid fine-tuning on high-fidelity meshes reconstructed from real-world captures. Specifically, we develop a feed-forward, efficient, and high-fidelity geometry reconstruction pipeline using RGB-D inputs, ensuring both speed and quality during test-time training. We demonstrate that TTT-Parkour empowers humanoid robots to master complex obstacles, including wedges, stakes, boxes, trapezoids, and narrow beams. The whole pipeline of capturing, reconstructing, and test-time training requires less than 10 minutes on most tested terrains. Extensive experiments show that the policy after test-time training exhibits robust zero-shot sim-to-real transfer capability.

</details>


### [506] [Mapping-Guided Task Discovery and Allocation for Robotic Inspection of Underwater Structures](https://arxiv.org/abs/2602.02389)
*Marina Ruediger,Ashis G. Banerjee*

Main category: cs.RO

TL;DR: Automated task generation for underwater multi-robot inspections using SLAM data without prior geometric knowledge, optimized through keypoint scoring and distance pruning.


<details>
  <summary>Details</summary>
Motivation: To enable autonomous underwater inspections without requiring prior knowledge of existing geometry, allowing robots to adapt to unexpected structures and focus on areas likely to have defects.

Method: Generate inspection tasks from SLAM meshes, optimize using expected keypoint scores and distance-based pruning, validate through in-water tests, and compare with simulated Voronoi partitions and boustrophedon patterns.

Result: The algorithm effectively generates inspection tasks, with in-water tests demonstrating its effectiveness and helping determine appropriate parameters. The method outperforms traditional coverage patterns by maintaining coverage while focusing on defect-prone areas.

Conclusion: The presented task discovery method provides adaptability to unexpected geometry and maintains coverage while focusing on areas more likely to present defects or damage, offering advantages over conventional inspection patterns.

Abstract: Task generation for underwater multi-robot inspections without prior knowledge of existing geometry can be achieved and optimized through examination of simultaneous localization and mapping (SLAM) data. By considering hardware parameters and environmental conditions, a set of tasks is generated from SLAM meshes and optimized through expected keypoint scores and distance-based pruning. In-water tests are used to demonstrate the effectiveness of the algorithm and determine the appropriate parameters. These results are compared to simulated Voronoi partitions and boustrophedon patterns for inspection coverage on a model of the test environment. The key benefits of the presented task discovery method include adaptability to unexpected geometry and distributions that maintain coverage while focusing on areas more likely to present defects or damage.

</details>


### [507] [PRISM: Performer RS-IMLE for Single-pass Multisensory Imitation Learning](https://arxiv.org/abs/2602.02396)
*Amisha Bhaskar,Pratap Tokekar,Stefano Di Cairano,Alexander Schperberg*

Main category: cs.RO

TL;DR: PRISM is a real-time robotic imitation learning policy using batch-global rejection-sampling IMLE with multisensory encoding and linear-attention generation, outperforming diffusion policies by 10-25% in success rates while maintaining 30-50 Hz control.


<details>
  <summary>Details</summary>
Motivation: Robotic imitation learning needs models that handle multimodal action distributions, real-time control rates, and multiple sensing modalities simultaneously. Current generative approaches (diffusion, flow matching, IMLE) only satisfy subsets of these requirements.

Method: PRISM uses batch-global rejection-sampling variant of IMLE with a temporal multisensory encoder (RGB, depth, tactile, audio, proprioception) coupled with a linear-attention generator using Performer architecture.

Result: Outperforms SOTA diffusion policies by 10-25% in success rate on real hardware (Unitree Go2 with D1 arm, UR5 manipulator) while maintaining 30-50 Hz control. On CALVIN (10% data), improves success rates by ~25% over diffusion and ~20% over flow matching, while reducing trajectory jerk by 20x-50x.

Conclusion: PRISM is a fast, accurate, multisensory imitation policy that retains multimodal action coverage without iterative sampling latency, positioning it as superior to existing generative approaches for real-time robotic control.

Abstract: Robotic imitation learning typically requires models that capture multimodal action distributions while operating at real-time control rates and accommodating multiple sensing modalities. Although recent generative approaches such as diffusion models, flow matching, and Implicit Maximum Likelihood Estimation (IMLE) have achieved promising results, they often satisfy only a subset of these requirements. To address this, we introduce PRISM, a single-pass policy based on a batch-global rejection-sampling variant of IMLE. PRISM couples a temporal multisensory encoder (integrating RGB, depth, tactile, audio, and proprioception) with a linear-attention generator using a Performer architecture. We demonstrate the efficacy of PRISM on a diverse real-world hardware suite, including loco-manipulation using a Unitree Go2 with a 7-DoF arm D1 and tabletop manipulation with a UR5 manipulator. Across challenging physical tasks such as pre-manipulation parking, high-precision insertion, and multi-object pick-and-place, PRISM outperforms state-of-the-art diffusion policies by 10-25% in success rate while maintaining high-frequency (30-50 Hz) closed-loop control. We further validate our approach on large-scale simulation benchmarks, including CALVIN, MetaWorld, and Robomimic. In CALVIN (10% data split), PRISM improves success rates by approximately 25% over diffusion and approximately 20% over flow matching, while simultaneously reducing trajectory jerk by 20x-50x. These results position PRISM as a fast, accurate, and multisensory imitation policy that retains multimodal action coverage without the latency of iterative sampling.

</details>


### [508] [SoMA: A Real-to-Sim Neural Simulator for Robotic Soft-body Manipulation](https://arxiv.org/abs/2602.02402)
*Mu Huang,Hui Wang,Kerui Ren,Linning Xu,Yunsong Zhou,Mulin Yu,Bo Dai,Jiangmiao Pang*

Main category: cs.RO

TL;DR: SoMA is a 3D Gaussian Splat simulator for soft-body manipulation that couples deformable dynamics, environmental forces, and robot actions in a unified latent neural space for end-to-end real-to-sim simulation.


<details>
  <summary>Details</summary>
Motivation: Existing simulators for deformable objects rely on predefined physics or data-driven dynamics without robot-conditioned control, which limits accuracy, stability, and generalization for real-to-sim robot manipulation.

Method: SoMA uses 3D Gaussian Splatting to model deformable dynamics, environmental forces, and robot joint actions in a unified latent neural space, enabling end-to-end real-to-sim simulation without predefined physical models.

Result: SoMA improves resimulation accuracy and generalization on real-world robot manipulation by 20%, enabling stable simulation of complex tasks such as long-horizon cloth folding.

Conclusion: SoMA enables controllable, stable long-horizon manipulation and generalization beyond observed trajectories, addressing fundamental challenges in simulating deformable objects under rich interactions for robot manipulation.

Abstract: Simulating deformable objects under rich interactions remains a fundamental challenge for real-to-sim robot manipulation, with dynamics jointly driven by environmental effects and robot actions. Existing simulators rely on predefined physics or data-driven dynamics without robot-conditioned control, limiting accuracy, stability, and generalization. This paper presents SoMA, a 3D Gaussian Splat simulator for soft-body manipulation. SoMA couples deformable dynamics, environmental forces, and robot joint actions in a unified latent neural space for end-to-end real-to-sim simulation. Modeling interactions over learned Gaussian splats enables controllable, stable long-horizon manipulation and generalization beyond observed trajectories without predefined physical models. SoMA improves resimulation accuracy and generalization on real-world robot manipulation by 20%, enabling stable simulation of complex tasks such as long-horizon cloth folding.

</details>


### [509] [Multi-Agent Monte Carlo Tree Search for Makespan-Efficient Object Rearrangement in Cluttered Spaces](https://arxiv.org/abs/2602.02411)
*Hanwen Ren,Junyong Kim,Aathman Tharmasanthiran,Ahmed H. Qureshi*

Main category: cs.RO

TL;DR: CAM-MCTS: A centralized, asynchronous multi-agent MCTS framework for efficient object rearrangement in cluttered environments that reduces makespan by minimizing idle time and synchronization delays.


<details>
  <summary>Details</summary>
Motivation: Real-world object rearrangement tasks are often non-monotone (objects block each other requiring temporary relocation), and effective multi-agent collaboration can substantially reduce completion time, but prior work focuses mainly on monotone instances.

Method: Centralized, Asynchronous, Multi-agent Monte Carlo Tree Search (CAM-MCTS) combines centralized task assignment (agents aware of each other's intended actions for global optimization) with asynchronous execution (agents take new tasks at appropriate times without waiting), guided by one-step look-ahead cost estimates.

Result: CAM-MCTS demonstrates consistent reductions in makespan compared to strong baselines across diverse monotone and non-monotone tasks in cluttered environments, and shows effectiveness and robustness in real-world multi-agent system validation.

Conclusion: CAM-MCTS provides an effective framework for makespan-efficient object rearrangement planning in challenging environments by minimizing idle time, preventing unnecessary synchronization delays, and enhancing overall system efficiency through centralized optimization and asynchronous execution.

Abstract: Object rearrangement planning in complex, cluttered environments is a common challenge in warehouses, households, and rescue sites. Prior studies largely address monotone instances, whereas real-world tasks are often non-monotone-objects block one another and must be temporarily relocated to intermediate positions before reaching their final goals. In such settings, effective multi-agent collaboration can substantially reduce the time required to complete tasks. This paper introduces Centralized, Asynchronous, Multi-agent Monte Carlo Tree Search (CAM-MCTS), a novel framework for general-purpose makespan-efficient object rearrangement planning in challenging environments. CAM-MCTS combines centralized task assignment-where agents remain aware of each other's intended actions to facilitate globally optimized planning-with an asynchronous task execution strategy that enables agents to take on new tasks at appropriate time steps, rather than waiting for others, guided by a one-step look-ahead cost estimate. This design minimizes idle time, prevents unnecessary synchronization delays, and enhances overall system efficiency. We evaluate CAM-MCTS across a diverse set of monotone and non-monotone tasks in cluttered environments, demonstrating consistent reductions in makespan compared to strong baselines. Finally, we validate our approach on a real-world multi-agent system under different configurations, further confirming its effectiveness and robustness.

</details>


### [510] [3D Foundation Model-Based Loop Closing for Decentralized Collaborative SLAM](https://arxiv.org/abs/2602.02430)
*Pierre-Yves Lajoie,Benjamin Ramtoula,Daniele De Martini,Giovanni Beltrame*

Main category: cs.RO

TL;DR: A decentralized C-SLAM approach using 3D foundation models for robust loop closing across robots with large viewpoint differences, improving accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Decentralized C-SLAM struggles with identifying map overlaps due to significant viewpoint variations among robots. Recent 3D foundation models can register images despite large viewpoint differences, offering a solution.

Method: Integrates 3D foundation models into existing SLAM pipelines to estimate relative poses from monocular image pairs, with robust outlier mitigation techniques and specialized pose graph optimization to resolve scale ambiguities.

Result: Demonstrates improvements in localization and mapping accuracy compared to state-of-the-art approaches, with significant gains in computational and memory efficiency.

Conclusion: The approach shows potential for deployment in large-scale multi-robot scenarios by leveraging 3D foundation models for robust loop closing in decentralized C-SLAM systems.

Abstract: Decentralized Collaborative Simultaneous Localization And Mapping (C-SLAM) techniques often struggle to identify map overlaps due to significant viewpoint variations among robots. Motivated by recent advancements in 3D foundation models, which can register images despite large viewpoint differences, we propose a robust loop closing approach that leverages these models to establish inter-robot measurements. In contrast to resource-intensive methods requiring full 3D reconstruction within a centralized map, our approach integrates foundation models into existing SLAM pipelines, yielding scalable and robust multi-robot mapping. Our contributions include: (1) integrating 3D foundation models to reliably estimate relative poses from monocular image pairs within decentralized C-SLAM; (2) introducing robust outlier mitigation techniques critical to the use of these relative poses; and (3) developing specialized pose graph optimization formulations that efficiently resolve scale ambiguities. We evaluate our method against state-of-the-art approaches, demonstrating improvements in localization and mapping accuracy, alongside significant gains in computational and memory efficiency. These results highlight the potential of our approach for deployment in large-scale multi-robot scenarios.

</details>


### [511] [World-Gymnast: Training Robots with Reinforcement Learning in a World Model](https://arxiv.org/abs/2602.02454)
*Ansh Kumar Sharma,Yixiang Sun,Ninghao Lu,Yunzhe Zhang,Jiarao Liu,Sherry Yang*

Main category: cs.RO

TL;DR: World-Gymnast uses RL in a video world model to train robot policies, outperforming supervised learning and software simulation by up to 18x and 2x respectively.


<details>
  <summary>Details</summary>
Motivation: Physical robot learning is expensive, supervised finetuning is limited by expert data, and RL in software simulators suffers from sim-to-real gap. World models learned from real-world video-action data offer a promising alternative.

Method: Proposes World-Gymnast: RL finetuning of a vision-language-action policy by rolling out in an action-conditioned video world model, rewarding rollouts with a vision-language model.

Result: On Bridge robot setup, outperforms SFT by up to 18x and software simulator by up to 2x. Shows capabilities like training on diverse language instructions, novel scenes, test-time training, and online iterative improvement.

Conclusion: Learning world models and training robot policies in the cloud could bridge the gap between demonstration-only robots and household-capable robots.

Abstract: Robot learning from interacting with the physical world is fundamentally bottlenecked by the cost of physical interaction. The two alternatives, supervised finetuning (SFT) from expert demonstrations and reinforcement learning (RL) in a software-based simulator, are limited by the amount of expert data available and the sim-to-real gap for manipulation. With the recent emergence of world models learned from real-world video-action data, we ask the question of whether training a policy in a world model can be more effective than supervised learning or software simulation in achieving better real-robot performance. We propose World-Gymnast, which performs RL finetuning of a vision-language-action (VLA) policy by rolling out the policy in an action-conditioned video world model and rewarding the rollouts with a vision-language model (VLM). On the Bridge robot setup, World-Gymnast outperforms SFT by as much as 18x and outperforms software simulator by as much as 2x. More importantly, World-Gymnast demonstrates intriguing capabilities of RL with a world model, including training on diverse language instructions and novel scenes from the world model, test-time training in a novel scene, and online iterative world model and policy improvement. Our results suggest learning a world model and training robot policies in the cloud could be the key to bridging the gap between robots that work in demonstrations and robots that can work in anyone's household.

</details>


### [512] [Relationship-Aware Hierarchical 3D Scene Graph for Task Reasoning](https://arxiv.org/abs/2602.02456)
*Albert Gassol Puigjaner,Angelos Zacharia,Kostas Alexis*

Main category: cs.RO

TL;DR: Enhanced hierarchical 3D scene graph with open-vocabulary features and object-relational reasoning for intelligent robotic task execution


<details>
  <summary>Details</summary>
Motivation: Traditional SLAM methods lack higher-level abstraction and relational reasoning needed for intelligent autonomous agents to navigate and interact with environments

Method: Enhanced hierarchical 3D scene graph integrating open-vocabulary features across multiple abstraction levels, using Vision Language Model for semantic relationships, and task reasoning module combining LLM and VLM for scene interpretation

Result: Validated on quadruped robot in multiple environments and tasks, demonstrating ability to reason about tasks and interact intelligently with environment

Conclusion: Proposed approach enables more intelligent robotic task reasoning and environment interaction through enhanced 3D scene graph representation with semantic and relational understanding

Abstract: Representing and understanding 3D environments in a structured manner is crucial for autonomous agents to navigate and reason about their surroundings. While traditional Simultaneous Localization and Mapping (SLAM) methods generate metric reconstructions and can be extended to metric-semantic mapping, they lack a higher level of abstraction and relational reasoning. To address this gap, 3D scene graphs have emerged as a powerful representation for capturing hierarchical structures and object relationships. In this work, we propose an enhanced hierarchical 3D scene graph that integrates open-vocabulary features across multiple abstraction levels and supports object-relational reasoning. Our approach leverages a Vision Language Model (VLM) to infer semantic relationships. Notably, we introduce a task reasoning module that combines Large Language Models (LLM) and a VLM to interpret the scene graph's semantic and relational information, enabling agents to reason about tasks and interact with their environment more intelligently. We validate our method by deploying it on a quadruped robot in multiple environments and tasks, highlighting its ability to reason about them.

</details>


### [513] [TIC-VLA: A Think-in-Control Vision-Language-Action Model for Robot Navigation in Dynamic Environments](https://arxiv.org/abs/2602.02459)
*Zhiyu Huang,Yun Zhang,Johnson Liu,Rui Song,Chen Tang,Jiaqi Ma*

Main category: cs.RO

TL;DR: TIC-VLA is a latency-aware vision-language-action framework that explicitly models delayed semantic reasoning during real-time robot control, outperforming prior VLA models under multi-second reasoning latency.


<details>
  <summary>Details</summary>
Motivation: Robots need to follow language instructions in dynamic environments while maintaining real-time reactive control, but current VLA models assume temporally aligned reasoning and control despite semantic inference being inherently delayed relative to real-time action.

Method: Introduces TIC-VLA with a delayed semantic-control interface that conditions action generation on delayed vision-language semantic states and explicit latency metadata, plus a latency-consistent training pipeline that injects reasoning delays during imitation and reinforcement learning.

Result: Extensive experiments in simulation (using new DynaNav benchmark) and on real robots show TIC-VLA consistently outperforms prior VLA models while maintaining robust real-time control under multi-second reasoning latency.

Conclusion: TIC-VLA successfully addresses the asynchronous reasoning challenge in VLA models, enabling robots to maintain real-time control despite semantic inference delays, with applications in dynamic, human-centric environments.

Abstract: Robots in dynamic, human-centric environments must follow language instructions while maintaining real-time reactive control. Vision-language-action (VLA) models offer a promising framework, but they assume temporally aligned reasoning and control, despite semantic inference being inherently delayed relative to real-time action. We introduce Think-in-Control (TIC)-VLA, a latency-aware framework that explicitly models delayed semantic reasoning during action generation. TIC-VLA defines a delayed semantic-control interface that conditions action generation on delayed vision-language semantic states and explicit latency metadata, in addition to current observations, enabling policies to compensate for asynchronous reasoning. We further propose a latency-consistent training pipeline that injects reasoning inference delays during imitation learning and online reinforcement learning, aligning training with asynchronous deployment. To support realistic evaluation, we present DynaNav, a physics-accurate, photo-realistic simulation suite for language-guided navigation in dynamic environments. Extensive experiments in simulation and on a real robot show that TIC-VLA consistently outperforms prior VLA models while maintaining robust real-time control under multi-second reasoning latency. Project website: https://ucla-mobility.github.io/TIC-VLA/

</details>


### [514] [HumanX: Toward Agile and Generalizable Humanoid Interaction Skills from Human Videos](https://arxiv.org/abs/2602.02473)
*Yinhuai Wang,Qihan Zhao,Yuen Fui Lau,Runyi Yu,Hok Wai Tsui,Qifeng Chen,Jingbo Wang,Jiangmiao Pang,Ping Tan*

Main category: cs.RO

TL;DR: HumanX is a framework that converts human videos into generalizable interaction skills for humanoid robots without task-specific rewards, achieving 8x better generalization than prior methods.


<details>
  <summary>Details</summary>
Motivation: Current humanoid robot approaches are limited by scarce realistic interaction data and the need for meticulous task-specific reward engineering, which hinders scalability for agile and adaptive interactive tasks.

Method: HumanX integrates two co-designed components: XGen (data generation pipeline that synthesizes diverse, physically plausible robot interaction data from videos with scalable augmentation) and XMimic (unified imitation learning framework for learning generalizable interaction skills).

Result: The framework successfully acquired 10 different skills across five domains (basketball, football, badminton, cargo pickup, reactive fighting) and transferred them zero-shot to a physical Unitree G1 humanoid, achieving over 8 times higher generalization success than prior methods.

Conclusion: HumanX demonstrates a scalable and task-agnostic pathway for learning versatile, real-world robot interactive skills, enabling complex maneuvers and sustained human-robot interactions from single video demonstrations without external perception.

Abstract: Enabling humanoid robots to perform agile and adaptive interactive tasks has long been a core challenge in robotics. Current approaches are bottlenecked by either the scarcity of realistic interaction data or the need for meticulous, task-specific reward engineering, which limits their scalability. To narrow this gap, we present HumanX, a full-stack framework that compiles human video into generalizable, real-world interaction skills for humanoids, without task-specific rewards. HumanX integrates two co-designed components: XGen, a data generation pipeline that synthesizes diverse and physically plausible robot interaction data from video while supporting scalable data augmentation; and XMimic, a unified imitation learning framework that learns generalizable interaction skills. Evaluated across five distinct domains--basketball, football, badminton, cargo pickup, and reactive fighting--HumanX successfully acquires 10 different skills and transfers them zero-shot to a physical Unitree G1 humanoid. The learned capabilities include complex maneuvers such as pump-fake turnaround fadeaway jumpshots without any external perception, as well as interactive tasks like sustained human-robot passing sequences over 10 consecutive cycles--learned from a single video demonstration. Our experiments show that HumanX achieves over 8 times higher generalization success than prior methods, demonstrating a scalable and task-agnostic pathway for learning versatile, real-world robot interactive skills.

</details>


### [515] [Flow Policy Gradients for Robot Control](https://arxiv.org/abs/2602.02481)
*Brent Yi,Hongsuk Choi,Himanshu Gaurav Singh,Xiaoyu Huang,Takara E. Truong,Carmelo Sferrazza,Yi Ma,Rocky Duan,Pieter Abbeel,Guanya Shi,Karen Liu,Angjoo Kanazawa*

Main category: cs.RO

TL;DR: Flow matching policy gradients enable training expressive robot control policies without likelihood computation, showing effectiveness in locomotion, manipulation, and sim-to-real transfer.


<details>
  <summary>Details</summary>
Motivation: Traditional likelihood-based policy gradient methods are limited to simple distributions like Gaussians, constraining policy expressiveness. The authors aim to enable more expressive policies for challenging robot control tasks.

Method: Flow matching policy gradients framework that bypasses likelihood computation, with an improved objective designed for robot control settings. The approach enables training expressive policies without differentiable action likelihood constraints.

Result: Successful application to legged locomotion, humanoid motion tracking, and manipulation tasks, with robust sim-to-real transfer on two humanoid robots. Policies exploit flow representation for exploration and show improved fine-tuning robustness over baselines.

Conclusion: Flow matching policy gradients provide an effective alternative to likelihood-based methods, enabling more expressive policies for complex robot control tasks with better exploration and fine-tuning capabilities.

Abstract: Likelihood-based policy gradient methods are the dominant approach for training robot control policies from rewards. These methods rely on differentiable action likelihoods, which constrain policy outputs to simple distributions like Gaussians. In this work, we show how flow matching policy gradients -- a recent framework that bypasses likelihood computation -- can be made effective for training and fine-tuning more expressive policies in challenging robot control settings. We introduce an improved objective that enables success in legged locomotion, humanoid motion tracking, and manipulation tasks, as well as robust sim-to-real transfer on two humanoid robots. We then present ablations and analysis on training dynamics. Results show how policies can exploit the flow representation for exploration when training from scratch, as well as improved fine-tuning robustness over baselines.

</details>
