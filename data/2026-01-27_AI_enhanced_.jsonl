{"id": "2601.16985", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16985", "abs": "https://arxiv.org/abs/2601.16985", "authors": ["Pierrick Lorang"], "title": "Breaking Task Impasses Quickly: Adaptive Neuro-Symbolic Learning for Open-World Robotics", "comment": "IEEE ICRA 2025 Doctoral Consortium", "summary": "Adapting to unforeseen novelties in open-world environments remains a major challenge for autonomous systems. While hybrid planning and reinforcement learning (RL) approaches show promise, they often suffer from sample inefficiency, slow adaptation, and catastrophic forgetting. We present a neuro-symbolic framework integrating hierarchical abstractions, task and motion planning (TAMP), and reinforcement learning to enable rapid adaptation in robotics. Our architecture combines symbolic goal-oriented learning and world model-based exploration to facilitate rapid adaptation to environmental changes. Validated in robotic manipulation and autonomous driving, our approach achieves faster convergence, improved sample efficiency, and superior robustness over state-of-the-art hybrid methods, demonstrating its potential for real-world deployment.", "AI": {"tldr": "Neuro-symbolic framework combining hierarchical abstractions, TAMP, and RL enables rapid adaptation to novelties in robotics with improved sample efficiency and robustness.", "motivation": "Autonomous systems struggle with unforeseen novelties in open-world environments. Existing hybrid planning and RL approaches suffer from sample inefficiency, slow adaptation, and catastrophic forgetting.", "method": "Neuro-symbolic framework integrating hierarchical abstractions, task and motion planning (TAMP), and reinforcement learning. Combines symbolic goal-oriented learning with world model-based exploration for rapid adaptation.", "result": "Validated in robotic manipulation and autonomous driving. Achieves faster convergence, improved sample efficiency, and superior robustness compared to state-of-the-art hybrid methods.", "conclusion": "The framework demonstrates potential for real-world deployment by enabling rapid adaptation to environmental changes in robotics applications."}}
{"id": "2601.17219", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.17219", "abs": "https://arxiv.org/abs/2601.17219", "authors": ["David Wireko Atibila", "Vineet R. Kamat", "Carol C. Menassa"], "title": "Advancing Improvisation in Human-Robot Construction Collaboration: Taxonomy and Research Roadmap", "comment": "73 pages, 8 figures", "summary": "The construction industry faces productivity stagnation, skilled labor shortages, and safety concerns. While robotic automation offers solutions, construction robots struggle to adapt to unstructured, dynamic sites. Central to this is improvisation, adapting to unexpected situations through creative problem-solving, which remains predominantly human. In construction's unpredictable environments, collaborative human-robot improvisation is essential for workflow continuity. This research develops a six-level taxonomy classifying human-robot collaboration (HRC) based on improvisation capabilities. Through systematic review of 214 articles (2010-2025), we categorize construction robotics across: Manual Work (Level 0), Human-Controlled Execution (Level 1), Adaptive Manipulation (Level 2), Imitation Learning (Level 3), Human-in-Loop BIM Workflow (Level 4), Cloud-Based Knowledge Integration (Level 5), and True Collaborative Improvisation (Level 6). Analysis reveals current research concentrates at lower levels, with critical gaps in experiential learning and limited progression toward collaborative improvisation. A five-dimensional radar framework illustrates progressive evolution of Planning, Cognitive Role, Physical Execution, Learning Capability, and Improvisation, demonstrating how complementary human-robot capabilities create team performance exceeding individual contributions. The research identifies three fundamental barriers: technical limitations in grounding and dialogic reasoning, conceptual gaps between human improvisation and robotics research, and methodological challenges. We recommend future research emphasizing improved human-robot communication via Augmented/Virtual Reality interfaces, large language model integration, and cloud-based knowledge systems to advance toward true collaborative improvisation.", "AI": {"tldr": "Develops a 6-level taxonomy for human-robot collaboration in construction based on improvisation capabilities, identifying current gaps and recommending future directions for true collaborative improvisation.", "motivation": "Construction industry faces productivity stagnation, skilled labor shortages, and safety issues. While robotic automation offers solutions, current construction robots struggle to adapt to unstructured, dynamic environments where human improvisation remains essential for workflow continuity.", "method": "Systematic review of 214 articles (2010-2025) to develop a six-level taxonomy classifying human-robot collaboration based on improvisation capabilities. Created a five-dimensional radar framework analyzing Planning, Cognitive Role, Physical Execution, Learning Capability, and Improvisation.", "result": "Current research concentrates at lower collaboration levels (Manual Work, Human-Controlled Execution), with critical gaps in experiential learning and limited progression toward collaborative improvisation. Identified three fundamental barriers: technical limitations, conceptual gaps, and methodological challenges.", "conclusion": "Future research should emphasize improved human-robot communication via AR/VR interfaces, large language model integration, and cloud-based knowledge systems to advance toward true collaborative improvisation where complementary human-robot capabilities create team performance exceeding individual contributions."}}
{"id": "2601.17227", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.17227", "abs": "https://arxiv.org/abs/2601.17227", "authors": ["Avraiem Iskandar", "Shamak Dutta", "Kevin Murrant", "Yash Vardhan Pant", "Stephen L. Smith"], "title": "Hierarchical Informative Path Planning via Graph Guidance and Trajectory Optimization", "comment": null, "summary": "We study informative path planning (IPP) with travel budgets in cluttered environments, where an agent collects measurements of a latent field modeled as a Gaussian process (GP) to reduce uncertainty at target locations. Graph-based solvers provide global guarantees but assume pre-selected measurement locations, while continuous trajectory optimization supports path-based sensing but is computationally intensive and sensitive to initialization in obstacle-dense settings. We propose a hierarchical framework with three stages: (i) graph-based global planning, (ii) segment-wise budget allocation using geometric and kernel bounds, and (iii) spline-based refinement of each segment with hard constraints and obstacle pruning. By combining global guidance with local refinement, our method achieves lower posterior uncertainty than graph-only and continuous baselines, while running faster than continuous-space solvers (up to 9x faster than gradient-based methods and 20x faster than black-box optimizers) across synthetic cluttered environments and Arctic datasets.", "AI": {"tldr": "Hierarchical framework for informative path planning in cluttered environments that combines graph-based global planning with continuous trajectory optimization to achieve better uncertainty reduction than graph-only methods while being much faster than continuous-only solvers.", "motivation": "Existing methods for informative path planning have limitations: graph-based solvers assume pre-selected measurement locations and lack path-based sensing, while continuous trajectory optimization is computationally intensive and sensitive to initialization in obstacle-dense environments.", "method": "Three-stage hierarchical approach: (1) graph-based global planning, (2) segment-wise budget allocation using geometric and kernel bounds, and (3) spline-based refinement of each segment with hard constraints and obstacle pruning.", "result": "Achieves lower posterior uncertainty than graph-only and continuous baselines, with significant speed improvements: up to 9x faster than gradient-based methods and 20x faster than black-box optimizers across synthetic cluttered environments and Arctic datasets.", "conclusion": "The hierarchical framework successfully combines the global guarantees of graph-based methods with the flexibility of continuous optimization, providing an efficient solution for informative path planning in cluttered environments with travel budgets."}}
{"id": "2601.17231", "categories": ["cs.RO", "cs.AR"], "pdf": "https://arxiv.org/pdf/2601.17231", "abs": "https://arxiv.org/abs/2601.17231", "authors": ["Tanmay Desai", "Brian Plancher", "R. Iris Bahar"], "title": "Real-Time, Energy-Efficient, Sampling-Based Optimal Control via FPGA Acceleration", "comment": "8 pages, 5 figures", "summary": "Autonomous mobile robots (AMRs), used for search-and-rescue and remote exploration, require fast and robust planning and control schemes. Sampling-based approaches for Model Predictive Control, especially approaches based on the Model Predictive Path Integral Control (MPPI) algorithm, have recently proven both to be highly effective for such applications and to map naturally to GPUs for hardware acceleration. However, both GPU and CPU implementations of such algorithms can struggle to meet tight energy and latency budgets on battery-constrained AMR platforms that leverage embedded compute. To address this issue, we present an FPGA-optimized MPPI design that exposes fine-grained parallelism and eliminates synchronization bottlenecks via deep pipelining and parallelism across algorithmic stages. This results in an average 3.1x to 7.5x speedup over optimized implementations on an embedded GPU and CPU, respectively, while simultaneously achieving a 2.5x to 5.4x reduction in energy usage. These results demonstrate that FPGA architectures are a promising direction for energy-efficient and high-performance edge robotics.", "AI": {"tldr": "FPGA-optimized MPPI design for autonomous mobile robots achieves 3.1-7.5x speedup and 2.5-5.4x energy reduction over GPU/CPU implementations.", "motivation": "Autonomous mobile robots need fast, robust planning/control for search-and-rescue/exploration, but existing GPU/CPU implementations struggle with energy/latency constraints on battery-powered embedded platforms.", "method": "Developed FPGA-optimized Model Predictive Path Integral Control (MPPI) design with fine-grained parallelism, deep pipelining, and parallelism across algorithmic stages to eliminate synchronization bottlenecks.", "result": "Achieved average 3.1x to 7.5x speedup over optimized embedded GPU/CPU implementations, with simultaneous 2.5x to 5.4x reduction in energy usage.", "conclusion": "FPGA architectures are promising for energy-efficient, high-performance edge robotics, offering significant advantages over traditional GPU/CPU approaches for autonomous mobile robot control."}}
{"id": "2601.17027", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17027", "abs": "https://arxiv.org/abs/2601.17027", "authors": ["Honglin Lin", "Chonghan Qin", "Zheng Liu", "Qizhi Pei", "Yu Li", "Zhanping Zhong", "Xin Gao", "Yanfeng Wang", "Conghui He", "Lijun Wu"], "title": "Scientific Image Synthesis: Benchmarking, Methodologies, and Downstream Utility", "comment": null, "summary": "While synthetic data has proven effective for improving scientific reasoning in the text domain, multimodal reasoning remains constrained by the difficulty of synthesizing scientifically rigorous images. Existing Text-to-Image (T2I) models often produce outputs that are visually plausible yet scientifically incorrect, resulting in a persistent visual-logic divergence that limits their value for downstream reasoning. Motivated by recent advances in next-generation T2I models, we conduct a systematic study of scientific image synthesis across generation paradigms, evaluation, and downstream use. We analyze both direct pixel-based generation and programmatic synthesis, and propose ImgCoder, a logic-driven framework that follows an explicit \"understand - plan - code\" workflow to improve structural precision. To rigorously assess scientific correctness, we introduce SciGenBench, which evaluates generated images based on information utility and logical validity. Our evaluation reveals systematic failure modes in pixel-based models and highlights a fundamental expressiveness-precision trade-off. Finally, we show that fine-tuning Large Multimodal Models (LMMs) on rigorously verified synthetic scientific images yields consistent reasoning gains, with potential scaling trends analogous to the text domain, validating high-fidelity scientific synthesis as a viable path to unlocking massive multimodal reasoning capabilities.", "AI": {"tldr": "ImgCoder framework improves scientific image synthesis via logic-driven \"understand-plan-code\" workflow, with SciGenBench for evaluation, showing synthetic scientific images can boost multimodal reasoning when properly verified.", "motivation": "Multimodal reasoning is limited by poor scientific image synthesis - existing T2I models produce visually plausible but scientifically incorrect images, creating visual-logic divergence that hinders downstream reasoning applications.", "method": "Propose ImgCoder framework with explicit \"understand-plan-code\" workflow for logic-driven synthesis; analyze both pixel-based and programmatic generation; introduce SciGenBench for rigorous evaluation of scientific correctness; fine-tune LMMs on verified synthetic images.", "result": "Reveal systematic failure modes in pixel-based models and fundamental expressiveness-precision trade-off; show fine-tuning LMMs on verified synthetic scientific images yields consistent reasoning gains with potential scaling trends similar to text domain.", "conclusion": "High-fidelity scientific image synthesis is a viable path to unlocking massive multimodal reasoning capabilities, with rigorous verification enabling synthetic data to improve multimodal reasoning similar to how it improved text-based reasoning."}}
{"id": "2601.17009", "categories": ["cs.AI", "cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.17009", "abs": "https://arxiv.org/abs/2601.17009", "authors": ["Yanhua Zhao"], "title": "Online parameter estimation for the Crazyflie quadcopter through an EM algorithm", "comment": "20 pages, 37 figures", "summary": "Drones are becoming more and more popular nowadays. They are small in size, low in cost, and reliable in operation. They contain a variety of sensors and can perform a variety of flight tasks, reaching places that are difficult or inaccessible for humans. Earthquakes damage a lot of infrastructure, making it impossible for rescuers to reach some areas. But drones can help. Many amateur and professional photographers like to use drones for aerial photography. Drones play a non-negligible role in agriculture and transportation too. Drones can be used to spray pesticides, and they can also transport supplies. A quadcopter is a four-rotor drone and has been studied in this paper. In this paper, random noise is added to the quadcopter system and its effects on the drone system are studied. An extended Kalman filter has been used to estimate the state based on noisy observations from the sensor. Based on a SDE system, a linear quadratic Gaussian controller has been implemented. The expectation maximization algorithm has been applied for parameter estimation of the quadcopter. The results of offline parameter estimation and online parameter estimation are presented. The results show that the online parameter estimation has a slightly larger range of convergence values than the offline parameter estimation.", "AI": {"tldr": "This paper studies the effects of random noise on quadcopter systems, using extended Kalman filtering for state estimation, LQG control, and expectation maximization for parameter estimation, comparing offline vs online approaches.", "motivation": "Drones are increasingly important for various applications (rescue operations, photography, agriculture, transportation), but their performance can be affected by random noise. Understanding and mitigating noise effects is crucial for reliable drone operation in real-world scenarios.", "method": "The study uses an extended Kalman filter to estimate system states from noisy sensor observations, implements a linear quadratic Gaussian controller based on a stochastic differential equation system, and applies the expectation maximization algorithm for parameter estimation of the quadcopter.", "result": "The paper presents results for both offline and online parameter estimation, showing that online parameter estimation has a slightly larger range of convergence values compared to offline parameter estimation.", "conclusion": "The study demonstrates effective noise handling and parameter estimation techniques for quadcopter systems, with online parameter estimation showing different convergence characteristics than offline approaches, which has implications for real-time drone control systems."}}
{"id": "2601.17249", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.17249", "abs": "https://arxiv.org/abs/2601.17249", "authors": ["Peter Bryan", "Rejin John Varghese", "Dario Farina"], "title": "Quantifying Ergonomics in the Elevate Soft Robotic Suit", "comment": "5 pages, 3 figures. Submitted to IEEE-EMBC 2026", "summary": "Soft robotic suits have the potential to rehabilitate, assist, and augment the human body. The low weight, cost, and minimal form-factor of these devices make them ideal for daily use by both healthy and impaired individuals. However, challenges associated with data-driven, user-specific, and comfort-first design of human-robot interfaces using soft materials limit their widespread translation and adoption. In this work, we present the quantitative evaluation of ergonomics and comfort of the Elevate suit - a cable driven soft robotic suit that assists shoulder elevation. Using a motion-capture system and force sensors, we measured the suit's ergonomics during assisted shoulder elevation up to 70 degrees. Two 4-hour sessions were conducted with one subject, involving transmitting cable tensions of up to 200N with no discomfort reported. We estimated that the pressure applied to the shoulder during assisted movements was within the range seen in a human grasp (approximately 69.1-85.1kPa), and estimated volumetric compression of <3% and <8% across the torso and upper arm, respectively. These results provide early validation of Elevate's ergonomic design in preparation for future studies with patient groups.", "AI": {"tldr": "Evaluated ergonomics and comfort of Elevate soft robotic suit for shoulder assistance using motion capture and force sensors during 4-hour sessions with up to 200N cable tension.", "motivation": "Soft robotic suits have potential for rehabilitation and assistance but face challenges in data-driven, user-specific, comfort-first design that limit widespread adoption.", "method": "Used motion-capture system and force sensors to measure ergonomics during assisted shoulder elevation up to 70 degrees, conducted two 4-hour sessions with one subject transmitting cable tensions up to 200N.", "result": "No discomfort reported during sessions, shoulder pressure estimated within human grasp range (69.1-85.1kPa), volumetric compression <3% on torso and <8% on upper arm.", "conclusion": "Results provide early validation of Elevate's ergonomic design, preparing for future studies with patient groups."}}
{"id": "2601.17031", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17031", "abs": "https://arxiv.org/abs/2601.17031", "authors": ["Yunhao Xu", "Fuquan Zong", "Yexuan Xing", "Chulong Zhang", "Guang Yang", "Shilong Yang", "Xiaokun Liang", "Juan Yu"], "title": "Data-Efficient Meningioma Segmentation via Implicit Spatiotemporal Mixing and Sim2Real Semantic Injection", "comment": null, "summary": "The performance of medical image segmentation is increasingly defined by the efficiency of data utilization rather than merely the volume of raw data. Accurate segmentation, particularly for complex pathologies like meningiomas, demands that models fully exploit the latent information within limited high-quality annotations. To maximize the value of existing datasets, we propose a novel dual-augmentation framework that synergistically integrates spatial manifold expansion and semantic object injection. Specifically, we leverage Implicit Neural Representations (INR) to model continuous velocity fields. Unlike previous methods, we perform linear mixing on the integrated deformation fields, enabling the efficient generation of anatomically plausible variations by interpolating within the deformation space. This approach allows for the extensive exploration of structural diversity from a small set of anchors. Furthermore, we introduce a Sim2Real lesion injection module. This module constructs a high-fidelity simulation domain by transplanting lesion textures into healthy anatomical backgrounds, effectively bridging the gap between synthetic augmentation and real-world pathology. Comprehensive experiments on a hybrid dataset demonstrate that our framework significantly enhances the data efficiency and robustness of state-of-the-art models, including nnU-Net and U-Mamba, offering a potent strategy for high-performance medical image analysis with limited annotation budgets.", "AI": {"tldr": "Novel dual-augmentation framework combining spatial manifold expansion via INR-based deformation mixing and Sim2Real lesion injection to maximize data efficiency for medical image segmentation with limited annotations.", "motivation": "Medical image segmentation performance depends more on efficient data utilization than raw data volume. Complex pathologies like meningiomas require models to fully exploit limited high-quality annotations, necessitating methods to maximize value from existing datasets.", "method": "Proposes dual-augmentation framework: 1) Spatial manifold expansion using Implicit Neural Representations (INR) to model continuous velocity fields and perform linear mixing on integrated deformation fields for anatomically plausible variations; 2) Sim2Real lesion injection module that transplants lesion textures into healthy backgrounds to bridge synthetic-real gap.", "result": "Comprehensive experiments on hybrid dataset show framework significantly enhances data efficiency and robustness of state-of-the-art models (nnU-Net, U-Mamba), offering potent strategy for high-performance medical image analysis with limited annotation budgets.", "conclusion": "The proposed synergistic dual-augmentation framework effectively maximizes data utilization from limited annotations, enabling high-performance medical image segmentation through efficient exploration of structural diversity and realistic pathology simulation."}}
{"id": "2601.17168", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.17168", "abs": "https://arxiv.org/abs/2601.17168", "authors": ["Judy Zhu", "Dhari Gandhi", "Himanshu Joshi", "Ahmad Rezaie Mianroodi", "Sedef Akinli Kocak", "Dhanesh Ramachandran"], "title": "Interpreting Agentic Systems: Beyond Model Explanations to System-Level Accountability", "comment": null, "summary": "Agentic systems have transformed how Large Language Models (LLMs) can be leveraged to create autonomous systems with goal-directed behaviors, consisting of multi-step planning and the ability to interact with different environments. These systems differ fundamentally from traditional machine learning models, both in architecture and deployment, introducing unique AI safety challenges, including goal misalignment, compounding decision errors, and coordination risks among interacting agents, that necessitate embedding interpretability and explainability by design to ensure traceability and accountability across their autonomous behaviors. Current interpretability techniques, developed primarily for static models, show limitations when applied to agentic systems. The temporal dynamics, compounding decisions, and context-dependent behaviors of agentic systems demand new analytical approaches. This paper assesses the suitability and limitations of existing interpretability methods in the context of agentic systems, identifying gaps in their capacity to provide meaningful insight into agent decision-making. We propose future directions for developing interpretability techniques specifically designed for agentic systems, pinpointing where interpretability is required to embed oversight mechanisms across the agent lifecycle from goal formation, through environmental interaction, to outcome evaluation. These advances are essential to ensure the safe and accountable deployment of agentic AI systems.", "AI": {"tldr": "Current interpretability methods for static ML models are inadequate for agentic AI systems, requiring new approaches to address temporal dynamics, compounding decisions, and context-dependent behaviors for safe deployment.", "motivation": "Agentic AI systems introduce unique safety challenges (goal misalignment, compounding errors, coordination risks) that demand interpretability by design, but existing interpretability techniques developed for static models are insufficient for these dynamic, autonomous systems.", "method": "The paper assesses the suitability and limitations of current interpretability methods for agentic systems, identifies gaps in providing meaningful insight into agent decision-making, and proposes future directions for developing specialized interpretability techniques.", "result": "Current interpretability techniques show limitations when applied to agentic systems due to their temporal dynamics, compounding decisions, and context-dependent behaviors, revealing significant gaps in analytical approaches.", "conclusion": "New interpretability techniques specifically designed for agentic systems are essential to embed oversight mechanisms across the agent lifecycle (goal formation, environmental interaction, outcome evaluation) for safe and accountable deployment of agentic AI."}}
{"id": "2601.17251", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.17251", "abs": "https://arxiv.org/abs/2601.17251", "authors": ["Yunuo Chen", "Yafei Hu", "Lingfeng Sun", "Tushar Kusnur", "Laura Herlant", "Chenfanfu Jiang"], "title": "EMPM: Embodied MPM for Modeling and Simulation of Deformable Objects", "comment": null, "summary": "Modeling deformable objects - especially continuum materials - in a way that is physically plausible, generalizable, and data-efficient remains challenging across 3D vision, graphics, and robotic manipulation. Many existing methods oversimplify the rich dynamics of deformable objects or require large training sets, which often limits generalization. We introduce embodied MPM (EMPM), a deformable object modeling and simulation framework built on a differentiable Material Point Method (MPM) simulator that captures the dynamics of challenging materials. From multi-view RGB-D videos, our approach reconstructs geometry and appearance, then uses an MPM physics engine to simulate object behavior by minimizing the mismatch between predicted and observed visual data. We further optimize MPM parameters online using sensory feedback, enabling adaptive, robust, and physics-aware object representations that open new possibilities for robotic manipulation of complex deformables. Experiments show that EMPM outperforms spring-mass baseline models. Project website: https://embodied-mpm.github.io.", "AI": {"tldr": "EMPM is a differentiable MPM-based framework for modeling deformable objects from multi-view RGB-D videos, enabling physics-aware simulation and online parameter optimization for robotic manipulation.", "motivation": "Existing methods for modeling deformable objects either oversimplify rich dynamics or require large training datasets, limiting generalization and practical application in robotics and vision tasks.", "method": "Uses differentiable Material Point Method (MPM) simulator to reconstruct geometry and appearance from multi-view RGB-D videos, then minimizes mismatch between predicted and observed visual data through physics simulation with online parameter optimization.", "result": "EMPM outperforms spring-mass baseline models and enables adaptive, robust, physics-aware object representations suitable for robotic manipulation of complex deformable materials.", "conclusion": "The framework provides a physically plausible, generalizable, and data-efficient approach to deformable object modeling that opens new possibilities for robotic manipulation of complex materials."}}
{"id": "2601.17032", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17032", "abs": "https://arxiv.org/abs/2601.17032", "authors": ["Wilkie Delgado-Font", "Miriela Escobedo-Nicot", "Manuel Gonz\u00e1lez-Hidalgo", "Silena Herold-Garcia", "Antoni Jaume-i-Cap\u00f3", "Arnau Mir"], "title": "Diagnosis Support of Sickle Cell Anemia by Classifying Red Blood Cell Shape in Peripheral Blood Images", "comment": null, "summary": "Red blood cell (RBC) deformation is the consequence of several diseases, including sickle cell anemia, which causes recurring episodes of pain and severe pronounced anemia. Monitoring patients with these diseases involves the observation of peripheral blood samples under a microscope, a time-consuming procedure. Moreover, a specialist is required to perform this technique, and owing to the subjective nature of the observation of isolated RBCs, the error rate is high. In this paper, we propose an automated method for differentially enumerating RBCs that uses peripheral blood smear image analysis. In this method, the objects of interest in the image are segmented using a Chan-Vese active contour model. An analysis is then performed to classify the RBCs, also called erythrocytes, as normal or elongated or having other deformations, using the basic shape analysis descriptors: circular shape factor (CSF) and elliptical shape factor (ESF). To analyze cells that become partially occluded in a cluster during sample preparation, an elliptical adjustment is performed to allow the analysis of erythrocytes with discoidal and elongated shapes. The images of patient blood samples used in the study were acquired by a clinical laboratory specialist in the Special Hematology Department of the ``Dr. Juan Bruno Zayas'' General Hospital in Santiago de Cuba. A comparison of the results obtained by the proposed method in our experiments with those obtained by some state-of-the-art methods showed that the proposed method is superior for the diagnosis of sickle cell anemia. This superiority is achieved for evidenced by the obtained F-measure value (0.97 for normal cells and 0.95 for elongated ones) and several overall multiclass performance measures. The results achieved by the proposed method are suitable for the purpose of clinical treatment and diagnostic support of sickle cell anemia.", "AI": {"tldr": "Automated method for classifying RBCs in blood smear images using active contour segmentation and shape analysis to diagnose sickle cell anemia.", "motivation": "Current manual microscopy for monitoring RBC deformation in diseases like sickle cell anemia is time-consuming, requires specialists, and has high error rates due to subjective observation.", "method": "Uses Chan-Vese active contour model for segmentation, then classifies RBCs as normal or deformed using circular shape factor (CSF) and elliptical shape factor (ESF). Includes elliptical adjustment for partially occluded cells in clusters.", "result": "Achieved F-measure of 0.97 for normal cells and 0.95 for elongated cells, outperforming state-of-the-art methods. Suitable for clinical treatment and diagnostic support.", "conclusion": "The proposed automated method is superior for sickle cell anemia diagnosis compared to existing methods and can effectively support clinical treatment."}}
{"id": "2601.17188", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17188", "abs": "https://arxiv.org/abs/2601.17188", "authors": ["Swapn Shah", "Wlodek Zadrozny"], "title": "Implementing Tensor Logic: Unifying Datalog and Neural Reasoning via Tensor Contraction", "comment": null, "summary": "The unification of symbolic reasoning and neural networks remains a central challenge in artificial intelligence. Symbolic systems offer reliability and interpretability but lack scalability, while neural networks provide learning capabilities but sacrifice transparency. Tensor Logic, proposed by Domingos, suggests that logical rules and Einstein summation are mathematically equivalent, offering a principled path toward unification. This paper provides empirical validation of this framework through three experiments. First, we demonstrate the equivalence between recursive Datalog rules and iterative tensor contractions by computing the transitive closure of a biblical genealogy graph containing 1,972 individuals and 1,727 parent-child relationships, converging in 74 iterations to discover 33,945 ancestor relationships. Second, we implement reasoning in embedding space by training a neural network with learnable transformation matrices, demonstrating successful zero-shot compositional inference on held-out queries. Third, we validate the Tensor Logic superposition construction on FB15k-237, a large-scale knowledge graph with 14,541 entities and 237 relations. Using Domingos's relation matrix formulation $R_r = E^\\top A_r E$, we achieve MRR of 0.3068 on standard link prediction and MRR of 0.3346 on a compositional reasoning benchmark where direct edges are removed during training, demonstrating that matrix composition enables multi-hop inference without direct training examples.", "AI": {"tldr": "Tensor Logic unifies symbolic reasoning and neural networks by showing logical rules are equivalent to Einstein summation, validated through three experiments on genealogy graphs, embedding space reasoning, and knowledge graph link prediction.", "motivation": "The unification of symbolic reasoning (reliable, interpretable) and neural networks (scalable, learnable) remains a central AI challenge. Symbolic systems lack scalability while neural networks sacrifice transparency.", "method": "Three experiments validate Tensor Logic: 1) Show equivalence between Datalog rules and tensor contractions on biblical genealogy graph, 2) Implement embedding space reasoning with learnable transformation matrices, 3) Validate superposition construction on FB15k-237 knowledge graph using relation matrix formulation R_r = E^T A_r E.", "result": "1) Transitive closure computation converged in 74 iterations discovering 33,945 ancestor relationships. 2) Successful zero-shot compositional inference on held-out queries. 3) Achieved MRR of 0.3068 on link prediction and 0.3346 on compositional reasoning benchmark where direct edges were removed during training.", "conclusion": "Tensor Logic provides a principled path toward unifying symbolic reasoning and neural networks, demonstrating that matrix composition enables multi-hop inference without direct training examples, bridging the gap between reliability/interpretability and scalability/learning."}}
{"id": "2601.17287", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.17287", "abs": "https://arxiv.org/abs/2601.17287", "authors": ["Yanrong Chen", "Xihan Bian"], "title": "Real-Time Synchronized Interaction Framework for Emotion-Aware Humanoid Robots", "comment": "6 pages, 6 figures", "summary": "As humanoid robots increasingly introduced into social scene, achieving emotionally synchronized multimodal interaction remains a significant challenges. To facilitate the further adoption and integration of humanoid robots into service roles, we present a real-time framework for NAO robots that synchronizes speech prosody with full-body gestures through three key innovations: (1) A dual-channel emotion engine where large language model (LLM) simultaneously generates context-aware text responses and biomechanically feasible motion descriptors, constrained by a structured joint movement library; (2) Duration-aware dynamic time warping for precise temporal alignment of speech output and kinematic motion keyframes; (3) Closed-loop feasibility verification ensuring gestures adhere to NAO's physical joint limits through real-time adaptation. Evaluations show 21% higher emotional alignment compared to rule-based systems, achieved by coordinating vocal pitch (arousal-driven) with upper-limb kinematics while maintaining lower-body stability. By enabling seamless sensorimotor coordination, this framework advances the deployment of context-aware social robots in dynamic applications such as personalized healthcare, interactive education, and responsive customer service platforms.", "AI": {"tldr": "Real-time framework for NAO robots synchronizes speech prosody with full-body gestures using LLM-based emotion engine, dynamic time warping, and physical feasibility verification.", "motivation": "Humanoid robots need emotionally synchronized multimodal interaction for better social integration and service roles, but achieving this remains challenging.", "method": "Three innovations: (1) Dual-channel LLM emotion engine generating text and motion descriptors, (2) Duration-aware dynamic time warping for temporal alignment, (3) Closed-loop feasibility verification for physical constraints.", "result": "21% higher emotional alignment compared to rule-based systems, achieved by coordinating vocal pitch with upper-limb kinematics while maintaining lower-body stability.", "conclusion": "Framework enables seamless sensorimotor coordination, advancing deployment of context-aware social robots in healthcare, education, and customer service applications."}}
{"id": "2601.17037", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17037", "abs": "https://arxiv.org/abs/2601.17037", "authors": ["Aahana Basappa", "Pranay Goel", "Anusri Karra", "Anish Karra", "Asa Gilmore", "Kevin Zhu"], "title": "AMVICC: A Novel Benchmark for Cross-Modal Failure Mode Profiling for VLMs and IGMs", "comment": "Comments: 13 pages, 4 figures. Presented at the 39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: NeurIPS 2025 VLM4RWD. Authors Aahana Basappa and Pranay Goel contributed equally to this work. Code: https://github.com/AahanaB24/AMVICC, Data: https://doi.org/10.5281/zenodo.17646068", "summary": "We investigated visual reasoning limitations of both multimodal large language models (MLLMs) and image generation models (IGMs) by creating a novel benchmark to systematically compare failure modes across image-to-text and text-to-image tasks, enabling cross-modal evaluation of visual understanding. Despite rapid growth in machine learning, vision language models (VLMs) still fail to understand or generate basic visual concepts such as object orientation, quantity, or spatial relationships, which highlighted gaps in elementary visual reasoning. By adapting MMVP benchmark questions into explicit and implicit prompts, we create \\textit{AMVICC}, a novel benchmark for profiling failure modes across various modalities. After testing 11 MLLMs and 3 IGMs in nine categories of visual reasoning, our results show that failure modes are often shared between models and modalities, but certain failures are model-specific and modality-specific, and this can potentially be attributed to various factors. IGMs consistently struggled to manipulate specific visual components in response to prompts, especially in explicit prompts, suggesting poor control over fine-grained visual attributes. Our findings apply most directly to the evaluation of existing state-of-the-art models on structured visual reasoning tasks. This work lays the foundation for future cross-modal alignment studies, offering a framework to probe whether generation and interpretation failures stem from shared limitations to guide future improvements in unified vision-language modeling.", "AI": {"tldr": "Created AMVICC benchmark to compare failure modes of multimodal LLMs and image generation models on visual reasoning tasks, finding shared and modality-specific limitations in understanding/generating basic visual concepts.", "motivation": "Despite rapid progress, vision-language models still fail at basic visual reasoning (object orientation, quantity, spatial relationships), highlighting gaps in elementary visual understanding that need systematic evaluation.", "method": "Adapted MMVP benchmark questions into explicit/implicit prompts to create AMVICC benchmark, testing 11 MLLMs and 3 IGMs across nine visual reasoning categories to profile failure modes across modalities.", "result": "Failure modes are often shared between models/modalities, but some are model/modality-specific. IGMs struggle with fine-grained visual attribute control, especially in explicit prompts. Models show systematic limitations in basic visual reasoning.", "conclusion": "Provides framework for cross-modal evaluation of visual understanding, laying foundation for future alignment studies to determine if generation/interpretation failures stem from shared limitations, guiding improvements in unified vision-language modeling."}}
{"id": "2601.17310", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17310", "abs": "https://arxiv.org/abs/2601.17310", "authors": ["Yu Akagi", "Tomohisa Seki", "Hiromasa Ito", "Toru Takiguchi", "Kazuhiko Ohe", "Yoshimasa Kawazoe"], "title": "High-Fidelity Longitudinal Patient Simulation Using Real-World Data", "comment": null, "summary": "Simulation is a powerful tool for exploring uncertainty. Its potential in clinical medicine is transformative and includes personalized treatment planning and virtual clinical trials. However, simulating patient trajectories is challenging because of complex biological and sociocultural influences. Here, we show that real-world clinical records can be leveraged to empirically model patient timelines. We developed a generative simulator model that takes a patient's history as input and synthesizes fine-grained, realistic future trajectories. The model was pretrained on more than 200 million clinical records. It produced high-fidelity future timelines, closely matching event occurrence rates, laboratory test results, and temporal dynamics in real patient future data. It also accurately estimated future event probabilities, with observed-to-expected ratios consistently near 1.0 across diverse outcomes and time horizons. Our results reveal the untapped value of real-world data in electronic health records and introduce a scalable framework for in silico modeling of clinical care.", "AI": {"tldr": "Researchers developed a generative AI model trained on 200M+ clinical records that can simulate realistic patient trajectories for personalized treatment planning and virtual clinical trials.", "motivation": "Simulation has transformative potential in clinical medicine but is challenging due to complex biological and sociocultural factors. Real-world clinical data offers an untapped resource for empirically modeling patient timelines.", "method": "Developed a generative simulator model pretrained on over 200 million clinical records that takes patient history as input and synthesizes fine-grained, realistic future trajectories.", "result": "The model produced high-fidelity future timelines matching real patient data in event occurrence rates, lab results, and temporal dynamics. It accurately estimated future event probabilities with observed-to-expected ratios consistently near 1.0 across diverse outcomes and time horizons.", "conclusion": "The study reveals the untapped value of real-world EHR data and introduces a scalable framework for in silico modeling of clinical care, enabling personalized treatment planning and virtual clinical trials."}}
{"id": "2601.17404", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.17404", "abs": "https://arxiv.org/abs/2601.17404", "authors": ["Anke Fischer-Janzen", "Thomas M. Wendt", "Kristof Van Laerhoven"], "title": "Eye-Tracking-Driven Control in Daily Task Assistance for Assistive Robotic Arms", "comment": "23 pages, 6 figures, publication in review process", "summary": "Shared control improves Human-Robot Interaction by reducing the user's workload and increasing the robot's autonomy. It allows robots to perform tasks under the user's supervision. Current eye-tracking-driven approaches face several challenges. These include accuracy issues in 3D gaze estimation and difficulty interpreting gaze when differentiating between multiple tasks. We present an eye-tracking-driven control framework, aimed at enabling individuals with severe physical disabilities to perform daily tasks independently. Our system uses task pictograms as fiducial markers combined with a feature matching approach that transmits data of the selected object to accomplish necessary task related measurements with an eye-in-hand configuration. This eye-tracking control does not require knowledge of the user's position in relation to the object. The framework correctly interpreted object and task selection in up to 97.9% of measurements. Issues were found in the evaluation, that were improved and shared as lessons learned. The open-source framework can be adapted to new tasks and objects due to the integration of state-of-the-art object detection models.", "AI": {"tldr": "Eye-tracking control framework using task pictograms as fiducial markers enables individuals with severe disabilities to perform daily tasks independently with 97.9% accuracy in object/task selection.", "motivation": "Current eye-tracking approaches face challenges with 3D gaze estimation accuracy and difficulty interpreting gaze when differentiating between multiple tasks, limiting their effectiveness for individuals with severe physical disabilities.", "method": "Uses task pictograms as fiducial markers combined with feature matching to transmit selected object data for task-related measurements with eye-in-hand configuration, eliminating need for user position knowledge relative to objects.", "result": "Framework correctly interpreted object and task selection in up to 97.9% of measurements, with identified issues improved and shared as lessons learned.", "conclusion": "Open-source framework can be adapted to new tasks and objects through integration of state-of-the-art object detection models, providing accessible shared control for individuals with severe disabilities."}}
{"id": "2601.17038", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17038", "abs": "https://arxiv.org/abs/2601.17038", "authors": ["Obai Alashram", "Nejad Alagha", "Mahmoud AlKakuri", "Zeeshan Swaveel", "Abigail Copiaco"], "title": "Hybrid Deep Feature Extraction and ML for Construction and Demolition Debris Classification", "comment": null, "summary": "The construction industry produces significant volumes of debris, making effective sorting and classification critical for sustainable waste management and resource recovery. This study presents a hybrid vision-based pipeline that integrates deep feature extraction with classical machine learning (ML) classifiers for automated construction and demolition (C\\&D) debris classification. A novel dataset comprising 1,800 balanced, high-quality images representing four material categories, Ceramic/Tile, Concrete, Trash/Waste, and Wood was collected from real construction sites in the UAE, capturing diverse real-world conditions. Deep features were extracted using a pre-trained Xception network, and multiple ML classifiers, including SVM, kNN, Bagged Trees, LDA, and Logistic Regression, were systematically evaluated. The results demonstrate that hybrid pipelines using Xception features with simple classifiers such as Linear SVM, kNN, and Bagged Trees achieve state-of-the-art performance, with up to 99.5\\% accuracy and macro-F1 scores, surpassing more complex or end-to-end deep learning approaches. The analysis highlights the operational benefits of this approach for robust, field-deployable debris identification and provides pathways for future integration with robotics and onsite automation systems.", "AI": {"tldr": "Hybrid vision pipeline combining Xception deep features with classical ML classifiers achieves 99.5% accuracy for automated construction debris classification using real-world UAE dataset.", "motivation": "Construction industry generates massive debris volumes requiring effective sorting for sustainable waste management and resource recovery, but automated classification remains challenging.", "method": "Created novel dataset of 1,800 balanced images (Ceramic/Tile, Concrete, Trash/Waste, Wood) from UAE construction sites; extracted deep features using pre-trained Xception network; evaluated multiple ML classifiers (SVM, kNN, Bagged Trees, LDA, Logistic Regression).", "result": "Hybrid pipelines with Xception features and simple classifiers (Linear SVM, kNN, Bagged Trees) achieved state-of-the-art performance up to 99.5% accuracy and macro-F1 scores, outperforming complex end-to-end deep learning approaches.", "conclusion": "Hybrid vision-ML approach offers robust, field-deployable debris identification with operational benefits and pathways for integration with robotics and onsite automation systems in construction waste management."}}
{"id": "2601.17311", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17311", "abs": "https://arxiv.org/abs/2601.17311", "authors": ["Bang Liu", "Linglong Kong", "Jian Pei"], "title": "Phase Transition for Budgeted Multi-Agent Synergy", "comment": "55 pages, 12 figures", "summary": "Multi-agent systems can improve reliability, yet under a fixed inference budget they often help, saturate, or even collapse. We develop a minimal and calibratable theory that predicts these regimes from three binding constraints of modern agent stacks: finite context windows, lossy inter-agent communication, and shared failures among similar agents. Each leaf agent is summarized by a compute-performance scaling exponent $\u03b2$; communication is captured by a message-length fidelity curve $\u03b3(m)$; dependence is captured by an effective shared-error correlation $\u03c1$; and a context window $W$ imposes hard fan-in limits that make hierarchy necessary. For binary success/failure tasks with majority aggregation, we prove a sharp phase transition for deep $b$-ary trees with correlated inputs and lossy communication: a single scalar $\u03b1_\u03c1$ (combining $\u03b3(m)$, $\u03c1$, and fan-in $b$) determines whether weak signal is amplified to a nontrivial fixed point or washed out to chance. In the amplifying regime, we derive an organization exponent $s$ and show that budgeted synergy, i.e., outperforming the best single agent under the same total budget, occurs exactly when $s>\u03b2$, yielding closed-form compute allocation rules and explicit budget thresholds. We further characterize saturation via a mixing depth and provide a conservative clipped predictor that remains accurate across growth and saturation. A continuous-performance warm-up gives closed-form risks for star, chain, and tree organizations, making correlation- and communication-induced floors explicit and exposing the core design trade-offs in a smooth setting. Finally, we validate the predicted phase boundaries in controlled synthetic simulations and show how the same mechanisms explain the dominant bottlenecks reported in recent large-scale matched-budget studies of LLM agent-system scaling.", "AI": {"tldr": "Multi-agent systems show help, saturation, or collapse under fixed budgets; theory predicts regimes from context windows, lossy communication, and shared failures.", "motivation": "Multi-agent systems can improve reliability but often show unpredictable behavior under fixed inference budgets - sometimes helping, saturating, or collapsing. Need a minimal, calibratable theory to predict these regimes from fundamental constraints of modern agent stacks.", "method": "Develop theory with three key constraints: finite context windows (W), lossy inter-agent communication (\u03b3(m)), and shared failures among similar agents (\u03c1). Model leaf agents with compute-performance scaling exponent \u03b2. Analyze binary success/failure tasks with majority aggregation in deep b-ary trees. Derive phase transitions, organization exponent s, and compute allocation rules.", "result": "Prove sharp phase transition: single scalar \u03b1_\u03c1 determines whether weak signal amplifies to nontrivial fixed point or washes out to chance. Budgeted synergy occurs when s>\u03b2, yielding closed-form compute allocation rules and explicit budget thresholds. Characterize saturation via mixing depth. Validate predictions in synthetic simulations and explain bottlenecks in LLM agent-system scaling studies.", "conclusion": "The theory successfully predicts multi-agent system behavior regimes from fundamental constraints, providing practical design rules for compute allocation and organization structure. The framework explains observed bottlenecks in real LLM agent systems and offers conservative clipped predictors that remain accurate across growth and saturation regimes."}}
{"id": "2601.17412", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.17412", "abs": "https://arxiv.org/abs/2601.17412", "authors": ["Valerii Serpiva", "Artem Lykov", "Jeffrin Sam", "Aleksey Fedoseev", "Dzmitry Tsetserukou"], "title": "DiffusionCinema: Text-to-Aerial Cinematography", "comment": null, "summary": "We propose a novel Unmanned Aerial Vehicles (UAV) assisted creative capture system that leverages diffusion models to interpret high-level natural language prompts and automatically generate optimal flight trajectories for cinematic video recording. Instead of manually piloting the drone, the user simply describes the desired shot (e.g., \"orbit around me slowly from the right and reveal the background waterfall\"). Our system encodes the prompt along with an initial visual snapshot from the onboard camera, and a diffusion model samples plausible spatio-temporal motion plans that satisfy both the scene geometry and shot semantics. The generated flight trajectory is then executed autonomously by the UAV to record smooth, repeatable video clips that match the prompt. User evaluation using NASA-TLX showed a significantly lower overall workload with our interface (M = 21.6) compared to a traditional remote controller (M = 58.1), demonstrating a substantial reduction in perceived effort. Mental demand (M = 11.5 vs. 60.5) and frustration (M = 14.0 vs. 54.5) were also markedly lower for our system, confirming clear usability advantages in autonomous text-driven flight control. This project demonstrates a new interaction paradigm: text-to-cinema flight, where diffusion models act as the \"creative operator\" converting story intentions directly into aerial motion.", "AI": {"tldr": "A UAV system uses diffusion models to convert natural language prompts into autonomous flight trajectories for cinematic video recording, reducing user workload compared to manual control.", "motivation": "To simplify drone cinematography by enabling users to describe desired shots in natural language rather than manually piloting drones, making aerial videography more accessible and creative.", "method": "System encodes natural language prompts with initial visual snapshots from drone camera, uses diffusion models to sample spatio-temporal motion plans that satisfy both scene geometry and shot semantics, then executes generated trajectories autonomously.", "result": "NASA-TLX evaluation showed significantly lower overall workload (M=21.6 vs 58.1), mental demand (11.5 vs 60.5), and frustration (14.0 vs 54.5) compared to traditional remote control, demonstrating clear usability advantages.", "conclusion": "Demonstrates a new \"text-to-cinema flight\" interaction paradigm where diffusion models act as creative operators converting story intentions directly into aerial motion, making drone cinematography more intuitive and less demanding."}}
{"id": "2601.17039", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17039", "abs": "https://arxiv.org/abs/2601.17039", "authors": ["Junhyuk Heo", "Beomkyu Choi", "Hyunjin Shin", "Darongsae Kwon"], "title": "MANGO: A Global Single-Date Paired Dataset for Mangrove Segmentation", "comment": null, "summary": "Mangroves are critical for climate-change mitigation, requiring reliable monitoring for effective conservation. While deep learning has emerged as a powerful tool for mangrove detection, its progress is hindered by the limitations of existing datasets. In particular, many resources provide only annual map products without curated single-date image-mask pairs, limited to specific regions rather than global coverage, or remain inaccessible to the public. To address these challenges, we introduce MANGO, a large-scale global dataset comprising 42,703 labeled image-mask pairs across 124 countries. To construct this dataset, we retrieve all available Sentinel-2 imagery within the year 2020 for mangrove regions and select the best single-date observations that align with the mangrove annual mask. This selection is performed using a target detection-driven approach that leverages pixel-wise coordinate references to ensure adaptive and representative image-mask pairings. We also provide a benchmark across diverse semantic segmentation architectures under a country-disjoint split, establishing a foundation for scalable and reliable global mangrove monitoring.", "AI": {"tldr": "MANGO is a large-scale global dataset of 42,703 labeled image-mask pairs across 124 countries for mangrove detection, addressing limitations of existing datasets by providing curated single-date observations aligned with annual masks.", "motivation": "Existing mangrove datasets are limited - they often provide only annual map products without curated single-date image-mask pairs, have limited regional coverage rather than global scope, or remain inaccessible to the public, hindering progress in deep learning for mangrove monitoring.", "method": "Retrieved all available Sentinel-2 imagery within 2020 for mangrove regions, selected best single-date observations aligned with mangrove annual masks using a target detection-driven approach with pixel-wise coordinate references for adaptive and representative image-mask pairings.", "result": "Created MANGO dataset with 42,703 labeled image-mask pairs across 124 countries, and established benchmark across diverse semantic segmentation architectures under country-disjoint split.", "conclusion": "MANGO dataset provides foundation for scalable and reliable global mangrove monitoring, addressing critical limitations in existing resources and enabling better deep learning applications for mangrove conservation and climate-change mitigation."}}
{"id": "2601.17332", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17332", "abs": "https://arxiv.org/abs/2601.17332", "authors": ["Yicheng Tao", "Hongteng Xu"], "title": "TheoremForge: Scaling up Formal Data Synthesis with Low-Budget Agentic Workflow", "comment": null, "summary": "The high cost of agentic workflows in formal mathematics hinders large-scale data synthesis, exacerbating the scarcity of open-source corpora. To address this, we introduce \\textbf{TheoremForge}, a cost-effective formal data synthesis pipeline that decomposes the formalization process into five sub-tasks, which are \\textit{statement formalization}, \\textit{proof generation}, \\textit{premise selection}, \\textit{proof correction} and \\textit{proof sketching}. By implementing a \\textit{Decoupled Extraction Strategy}, the workflow recovers valid training signals from globally failed trajectories, effectively utilizing wasted computation. Experiments on a 2,000-problem benchmark demonstrate that TheoremForge achieves a Verified Rate of 12.6\\%, surpassing the 8.6\\% baseline, at an average cost of only \\textbf{\\$0.481} per successful trajectory using Gemini-3-Flash. Crucially, our strategy increases data yield by \\textbf{1.6$\\times$} for proof generation compared to standard filtering. These results establish TheoremForge as a scalable framework for constructing a data flywheel to train future expert models. Our code is available \\href{https://github.com/timechess/TheoremForge}{here}.", "AI": {"tldr": "TheoremForge is a cost-effective pipeline for synthesizing formal mathematics data by decomposing formalization into 5 sub-tasks and recovering training signals from failed trajectories, achieving 12.6% verified rate at $0.481 per success.", "motivation": "High cost of agentic workflows in formal mathematics limits large-scale data synthesis, creating scarcity of open-source corpora for training expert models.", "method": "Decomposes formalization into 5 sub-tasks (statement formalization, proof generation, premise selection, proof correction, proof sketching) and implements Decoupled Extraction Strategy to recover valid training signals from failed trajectories.", "result": "Achieves 12.6% Verified Rate (vs 8.6% baseline) on 2,000-problem benchmark at average cost of $0.481 per successful trajectory, increasing data yield by 1.6\u00d7 for proof generation compared to standard filtering.", "conclusion": "TheoremForge establishes a scalable framework for constructing a data flywheel to train future expert models in formal mathematics."}}
{"id": "2601.17428", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.17428", "abs": "https://arxiv.org/abs/2601.17428", "authors": ["Ziming Li", "Chenhao Li", "Marco Hutter"], "title": "Scaling Rough Terrain Locomotion with Automatic Curriculum Reinforcement Learning", "comment": null, "summary": "Curriculum learning has demonstrated substantial effectiveness in robot learning. However, it still faces limitations when scaling to complex, wide-ranging task spaces. Such task spaces often lack a well-defined difficulty structure, making the difficulty ordering required by previous methods challenging to define. We propose a Learning Progress-based Automatic Curriculum Reinforcement Learning (LP-ACRL) framework, which estimates the agent's learning progress online and adaptively adjusts the task-sampling distribution, thereby enabling automatic curriculum generation without prior knowledge of the difficulty distribution over the task space. Policies trained with LP-ACRL enable the ANYmal D quadruped to achieve and maintain stable, high-speed locomotion at 2.5 m/s linear velocity and 3.0 rad/s angular velocity across diverse terrains, including stairs, slopes, gravel, and low-friction flat surfaces--whereas previous methods have generally been limited to high speeds on flat terrain or low speeds on complex terrain. Experimental results demonstrate that LP-ACRL exhibits strong scalability and real-world applicability, providing a robust baseline for future research on curriculum generation in complex, wide-ranging robotic learning task spaces.", "AI": {"tldr": "LP-ACRL is a learning progress-based automatic curriculum RL framework that enables quadruped robots to achieve high-speed locomotion across diverse terrains without prior knowledge of task difficulty.", "motivation": "Curriculum learning is effective for robot learning but struggles with complex task spaces lacking clear difficulty structures, making traditional difficulty ordering challenging to define.", "method": "LP-ACRL estimates agent's learning progress online and adaptively adjusts task-sampling distribution, enabling automatic curriculum generation without prior knowledge of difficulty distribution.", "result": "ANYmal D quadruped achieves stable high-speed locomotion (2.5 m/s linear, 3.0 rad/s angular) across diverse terrains including stairs, slopes, gravel, and low-friction surfaces, surpassing previous methods limited to either high speeds on flat terrain or low speeds on complex terrain.", "conclusion": "LP-ACRL demonstrates strong scalability and real-world applicability, providing a robust baseline for future curriculum generation research in complex robotic learning task spaces."}}
{"id": "2601.17040", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17040", "abs": "https://arxiv.org/abs/2601.17040", "authors": ["H Neji", "J Nogueras-Iso", "J Lacasta", "M\u00c1 Latre", "FJ Garc\u00eda-Marco"], "title": "FP-THD: Full page transcription of historical documents", "comment": "Figure 1: FP-THD architecture Overview: Layout Analysis and Masked Auto-encoder with Vision Trans- former", "summary": "The transcription of historical documents written in Latin in XV and XVI centuries has special challenges as it must maintain the characters and special symbols that have distinct meanings to ensure that historical texts retain their original style and significance. This work proposes a pipeline for the transcription of historical documents preserving these special features. We propose to extend an existing text line recognition method with a layout analysis model. We analyze historical text images using a layout analysis model to extract text lines, which are then processed by an OCR model to generate a fully digitized page. We showed that our pipeline facilitates the processing of the page and produces an efficient result. We evaluated our approach on multiple datasets and demonstrate that the masked autoencoder effectively processes different types of text, including handwritten, printed and multi-language.", "AI": {"tldr": "Proposes a pipeline combining layout analysis and OCR to transcribe historical Latin documents while preserving special characters and symbols from XV-XVI centuries.", "motivation": "Historical Latin documents from XV-XVI centuries contain special characters and symbols with distinct meanings that must be preserved during transcription to maintain original style and significance, presenting unique challenges.", "method": "Extends existing text line recognition with layout analysis model: uses layout analysis to extract text lines from historical images, then processes them with OCR model to generate fully digitized pages. Employs masked autoencoder approach.", "result": "Pipeline facilitates page processing and produces efficient results. Evaluated on multiple datasets showing masked autoencoder effectively processes different text types including handwritten, printed, and multi-language documents.", "conclusion": "Proposed pipeline successfully addresses transcription challenges for historical Latin documents by preserving special features through combined layout analysis and OCR approach."}}
{"id": "2601.17335", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17335", "abs": "https://arxiv.org/abs/2601.17335", "authors": ["Angshul Majumdar"], "title": "The Relativity of AGI: Distributional Axioms, Fragility, and Undecidability", "comment": null, "summary": "We study whether Artificial General Intelligence (AGI) admits a coherent theoretical definition that supports absolute claims of existence, robustness, or self-verification. We formalize AGI axiomatically as a distributional, resource-bounded semantic predicate, indexed by a task family, a task distribution, a performance functional, and explicit resource budgets. Under this framework, we derive four classes of results. First, we show that generality is inherently relational: there is no distribution-independent notion of AGI. Second, we prove non-invariance results demonstrating that arbitrarily small perturbations of the task distribution can invalidate AGI properties via cliff sets, precluding universal robustness. Third, we establish bounded transfer guarantees, ruling out unbounded generalization across task families under finite resources. Fourth, invoking Rice-style and G\u00f6del--Tarski arguments, we prove that AGI is a nontrivial semantic property and therefore cannot be soundly and completely certified by any computable procedure, including procedures implemented by the agent itself. Consequently, recursive self-improvement schemes that rely on internal self-certification of AGI are ill-posed. Taken together, our results show that strong, distribution-independent claims of AGI are not false but undefined without explicit formal indexing, and that empirical progress in AI does not imply the attainability of self-certifying general intelligence.", "AI": {"tldr": "AGI cannot be defined independently of task distributions, lacks universal robustness, has bounded generalization, and cannot be self-certified computationally.", "motivation": "To determine if AGI can be coherently defined theoretically to support absolute claims about existence, robustness, or self-verification, addressing foundational questions about what constitutes general intelligence.", "method": "Formalize AGI axiomatically as a distributional, resource-bounded semantic predicate indexed by task family, distribution, performance functional, and resource budgets. Use mathematical proofs including Rice-style and G\u00f6del-Tarski arguments.", "result": "Four key findings: 1) Generality is relational (no distribution-independent AGI), 2) Non-invariance to task perturbations (no universal robustness), 3) Bounded transfer (finite resources limit generalization), 4) AGI cannot be soundly/completely certified by any computable procedure.", "conclusion": "Strong distribution-independent AGI claims are undefined without explicit formal indexing; empirical AI progress doesn't imply self-certifying general intelligence is attainable; recursive self-improvement relying on internal AGI certification is ill-posed."}}
{"id": "2601.17440", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.17440", "abs": "https://arxiv.org/abs/2601.17440", "authors": ["Xinru Cui", "Linxi Feng", "Yixuan Zhou", "Haoqi Han", "Zhe Liu", "Hesheng Wang"], "title": "PILOT: A Perceptive Integrated Low-level Controller for Loco-manipulation over Unstructured Scenes", "comment": "8 pages, 4 figures", "summary": "Humanoid robots hold great potential for diverse interactions and daily service tasks within human-centered environments, necessitating controllers that seamlessly integrate precise locomotion with dexterous manipulation. However, most existing whole-body controllers lack exteroceptive awareness of the surrounding environment, rendering them insufficient for stable task execution in complex, unstructured scenarios.To address this challenge, we propose PILOT, a unified single-stage reinforcement learning (RL) framework tailored for perceptive loco-manipulation, which synergizes perceptive locomotion and expansive whole-body control within a single policy. To enhance terrain awareness and ensure precise foot placement, we design a cross-modal context encoder that fuses prediction-based proprioceptive features with attention-based perceptive representations. Furthermore, we introduce a Mixture-of-Experts (MoE) policy architecture to coordinate diverse motor skills, facilitating better specialization across distinct motion patterns. Extensive experiments in both simulation and on the physical Unitree G1 humanoid robot validate the efficacy of our framework. PILOT demonstrates superior stability, command tracking precision, and terrain traversability compared to existing baselines. These results highlight its potential to serve as a robust, foundational low-level controller for loco-manipulation in unstructured scenes.", "AI": {"tldr": "PILOT is a unified reinforcement learning framework for humanoid robots that combines perceptive locomotion and whole-body manipulation in a single policy, using cross-modal encoding and Mixture-of-Experts architecture for better terrain awareness and skill coordination.", "motivation": "Existing whole-body controllers for humanoid robots lack exteroceptive awareness of the environment, making them insufficient for stable task execution in complex, unstructured scenarios where robots need to integrate precise locomotion with dexterous manipulation.", "method": "Proposes PILOT: a single-stage RL framework with 1) cross-modal context encoder that fuses proprioceptive features with perceptive representations for terrain awareness, and 2) Mixture-of-Experts policy architecture to coordinate diverse motor skills across different motion patterns.", "result": "Extensive experiments in simulation and on physical Unitree G1 humanoid robot show PILOT achieves superior stability, command tracking precision, and terrain traversability compared to existing baselines.", "conclusion": "PILOT demonstrates potential as a robust foundational low-level controller for loco-manipulation in unstructured scenes, successfully integrating perceptive locomotion with whole-body control in a unified framework."}}
{"id": "2601.17041", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17041", "abs": "https://arxiv.org/abs/2601.17041", "authors": ["Ghadeer Alanazi", "Abir Benabid"], "title": "Arabic Sign Language Recognition using Multimodal Approach", "comment": null, "summary": "Arabic Sign Language (ArSL) is an essential communication method for individuals in the Deaf and Hard-of-Hearing community. However, existing recognition systems face significant challenges due to their reliance on single sensor approaches like Leap Motion or RGB cameras. These systems struggle with limitations such as inadequate tracking of complex hand orientations and imprecise recognition of 3D hand movements. This research paper aims to investigate the potential of a multimodal approach that combines Leap Motion and RGB camera data to explore the feasibility of recognition of ArSL. The system architecture includes two parallel subnetworks: a custom dense neural network for Leap Motion data, incorporating dropout and L2 regularization, and an image subnetwork based on a fine-tuned VGG16 model enhanced with data augmentation techniques. Feature representations from both modalities are concatenated in a fusion model and passed through fully connected layers, with final classification performed via SoftMax activation to analyze spatial and temporal features of hand gestures. The system was evaluated on a custom dataset comprising 18 ArSL words, of which 13 were correctly recognized, yielding an overall accuracy of 78%. These results offer preliminary insights into the viability of multimodal fusion for sign language recognition and highlight areas for further optimization and dataset expansion.", "AI": {"tldr": "Multimodal fusion of Leap Motion and RGB camera data achieves 78% accuracy on 18 Arabic Sign Language words, showing promise for improved sign language recognition over single-sensor approaches.", "motivation": "Existing Arabic Sign Language recognition systems rely on single sensors (Leap Motion or RGB cameras) which struggle with complex hand orientations and 3D movement tracking, necessitating a multimodal approach for better accuracy.", "method": "Combines Leap Motion and RGB camera data using two parallel subnetworks: custom dense neural network with dropout/L2 regularization for Leap Motion data, and fine-tuned VGG16 with data augmentation for RGB images, followed by feature fusion and SoftMax classification.", "result": "Achieved 78% accuracy on custom dataset of 18 ArSL words, with 13 correctly recognized, demonstrating preliminary viability of multimodal fusion for sign language recognition.", "conclusion": "Multimodal fusion shows promise for Arabic Sign Language recognition but requires further optimization and dataset expansion to improve accuracy and robustness."}}
{"id": "2601.17343", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17343", "abs": "https://arxiv.org/abs/2601.17343", "authors": ["Wei Liu", "Haomei Xu", "Hongkai Liu", "Zhiying Deng", "Ruixuan Li", "Heng Huang", "Yee Whye Teh", "Wee Sun Lee"], "title": "Are We Evaluating the Edit Locality of LLM Model Editing Properly?", "comment": null, "summary": "Model editing has recently emerged as a popular paradigm for efficiently updating knowledge in LLMs. A central desideratum of updating knowledge is to balance editing efficacy, i.e., the successful injection of target knowledge, and specificity (also known as edit locality), i.e., the preservation of existing non-target knowledge. However, we find that existing specificity evaluation protocols are inadequate for this purpose. We systematically elaborated on the three fundamental issues it faces. Beyond the conceptual issues, we further empirically demonstrate that existing specificity metrics are weakly correlated with the strength of specificity regularizers. We also find that current metrics lack sufficient sensitivity, rendering them ineffective at distinguishing the specificity performance of different methods. Finally, we propose a constructive evaluation protocol. Under this protocol, the conflict between open-ended LLMs and the assumption of determined answers is eliminated, query-independent fluency biases are avoided, and the evaluation strictness can be smoothly adjusted within a near-continuous space. Experiments across various LLMs, datasets, and editing methods show that metrics derived from the proposed protocol are more sensitive to changes in the strength of specificity regularizers and exhibit strong correlation with them, enabling more fine-grained discrimination of different methods' knowledge preservation capabilities.", "AI": {"tldr": "Paper critiques existing specificity evaluation protocols for model editing in LLMs, identifies three fundamental issues, and proposes a new constructive evaluation protocol that better measures knowledge preservation.", "motivation": "Existing specificity evaluation protocols for model editing are inadequate for properly measuring the balance between editing efficacy (successful knowledge injection) and specificity (preservation of existing non-target knowledge). Current metrics have conceptual flaws and empirical limitations.", "method": "The paper systematically analyzes three fundamental issues with existing specificity evaluation protocols, empirically demonstrates their weaknesses, and proposes a new constructive evaluation protocol that eliminates conflicts between open-ended LLMs and determined answers, avoids query-independent fluency biases, and allows smooth adjustment of evaluation strictness.", "result": "Experiments across various LLMs, datasets, and editing methods show that metrics from the proposed protocol are more sensitive to changes in specificity regularizer strength, exhibit strong correlation with them, and enable finer-grained discrimination of different methods' knowledge preservation capabilities.", "conclusion": "The proposed constructive evaluation protocol provides a more reliable and sensitive framework for assessing specificity in model editing, addressing fundamental issues with existing approaches and enabling better measurement of knowledge preservation capabilities."}}
{"id": "2601.17486", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.17486", "abs": "https://arxiv.org/abs/2601.17486", "authors": ["Zhiyuan Zhang", "Yu She"], "title": "EquiForm: Noise-Robust SE(3)-Equivariant Policy Learning from 3D Point Clouds", "comment": "Project website: https://ZhangZhiyuanZhang.github.io/equiform-website/ Code will be released", "summary": "Visual imitation learning with 3D point clouds has advanced robotic manipulation by providing geometry-aware, appearance-invariant observations. However, point cloud-based policies remain highly sensitive to sensor noise, pose perturbations, and occlusion-induced artifacts, which distort geometric structure and break the equivariance assumptions required for robust generalization. Existing equivariant approaches primarily encode symmetry constraints into neural architectures, but do not explicitly correct noise-induced geometric deviations or enforce equivariant consistency in learned representations. We introduce EquiForm, a noise-robust SE(3)-equivariant policy learning framework for point cloud-based manipulation. EquiForm formalizes how noise-induced geometric distortions lead to equivariance deviations in observation-to-action mappings, and introduces a geometric denoising module to restore consistent 3D structure under noisy or incomplete observations. In addition, we propose a contrastive equivariant alignment objective that enforces representation consistency under both rigid transformations and noise perturbations. Built upon these components, EquiForm forms a flexible policy learning pipeline that integrates noise-robust geometric reasoning with modern generative models. We evaluate EquiForm on 16 simulated tasks and 4 real-world manipulation tasks across diverse objects and scene layouts. Compared to state-of-the-art point cloud imitation learning methods, EquiForm achieves an average improvement of 17.2% in simulation and 28.1% in real-world experiments, demonstrating strong noise robustness and spatial generalization.", "AI": {"tldr": "EquiForm is an SE(3)-equivariant policy learning framework for point cloud-based robotic manipulation that improves noise robustness and spatial generalization by addressing geometric distortions from sensor noise, pose perturbations, and occlusion artifacts.", "motivation": "Point cloud-based policies for visual imitation learning are sensitive to sensor noise, pose perturbations, and occlusion artifacts, which distort geometric structure and break the equivariance assumptions needed for robust generalization. Existing equivariant approaches encode symmetry constraints but don't correct noise-induced geometric deviations or enforce equivariant consistency in learned representations.", "method": "EquiForm introduces: 1) A geometric denoising module to restore consistent 3D structure under noisy/incomplete observations, 2) A contrastive equivariant alignment objective that enforces representation consistency under both rigid transformations and noise perturbations, and 3) A flexible policy learning pipeline integrating noise-robust geometric reasoning with modern generative models.", "result": "Evaluated on 16 simulated tasks and 4 real-world manipulation tasks across diverse objects and scene layouts. Achieved average improvements of 17.2% in simulation and 28.1% in real-world experiments compared to state-of-the-art point cloud imitation learning methods, demonstrating strong noise robustness and spatial generalization.", "conclusion": "EquiForm successfully addresses the noise sensitivity problem in point cloud-based policies by formalizing how noise-induced geometric distortions lead to equivariance deviations and introducing mechanisms to restore geometric consistency, resulting in significantly improved performance and generalization capabilities for robotic manipulation tasks."}}
{"id": "2601.17042", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17042", "abs": "https://arxiv.org/abs/2601.17042", "authors": ["Tianyuan Liu", "Libin Hou", "Linyuan Wang", "Bin Yan"], "title": "Interpretable and Sparse Linear Attention with Decoupled Membership-Subspace Modeling via MCR2 Objective", "comment": "8 pages with 6 figures", "summary": "Maximal Coding Rate Reduction (MCR2)-driven white-box transformer, grounded in structured representation learning, unifies interpretability and efficiency, providing a reliable white-box solution for visual modeling. However, in existing designs, tight coupling between \"membership matrix\" and \"subspace matrix U\" in MCR2 causes redundant coding under incorrect token projection. To this end, we decouple the functional relationship between the \"membership matrix\" and \"subspaces U\" in the MCR2 objective and derive an interpretable sparse linear attention operator from unrolled gradient descent of the optimized objective. Specifically, we propose to directly learn the membership matrix from inputs and subsequently derive sparse subspaces from the fullspace S. Consequently, gradient unrolling of the optimized MCR2 objective yields an interpretable sparse linear attention operator: Decoupled Membership-Subspace Attention (DMSA). Experimental results on visual tasks show that simply replacing the attention module in Token Statistics Transformer (ToST) with DMSA (we refer to as DMST) not only achieves a faster coding reduction rate but also outperforms ToST by 1.08%-1.45% in top-1 accuracy on the ImageNet-1K dataset. Compared with vanilla Transformer architectures, DMST exhibits significantly higher computational efficiency and interpretability.", "AI": {"tldr": "Proposes Decoupled Membership-Subspace Attention (DMSA), an interpretable sparse linear attention operator derived from decoupling MCR2 objective, improving efficiency and accuracy over existing transformers.", "motivation": "Existing MCR2-driven transformers suffer from tight coupling between membership matrix and subspace matrix, causing redundant coding under incorrect token projection. This limits interpretability and efficiency.", "method": "Decouple functional relationship between membership matrix and subspaces in MCR2 objective. Learn membership matrix directly from inputs, derive sparse subspaces from fullspace S. Gradient unrolling yields interpretable sparse linear attention operator (DMSA).", "result": "Replacing attention in ToST with DMSA (DMST) achieves faster coding reduction rate and outperforms ToST by 1.08%-1.45% top-1 accuracy on ImageNet-1K. Shows significantly higher computational efficiency and interpretability compared to vanilla transformers.", "conclusion": "DMSA provides an interpretable, efficient attention mechanism through MCR2 objective decoupling, demonstrating improved performance and interpretability for visual modeling tasks."}}
{"id": "2601.17346", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17346", "abs": "https://arxiv.org/abs/2601.17346", "authors": ["Haoxin Xu", "Changyong Qi", "Tong Liu", "Bohao Zhang", "Anna He", "Bingqian Jiang", "Longwei Zheng", "Xiaoqing Gu"], "title": "Multi-Agent Learning Path Planning via LLMs", "comment": null, "summary": "The integration of large language models (LLMs) into intelligent tutoring systems offers transformative potential for personalized learning in higher education. However, most existing learning path planning approaches lack transparency, adaptability, and learner-centered explainability. To address these challenges, this study proposes a novel Multi-Agent Learning Path Planning (MALPP) framework that leverages a role- and rule-based collaboration mechanism among intelligent agents, each powered by LLMs. The framework includes three task-specific agents: a learner analytics agent, a path planning agent, and a reflection agent. These agents collaborate via structured prompts and predefined rules to analyze learning profiles, generate tailored learning paths, and iteratively refine them with interpretable feedback. Grounded in Cognitive Load Theory and Zone of Proximal Development, the system ensures that recommended paths are cognitively aligned and pedagogically meaningful. Experiments conducted on the MOOCCubeX dataset using seven LLMs show that MALPP significantly outperforms baseline models in path quality, knowledge sequence consistency, and cognitive load alignment. Ablation studies further validate the effectiveness of the collaborative mechanism and theoretical constraints. This research contributes to the development of trustworthy, explainable AI in education and demonstrates a scalable approach to learner-centered adaptive instruction powered by LLMs.", "AI": {"tldr": "MALPP framework uses multi-agent LLM collaboration for transparent, adaptive learning path planning in higher education, outperforming baselines on path quality and cognitive alignment.", "motivation": "Existing learning path planning approaches lack transparency, adaptability, and learner-centered explainability, limiting the transformative potential of LLMs in intelligent tutoring systems for personalized higher education.", "method": "Proposes Multi-Agent Learning Path Planning (MALPP) framework with three LLM-powered agents (learner analytics, path planning, reflection) collaborating via structured prompts and predefined rules, grounded in Cognitive Load Theory and Zone of Proximal Development.", "result": "Experiments on MOOCCubeX dataset with seven LLMs show MALPP significantly outperforms baseline models in path quality, knowledge sequence consistency, and cognitive load alignment; ablation studies validate collaborative mechanism effectiveness.", "conclusion": "MALPP contributes to trustworthy, explainable AI in education and demonstrates a scalable approach to learner-centered adaptive instruction powered by LLMs, addressing transparency and adaptability gaps in existing systems."}}
{"id": "2601.17507", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.17507", "abs": "https://arxiv.org/abs/2601.17507", "authors": ["Yutong Shen", "Hangxu Liu", "Kailin Pei", "Ruizhe Xia", "Tongtong Feng"], "title": "MetaWorld: Skill Transfer and Composition in a Hierarchical World Model for Grounding High-Level Instructions", "comment": "8 pages, 4 figures, Submitted to ICLR 2026 World Model Workshop", "summary": "Humanoid robot loco-manipulation remains constrained by the semantic-physical gap. Current methods face three limitations: Low sample efficiency in reinforcement learning, poor generalization in imitation learning, and physical inconsistency in VLMs. We propose MetaWorld, a hierarchical world model that integrates semantic planning and physical control via expert policy transfer. The framework decouples tasks into a VLM-driven semantic layer and a latent dynamics model operating in a compact state space. Our dynamic expert selection and motion prior fusion mechanism leverages a pre-trained multi-expert policy library as transferable knowledge, enabling efficient online adaptation via a two-stage framework. VLMs serve as semantic interfaces, mapping instructions to executable skills and bypassing symbol grounding. Experiments on Humanoid-Bench show MetaWorld outperforms world model-based RL in task completion and motion coherence. Our code will be found at https://anonymous.4open.science/r/metaworld-2BF4/", "AI": {"tldr": "MetaWorld: Hierarchical world model integrating semantic planning and physical control via expert policy transfer for humanoid robot loco-manipulation.", "motivation": "Address limitations in current humanoid robot loco-manipulation methods: low sample efficiency in RL, poor generalization in imitation learning, and physical inconsistency in vision-language models.", "method": "Hierarchical framework decoupling tasks into VLM-driven semantic layer and latent dynamics model in compact state space, with dynamic expert selection and motion prior fusion using pre-trained multi-expert policy library.", "result": "Outperforms world model-based RL in task completion and motion coherence on Humanoid-Bench.", "conclusion": "MetaWorld effectively bridges semantic-physical gap through hierarchical integration of semantic planning and physical control with transferable expert knowledge."}}
{"id": "2601.17046", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17046", "abs": "https://arxiv.org/abs/2601.17046", "authors": ["Matan Leibovich", "Mai Tan", "Adria Marcos-Morales", "Sreyas Mohan", "Peter A. Crozier", "Carlos Fernandez-Granda"], "title": "Atomic Depth Estimation From Noisy Electron Microscopy Data Via Deep Learning", "comment": null, "summary": "We present a novel approach for extracting 3D atomic-level information from transmission electron microscopy (TEM) images affected by significant noise. The approach is based on formulating depth estimation as a semantic segmentation problem. We address the resulting segmentation problem by training a deep convolutional neural network to generate pixel-wise depth segmentation maps using simulated data corrupted by synthetic noise. The proposed method was applied to estimate the depth of atomic columns in CeO2 nanoparticles from simulated images and real-world TEM data. Our experiments show that the resulting depth estimates are accurate, calibrated and robust to noise.", "AI": {"tldr": "Deep learning approach for 3D atomic depth estimation from noisy TEM images using semantic segmentation with simulated training data.", "motivation": "Need to extract accurate 3D atomic-level information from transmission electron microscopy (TEM) images that are often affected by significant noise, which makes depth estimation challenging.", "method": "Formulate depth estimation as semantic segmentation problem, train deep convolutional neural network on simulated TEM data corrupted by synthetic noise to generate pixel-wise depth segmentation maps.", "result": "Method successfully applied to estimate depth of atomic columns in CeO2 nanoparticles from both simulated and real-world TEM data, producing accurate, calibrated and noise-robust depth estimates.", "conclusion": "The proposed deep learning-based semantic segmentation approach effectively addresses 3D atomic depth estimation from noisy TEM images, demonstrating practical applicability to real experimental data."}}
{"id": "2601.17348", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17348", "abs": "https://arxiv.org/abs/2601.17348", "authors": ["Srikant Panda", "Sourabh Singh Yadav", "Palkesh Malviya"], "title": "Auditing Disability Representation in Vision-Language Models", "comment": null, "summary": "Vision-language models (VLMs) are increasingly deployed in socially sensitive applications, yet their behavior with respect to disability remains underexplored. We study disability aware descriptions for person centric images, where models often transition from evidence grounded factual description to interpretation shift including introduction of unsupported inferences beyond observable visual evidence. To systematically analyze this phenomenon, we introduce a benchmark based on paired Neutral Prompts (NP) and Disability-Contextualised Prompts (DP) and evaluate 15 state-of-the-art open- and closed-source VLMs under a zero-shot setting across 9 disability categories. Our evaluation framework treats interpretive fidelity as core objective and combines standard text-based metrics capturing affective degradation through shifts in sentiment, social regard and response length with an LLM-as-judge protocol, validated by annotators with lived experience of disability. We find that introducing disability context consistently degrades interpretive fidelity, inducing interpretation shifts characterised by speculative inference, narrative elaboration, affective degradation and deficit oriented framing. These effects are further amplified along race and gender dimension. Finally, we demonstrate targeted prompting and preference fine-tuning effectively improves interpretive fidelity and reduces substantially interpretation shifts.", "AI": {"tldr": "VLMs show systematic interpretation shifts when describing people with disabilities, introducing unsupported inferences and negative framing that worsen with intersectional identities, but targeted interventions can improve fidelity.", "motivation": "VLMs are increasingly used in sensitive social applications, but their behavior regarding disability remains poorly understood. There's a need to systematically analyze how these models handle disability context in person-centric image descriptions, particularly when they shift from factual observation to unsupported interpretive inferences.", "method": "Created a benchmark with paired Neutral Prompts (NP) and Disability-Contextualised Prompts (DP) for 9 disability categories. Evaluated 15 state-of-the-art open/closed VLMs in zero-shot settings. Used text-based metrics for affective degradation (sentiment, social regard, length) plus LLM-as-judge protocol validated by disabled annotators.", "result": "Disability context consistently degrades interpretive fidelity, causing interpretation shifts including speculative inference, narrative elaboration, affective degradation, and deficit-oriented framing. These effects amplify along race and gender dimensions (intersectional bias).", "conclusion": "Targeted prompting and preference fine-tuning effectively improve interpretive fidelity and substantially reduce interpretation shifts, offering practical interventions for more accurate disability representation in VLMs."}}
{"id": "2601.17550", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.17550", "abs": "https://arxiv.org/abs/2601.17550", "authors": ["Deepak Singh", "Shreyas Khobragade", "Nitin J. Sanket"], "title": "AsterNav: Autonomous Aerial Robot Navigation In Darkness Using Passive Computation", "comment": "8 pages, 10 figures, Published in IEEE Robotics And Automation Letters", "summary": "Autonomous aerial navigation in absolute darkness is crucial for post-disaster search and rescue operations, which often occur from disaster-zone power outages. Yet, due to resource constraints, tiny aerial robots, perfectly suited for these operations, are unable to navigate in the darkness to find survivors safely. In this paper, we present an autonomous aerial robot for navigation in the dark by combining an Infra-Red (IR) monocular camera with a large-aperture coded lens and structured light without external infrastructure like GPS or motion-capture. Our approach obtains depth-dependent defocus cues (each structured light point appears as a pattern that is depth dependent), which acts as a strong prior for our AsterNet deep depth estimation model. The model is trained in simulation by generating data using a simple optical model and transfers directly to the real world without any fine-tuning or retraining. AsterNet runs onboard the robot at 20 Hz on an NVIDIA Jetson Orin$^\\text{TM}$ Nano. Furthermore, our network is robust to changes in the structured light pattern and relative placement of the pattern emitter and IR camera, leading to simplified and cost-effective construction. We successfully evaluate and demonstrate our proposed depth navigation approach AsterNav using depth from AsterNet in many real-world experiments using only onboard sensing and computation, including dark matte obstacles and thin ropes (diameter 6.25mm), achieving an overall success rate of 95.5% with unknown object shapes, locations and materials. To the best of our knowledge, this is the first work on monocular, structured-light-based quadrotor navigation in absolute darkness.", "AI": {"tldr": "AsterNav: Autonomous aerial robot navigation in absolute darkness using IR camera with coded lens and structured light for depth estimation without external infrastructure.", "motivation": "Post-disaster search and rescue operations often occur in darkness due to power outages, requiring tiny aerial robots to navigate safely without GPS or motion-capture systems.", "method": "Combines IR monocular camera with large-aperture coded lens and structured light to obtain depth-dependent defocus cues, uses AsterNet deep depth estimation model trained in simulation with simple optical model, runs onboard at 20 Hz.", "result": "Achieves 95.5% success rate in real-world experiments with dark matte obstacles and thin ropes (6.25mm diameter), robust to pattern changes and emitter-camera placement, transfers directly from simulation to real world without fine-tuning.", "conclusion": "First monocular structured-light-based quadrotor navigation system for absolute darkness, enabling cost-effective autonomous aerial navigation in post-disaster scenarios without external infrastructure."}}
{"id": "2601.17047", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2601.17047", "abs": "https://arxiv.org/abs/2601.17047", "authors": ["Yuanjie Gu", "Yiqun Wang", "Chaohui Yu", "Ang Xuan", "Fan Wang", "Zhi Lu", "Biqin Dong"], "title": "A Contrastive Pre-trained Foundation Model for Deciphering Imaging Noisomics across Modalities", "comment": null, "summary": "Characterizing imaging noise is notoriously data-intensive and device-dependent, as modern sensors entangle physical signals with complex algorithmic artifacts. Current paradigms struggle to disentangle these factors without massive supervised datasets, often reducing noise to mere interference rather than an information resource. Here, we introduce \"Noisomics\", a framework shifting the focus from suppression to systematic noise decoding via the Contrastive Pre-trained (CoP) Foundation Model. By leveraging the manifold hypothesis and synthetic noise genome, CoP employs contrastive learning to disentangle semantic signals from stochastic perturbations. Crucially, CoP breaks traditional deep learning scaling laws, achieving superior performance with only 100 training samples, outperforming supervised baselines trained on 100,000 samples, thereby reducing data and computational dependency by three orders of magnitude. Extensive benchmarking across 12 diverse out-of-domain datasets confirms its robust zero-shot generalization, demonstrating a 63.8% reduction in estimation error and an 85.1% improvement in the coefficient of determination compared to the conventional training strategy. We demonstrate CoP's utility across scales: from deciphering non-linear hardware-noise interplay in consumer photography to optimizing photon-efficient protocols for deep-tissue microscopy. By decoding noise as a multi-parametric footprint, our work redefines stochastic degradation as a vital information resource, empowering precise imaging diagnostics without prior device calibration.", "AI": {"tldr": "Noisomics framework uses contrastive pre-trained model to decode imaging noise as information resource, achieving 1000x data/compute reduction and strong zero-shot generalization across imaging domains.", "motivation": "Current noise characterization methods are data-intensive and device-dependent, treating noise as interference rather than information. There's a need to disentangle physical signals from algorithmic artifacts without massive supervised datasets.", "method": "Introduces \"Noisomics\" framework with Contrastive Pre-trained (CoP) Foundation Model that uses contrastive learning and synthetic noise genome to disentangle semantic signals from stochastic perturbations on the data manifold.", "result": "CoP breaks deep learning scaling laws, achieving superior performance with only 100 training samples vs. 100,000 for baselines (1000x reduction). Shows 63.8% error reduction and 85.1% R\u00b2 improvement, with robust zero-shot generalization across 12 diverse datasets.", "conclusion": "Noisomics redefines noise as a multi-parametric information resource rather than degradation, enabling precise imaging diagnostics without device calibration and empowering applications from consumer photography to deep-tissue microscopy."}}
{"id": "2601.17426", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2601.17426", "abs": "https://arxiv.org/abs/2601.17426", "authors": ["Zhengqing Zang", "Yuqi Ding", "Yanmei Gu", "Changkai Song", "Zhengkai Yang", "Guoping Du", "Junbo Zhao", "Haobo Wang"], "title": "A Syllogistic Probe: Tracing the Evolution of Logic Reasoning in Large Language Models", "comment": null, "summary": "Human logic has gradually shifted from intuition-driven inference to rigorous formal systems. Motivated by recent advances in large language models (LLMs), we explore whether LLMs exhibit a similar evolution in the underlying logical framework. Using existential import as a probe, we for evaluate syllogism under traditional and modern logic. Through extensive experiments of testing SOTA LLMs on a new syllogism dataset, we have some interesting findings: (i) Model size scaling promotes the shift toward modern logic; (ii) Thinking serves as an efficient accelerator beyond parameter scaling; (iii) the Base model plays a crucial role in determining how easily and stably this shift can emerge. Beyond these core factors, we conduct additional experiments for in-depth analysis of properties of current LLMs on syllogistic reasoning.", "AI": {"tldr": "LLMs show evolution in logical frameworks from traditional to modern logic, influenced by model size scaling, thinking processes, and base model architecture.", "motivation": "To investigate whether large language models (LLMs) exhibit a similar evolution in logical frameworks as humans - from intuition-driven inference to rigorous formal systems - using existential import as a probe for syllogistic reasoning.", "method": "Using existential import to evaluate syllogism under traditional vs. modern logic, testing state-of-the-art LLMs on a new syllogism dataset with extensive experiments examining model size scaling, thinking processes, and base model architecture.", "result": "Three key findings: (1) Model size scaling promotes shift toward modern logic, (2) Thinking serves as efficient accelerator beyond parameter scaling, (3) Base model plays crucial role in determining how easily and stably this shift emerges.", "conclusion": "LLMs demonstrate evolution in logical frameworks similar to human cognitive development, with model architecture, scaling, and reasoning processes significantly influencing their transition from traditional to modern logical reasoning patterns."}}
{"id": "2601.17556", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.17556", "abs": "https://arxiv.org/abs/2601.17556", "authors": ["Ulices Santa Cruz", "Mahmoud Elfar", "Yasser Shoukry"], "title": "Correct-by-Construction Vision-based Pose Estimation using Geometric Generative Models", "comment": null, "summary": "We consider the problem of vision-based pose estimation for autonomous systems. While deep neural networks have been successfully used for vision-based tasks, they inherently lack provable guarantees on the correctness of their output, which is crucial for safety-critical applications. We present a framework for designing certifiable neural networks (NNs) for perception-based pose estimation that integrates physics-driven modeling with learning-based estimation. The proposed framework begins by leveraging the known geometry of planar objects commonly found in the environment, such as traffic signs and runway markings, referred to as target objects. At its core, it introduces a geometric generative model (GGM), a neural-network-like model whose parameters are derived from the image formation process of a target object observed by a camera. Once designed, the GGM can be used to train NN-based pose estimators with certified guarantees in terms of their estimation errors. We first demonstrate this framework in uncluttered environments, where the target object is the only object present in the camera's field of view. We extend this using ideas from NN reachability analysis to design certified object NN that can detect the presence of the target object in cluttered environments. Subsequently, the framework consolidates the certified object detector with the certified pose estimator to design a multi-stage perception pipeline that generalizes the proposed approach to cluttered environments, while maintaining its certified guarantees. We evaluate the proposed framework using both synthetic and real images of various planar objects commonly encountered by autonomous vehicles. Using images captured by an event-based camera, we show that the trained encoder can effectively estimate the pose of a traffic sign in accordance with the certified bound provided by the framework.", "AI": {"tldr": "A framework for designing certifiable neural networks for vision-based pose estimation that combines physics-driven modeling with learning to provide provable guarantees on estimation errors, even in cluttered environments.", "motivation": "Deep neural networks for vision-based pose estimation lack provable guarantees on correctness, which is crucial for safety-critical autonomous systems. There's a need for certifiable neural networks that can provide guaranteed error bounds.", "method": "Proposes a framework using geometric generative models (GGMs) - neural-network-like models whose parameters come from the image formation process of planar target objects. Uses physics-driven modeling combined with learning, and extends to cluttered environments using NN reachability analysis for certified object detection.", "result": "The framework successfully trains neural network pose estimators with certified error bounds. Evaluated on synthetic and real images of planar objects, including event-based camera images of traffic signs, showing effective pose estimation within certified bounds.", "conclusion": "The proposed framework enables the design of certifiable neural networks for perception-based pose estimation that maintain provable guarantees even in cluttered environments, addressing a critical need for safety-critical autonomous systems."}}
{"id": "2601.17048", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17048", "abs": "https://arxiv.org/abs/2601.17048", "authors": ["Jing Jie Tan", "Rupert Schreiner", "Matthias Hausladen", "Ali Asgharzade", "Simon Edler", "Julian Bartsch", "Michael Bachmann", "Andreas Schels", "Ban-Hoe Kwan", "Danny Wee-Kiat Ng", "Yan-Chai Hum"], "title": "SiMiC: Context-Aware Silicon Microstructure Characterization Using Attention-Based Convolutional Neural Networks for Field-Emission Tip Analysis", "comment": null, "summary": "Accurate characterization of silicon microstructures is essential for advancing microscale fabrication, quality control, and device performance. Traditional analysis using Scanning Electron Microscopy (SEM) often requires labor-intensive, manual evaluation of feature geometry, limiting throughput and reproducibility. In this study, we propose SiMiC: Context-Aware Silicon Microstructure Characterization Using Attention-Based Convolutional Neural Networks for Field-Emission Tip Analysis. By leveraging deep learning, our approach efficiently extracts morphological features-such as size, shape, and apex curvature-from SEM images, significantly reducing human intervention while improving measurement consistency. A specialized dataset of silicon-based field-emitter tips was developed, and a customized CNN architecture incorporating attention mechanisms was trained for multi-class microstructure classification and dimensional prediction. Comparative analysis with classical image processing techniques demonstrates that SiMiC achieves high accuracy while maintaining interpretability. The proposed framework establishes a foundation for data-driven microstructure analysis directly linked to field-emission performance, opening avenues for correlating emitter geometry with emission behavior and guiding the design of optimized cold-cathode and SEM electron sources. The related dataset and algorithm repository that could serve as a baseline in this area can be found at https://research.jingjietan.com/?q=SIMIC", "AI": {"tldr": "SiMiC uses attention-based CNNs to automatically analyze silicon microstructure features from SEM images, reducing manual labor and improving consistency for field-emitter tip characterization.", "motivation": "Traditional SEM analysis requires labor-intensive manual evaluation of feature geometry, limiting throughput and reproducibility in silicon microstructure characterization for field-emission applications.", "method": "Developed a specialized dataset of silicon field-emitter tips and trained a customized CNN architecture with attention mechanisms for multi-class microstructure classification and dimensional prediction from SEM images.", "result": "SiMiC achieves high accuracy compared to classical image processing techniques while maintaining interpretability, significantly reducing human intervention and improving measurement consistency.", "conclusion": "The framework establishes a foundation for data-driven microstructure analysis linked to field-emission performance, enabling correlation of emitter geometry with emission behavior and guiding optimized electron source design."}}
{"id": "2601.17481", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17481", "abs": "https://arxiv.org/abs/2601.17481", "authors": ["Emily Broadhurst", "Tawab Safi", "Joseph Edell", "Vashisht Ganesh", "Karime Maamari"], "title": "Lattice: Generative Guardrails for Conversational Agents", "comment": null, "summary": "Conversational AI systems require guardrails to prevent harmful outputs, yet existing approaches use static rules that cannot adapt to new threats or deployment contexts. We introduce Lattice, a framework for self-constructing and continuously improving guardrails. Lattice operates in two stages: construction builds initial guardrails from labeled examples through iterative simulation and optimization; continuous improvement autonomously adapts deployed guardrails through risk assessment, adversarial testing, and consolidation. Evaluated on the ProsocialDialog dataset, Lattice achieves 91% F1 on held-out data, outperforming keyword baselines by 43pp, LlamaGuard by 25pp, and NeMo by 4pp. The continuous improvement stage achieves 7pp F1 improvement on cross-domain data through closed-loop optimization. Our framework shows that effective guardrails can be self-constructed through iterative optimization.", "AI": {"tldr": "Lattice is a self-constructing framework for conversational AI guardrails that builds initial protections from examples and continuously improves them through autonomous adaptation, outperforming existing methods by significant margins.", "motivation": "Existing conversational AI guardrails use static rules that cannot adapt to new threats or different deployment contexts, creating vulnerabilities in dynamic environments.", "method": "Two-stage framework: 1) Construction builds initial guardrails from labeled examples through iterative simulation and optimization; 2) Continuous improvement autonomously adapts deployed guardrails through risk assessment, adversarial testing, and consolidation.", "result": "Achieves 91% F1 on ProsocialDialog dataset, outperforming keyword baselines by 43 percentage points, LlamaGuard by 25pp, and NeMo by 4pp. Continuous improvement stage achieves 7pp F1 improvement on cross-domain data.", "conclusion": "Effective conversational AI guardrails can be self-constructed through iterative optimization, demonstrating that autonomous adaptation significantly improves safety performance over static approaches."}}
{"id": "2601.17812", "categories": ["cs.RO", "cs.HC", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.17812", "abs": "https://arxiv.org/abs/2601.17812", "authors": ["Mingtian Du", "Suhas Raghavendra Kulkarni", "Bernardo Noronha", "Domenico Campolo"], "title": "Delay-Compensated Stiffness Estimation for Robot-Mediated Dyadic Interaction", "comment": null, "summary": "Robot-mediated human-human (dyadic) interactions enable therapists to provide physical therapy remotely, yet an accurate perception of patient stiffness remains challenging due to network-induced haptic delays. Conventional stiffness estimation methods, which neglect delay, suffer from temporal misalignment between force and position signals, leading to significant estimation errors as delays increase. To address this, we propose a robust, delay-compensated stiffness estimation framework by deriving an algebraic estimator based on quasi-static equilibrium that explicitly accounts for temporally aligning the expert's input with the novice's response. A Normalised Weighted Least Squares (NWLS) implementation is then introduced to robustly filter dynamic bias resulting from the algebraic derivation. Experiments using commercial rehabilitation robots (H-MAN) as the platform demonstrate that the proposed method significantly outperforms the standard estimator, maintaining consistent tracking accuracy under multiple introduced delays. These findings offer a promising solution for achieving high-fidelity haptic perception in remote dyadic interaction, potentially facilitating reliable stiffness assessment in therapeutic settings across networks.", "AI": {"tldr": "Proposed delay-compensated stiffness estimation framework for remote rehabilitation robots that maintains accuracy despite network delays by temporally aligning force and position signals.", "motivation": "Network-induced haptic delays in robot-mediated dyadic interactions cause temporal misalignment between force and position signals, leading to inaccurate stiffness estimation in remote physical therapy settings.", "method": "Developed algebraic estimator based on quasi-static equilibrium that explicitly accounts for temporal alignment, then implemented Normalised Weighted Least Squares (NWLS) to filter dynamic bias from algebraic derivation.", "result": "Experiments on H-MAN rehabilitation robots show proposed method significantly outperforms standard estimator, maintaining consistent tracking accuracy under multiple introduced delays.", "conclusion": "The framework offers promising solution for high-fidelity haptic perception in remote dyadic interactions, potentially enabling reliable stiffness assessment in therapeutic settings across networks."}}
{"id": "2601.17049", "categories": ["cs.CV", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.17049", "abs": "https://arxiv.org/abs/2601.17049", "authors": ["Christina Garcia", "Nhat Tan Le", "Taihei Fujioka", "Umang Dobhal", "Milyun Ni'ma Shoumi", "Thanh Nha Nguyen", "Sozo Inoue"], "title": "Summary of the Unusual Activity Recognition Challenge for Developmental Disability Support", "comment": "14 pages, 7 figures, 3 tables. Summary paper for a coding challenge hosted in ISAS 2025", "summary": "This paper presents an overview of the Recognize the Unseen: Unusual Behavior Recognition from Pose Data Challenge, hosted at ISAS 2025. The challenge aims to address the critical need for automated recognition of unusual behaviors in facilities for individuals with developmental disabilities using non-invasive pose estimation data. Participating teams were tasked with distinguishing between normal and unusual activities based on skeleton keypoints extracted from video recordings of simulated scenarios. The dataset reflects real-world imbalance and temporal irregularities in behavior, and the evaluation adopted a Leave-One-Subject-Out (LOSO) strategy to ensure subject-agnostic generalization. The challenge attracted broad participation from 40 teams applying diverse approaches ranging from classical machine learning to deep learning architectures. Submissions were assessed primarily using macro-averaged F1 scores to account for class imbalance. The results highlight the difficulty of modeling rare, abrupt actions in noisy, low-dimensional data, and emphasize the importance of capturing both temporal and contextual nuances in behavior modeling. Insights from this challenge may contribute to future developments in socially responsible AI applications for healthcare and behavior monitoring.", "AI": {"tldr": "Challenge for recognizing unusual behaviors from pose data in developmental disability facilities using skeleton keypoints, with 40 teams applying ML/DL methods evaluated via LOSO strategy and macro-F1 scores.", "motivation": "Address the critical need for automated recognition of unusual behaviors in facilities for individuals with developmental disabilities using non-invasive pose estimation data, enabling better monitoring and care.", "method": "Challenge framework where teams distinguish normal vs unusual activities using skeleton keypoints from video recordings, with LOSO evaluation strategy to ensure subject-agnostic generalization, assessed via macro-averaged F1 scores.", "result": "Broad participation from 40 teams using diverse approaches (classical ML to deep learning), highlighting difficulty of modeling rare, abrupt actions in noisy, low-dimensional data, with emphasis on temporal and contextual nuance importance.", "conclusion": "Challenge insights contribute to future socially responsible AI applications for healthcare and behavior monitoring, demonstrating the complexity of unusual behavior recognition from pose data and need for robust temporal-contextual modeling."}}
{"id": "2601.17542", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.17542", "abs": "https://arxiv.org/abs/2601.17542", "authors": ["Vinoth Punniyamoorthy", "Nitin Saksena", "Srivenkateswara Reddy Sankiti", "Nachiappan Chockalingam", "Aswathnarayan Muthukrishnan Kirubakaran", "Shiva Kumar Reddy Carimireddy", "Durgaraman Maruthavanan"], "title": "Cognitive Platform Engineering for Autonomous Cloud Operations", "comment": null, "summary": "Modern DevOps practices have accelerated software delivery through automation, CI/CD pipelines, and observability tooling,but these approaches struggle to keep pace with the scale and dynamism of cloud-native systems. As telemetry volume grows and configuration drift increases, traditional, rule-driven automation often results in reactive operations, delayed remediation, and dependency on manual expertise. This paper introduces Cognitive Platform Engineering, a next-generation paradigm that integrates sensing, reasoning, and autonomous action directly into the platform lifecycle. This paper propose a four-plane reference architecture that unifies data collection, intelligent inference, policy-driven orchestration, and human experience layers within a continuous feedback loop. A prototype implementation built with Kubernetes, Terraform, Open Policy Agent, and ML-based anomaly detection demonstrates improvements in mean time to resolution, resource efficiency, and compliance. The results show that embedding intelligence into platform operations enables resilient, self-adjusting, and intent-aligned cloud environments. The paper concludes with research opportunities in reinforcement learning, explainable governance, and sustainable self-managing cloud ecosystems.", "AI": {"tldr": "Cognitive Platform Engineering integrates AI-driven sensing, reasoning, and autonomous action into DevOps to create self-adjusting cloud platforms that overcome limitations of traditional rule-based automation.", "motivation": "Traditional DevOps automation struggles with cloud-native scale and dynamism, leading to reactive operations, delayed remediation, and dependency on manual expertise as telemetry volume grows and configuration drift increases.", "method": "Proposes a four-plane reference architecture unifying data collection, intelligent inference, policy-driven orchestration, and human experience layers within a continuous feedback loop, implemented with Kubernetes, Terraform, Open Policy Agent, and ML-based anomaly detection.", "result": "Prototype demonstrates improvements in mean time to resolution, resource efficiency, and compliance, showing that embedding intelligence enables resilient, self-adjusting, and intent-aligned cloud environments.", "conclusion": "Cognitive Platform Engineering enables resilient cloud ecosystems with research opportunities in reinforcement learning, explainable governance, and sustainable self-managing cloud systems."}}
{"id": "2601.17815", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.17815", "abs": "https://arxiv.org/abs/2601.17815", "authors": ["Yves Inglin", "Jonas Frey", "Changan Chen", "Marco Hutter"], "title": "Less Is More: Scalable Visual Navigation from Limited Data", "comment": null, "summary": "Imitation learning provides a powerful framework for goal-conditioned visual navigation in mobile robots, enabling obstacle avoidance while respecting human preferences and social norms. However, its effectiveness depends critically on the quality and diversity of training data. In this work, we show how classical geometric planners can be leveraged to generate synthetic trajectories that complement costly human demonstrations. We train Less is More (LiMo), a transformer-based visual navigation policy that predicts goal-conditioned SE(2) trajectories from a single RGB observation, and find that augmenting limited expert demonstrations with planner-generated supervision yields substantial performance gains. Through ablations and complementary qualitative and quantitative analyses, we characterize how dataset scale and diversity affect planning performance. We demonstrate real-robot deployment and argue that robust visual navigation is enabled not by simply collecting more demonstrations, but by strategically curating diverse, high-quality datasets. Our results suggest that scalable, embodiment-specific geometric supervision is a practical path toward data-efficient visual navigation.", "AI": {"tldr": "Transformer-based visual navigation policy (LiMo) uses geometric planner-generated synthetic trajectories to augment limited human demonstrations, achieving better performance with strategically curated diverse datasets rather than just more data.", "motivation": "Imitation learning for visual navigation depends on quality/diversity of training data, but human demonstrations are costly to collect. Need more efficient approach to training visual navigation policies.", "method": "Train LiMo (transformer-based visual navigation policy) that predicts goal-conditioned SE(2) trajectories from single RGB observation. Augment limited expert demonstrations with synthetic trajectories generated by classical geometric planners.", "result": "Augmenting limited expert demonstrations with planner-generated supervision yields substantial performance gains. Strategic curation of diverse, high-quality datasets outperforms simply collecting more demonstrations. Real-robot deployment demonstrated.", "conclusion": "Scalable, embodiment-specific geometric supervision is a practical path toward data-efficient visual navigation. Robust visual navigation enabled by strategically curating diverse datasets rather than collecting more demonstrations."}}
{"id": "2601.17050", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17050", "abs": "https://arxiv.org/abs/2601.17050", "authors": ["Hongjun An", "Yiliang Song", "Jiawei Shao", "Zhe Sun", "Xuelong Li"], "title": "Single-Pixel Vision-Language Model for Intrinsic Privacy-Preserving Behavioral Intelligence", "comment": "Initial Version, Pending Updates. We welcome any feedback and suggestions for improvement. Please feel free to contact us at an.hongjun@foxmail.com", "summary": "Adverse social interactions, such as bullying, harassment, and other illicit activities, pose significant threats to individual well-being and public safety, leaving profound impacts on physical and mental health. However, these critical events frequently occur in privacy-sensitive environments like restrooms, and changing rooms, where conventional surveillance is prohibited or severely restricted by stringent privacy regulations and ethical concerns. Here, we propose the Single-Pixel Vision-Language Model (SP-VLM), a novel framework that reimagines secure environmental monitoring. It achieves intrinsic privacy-by-design by capturing human dynamics through inherently low-dimensional single-pixel modalities and inferring complex behavioral patterns via seamless vision-language integration. Building on this framework, we demonstrate that single-pixel sensing intrinsically suppresses identity recoverability, rendering state-of-the-art face recognition systems ineffective below a critical sampling rate. We further show that SP-VLM can nonetheless extract meaningful behavioral semantics, enabling robust anomaly detection, people counting, and activity understanding from severely degraded single-pixel observations. Combining these findings, we identify a practical sampling-rate regime in which behavioral intelligence emerges while personal identity remains strongly protected. Together, these results point to a human-rights-aligned pathway for safety monitoring that can support timely intervention without normalizing intrusive surveillance in privacy-sensitive spaces.", "AI": {"tldr": "SP-VLM uses single-pixel sensing + vision-language models for privacy-preserving monitoring in sensitive spaces, enabling behavior analysis while preventing identity recovery.", "motivation": "Need for safety monitoring in privacy-sensitive environments (restrooms, changing rooms) where conventional surveillance is prohibited due to privacy regulations and ethical concerns.", "method": "Single-Pixel Vision-Language Model (SP-VLM) framework that captures human dynamics through low-dimensional single-pixel modalities and infers behavioral patterns via vision-language integration.", "result": "Single-pixel sensing suppresses identity recoverability (face recognition ineffective below critical sampling rate), while SP-VLM can still extract behavioral semantics for anomaly detection, people counting, and activity understanding.", "conclusion": "Identifies practical sampling-rate regime where behavioral intelligence emerges while personal identity remains protected, offering human-rights-aligned pathway for safety monitoring without intrusive surveillance."}}
{"id": "2601.17564", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17564", "abs": "https://arxiv.org/abs/2601.17564", "authors": ["Aadam", "Monu Verma", "Mohamed Abdel-Mottaleb"], "title": "JaxARC: A High-Performance JAX-based Environment for Abstraction and Reasoning Research", "comment": null, "summary": "The Abstraction and Reasoning Corpus (ARC) tests AI systems' ability to perform human-like inductive reasoning from a few demonstration pairs. Existing Gymnasium-based RL environments severely limit experimental scale due to computational bottlenecks. We present JaxARC, an open-source, high-performance RL environment for ARC implemented in JAX. Its functional, stateless architecture enables massive parallelism, achieving 38-5,439x speedup over Gymnasium at matched batch sizes, with peak throughput of 790M steps/second. JaxARC supports multiple ARC datasets, flexible action spaces, composable wrappers, and configuration-driven reproducibility, enabling large-scale RL research previously computationally infeasible. JaxARC is available at https://github.com/aadimator/JaxARC.", "AI": {"tldr": "JaxARC is a high-performance JAX-based RL environment for the Abstraction and Reasoning Corpus that achieves massive speedups over existing implementations, enabling previously infeasible large-scale RL research.", "motivation": "Existing Gymnasium-based RL environments for ARC suffer from computational bottlenecks that severely limit experimental scale, making large-scale RL research on ARC infeasible.", "method": "Implemented a functional, stateless architecture in JAX that enables massive parallelism, supports multiple ARC datasets, flexible action spaces, composable wrappers, and configuration-driven reproducibility.", "result": "Achieved 38-5,439x speedup over Gymnasium at matched batch sizes, with peak throughput of 790M steps/second, making large-scale RL research on ARC computationally feasible.", "conclusion": "JaxARC provides an open-source, high-performance RL environment that enables previously infeasible large-scale RL research on ARC by overcoming computational bottlenecks through JAX-based parallelization."}}
{"id": "2601.17991", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.17991", "abs": "https://arxiv.org/abs/2601.17991", "authors": ["Roman Akinshin", "Elizaveta Lopatina", "Kirill Bogatikov", "Nikolai Kiz", "Anna V. Makarova", "Mikhail Lebedev", "Miguel Altamirano Cabrera", "Dzmitry Tsetserukou", "Valerii Kangler"], "title": "NeuroManip: Prosthetic Hand Manipulation System Based on EMG and Eye Tracking Powered by the Neuromorphic Processor AltAi", "comment": "This paper has been accepted for publication at LBR of HRI 2026 conference", "summary": "This paper presents a novel neuromorphic control architecture for upper-limb prostheses that combines surface electromyography (sEMG) with gaze-guided computer vision. The system uses a spiking neural network deployed on the neuromorphic processor AltAi to classify EMG patterns in real time while an eye-tracking headset and scene camera identify the object within the user's focus. In our prototype, the same EMG recognition model that was originally developed for a conventional GPU is deployed as a spiking network on AltAi, achieving comparable accuracy while operating in a sub-watt power regime, which enables a lightweight, wearable implementation. For six distinct functional gestures recorded from upper-limb amputees, the system achieves robust recognition performance comparable to state-of-the-art myoelectric interfaces. When the vision pipeline restricts the decision space to three context-appropriate gestures for the currently viewed object, recognition accuracy increases to roughly 95% while excluding unsafe, object-inappropriate grasps. These results indicate that the proposed neuromorphic, context-aware controller can provide energy-efficient and reliable prosthesis control and has the potential to improve safety and usability in everyday activities for people with upper-limb amputation.", "AI": {"tldr": "Neuromorphic prosthesis control combining EMG and gaze-guided vision achieves 95% accuracy with sub-watt power consumption.", "motivation": "To create an energy-efficient, wearable upper-limb prosthesis controller that improves safety and usability by combining EMG pattern recognition with context-aware vision guidance.", "method": "Uses spiking neural network on AltAi processor for real-time EMG classification, combined with eye-tracking headset and scene camera to identify objects in user's focus, restricting gesture options to context-appropriate ones.", "result": "Achieves robust recognition for six functional gestures comparable to state-of-the-art myoelectric interfaces. When vision restricts decision space to three context-appropriate gestures, accuracy increases to ~95% while excluding unsafe grasps, all operating in sub-watt power regime.", "conclusion": "The neuromorphic, context-aware controller provides energy-efficient, reliable prosthesis control with potential to improve safety and usability for upper-limb amputees in daily activities."}}
{"id": "2601.17053", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17053", "abs": "https://arxiv.org/abs/2601.17053", "authors": ["Shuhao Que", "Dieuwke van Dartel", "Ilse Heeringa", "Han Hegeman", "Miriam Vollenbroek-Hutten", "Ying Wang"], "title": "Synthetic Data Guided Feature Selection for Robust Activity Recognition in Older Adults", "comment": "This paper has been submitted to Nordic Conference on Digital Health and Wireless Solutions 2026, currently under review", "summary": "Physical activity during hip fracture rehabilitation is essential for mitigating long-term functional decline in geriatric patients. However, it is rarely quantified in clinical practice. Existing continuous monitoring systems with commercially available wearable activity trackers are typically developed in middle-aged adults and therefore perform unreliably in older adults with slower and more variable gait patterns. This study aimed to develop a robust human activity recognition (HAR) system to improve continuous physical activity recognition in the context of hip fracture rehabilitation. 24 healthy older adults aged over 80 years were included to perform activities of daily living (walking, standing, sitting, lying down, and postural transfers) under simulated free-living conditions for 75 minutes while wearing two accelerometers positioned on the lower back and anterior upper thigh. Model robustness was evaluated using leave-one-subject-out cross-validation. The synthetic data demonstrated potential to improve generalization across participants. The resulting feature intervention model (FIM), aided by synthetic data guidance, achieved reliable activity recognition with mean F1-scores of 0.896 for walking, 0.927 for standing, 0.997 for sitting, 0.937 for lying down, and 0.816 for postural transfers. Compared with a control condition model without synthetic data, the FIM significantly improved the postural transfer detection, i.e., an activity class of high clinical relevance that is often overlooked in existing HAR literature. In conclusion, these preliminary results demonstrate the feasibility of robust activity recognition in older adults. Further validation in hip fracture patient populations is required to assess the clinical utility of the proposed monitoring system.", "AI": {"tldr": "Researchers developed a robust human activity recognition system using synthetic data to improve physical activity monitoring in older adults during hip fracture rehabilitation, achieving high accuracy across daily living activities.", "motivation": "Physical activity monitoring during hip fracture rehabilitation is crucial but rarely quantified in clinical practice. Existing wearable systems developed for middle-aged adults perform poorly in older adults with slower, more variable gait patterns.", "method": "Studied 24 healthy adults over 80 performing daily activities (walking, standing, sitting, lying down, postural transfers) for 75 minutes while wearing accelerometers on lower back and upper thigh. Used leave-one-subject-out cross-validation and developed a feature intervention model (FIM) aided by synthetic data to improve generalization.", "result": "FIM achieved mean F1-scores: 0.896 (walking), 0.927 (standing), 0.997 (sitting), 0.937 (lying down), 0.816 (postural transfers). Synthetic data significantly improved postural transfer detection compared to control model without synthetic data.", "conclusion": "Preliminary results demonstrate feasibility of robust activity recognition in older adults. Further validation needed in hip fracture patient populations to assess clinical utility of the monitoring system."}}
{"id": "2601.17587", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17587", "abs": "https://arxiv.org/abs/2601.17587", "authors": ["Azza Fadhel", "Nathaniel W. Zuckschwerdt", "Aryan Deshwal", "Susmita Bose", "Amit Bandyopadhyay", "Jana Doppa"], "title": "Discovery of Feasible 3D Printing Configurations for Metal Alloys via AI-driven Adaptive Experimental Design", "comment": "Proceedings of Innovative Applications of AI (IAAI) 2026 Conference", "summary": "Configuring the parameters of additive manufacturing processes for metal alloys is a challenging problem due to complex relationships between input parameters (e.g., laser power, scan speed) and quality of printed outputs. The standard trial-and-error approach to find feasible parameter configurations is highly inefficient because validating each configuration is expensive in terms of resources (physical and human labor) and the configuration space is very large. This paper combines the general principles of AI-driven adaptive experimental design with domain knowledge to address the challenging problem of discovering feasible configurations. The key idea is to build a surrogate model from past experiments to intelligently select a small batch of input configurations for validation in each iteration. To demonstrate the effectiveness of this methodology, we deploy it for Directed Energy Deposition process to print GRCop--42, a high-performance copper--chromium--niobium alloy developed by NASA for aerospace applications. Within three months, our approach yielded multiple defect-free outputs across a range of laser powers dramatically reducing time to result and resource expenditure compared to several months of manual experimentation by domain scientists with no success. By enabling high-quality GRCop--42 fabrication on readily available infrared laser platforms for the first time, we democratize access to this critical alloy, paving the way for cost-effective, decentralized production for aerospace applications.", "AI": {"tldr": "AI-driven adaptive experimental design with domain knowledge dramatically accelerates discovery of feasible additive manufacturing parameters for metal alloys, successfully enabling high-quality GRCop-42 fabrication.", "motivation": "Traditional trial-and-error parameter configuration for metal additive manufacturing is highly inefficient and resource-intensive, with large configuration spaces making manual experimentation impractical for discovering feasible settings.", "method": "Combines AI-driven adaptive experimental design with domain knowledge, using surrogate models from past experiments to intelligently select small batches of input configurations for validation in each iteration.", "result": "Within three months, the approach yielded multiple defect-free outputs across a range of laser powers for GRCop-42 alloy, dramatically reducing time and resources compared to several months of manual experimentation with no success.", "conclusion": "The methodology democratizes access to critical aerospace alloys by enabling high-quality fabrication on readily available infrared laser platforms, paving the way for cost-effective, decentralized production for aerospace applications."}}
{"id": "2601.18121", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18121", "abs": "https://arxiv.org/abs/2601.18121", "authors": ["Byeonggyeol Choi", "Woojin Oh", "Jongwoo Lim"], "title": "Grasp-and-Lift: Executable 3D Hand-Object Interaction Reconstruction via Physics-in-the-Loop Optimization", "comment": "13 pages, 7 figures", "summary": "Dexterous hand manipulation increasingly relies on large-scale motion datasets with precise hand-object trajectory data. However, existing resources such as DexYCB and HO3D are primarily optimized for visual alignment but often yield physically implausible interactions when replayed in physics simulators, including penetration, missed contact, and unstable grasps.\n  We propose a simulation-in-the-loop refinement framework that converts these visually aligned trajectories into physically executable ones. Our core contribution is to formulate this as a tractable black-box optimization problem. We parameterize the hand's motion using a low-dimensional, spline-based representation built on sparse temporal keyframes. This allows us to use a powerful gradient-free optimizer, CMA-ES, to treat the high-fidelity physics engine as a black-box objective function. Our method finds motions that simultaneously maximize physical success (e.g., stable grasp and lift) while minimizing deviation from the original human demonstration.\n  Compared to MANIPTRANS-recent transfer pipelines, our approach achieves lower hand and object pose errors during replay and more accurately recovers hand-object physical interactions. Our approach provides a general and scalable method for converting visual demonstrations into physically valid trajectories, enabling the generation of high-fidelity data crucial for robust policy learning.", "AI": {"tldr": "Simulation-in-the-loop refinement framework converts visually aligned hand-object trajectories into physically executable ones using black-box optimization with CMA-ES and spline-based motion parameterization.", "motivation": "Existing hand manipulation datasets (DexYCB, HO3D) are optimized for visual alignment but produce physically implausible interactions in physics simulators (penetration, missed contact, unstable grasps), limiting their usefulness for robust policy learning.", "method": "Formulates refinement as tractable black-box optimization problem using low-dimensional spline-based motion representation with sparse temporal keyframes. Uses gradient-free CMA-ES optimizer to treat high-fidelity physics engine as black-box objective function, maximizing physical success while minimizing deviation from original demonstrations.", "result": "Outperforms MANIPTRANS transfer pipelines with lower hand and object pose errors during replay and more accurate recovery of hand-object physical interactions.", "conclusion": "Provides general and scalable method for converting visual demonstrations into physically valid trajectories, enabling generation of high-fidelity data crucial for robust policy learning in dexterous hand manipulation."}}
{"id": "2601.17056", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17056", "abs": "https://arxiv.org/abs/2601.17056", "authors": ["Zahra Vaseqi", "James Clark"], "title": "Ego4OOD: Rethinking Egocentric Video Domain Generalization via Covariate Shift Scoring", "comment": null, "summary": "Egocentric video action recognition under domain shifts remains challenging due to large intra-class spatio-temporal variability, long-tailed feature distributions, and strong correlations between actions and environments. Existing benchmarks for egocentric domain generalization often conflate covariate shifts with concept shifts, making it difficult to reliably evaluate a model's ability to generalize across input distributions. To address this limitation, we introduce Ego4OOD, a domain generalization benchmark derived from Ego4D that emphasizes measurable covariate diversity while reducing concept shift through semantically coherent, moment-level action categories. Ego4OOD spans eight geographically distinct domains and is accompanied by a clustering-based covariate shift metric that provides a quantitative proxy for domain difficulty. We further leverage a one-vs-all binary training objective that decomposes multi-class action recognition into independent binary classification tasks. This formulation is particularly well-suited for covariate shift by reducing interference between visually similar classes under feature distribution shift. Using this formulation, we show that a lightweight two-layer fully connected network achieves performance competitive with state-of-the-art egocentric domain generalization methods on both Argo1M and Ego4OOD, despite using fewer parameters and no additional modalities. Our empirical analysis demonstrates a clear relationship between measured covariate shift and recognition performance, highlighting the importance of controlled benchmarks and quantitative domain characterization for studying out-of-distribution generalization in egocentric video.", "AI": {"tldr": "Ego4OOD: A new domain generalization benchmark for egocentric video action recognition that focuses on covariate shifts, with a clustering-based metric to quantify domain difficulty and a binary training approach that achieves competitive performance with minimal parameters.", "motivation": "Existing benchmarks for egocentric domain generalization often conflate covariate shifts with concept shifts, making it difficult to reliably evaluate models' ability to generalize across input distributions. Egocentric video faces challenges like large intra-class variability, long-tailed distributions, and strong action-environment correlations.", "method": "1) Introduce Ego4OOD benchmark from Ego4D with eight geographically distinct domains, using semantically coherent moment-level action categories to reduce concept shift. 2) Develop clustering-based covariate shift metric to quantify domain difficulty. 3) Propose one-vs-all binary training objective that decomposes multi-class recognition into independent binary classification tasks.", "result": "A lightweight two-layer fully connected network achieves performance competitive with state-of-the-art egocentric domain generalization methods on both Argo1M and Ego4OOD, despite using fewer parameters and no additional modalities. Shows clear relationship between measured covariate shift and recognition performance.", "conclusion": "The paper demonstrates the importance of controlled benchmarks and quantitative domain characterization for studying out-of-distribution generalization in egocentric video. The binary training formulation is particularly effective for covariate shift scenarios by reducing interference between visually similar classes under feature distribution shifts."}}
{"id": "2601.17588", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17588", "abs": "https://arxiv.org/abs/2601.17588", "authors": ["Marcus Ma", "Shrikanth Narayanan"], "title": "Intelligence Requires Grounding But Not Embodiment", "comment": null, "summary": "Recent advances in LLMs have reignited scientific debate over whether embodiment is necessary for intelligence. We present the argument that intelligence requires grounding, a phenomenon entailed by embodiment, but not embodiment itself. We define intelligence as the possession of four properties -- motivation, predictive ability, understanding of causality, and learning from experience -- and argue that each can be achieved by a non-embodied, grounded agent. We use this to conclude that grounding, not embodiment, is necessary for intelligence. We then present a thought experiment of an intelligent LLM agent in a digital environment and address potential counterarguments.", "AI": {"tldr": "The paper argues that grounding (not embodiment) is necessary for intelligence, defining intelligence through four properties that can be achieved by non-embodied but grounded agents.", "motivation": "To address the scientific debate about whether embodiment is necessary for intelligence in light of recent LLM advances, proposing that grounding (which embodiment entails) is the actual requirement.", "method": "Defines intelligence as possessing four properties (motivation, predictive ability, understanding causality, learning from experience), argues each can be achieved by non-embodied grounded agents, and presents a thought experiment of an intelligent LLM agent in a digital environment.", "result": "Concludes that grounding, not embodiment, is necessary for intelligence, and addresses potential counterarguments to this position.", "conclusion": "Intelligence requires grounding (which embodiment provides) rather than embodiment itself, meaning non-embodied but grounded agents (like LLMs in digital environments) can be intelligent."}}
{"id": "2601.18289", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.18289", "abs": "https://arxiv.org/abs/2601.18289", "authors": ["Jialong Li", "Zhenguo Wang", "Tianci Wang", "Maj Stenmark", "Volker Krueger"], "title": "Quest2ROS2: A ROS 2 Framework for Bi-manual VR Teleoperation", "comment": "HRI 2026", "summary": "Quest2ROS2 is an open-source ROS2 framework for bi-manual teleoperation designed to scale robot data collection. Extending Quest2ROS, it overcomes workspace limitations via relative motion-based control, calculating robot movement from VR controller pose changes to enable intuitive, pose-independent operation. The framework integrates essential usability and safety features, including real-time RViz visualization, streamlined gripper control, and a pause-and-reset function for smooth transitions. We detail a modular architecture that supports \"Side-by-Side\" and \"Mirror\" control modes to optimize operator experience across diverse platforms. Code is available at: https://github.com/Taokt/Quest2ROS2.", "AI": {"tldr": "Quest2ROS2 is an open-source ROS2 framework for bi-manual teleoperation that enables intuitive VR-based robot control with relative motion, overcoming workspace limitations for scalable robot data collection.", "motivation": "The paper addresses the need for scalable robot data collection through teleoperation, specifically overcoming workspace limitations in existing VR teleoperation systems to enable more flexible and intuitive control of bi-manual robots.", "method": "The framework extends Quest2ROS with relative motion-based control that calculates robot movement from VR controller pose changes, enabling pose-independent operation. It features a modular architecture supporting \"Side-by-Side\" and \"Mirror\" control modes, with integrated real-time RViz visualization, streamlined gripper control, and pause-and-reset functionality.", "result": "The developed Quest2ROS2 framework provides an open-source solution for bi-manual teleoperation that enables intuitive, workspace-unconstrained control of robots, with essential usability and safety features for practical deployment.", "conclusion": "Quest2ROS2 successfully addresses workspace limitations in VR teleoperation through relative motion control, providing a scalable framework for robot data collection with improved operator experience across diverse robotic platforms."}}
{"id": "2601.17062", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17062", "abs": "https://arxiv.org/abs/2601.17062", "authors": ["Robert M. Belcher", "Brendan C. Degryse", "Leonard R. Kosta", "Christopher J. Lowrance"], "title": "A Computer Vision Pipeline for Iterative Bullet Hole Tracking in Rifle Zeroing", "comment": "Presented at the 2025 MIT Undergraduate Research Technology Conference (URTC)", "summary": "Adjusting rifle sights, a process commonly called \"zeroing,\" requires shooters to identify and differentiate bullet holes from multiple firing iterations. Traditionally, this process demands physical inspection, introducing delays due to range safety protocols and increasing the risk of human error. We present an end-to-end computer vision system for automated bullet hole detection and iteration-based tracking directly from images taken at the firing line. Our approach combines YOLOv8 for accurate small-object detection with Intersection over Union (IoU) analysis to differentiate bullet holes across sequential images. To address the scarcity of labeled sequential data, we propose a novel data augmentation technique that removes rather than adds objects to simulate realistic firing sequences. Additionally, we introduce a preprocessing pipeline that standardizes target orientation using ORB-based perspective correction, improving model accuracy. Our system achieves 97.0% mean average precision on bullet hole detection and 88.8% accuracy in assigning bullet holes to the correct firing iteration. While designed for rifle zeroing, this framework offers broader applicability in domains requiring the temporal differentiation of visually similar objects.", "AI": {"tldr": "Automated computer vision system for bullet hole detection and tracking across firing iterations using YOLOv8, IoU analysis, and novel data augmentation for rifle zeroing.", "motivation": "Traditional rifle zeroing requires physical inspection of targets, causing delays due to range safety protocols and increasing human error risk. There's a need for automated visual analysis to streamline the process.", "method": "Combines YOLOv8 for small-object detection with IoU analysis for temporal tracking across sequential images. Uses novel data augmentation that removes objects to simulate firing sequences, and ORB-based perspective correction for target orientation standardization.", "result": "Achieves 97.0% mean average precision on bullet hole detection and 88.8% accuracy in assigning bullet holes to correct firing iteration.", "conclusion": "System successfully automates rifle zeroing process with high accuracy. Framework has broader applicability for temporal differentiation of visually similar objects beyond firearms training."}}
{"id": "2601.17642", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17642", "abs": "https://arxiv.org/abs/2601.17642", "authors": ["Zhihao Zhang", "Liting Huang", "Guanghao Wu", "Preslav Nakov", "Heng Ji", "Usman Naseem"], "title": "Health-ORSC-Bench: A Benchmark for Measuring Over-Refusal and Safety Completion in Health Context", "comment": "Preprint", "summary": "Safety alignment in Large Language Models is critical for healthcare; however, reliance on binary refusal boundaries often results in \\emph{over-refusal} of benign queries or \\emph{unsafe compliance} with harmful ones. While existing benchmarks measure these extremes, they fail to evaluate Safe Completion: the model's ability to maximise helpfulness on dual-use or borderline queries by providing safe, high-level guidance without crossing into actionable harm. We introduce \\textbf{Health-ORSC-Bench}, the first large-scale benchmark designed to systematically measure \\textbf{Over-Refusal} and \\textbf{Safe Completion} quality in healthcare. Comprising 31,920 benign boundary prompts across seven health categories (e.g., self-harm, medical misinformation), our framework uses an automated pipeline with human validation to test models at varying levels of intent ambiguity. We evaluate 30 state-of-the-art LLMs, including GPT-5 and Claude-4, revealing a significant tension: safety-optimised models frequently refuse up to 80\\% of \"Hard\" benign prompts, while domain-specific models often sacrifice safety for utility. Our findings demonstrate that model family and size significantly influence calibration: larger frontier models (e.g., GPT-5, Llama-4) exhibit \"safety-pessimism\" and higher over-refusal than smaller or MoE-based counterparts (e.g., Qwen-3-Next), highlighting that current LLMs struggle to balance refusal and compliance. Health-ORSC-Bench provides a rigorous standard for calibrating the next generation of medical AI assistants toward nuanced, safe, and helpful completions. The code and data will be released upon acceptance. \\textcolor{red}{Warning: Some contents may include toxic or undesired contents.}", "AI": {"tldr": "Health-ORSC-Bench is the first large-scale benchmark to systematically measure over-refusal and safe completion quality in healthcare LLMs, revealing current models struggle to balance safety and helpfulness.", "motivation": "Current safety alignment in healthcare LLMs relies on binary refusal boundaries, causing over-refusal of benign queries or unsafe compliance with harmful ones. Existing benchmarks fail to evaluate Safe Completion - the ability to provide safe, high-level guidance on dual-use or borderline queries without crossing into actionable harm.", "method": "Created Health-ORSC-Bench with 31,920 benign boundary prompts across seven health categories (self-harm, medical misinformation, etc.). Used automated pipeline with human validation to test models at varying levels of intent ambiguity. Evaluated 30 state-of-the-art LLMs including GPT-5 and Claude-4.", "result": "Safety-optimized models refuse up to 80% of \"Hard\" benign prompts, while domain-specific models sacrifice safety for utility. Larger frontier models (GPT-5, Llama-4) show \"safety-pessimism\" and higher over-refusal than smaller or MoE-based models (Qwen-3-Next). Model family and size significantly influence calibration.", "conclusion": "Current LLMs struggle to balance refusal and compliance. Health-ORSC-Bench provides a rigorous standard for calibrating next-generation medical AI assistants toward nuanced, safe, and helpful completions."}}
{"id": "2601.18323", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.18323", "abs": "https://arxiv.org/abs/2601.18323", "authors": ["Weishi Mi", "Yong Bao", "Xiaowei Chi", "Xiaozhu Ju", "Zhiyuan Qin", "Kuangzhi Ge", "Kai Tang", "Peidong Jia", "Shanghang Zhang", "Jian Tang"], "title": "TC-IDM: Grounding Video Generation for Executable Zero-shot Robot Motion", "comment": null, "summary": "The vision-language-action (VLA) paradigm has enabled powerful robotic control by leveraging vision-language models, but its reliance on large-scale, high-quality robot data limits its generalization. Generative world models offer a promising alternative for general-purpose embodied AI, yet a critical gap remains between their pixel-level plans and physically executable actions.\n  To this end, we propose the Tool-Centric Inverse Dynamics Model (TC-IDM). By focusing on the tool's imagined trajectory as synthesized by the world model, TC-IDM establishes a robust intermediate representation that bridges the gap between visual planning and physical control.\n  TC-IDM extracts the tool's point cloud trajectories via segmentation and 3D motion estimation from generated videos. Considering diverse tool attributes, our architecture employs decoupled action heads to project these planned trajectories into 6-DoF end-effector motions and corresponding control signals.\n  This plan-and-translate paradigm not only supports a wide range of end-effectors but also significantly improves viewpoint invariance. Furthermore, it exhibits strong generalization capabilities across long-horizon and out-of-distribution tasks, including interacting with deformable objects.\n  In real-world evaluations, the world model with TC-IDM achieves an average success rate of 61.11 percent, with 77.7 percent on simple tasks and 38.46 percent on zero-shot deformable object tasks. It substantially outperforms end-to-end VLA-style baselines and other inverse dynamics models.", "AI": {"tldr": "TC-IDM bridges world model visual planning with physical robot control by translating tool trajectories from generated videos into executable actions.", "motivation": "VLA models are limited by data requirements, while world models produce pixel-level plans that can't be directly executed as physical actions.", "method": "Extracts tool point cloud trajectories from generated videos via segmentation and 3D motion estimation, then uses decoupled action heads to project trajectories into 6-DoF end-effector motions.", "result": "Achieves 61.11% average success rate (77.7% on simple tasks, 38.46% on zero-shot deformable tasks), outperforming VLA baselines and other inverse dynamics models.", "conclusion": "TC-IDM's plan-and-translate paradigm effectively bridges visual planning with physical control, enabling generalization across diverse tasks and end-effectors."}}
{"id": "2601.17067", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17067", "abs": "https://arxiv.org/abs/2601.17067", "authors": ["Luozhou Wang", "Zhifei Chen", "Yihua Du", "Dongyu Yan", "Wenhang Ge", "Guibao Shen", "Xinli Xu", "Leyi Wu", "Man Chen", "Tianshuo Xu", "Peiran Ren", "Xin Tao", "Pengfei Wan", "Ying-Cong Chen"], "title": "A Mechanistic View on Video Generation as World Models: State and Dynamics", "comment": null, "summary": "Large-scale video generation models have demonstrated emergent physical coherence, positioning them as potential world models. However, a gap remains between contemporary \"stateless\" video architectures and classic state-centric world model theories. This work bridges this gap by proposing a novel taxonomy centered on two pillars: State Construction and Dynamics Modeling. We categorize state construction into implicit paradigms (context management) and explicit paradigms (latent compression), while dynamics modeling is analyzed through knowledge integration and architectural reformulation. Furthermore, we advocate for a transition in evaluation from visual fidelity to functional benchmarks, testing physical persistence and causal reasoning. We conclude by identifying two critical frontiers: enhancing persistence via data-driven memory and compressed fidelity, and advancing causality through latent factor decoupling and reasoning-prior integration. By addressing these challenges, the field can evolve from generating visually plausible videos to building robust, general-purpose world simulators.", "AI": {"tldr": "Paper proposes taxonomy for video generation models as world models, focusing on state construction and dynamics modeling, advocating for functional benchmarks over visual fidelity.", "motivation": "Bridging the gap between contemporary \"stateless\" video generation architectures and classic state-centric world model theories, as large-scale video generation models show emergent physical coherence but lack proper state representation.", "method": "Proposes a novel taxonomy with two pillars: State Construction (implicit paradigms like context management vs explicit paradigms like latent compression) and Dynamics Modeling (analyzed through knowledge integration and architectural reformulation).", "result": "Identifies the need for transition from visual fidelity evaluation to functional benchmarks testing physical persistence and causal reasoning, and outlines critical frontiers for future research.", "conclusion": "By addressing challenges in persistence (via data-driven memory and compressed fidelity) and causality (through latent factor decoupling and reasoning-prior integration), the field can evolve from generating visually plausible videos to building robust, general-purpose world simulators."}}
{"id": "2601.17678", "categories": ["cs.AI", "cs.GT"], "pdf": "https://arxiv.org/pdf/2601.17678", "abs": "https://arxiv.org/abs/2601.17678", "authors": ["Zhiyu An", "Wan Du"], "title": "DIML: Differentiable Inverse Mechanism Learning from Behaviors of Multi-Agent Learning Trajectories", "comment": null, "summary": "We study inverse mechanism learning: recovering an unknown incentive-generating mechanism from observed strategic interaction traces of self-interested learning agents. Unlike inverse game theory and multi-agent inverse reinforcement learning, which typically infer utility/reward parameters inside a structured mechanism, our target includes unstructured mechanism -- a (possibly neural) mapping from joint actions to per-agent payoffs. Unlike differentiable mechanism design, which optimizes mechanisms forward, we infer mechanisms from behavior in an observational setting. We propose DIML, a likelihood-based framework that differentiates through a model of multi-agent learning dynamics and uses the candidate mechanism to generate counterfactual payoffs needed to predict observed actions. We establish identifiability of payoff differences under a conditional logit response model and prove statistical consistency of maximum likelihood estimation under standard regularity conditions. We evaluate DIML with simulated interactions of learning agents across unstructured neural mechanisms, congestion tolling, public goods subsidies, and large-scale anonymous games. DIML reliably recovers identifiable incentive differences and supports counterfactual prediction, where its performance rivals tabular enumeration oracle in small environments and its convergence scales to large, hundred-participant environments. Code to reproduce our experiments is open-sourced.", "AI": {"tldr": "DIML: A likelihood-based framework for inverse mechanism learning that recovers unknown incentive mechanisms from observed strategic interactions of self-interested learning agents.", "motivation": "Existing approaches like inverse game theory and multi-agent inverse RL focus on structured mechanisms with known utility parameters, but real-world mechanisms can be unstructured (neural mappings). Also, differentiable mechanism design optimizes forward rather than inferring from observed behavior.", "method": "DIML uses likelihood-based framework that differentiates through multi-agent learning dynamics, generating counterfactual payoffs to predict observed actions. Establishes identifiability under conditional logit response model and proves statistical consistency of maximum likelihood estimation.", "result": "DIML reliably recovers identifiable incentive differences, supports counterfactual prediction, rivals tabular enumeration oracle in small environments, and scales to large hundred-participant environments across neural mechanisms, congestion tolling, public goods subsidies, and anonymous games.", "conclusion": "DIML provides a practical framework for inferring unstructured incentive mechanisms from observed strategic behavior, bridging observational inference with mechanism design, with proven theoretical guarantees and demonstrated empirical scalability."}}
{"id": "2601.18442", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.18442", "abs": "https://arxiv.org/abs/2601.18442", "authors": ["Hongyi Zhao", "Shuo Wang", "Qijie He", "Ziyuan Pu"], "title": "SG-CADVLM: A Context-Aware Decoding Powered Vision Language Model for Safety-Critical Scenario Generation", "comment": null, "summary": "Autonomous vehicle safety validation requires testing on safety-critical scenarios, but these events are rare in real-world driving and costly to test due to collision risks. Crash reports provide authentic specifications of safety-critical events, offering a vital alternative to scarce real-world collision trajectory data. This makes them valuable sources for generating realistic high-risk scenarios through simulation. Existing approaches face significant limitations because data-driven methods lack diversity due to their reliance on existing latent distributions, whereas adversarial methods often produce unrealistic scenarios lacking physical fidelity. Large Language Model (LLM) and Vision Language Model (VLM)-based methods show significant promise. However, they suffer from context suppression issues where internal parametric knowledge overrides crash specifications, producing scenarios that deviate from actual accident characteristics. This paper presents SG-CADVLM (A Context-Aware Decoding Powered Vision Language Model for Safety-Critical Scenario Generation), a framework that integrates Context-Aware Decoding with multi-modal input processing to generate safety-critical scenarios from crash reports and road network diagrams. The framework mitigates VLM hallucination issues while enabling the simultaneous generation of road geometry and vehicle trajectories. The experimental results demonstrate that SG-CADVLM generates critical risk scenarios at a rate of 84.4% compared to 12.5% for the baseline methods, representing an improvement of 469%, while producing executable simulations for autonomous vehicle testing.", "AI": {"tldr": "SG-CADVLM is a context-aware VLM framework that generates realistic safety-critical driving scenarios from crash reports and road diagrams, achieving 84.4% critical risk scenario generation rate (469% improvement over baselines).", "motivation": "Safety-critical scenarios for autonomous vehicle testing are rare and costly to obtain from real-world data. Crash reports provide authentic specifications but existing methods have limitations: data-driven approaches lack diversity, adversarial methods lack realism, and LLM/VLM methods suffer from context suppression where parametric knowledge overrides actual crash specifications.", "method": "SG-CADVLM integrates Context-Aware Decoding with multi-modal input processing to generate safety-critical scenarios from crash reports and road network diagrams. The framework mitigates VLM hallucination issues while enabling simultaneous generation of both road geometry and vehicle trajectories.", "result": "The framework generates critical risk scenarios at a rate of 84.4% compared to 12.5% for baseline methods, representing a 469% improvement. It produces executable simulations for autonomous vehicle testing while maintaining physical fidelity and realism.", "conclusion": "SG-CADVLM effectively addresses the limitations of existing methods by combining context-aware decoding with multi-modal inputs, enabling realistic and diverse safety-critical scenario generation from authentic crash data for autonomous vehicle validation."}}
{"id": "2601.17071", "categories": ["cs.CV", "math.PR"], "pdf": "https://arxiv.org/pdf/2601.17071", "abs": "https://arxiv.org/abs/2601.17071", "authors": ["Jisui Huang", "Andreas Alpers", "Ke Chen", "Na Lei"], "title": "Superpixel-Based Image Segmentation Using Squared 2-Wasserstein Distances", "comment": "34 pages, 11 figures", "summary": "We present an efficient method for image segmentation in the presence of strong inhomogeneities. The approach can be interpreted as a two-level clustering procedure: pixels are first grouped into superpixels via a linear least-squares assignment problem, which can be viewed as a special case of a discrete optimal transport (OT) problem, and these superpixels are subsequently greedily merged into object-level segments using the squared 2-Wasserstein distance between their empirical distributions. In contrast to conventional superpixel merging strategies based on mean-color distances, our framework employs a distributional OT distance, yielding a mathematically unified formulation across both clustering levels. Numerical experiments demonstrate that this perspective leads to improved segmentation accuracy on challenging images while retaining high computational efficiency.", "AI": {"tldr": "Efficient image segmentation method using two-level clustering: superpixel grouping via discrete optimal transport followed by greedy merging using Wasserstein distance.", "motivation": "To address image segmentation challenges in the presence of strong inhomogeneities, where conventional methods based on mean-color distances may be insufficient.", "method": "Two-level clustering: 1) Group pixels into superpixels via linear least-squares assignment (special case of discrete optimal transport), 2) Greedily merge superpixels using squared 2-Wasserstein distance between empirical distributions.", "result": "Improved segmentation accuracy on challenging images while maintaining high computational efficiency compared to conventional approaches.", "conclusion": "The distributional optimal transport framework provides a mathematically unified formulation for segmentation that outperforms mean-color distance methods, especially for images with strong inhomogeneities."}}
{"id": "2601.17699", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17699", "abs": "https://arxiv.org/abs/2601.17699", "authors": ["Harper Hua", "Zhen Han", "Zhengyuan Shen", "Jeremy Lee", "Patrick Guan", "Qi Zhu", "Sullam Jeoung", "Yueyan Chen", "Yunfei Bai", "Shuai Wang", "Vassilis Ioannidis", "Huzefa Rangwala"], "title": "SQL-Trail: Multi-Turn Reinforcement Learning with Interleaved Feedback for Text-to-SQL", "comment": null, "summary": "While large language models (LLMs) have substantially improved Text-to-SQL generation, a pronounced gap remains between AI systems and human experts on challenging benchmarks such as BIRD-SQL. We argue this gap stems largely from the prevailing single-pass paradigm, which lacks the iterative reasoning, schema exploration, and error-correction behaviors that humans naturally employ. To address this limitation, we introduce SQL-Trail, a multi-turn reinforcement learning (RL) agentic framework for Text-to-SQL. Rather than producing a query in one shot, SQL-Trail interacts with the database environment and uses execution feedback to iteratively refine its predictions. Our approach centers on two key ideas: (i) an adaptive turn-budget allocation mechanism that scales the agent's interaction depth to match question difficulty, and (ii) a composite reward panel that jointly incentivizes SQL correctness and efficient exploration. Across benchmarks, SQL-Trail sets a new state of the art and delivers strong data efficiency--up to 18x higher than prior single-pass RL state-of-the-art methods. Notably, our 7B and 14B models outperform substantially larger proprietary systems by 5% on average, underscoring the effectiveness of interactive, agentic workflows for robust Text-to-SQL generation.", "AI": {"tldr": "SQL-Trail introduces a multi-turn RL agentic framework for Text-to-SQL that uses iterative refinement with database execution feedback, outperforming single-pass methods and larger proprietary systems.", "motivation": "Current single-pass Text-to-SQL methods lack the iterative reasoning, schema exploration, and error-correction behaviors that human experts naturally employ, creating a performance gap on challenging benchmarks like BIRD-SQL.", "method": "SQL-Trail is a multi-turn reinforcement learning agentic framework that interacts with database environments and uses execution feedback for iterative refinement. Key innovations include adaptive turn-budget allocation that scales interaction depth to question difficulty, and a composite reward panel that jointly incentivizes SQL correctness and efficient exploration.", "result": "SQL-Trail sets new state-of-the-art results across benchmarks with strong data efficiency (up to 18x higher than prior single-pass RL methods). Notably, 7B and 14B models outperform substantially larger proprietary systems by 5% on average.", "conclusion": "Interactive, agentic workflows with iterative refinement and execution feedback are highly effective for robust Text-to-SQL generation, enabling smaller models to outperform much larger proprietary systems through intelligent exploration and adaptive reasoning."}}
{"id": "2601.18492", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.18492", "abs": "https://arxiv.org/abs/2601.18492", "authors": ["Zijun Li", "Shijie Li", "Zhenxi Zhang", "Bin Li", "Shoujun Zhou"], "title": "DV-VLN: Dual Verification for Reliable LLM-Based Vision-and-Language Navigation", "comment": null, "summary": "Vision-and-Language Navigation (VLN) requires an embodied agent to navigate in a complex 3D environment according to natural language instructions. Recent progress in large language models (LLMs) has enabled language-driven navigation with improved interpretability. However, most LLM-based agents still rely on single-shot action decisions, where the model must choose one option from noisy, textualized multi-perspective observations. Due to local mismatches and imperfect intermediate reasoning, such decisions can easily deviate from the correct path, leading to error accumulation and reduced reliability in unseen environments. In this paper, we propose DV-VLN, a new VLN framework that follows a generate-then-verify paradigm. DV-VLN first performs parameter-efficient in-domain adaptation of an open-source LLaMA-2 backbone to produce a structured navigational chain-of-thought, and then verifies candidate actions with two complementary channels: True-False Verification (TFV) and Masked-Entity Verification (MEV). DV-VLN selects actions by aggregating verification successes across multiple samples, yielding interpretable scores for reranking. Experiments on R2R, RxR (English subset), and REVERIE show that DV-VLN consistently improves over direct prediction and sampling-only baselines, achieving competitive performance among language-only VLN agents and promising results compared with several cross-modal systems.Code is available at https://github.com/PlumJun/DV-VLN.", "AI": {"tldr": "DV-VLN introduces a generate-then-verify framework for Vision-and-Language Navigation that first generates navigational chain-of-thought reasoning, then verifies candidate actions through True-False and Masked-Entity verification channels to improve reliability in unseen environments.", "motivation": "Current LLM-based VLN agents rely on single-shot action decisions from noisy textual observations, which can easily deviate from correct paths due to local mismatches and imperfect reasoning, leading to error accumulation and reduced reliability in unseen environments.", "method": "DV-VLN uses parameter-efficient in-domain adaptation of LLaMA-2 to generate structured navigational chain-of-thought, then verifies candidate actions with two complementary channels: True-False Verification (TFV) and Masked-Entity Verification (MEV), aggregating verification successes across multiple samples for action selection.", "result": "Experiments on R2R, RxR (English subset), and REVERIE show DV-VLN consistently improves over direct prediction and sampling-only baselines, achieving competitive performance among language-only VLN agents and promising results compared with several cross-modal systems.", "conclusion": "The generate-then-verify paradigm with complementary verification channels provides a more reliable approach for language-driven navigation, reducing error accumulation and improving performance in unseen environments while maintaining interpretability."}}
{"id": "2601.17088", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17088", "abs": "https://arxiv.org/abs/2601.17088", "authors": ["Rui-Yang Ju", "Jen-Shiun Chiang"], "title": "GlassesGB: Controllable 2D GAN-Based Eyewear Personalization for 3D Gaussian Blendshapes Head Avatars", "comment": "IEEE VR 2026 Poster", "summary": "Virtual try-on systems allow users to interactively try different products within VR scenarios. However, most existing VTON methods operate only on predefined eyewear templates and lack support for fine-grained, user-driven customization. While GlassesGAN enables personalized 2D eyewear design, its capability remains limited to 2D image generation. Motivated by the success of 3D Gaussian Blendshapes in head reconstruction, we integrate these two techniques and propose GlassesGB, a framework that supports customizable eyewear generation for 3D head avatars. GlassesGB effectively bridges 2D generative customization with 3D head avatar rendering, addressing the challenge in achieving personalized eyewear design for VR applications. The implementation code is available at https://ruiyangju.github.io/GlassesGB.", "AI": {"tldr": "GlassesGB bridges 2D generative customization with 3D head avatar rendering for personalized eyewear design in VR applications.", "motivation": "Existing VTON systems lack fine-grained user-driven customization and operate only on predefined templates, while 2D methods like GlassesGAN can't support 3D VR applications.", "method": "Integrates 3D Gaussian Blendshapes for head reconstruction with 2D generative customization to create GlassesGB framework for customizable eyewear generation on 3D head avatars.", "result": "GlassesGB effectively bridges 2D generative customization with 3D head avatar rendering, addressing personalized eyewear design challenges for VR applications.", "conclusion": "The proposed framework enables customizable eyewear generation for 3D head avatars, advancing virtual try-on capabilities for personalized VR experiences."}}
{"id": "2601.17717", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17717", "abs": "https://arxiv.org/abs/2601.17717", "authors": ["Kaituo Zhang", "Mingzhi Hu", "Hoang Anh Duy Le", "Fariha Kabir Torsha", "Zhimeng Jiang", "Minh Khai Bui", "Chia-Yuan Chang", "Yu-Neng Chuang", "Zhen Xiong", "Ying Lin", "Guanchu Wang", "Na Zou"], "title": "The LLM Data Auditor: A Metric-oriented Survey on Quality and Trustworthiness in Evaluating Synthetic Data", "comment": null, "summary": "Large Language Models (LLMs) have emerged as powerful tools for generating data across various modalities. By transforming data from a scarce resource into a controllable asset, LLMs mitigate the bottlenecks imposed by the acquisition costs of real-world data for model training, evaluation, and system iteration. However, ensuring the high quality of LLM-generated synthetic data remains a critical challenge. Existing research primarily focuses on generation methodologies, with limited direct attention to the quality of the resulting data. Furthermore, most studies are restricted to single modalities, lacking a unified perspective across different data types. To bridge this gap, we propose the \\textbf{LLM Data Auditor framework}. In this framework, we first describe how LLMs are utilized to generate data across six distinct modalities. More importantly, we systematically categorize intrinsic metrics for evaluating synthetic data from two dimensions: quality and trustworthiness. This approach shifts the focus from extrinsic evaluation, which relies on downstream task performance, to the inherent properties of the data itself. Using this evaluation system, we analyze the experimental evaluations of representative generation methods for each modality and identify substantial deficiencies in current evaluation practices. Based on these findings, we offer concrete recommendations for the community to improve the evaluation of data generation. Finally, the framework outlines methodologies for the practical application of synthetic data across different modalities.", "AI": {"tldr": "Proposes LLM Data Auditor framework to systematically evaluate quality and trustworthiness of LLM-generated synthetic data across six modalities, shifting focus from extrinsic task performance to intrinsic data properties.", "motivation": "LLMs enable synthetic data generation but ensuring high quality remains challenging. Existing research focuses on generation methods rather than data quality evaluation, lacks unified perspective across modalities, and relies too much on extrinsic downstream task performance.", "method": "Proposes LLM Data Auditor framework that: 1) describes LLM data generation across six modalities, 2) systematically categorizes intrinsic evaluation metrics for quality and trustworthiness, 3) analyzes experimental evaluations of representative methods, and 4) outlines practical application methodologies.", "result": "Analysis reveals substantial deficiencies in current evaluation practices for synthetic data. Framework identifies gaps and provides concrete recommendations for improving data generation evaluation.", "conclusion": "The LLM Data Auditor framework addresses critical gaps in synthetic data evaluation by providing systematic intrinsic assessment across modalities, shifting focus from task performance to data properties, and offering practical guidance for the community."}}
{"id": "2601.18537", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18537", "abs": "https://arxiv.org/abs/2601.18537", "authors": ["Linyong Gan", "Zimo Li", "Wenxin Xu", "Xingjian Li", "Jianhua Z. Huang", "Enmei Tu", "Shuhang Chen"], "title": "SKETCH: Semantic Key-Point Conditioning for Long-Horizon Vessel Trajectory Prediction", "comment": null, "summary": "Accurate long-horizon vessel trajectory prediction remains challenging due to compounded uncertainty from complex navigation behaviors and environmental factors. Existing methods often struggle to maintain global directional consistency, leading to drifting or implausible trajectories when extrapolated over long time horizons. To address this issue, we propose a semantic-key-point-conditioned trajectory modeling framework, in which future trajectories are predicted by conditioning on a high-level Next Key Point (NKP) that captures navigational intent. This formulation decomposes long-horizon prediction into global semantic decision-making and local motion modeling, effectively restricting the support of future trajectories to semantically feasible subsets. To efficiently estimate the NKP prior from historical observations, we adopt a pretrain-finetune strategy. Extensive experiments on real-world AIS data demonstrate that the proposed method consistently outperforms state-of-the-art approaches, particularly for long travel durations, directional accuracy, and fine-grained trajectory prediction.", "AI": {"tldr": "Proposes semantic-key-point-conditioned trajectory modeling for vessels, using Next Key Point (NKP) to capture navigational intent and decompose long-horizon prediction into global decision-making and local motion modeling.", "motivation": "Existing vessel trajectory prediction methods struggle with global directional consistency, leading to drifting or implausible trajectories over long time horizons due to compounded uncertainty from complex navigation behaviors and environmental factors.", "method": "Semantic-key-point-conditioned trajectory modeling framework that predicts future trajectories by conditioning on a high-level Next Key Point (NKP) capturing navigational intent, decomposing prediction into global semantic decision-making and local motion modeling. Uses pretrain-finetune strategy to estimate NKP prior from historical observations.", "result": "Extensive experiments on real-world AIS data show the method consistently outperforms state-of-the-art approaches, particularly for long travel durations, directional accuracy, and fine-grained trajectory prediction.", "conclusion": "The proposed NKP-conditioned framework effectively addresses long-horizon vessel trajectory prediction challenges by restricting future trajectories to semantically feasible subsets through decomposition of global intent and local motion."}}
{"id": "2601.17089", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17089", "abs": "https://arxiv.org/abs/2601.17089", "authors": ["Qigan Sun", "Chaoning Zhang", "Jianwei Zhang", "Xudong Wang", "Jiehui Xie", "Pengcheng Zheng", "Haoyu Wang", "Sungyoung Lee", "Chi-lok Andy Tai", "Yang Yang", "Heng Tao Shen"], "title": "GRASP: Guided Region-Aware Sparse Prompting for Adapting MLLMs to Remote Sensing", "comment": null, "summary": "In recent years, Multimodal Large Language Models (MLLMs) have made significant progress in visual question answering tasks. However, directly applying existing fine-tuning methods to remote sensing (RS) images often leads to issues such as overfitting on background noise or neglecting target details. This is primarily due to the large-scale variations, sparse target distributions, and complex regional semantic features inherent in RS images. These challenges limit the effectiveness of MLLMs in RS tasks. To address these challenges, we propose a parameter-efficient fine-tuning (PEFT) strategy called Guided Region-Aware Sparse Prompting (GRASP). GRASP introduces spatially structured soft prompts associated with spatial blocks extracted from a frozen visual token grid. Through a question-guided sparse fusion mechanism, GRASP dynamically aggregates task-specific context into a compact global prompt, enabling the model to focus on relevant regions while filtering out background noise. Extensive experiments on multiple RSVQA benchmarks show that GRASP achieves competitive performance compared to existing fine-tuning and prompt-based methods while maintaining high parameter efficiency.", "AI": {"tldr": "GRASP is a parameter-efficient fine-tuning method for multimodal LLMs on remote sensing images that uses guided region-aware sparse prompting to focus on relevant regions while filtering background noise.", "motivation": "Existing fine-tuning methods for MLLMs perform poorly on remote sensing images due to large-scale variations, sparse target distributions, and complex regional semantics, leading to overfitting on background noise or neglecting target details.", "method": "GRASP introduces spatially structured soft prompts associated with spatial blocks from frozen visual tokens, using question-guided sparse fusion to dynamically aggregate task-specific context into a compact global prompt.", "result": "Extensive experiments on multiple RSVQA benchmarks show GRASP achieves competitive performance compared to existing fine-tuning and prompt-based methods while maintaining high parameter efficiency.", "conclusion": "GRASP effectively addresses the challenges of applying MLLMs to remote sensing images by enabling focused attention on relevant regions while filtering background noise through parameter-efficient fine-tuning."}}
{"id": "2601.17722", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17722", "abs": "https://arxiv.org/abs/2601.17722", "authors": ["Ying Mo", "Yu Bai", "Dapeng Sun", "Yuqian Shi", "Yukai Miao", "Li Chen", "Dan Li"], "title": "EntWorld: A Holistic Environment and Benchmark for Verifiable Enterprise GUI Agents", "comment": null, "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have enabled agents to operate in open-ended web and operating system environments. However, existing benchmarks predominantly target consumer-oriented scenarios (e.g., e-commerce and travel booking), failing to capture the complexity and rigor of professional enterprise workflows. Enterprise systems pose distinct challenges, including high-density user interfaces, strict business logic constraints, and a strong reliance on precise, state-consistent information retrieval-settings in which current generalist agents often struggle. To address this gap, we introduce EntWorld, a large-scale benchmark consisting of 1,756 tasks across six representative enterprise domains, including customer relationship management (CRM), information technology infrastructure library (ITIL), and enterprise resource planning (ERP) systems. Unlike previous datasets that depend on fragile execution traces or extensive manual annotation, EntWorld adopts a schema-grounded task generation framework that directly reverse-engineers business logic from underlying database schemas, enabling the synthesis of realistic, long-horizon workflows. Moreover, we propose a SQL-based deterministic verification mechanism in building datasets that replaces ambiguous visual matching with rigorous state-transition validation. Experimental results demonstrate that state-of-the-art models (e.g., GPT-4.1) achieve 47.61% success rate on EntWorld, substantially lower than the human performance, highlighting a pronounced enterprise gap in current agentic capabilities and the necessity of developing domain-specific agents. We release EntWorld as a rigorous testbed to facilitate the development and evaluation of the next generation of enterprise-ready digital agents.", "AI": {"tldr": "EntWorld is a benchmark for evaluating enterprise AI agents on complex business workflows, showing current models perform poorly (47.6%) compared to humans.", "motivation": "Existing benchmarks focus on consumer scenarios and fail to capture the complexity of enterprise systems with high-density UIs, strict business logic, and precise state-consistent information retrieval requirements.", "method": "Created EntWorld benchmark with 1,756 tasks across six enterprise domains (CRM, ITIL, ERP, etc.) using schema-grounded task generation that reverse-engineers business logic from database schemas, with SQL-based deterministic verification instead of visual matching.", "result": "State-of-the-art models like GPT-4.1 achieve only 47.61% success rate on EntWorld, substantially lower than human performance, revealing a significant enterprise capability gap.", "conclusion": "Enterprise systems pose distinct challenges requiring domain-specific agents, and EntWorld provides a rigorous testbed for developing next-generation enterprise-ready digital agents."}}
{"id": "2601.18548", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.18548", "abs": "https://arxiv.org/abs/2601.18548", "authors": ["Yulin Li", "Zhiyuan Song", "Yiming Li", "Zhicheng Song", "Kai Chen", "Chunxin Zheng", "Zhihai Bi", "Jiahang Cao", "Sylvain Calinon", "Fan Shi", "Jun Ma"], "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field", "comment": null, "summary": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes.", "AI": {"tldr": "GCDF extends configuration space distance fields to mobile manipulators, enabling efficient whole-body trajectory optimization in cluttered spaces through neural representation and convex optimization.", "motivation": "Mobile manipulators need agile motion in cluttered spaces, but current trajectory optimization struggles with high-dimensional nonconvexity and collision reasoning. Configuration Space Distance Fields (CDF) work for fixed-base robots but don't extend to mobile manipulators with unbounded workspaces and tight base-arm coupling.", "method": "Develop Generalized Configuration Space Distance Fields (GCDF) that extend CDF to robots with translational/rotational joints in unbounded workspaces. Create data generation/training pipeline for continuous neural GCDFs with accurate gradients. Build sequential convex optimization framework using GCDF-based collision reasoning with online neural constraint specification, sparsity-aware active-set detection, and incremental constraint management.", "result": "GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space. The neural GCDFs support efficient GPU-batched queries. The optimization framework scales to large numbers of implicit constraints and enables rapid replanning under scene changes.", "conclusion": "GCDF successfully extends CDF to mobile manipulation, providing an effective representation for whole-body trajectory optimization in cluttered, confined spaces through neural distance fields and scalable convex optimization."}}
{"id": "2601.17095", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17095", "abs": "https://arxiv.org/abs/2601.17095", "authors": ["Xusheng Du", "Athiwat Kongkaeo", "Ye Zhang", "Haoran Xie"], "title": "LoD Sketch Extraction from Architectural Models Using Generative AI: Dataset Construction for Multi-Level Architectural Design Generation", "comment": "10 pages, 5 figures, Proceedings of CAADRIA 2026", "summary": "For architectural design, representation across multiple Levels of Details (LoD) is essential for achieving a smooth transition from conceptual massing to detailed modeling. However, traditional LoD modeling processes rely on manual operations that are time-consuming, labor-intensive, and prone to geometric inconsistencies. While the rapid advancement of generative artificial intelligence (AI) has opened new possibilities for generating multi-level architectural models from sketch inputs, its application remains limited by the lack of high-quality paired LoD training data. To address this issue, we propose an automatic LoD sketch extraction framework using generative AI models, which progressively simplifies high-detail architectural models to automatically generate geometrically consistent and hierarchically coherent multi-LoD representations. The proposed framework integrates computer vision techniques with generative AI methods to establish a progressive extraction pipeline that transitions from detailed representations to volumetric abstractions. Experimental results demonstrate that the method maintains strong geometric consistency across LoD levels, achieving SSIM values of 0.7319 and 0.7532 for the transitions from LoD3 to LoD2 and from LoD2 to LoD1, respectively, with corresponding normalized Hausdorff distances of 25.1% and 61.0% of the image diagonal, reflecting controlled geometric deviation during abstraction. These results verify that the proposed framework effectively preserves global structure while achieving progressive semantic simplification across different LoD levels, providing reliable data and technical support for AI-driven multi-level architectural generation and hierarchical modeling.", "AI": {"tldr": "An automatic framework using generative AI to extract multi-level architectural sketches from detailed models, solving the lack of paired LoD training data for AI-driven architectural design.", "motivation": "Traditional manual LoD modeling is time-consuming and inconsistent, while AI-based approaches lack high-quality paired training data across different detail levels for architectural design.", "method": "A generative AI framework that progressively simplifies high-detail architectural models to automatically generate geometrically consistent multi-LoD representations, integrating computer vision with generative AI for hierarchical abstraction.", "result": "Achieved strong geometric consistency with SSIM values of 0.7319 (LoD3\u2192LoD2) and 0.7532 (LoD2\u2192LoD1), with normalized Hausdorff distances of 25.1% and 61.0% of image diagonal, showing controlled geometric deviation during abstraction.", "conclusion": "The framework effectively preserves global structure while achieving progressive semantic simplification across LoD levels, providing reliable data and technical support for AI-driven multi-level architectural generation and hierarchical modeling."}}
{"id": "2601.17735", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17735", "abs": "https://arxiv.org/abs/2601.17735", "authors": ["Kyungho Kim", "Geon Lee", "Juyeon Kim", "Dongwon Choi", "Shinhwan Kang", "Kijung Shin"], "title": "ReFuGe: Feature Generation for Prediction Tasks on Relational Databases with LLM Agents", "comment": "Accepted in ACM WWW 2026 (Short Paper)", "summary": "Relational databases (RDBs) play a crucial role in many real-world web applications, supporting data management across multiple interconnected tables. Beyond typical retrieval-oriented tasks, prediction tasks on RDBs have recently gained attention. In this work, we address this problem by generating informative relational features that enhance predictive performance. However, generating such features is challenging: it requires reasoning over complex schemas and exploring a combinatorially large feature space, all without explicit supervision. To address these challenges, we propose ReFuGe, an agentic framework that leverages specialized large language model agents: (1) a schema selection agent identifies the tables and columns relevant to the task, (2) a feature generation agent produces diverse candidate features from the selected schema, and (3) a feature filtering agent evaluates and retains promising features through reasoning-based and validation-based filtering. It operates within an iterative feedback loop until performance converges. Experiments on RDB benchmarks demonstrate that ReFuGe substantially improves performance on various RDB prediction tasks. Our code and datasets are available at https://github.com/K-Kyungho/REFUGE.", "AI": {"tldr": "ReFuGe is an agentic framework that uses LLM agents to generate informative relational features for prediction tasks on relational databases, improving performance through iterative feature generation and filtering.", "motivation": "Prediction tasks on relational databases are gaining attention but require generating informative relational features, which is challenging due to complex schemas, combinatorially large feature spaces, and lack of explicit supervision.", "method": "ReFuGe uses three specialized LLM agents: (1) schema selection agent identifies relevant tables/columns, (2) feature generation agent produces diverse candidate features, and (3) feature filtering agent evaluates features through reasoning-based and validation-based filtering, operating in an iterative feedback loop until convergence.", "result": "Experiments on RDB benchmarks demonstrate that ReFuGe substantially improves performance on various RDB prediction tasks.", "conclusion": "The proposed agentic framework effectively addresses the challenges of relational feature generation for database prediction tasks, with code and datasets made publicly available."}}
{"id": "2601.18569", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18569", "abs": "https://arxiv.org/abs/2601.18569", "authors": ["Seokju Lee", "Kyung-Soo Kim"], "title": "Attention-Based Neural-Augmented Kalman Filter for Legged Robot State Estimation", "comment": "8 pages, 6 figures, Accepted to IEEE Robotics and Automation Letters (RA-L)", "summary": "In this letter, we propose an Attention-Based Neural-Augmented Kalman Filter (AttenNKF) for state estimation in legged robots. Foot slip is a major source of estimation error: when slip occurs, kinematic measurements violate the no-slip assumption and inject bias during the update step. Our objective is to estimate this slip-induced error and compensate for it. To this end, we augment an Invariant Extended Kalman Filter (InEKF) with a neural compensator that uses an attention mechanism to infer error conditioned on foot-slip severity and then applies this estimate as a post-update compensation to the InEKF state (i.e., after the filter update). The compensator is trained in a latent space, which aims to reduce sensitivity to raw input scales and encourages structured slip-conditioned compensations, while preserving the InEKF recursion. Experiments demonstrate improved performance compared to existing legged-robot state estimators, particularly under slip-prone conditions.", "AI": {"tldr": "AttenNKF: An attention-based neural-augmented Kalman filter that compensates for foot-slip errors in legged robot state estimation by using a neural compensator with attention mechanism.", "motivation": "Foot slip is a major source of estimation error in legged robots because when slip occurs, kinematic measurements violate the no-slip assumption and inject bias during the Kalman filter update step.", "method": "Augments an Invariant Extended Kalman Filter (InEKF) with a neural compensator that uses attention mechanism to infer slip-induced error conditioned on foot-slip severity, then applies this estimate as post-update compensation to the InEKF state. The compensator is trained in a latent space to reduce sensitivity to raw input scales and encourage structured slip-conditioned compensations while preserving the InEKF recursion.", "result": "Experiments demonstrate improved performance compared to existing legged-robot state estimators, particularly under slip-prone conditions.", "conclusion": "The proposed AttenNKF effectively compensates for foot-slip errors in legged robot state estimation by combining the robustness of InEKF with neural compensation using attention mechanisms, leading to better performance in challenging slip conditions."}}
{"id": "2601.17103", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17103", "abs": "https://arxiv.org/abs/2601.17103", "authors": ["Pascaline Andr\u00e9", "Charles Heitz", "Evangelia Christodoulou", "Annika Reinke", "Carole H. Sudre", "Michela Antonelli", "Patrick Godau", "M. Jorge Cardoso", "Antoine Gilson", "Sophie Tezenas du Montcel", "Ga\u00ebl Varoquaux", "Lena Maier-Hein", "Olivier Colliot"], "title": "Performance uncertainty in medical image analysis: a large-scale investigation of confidence intervals", "comment": null, "summary": "Performance uncertainty quantification is essential for reliable validation and eventual clinical translation of medical imaging artificial intelligence (AI). Confidence intervals (CIs) play a central role in this process by indicating how precise a reported performance estimate is. Yet, due to the limited amount of work examining CI behavior in medical imaging, the community remains largely unaware of how many diverse CI methods exist and how they behave in specific settings. The purpose of this study is to close this gap. To this end, we conducted a large-scale empirical analysis across a total of 24 segmentation and classification tasks, using 19 trained models per task group, a broad spectrum of commonly used performance metrics, multiple aggregation strategies, and several widely adopted CI methods. Reliability (coverage) and precision (width) of each CI method were estimated across all settings to characterize their dependence on study characteristics. Our analysis revealed five principal findings: 1) the sample size required for reliable CIs varies from a few dozens to several thousands of cases depending on study parameters; 2) CI behavior is strongly affected by the choice of performance metric; 3) aggregation strategy substantially influences the reliability of CIs, e.g. they require more observations for macro than for micro; 4) the machine learning problem (segmentation versus classification) modulates these effects; 5) different CI methods are not equally reliable and precise depending on the use case. These results form key components for the development of future guidelines on reporting performance uncertainty in medical imaging AI.", "AI": {"tldr": "Large-scale empirical analysis of confidence interval methods for medical imaging AI performance uncertainty quantification reveals key dependencies on sample size, metrics, aggregation strategies, and problem types.", "motivation": "The medical imaging AI community lacks awareness of diverse confidence interval methods and their behavior in specific settings, despite CI's central role in performance uncertainty quantification for reliable validation and clinical translation.", "method": "Conducted large-scale empirical analysis across 24 segmentation/classification tasks using 19 trained models per task group, multiple performance metrics, aggregation strategies, and widely adopted CI methods, evaluating reliability (coverage) and precision (width).", "result": "Five principal findings: 1) required sample size varies from dozens to thousands; 2) CI behavior strongly affected by metric choice; 3) aggregation strategy substantially influences reliability; 4) problem type (segmentation vs classification) modulates effects; 5) different CI methods vary in reliability/precision by use case.", "conclusion": "Results provide key components for developing future guidelines on reporting performance uncertainty in medical imaging AI, addressing the community's gap in understanding CI method behavior."}}
{"id": "2601.17744", "categories": ["cs.AI", "cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2601.17744", "abs": "https://arxiv.org/abs/2601.17744", "authors": ["Amjad Fatmi"], "title": "Faramesh: A Protocol-Agnostic Execution Control Plane for Autonomous Agent Systems", "comment": "40 pages, 10 figures. Preprint. Code: https://github.com/faramesh/faramesh-core", "summary": "Autonomous agent systems increasingly trigger real-world side effects: deploying infrastructure, modifying databases, moving money, and executing workflows. Yet most agent stacks provide no mandatory execution checkpoint where organizations can deterministically permit, deny, or defer an action before it changes reality. This paper introduces Faramesh, a protocol-agnostic execution control plane that enforces execution-time authorization for agent-driven actions via a non-bypassable Action Authorization Boundary (AAB). Faramesh canonicalizes agent intent into a Canonical Action Representation (CAR), evaluates actions deterministically against policy and state, and issues a decision artifact (PERMIT/DEFER/DENY) that executors must validate prior to execution. The system is designed to be framework- and model-agnostic, supports multi-agent and multi-tenant deployments, and remains independent of transport protocols (e.g., MCP). Faramesh further provides decision-centric, append-only provenance logging keyed by canonical action hashes, enabling auditability, verification, and deterministic replay without re-running agent reasoning. We show how these primitives yield enforceable, predictable governance for autonomous execution while avoiding hidden coupling to orchestration layers or observability-only approaches.", "AI": {"tldr": "Faramesh is a protocol-agnostic execution control plane that enforces mandatory authorization checkpoints for autonomous agent actions before they trigger real-world side effects.", "motivation": "Autonomous agent systems increasingly trigger real-world side effects (deploying infrastructure, modifying databases, moving money, etc.) without mandatory execution checkpoints where organizations can permit, deny, or defer actions before they change reality.", "method": "Introduces Faramesh with an Action Authorization Boundary (AAB) that canonicalizes agent intent into Canonical Action Representation (CAR), evaluates actions deterministically against policy and state, and issues PERMIT/DEFER/DENY decisions that executors must validate before execution. It's framework/model-agnostic, supports multi-agent/multi-tenant deployments, and includes append-only provenance logging keyed by action hashes.", "result": "Provides enforceable, predictable governance for autonomous execution with deterministic replay capability, auditability, and verification without hidden coupling to orchestration layers or observability-only approaches.", "conclusion": "Faramesh enables organizations to maintain control over autonomous agent actions through mandatory authorization checkpoints, ensuring deterministic governance while remaining independent of specific frameworks, models, or transport protocols."}}
{"id": "2601.18629", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.18629", "abs": "https://arxiv.org/abs/2601.18629", "authors": ["Yiming Wang", "Ruogu Zhang", "Minyang Li", "Hao Shi", "Junbo Wang", "Deyi Li", "Jieji Ren", "Wenhai Liu", "Weiming Wang", "Hao-Shu Fang"], "title": "ExoGS: A 4D Real-to-Sim-to-Real Framework for Scalable Manipulation Data Collection", "comment": null, "summary": "Real-to-Sim-to-Real technique is gaining increasing interest for robotic manipulation, as it can generate scalable data in simulation while having narrower sim-to-real gap. However, previous methods mainly focused on environment-level visual real-to-sim transfer, ignoring the transfer of interactions, which could be challenging and inefficient to obtain purely in simulation especially for contact-rich tasks. We propose ExoGS, a robot-free 4D Real-to-Sim-to-Real framework that captures both static environments and dynamic interactions in the real world and transfers them seamlessly to a simulated environment. It provides a new solution for scalable manipulation data collection and policy learning. ExoGS employs a self-designed robot-isomorphic passive exoskeleton AirExo-3 to capture kinematically consistent trajectories with millimeter-level accuracy and synchronized RGB observations during direct human demonstrations. The robot, objects, and environment are reconstructed as editable 3D Gaussian Splatting assets, enabling geometry-consistent replay and large-scale data augmentation. Additionally, a lightweight Mask Adapter injects instance-level semantics into the policy to enhance robustness under visual domain shifts. Real-world experiments demonstrate that ExoGS significantly improves data efficiency and policy generalization compared to teleoperation-based baselines. Code and hardware files have been released on https://github.com/zaixiabalala/ExoGS.", "AI": {"tldr": "ExoGS is a robot-free 4D Real-to-Sim-to-Real framework that captures real-world environments and interactions using a passive exoskeleton, reconstructs them as editable 3D Gaussian Splatting assets, and enables scalable manipulation data collection and policy learning with improved efficiency and generalization.", "motivation": "Previous Real-to-Sim-to-Real methods focused mainly on environment-level visual transfer but ignored interaction transfer, which is challenging to obtain purely in simulation for contact-rich tasks. There's a need for a framework that can capture both static environments and dynamic interactions from the real world and transfer them seamlessly to simulation for scalable manipulation data collection.", "method": "Uses a self-designed robot-isomorphic passive exoskeleton (AirExo-3) to capture kinematically consistent trajectories with millimeter accuracy and synchronized RGB observations during human demonstrations. Reconstructs robot, objects, and environment as editable 3D Gaussian Splatting assets for geometry-consistent replay and data augmentation. Includes a lightweight Mask Adapter to inject instance-level semantics into policies for robustness under visual domain shifts.", "result": "Real-world experiments show ExoGS significantly improves data efficiency and policy generalization compared to teleoperation-based baselines. The framework enables scalable manipulation data collection and policy learning with narrower sim-to-real gap.", "conclusion": "ExoGS provides a new solution for scalable manipulation data collection and policy learning by capturing both static environments and dynamic interactions from the real world and transferring them to simulation, addressing limitations of previous methods that ignored interaction transfer."}}
{"id": "2601.17107", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17107", "abs": "https://arxiv.org/abs/2601.17107", "authors": ["Qinkai Yu", "Chong Zhang", "Gaojie Jin", "Tianjin Huang", "Wei Zhou", "Wenhui Li", "Xiaobo Jin", "Bo Huang", "Yitian Zhao", "Guang Yang", "Gregory Y. H. Lip", "Yalin Zheng", "Aline Villavicencio", "Yanda Meng"], "title": "StealthMark: Harmless and Stealthy Ownership Verification for Medical Segmentation via Uncertainty-Guided Backdoors", "comment": "15 pages,7 figures. Accepted to IEEE Transactions on Image Processing (TIP) 2026", "summary": "Annotating medical data for training AI models is often costly and limited due to the shortage of specialists with relevant clinical expertise. This challenge is further compounded by privacy and ethical concerns associated with sensitive patient information. As a result, well-trained medical segmentation models on private datasets constitute valuable intellectual property requiring robust protection mechanisms. Existing model protection techniques primarily focus on classification and generative tasks, while segmentation models-crucial to medical image analysis-remain largely underexplored. In this paper, we propose a novel, stealthy, and harmless method, StealthMark, for verifying the ownership of medical segmentation models under black-box conditions. Our approach subtly modulates model uncertainty without altering the final segmentation outputs, thereby preserving the model's performance. To enable ownership verification, we incorporate model-agnostic explanation methods, e.g. LIME, to extract feature attributions from the model outputs. Under specific triggering conditions, these explanations reveal a distinct and verifiable watermark. We further design the watermark as a QR code to facilitate robust and recognizable ownership claims. We conducted extensive experiments across four medical imaging datasets and five mainstream segmentation models. The results demonstrate the effectiveness, stealthiness, and harmlessness of our method on the original model's segmentation performance. For example, when applied to the SAM model, StealthMark consistently achieved ASR above 95% across various datasets while maintaining less than a 1% drop in Dice and AUC scores, significantly outperforming backdoor-based watermarking methods and highlighting its strong potential for practical deployment. Our implementation code is made available at: https://github.com/Qinkaiyu/StealthMark.", "AI": {"tldr": "StealthMark: A stealthy watermarking method for protecting medical segmentation models by subtly modulating model uncertainty without affecting segmentation performance, enabling ownership verification under black-box conditions.", "motivation": "Medical data annotation is costly and limited due to specialist shortages and privacy concerns, making well-trained segmentation models valuable IP that needs protection. Existing protection methods focus on classification/generative tasks, leaving segmentation models underexplored.", "method": "StealthMark subtly modulates model uncertainty without altering segmentation outputs, uses model-agnostic explanation methods (like LIME) to extract feature attributions that reveal a QR code watermark under triggering conditions, enabling black-box ownership verification.", "result": "Extensive experiments across 4 medical imaging datasets and 5 segmentation models show effectiveness (ASR above 95% for SAM model), stealthiness, and harmlessness (<1% drop in Dice/AUC scores), outperforming backdoor-based watermarking methods.", "conclusion": "StealthMark provides a practical, effective solution for protecting medical segmentation model IP with minimal performance impact, demonstrating strong potential for real-world deployment in healthcare AI."}}
{"id": "2601.17767", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17767", "abs": "https://arxiv.org/abs/2601.17767", "authors": ["Rajan Das Gupta", "Xiaobin Wu", "Xun Liu", "Jiaqi He"], "title": "HyCARD-Net: A Synergistic Hybrid Intelligence Framework for Cardiovascular Disease Diagnosis", "comment": "Accepted and published in the 2025 4th International Conference on Image Processing, Computer Vision and Machine Learning (ICICML)", "summary": "Cardiovascular disease (CVD) remains the foremost cause of mortality worldwide, underscoring the urgent need for intelligent and data-driven diagnostic tools. Traditional predictive models often struggle to generalize across heterogeneous datasets and complex physiological patterns. To address this, we propose a hybrid ensemble framework that integrates deep learning architectures, Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM), with classical machine learning algorithms, including K-Nearest Neighbor (KNN) and Extreme Gradient Boosting (XGB), using an ensemble voting mechanism. This approach combines the representational power of deep networks with the interpretability and efficiency of traditional models. Experiments on two publicly available Kaggle datasets demonstrate that the proposed model achieves superior performance, reaching 82.30 percent accuracy on Dataset I and 97.10 percent on Dataset II, with consistent gains in precision, recall, and F1-score. These findings underscore the robustness and clinical potential of hybrid AI frameworks for predicting cardiovascular disease and facilitating early intervention. Furthermore, this study directly supports the United Nations Sustainable Development Goal 3 (Good Health and Well-being) by promoting early diagnosis, prevention, and management of non-communicable diseases through innovative, data-driven healthcare solutions.", "AI": {"tldr": "Hybrid ensemble framework combining CNN, LSTM, KNN, and XGBoost achieves superior CVD prediction accuracy (82.30% and 97.10%) on Kaggle datasets, supporting early diagnosis and UN SDG 3.", "motivation": "Cardiovascular disease is the leading global cause of mortality, requiring intelligent diagnostic tools. Traditional models struggle with generalization across heterogeneous datasets and complex physiological patterns.", "method": "Proposed hybrid ensemble framework integrates deep learning (CNN and LSTM) with classical machine learning (KNN and XGBoost) using ensemble voting mechanism, combining representational power of deep networks with interpretability and efficiency of traditional models.", "result": "Superior performance with 82.30% accuracy on Dataset I and 97.10% on Dataset II, with consistent gains in precision, recall, and F1-score across two publicly available Kaggle datasets.", "conclusion": "Hybrid AI frameworks show robustness and clinical potential for CVD prediction and early intervention, supporting UN Sustainable Development Goal 3 (Good Health and Well-being) through innovative data-driven healthcare solutions."}}
{"id": "2601.18639", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.18639", "abs": "https://arxiv.org/abs/2601.18639", "authors": ["Ojasva Mishra", "Xiaolong Wu", "Min Xu"], "title": "Constraint-Aware Discrete-Time PID Gain Optimization for Robotic Joint Control Under Actuator Saturation", "comment": null, "summary": "The precise regulation of rotary actuation is fundamental in autonomous robotics, yet practical PID loops deviate from continuous-time theory due to discrete-time execution, actuator saturation, and small delays and measurement imperfections. We present an implementation-aware analysis and tuning workflow for saturated discrete-time joint control. We (i) derive PI stability regions under Euler and exact zero-order-hold (ZOH) discretizations using the Jury criterion, (ii) evaluate a discrete back-calculation anti-windup realization under saturation-dominant regimes, and (iii) propose a hybrid-certified Bayesian optimization workflow that screens analytically unstable candidates and behaviorally unsafe transients while optimizing a robust IAE objective with soft penalties on overshoot and saturation duty. Baseline sweeps ($\u03c4=1.0$~s, $\u0394t=0.01$~s, $u\\in[-10,10]$) quantify rise/settle trends for P/PI/PID. Under a randomized model family emulating uncertainty, delay, noise, quantization, and tighter saturation, robustness-oriented tuning improves median IAE from $0.843$ to $0.430$ while keeping median overshoot below $2\\%$. In simulation-only tuning, the certification screen rejects $11.6\\%$ of randomly sampled gains within bounds before full robust evaluation, improving sample efficiency without hardware experiments.", "AI": {"tldr": "Implementation-aware PID tuning workflow for robotic joint control that addresses discrete-time execution, saturation, and delays using hybrid-certified Bayesian optimization.", "motivation": "Practical PID loops in robotics deviate from continuous-time theory due to discrete-time execution, actuator saturation, delays, and measurement imperfections, requiring implementation-aware analysis and tuning.", "method": "Derived PI stability regions using Jury criterion for Euler and ZOH discretizations, evaluated discrete back-calculation anti-windup, and proposed hybrid-certified Bayesian optimization that screens unstable candidates while optimizing robust IAE with overshoot/saturation penalties.", "result": "Robustness-oriented tuning improved median IAE from 0.843 to 0.430 while keeping median overshoot below 2% under randomized model uncertainty; certification screen rejected 11.6% of randomly sampled gains before full evaluation.", "conclusion": "The implementation-aware tuning workflow effectively addresses practical PID control challenges in robotics, improving robustness and sample efficiency without requiring hardware experiments."}}
{"id": "2601.17124", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17124", "abs": "https://arxiv.org/abs/2601.17124", "authors": ["Bin Lin", "Zongjian Li", "Yuwei Niu", "Kaixiong Gong", "Yunyang Ge", "Yunlong Lin", "Mingzhe Zheng", "JianWei Zhang", "Miles Yang", "Zhao Zhong", "Liefeng Bo", "Li Yuan"], "title": "iFSQ: Improving FSQ for Image Generation with 1 Line of Code", "comment": "Technical Report", "summary": "The field of image generation is currently bifurcated into autoregressive (AR) models operating on discrete tokens and diffusion models utilizing continuous latents. This divide, rooted in the distinction between VQ-VAEs and VAEs, hinders unified modeling and fair benchmarking. Finite Scalar Quantization (FSQ) offers a theoretical bridge, yet vanilla FSQ suffers from a critical flaw: its equal-interval quantization can cause activation collapse. This mismatch forces a trade-off between reconstruction fidelity and information efficiency. In this work, we resolve this dilemma by simply replacing the activation function in original FSQ with a distribution-matching mapping to enforce a uniform prior. Termed iFSQ, this simple strategy requires just one line of code yet mathematically guarantees both optimal bin utilization and reconstruction precision. Leveraging iFSQ as a controlled benchmark, we uncover two key insights: (1) The optimal equilibrium between discrete and continuous representations lies at approximately 4 bits per dimension. (2) Under identical reconstruction constraints, AR models exhibit rapid initial convergence, whereas diffusion models achieve a superior performance ceiling, suggesting that strict sequential ordering may limit the upper bounds of generation quality. Finally, we extend our analysis by adapting Representation Alignment (REPA) to AR models, yielding LlamaGen-REPA. Codes is available at https://github.com/Tencent-Hunyuan/iFSQ", "AI": {"tldr": "iFSQ improves FSQ quantization by replacing activation function with distribution-matching mapping, enabling optimal bin utilization and reconstruction. Using iFSQ as benchmark reveals 4 bits/dim as optimal discrete-continuous equilibrium, and shows AR models converge faster but diffusion models achieve higher performance ceilings.", "motivation": "The field is divided between autoregressive models (discrete tokens) and diffusion models (continuous latents), hindering unified modeling and fair benchmarking. FSQ bridges this gap theoretically but suffers from activation collapse due to equal-interval quantization, forcing trade-off between reconstruction fidelity and information efficiency.", "method": "Replace the activation function in original FSQ with a distribution-matching mapping to enforce a uniform prior (iFSQ). This simple one-line code change mathematically guarantees both optimal bin utilization and reconstruction precision. Use iFSQ as controlled benchmark to analyze discrete vs continuous representations, and extend analysis by adapting Representation Alignment (REPA) to AR models (LlamaGen-REPA).", "result": "Two key insights: (1) Optimal equilibrium between discrete and continuous representations lies at approximately 4 bits per dimension. (2) Under identical reconstruction constraints, AR models exhibit rapid initial convergence, while diffusion models achieve superior performance ceiling, suggesting strict sequential ordering may limit generation quality upper bounds.", "conclusion": "iFSQ resolves the FSQ quantization dilemma with minimal code change, providing a unified benchmark that reveals fundamental insights about discrete vs continuous representations in image generation. The findings suggest architectural differences between AR and diffusion models lead to different convergence patterns and performance ceilings."}}
{"id": "2601.17789", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17789", "abs": "https://arxiv.org/abs/2601.17789", "authors": ["Yiming Su", "Kunzhao Xu", "Yanjie Gao", "Fan Yang", "Cheng Li", "Mao Yang", "Tianyin Xu"], "title": "Neuro-Symbolic Verification on Instruction Following of LLMs", "comment": null, "summary": "A fundamental problem of applying Large Language Models (LLMs) to important applications is that LLMs do not always follow instructions, and violations are often hard to observe or check. In LLM-based agentic workflows, such violations can propagate and amplify along reasoning chains, causing task failures and system incidents. This paper presents NSVIF, a neuro-symbolic framework for verifying whether an LLM's output follows the instructions used to prompt the LLM. NSVIF is a universal, general-purpose verifier; it makes no assumption about the instruction or the LLM. NSVIF formulates instruction-following verification as a constraint-satisfaction problem by modeling user instructions as constraints. NSVIF models both logical and semantic constraints; constraint solving is done by a unified solver that orchestrates logical reasoning and semantic analysis. To evaluate NSVIF, we develop VIFBENCH, a new benchmark for instruction-following verifiers with fine-grained data labels. Experiments show that NSVIF significantly outperforms LLM-based approaches and provides interpretable feedback. We also show that feedback from NSVIF helps improve LLMs' instruction-following capability without post-training.", "AI": {"tldr": "NSVIF is a neuro-symbolic framework that verifies if LLM outputs follow instructions by modeling instructions as constraints and solving them through logical reasoning and semantic analysis.", "motivation": "LLMs don't always follow instructions, and violations are hard to detect. In agentic workflows, these violations can propagate and cause system failures, creating a need for reliable verification.", "method": "NSVIF models user instructions as both logical and semantic constraints, formulating verification as a constraint-satisfaction problem. It uses a unified solver that orchestrates logical reasoning and semantic analysis.", "result": "NSVIF significantly outperforms LLM-based approaches on VIFBENCH benchmark and provides interpretable feedback. The feedback also helps improve LLMs' instruction-following capability without post-training.", "conclusion": "NSVIF provides an effective universal framework for verifying LLM instruction-following, addressing a critical reliability problem in LLM applications with both strong performance and practical utility."}}
{"id": "2601.18692", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18692", "abs": "https://arxiv.org/abs/2601.18692", "authors": ["Wei Wu", "Fan Lu", "Yunnan Wang", "Shuai Yang", "Shi Liu", "Fangjing Wang", "Qian Zhu", "He Sun", "Yong Wang", "Shuailei Ma", "Yiyu Ren", "Kejia Zhang", "Hui Yu", "Jingmei Zhao", "Shuai Zhou", "Zhenqi Qiu", "Houlong Xiong", "Ziyu Wang", "Zechen Wang", "Ran Cheng", "Yong-Lu Li", "Yongtao Huang", "Xing Zhu", "Yujun Shen", "Kecheng Zheng"], "title": "A Pragmatic VLA Foundation Model", "comment": "Project Webpage: https://technology.robbyant.com/lingbot-vla/, Code: https://github.com/Robbyant/lingbot-vla/", "summary": "Offering great potential in robotic manipulation, a capable Vision-Language-Action (VLA) foundation model is expected to faithfully generalize across tasks and platforms while ensuring cost efficiency (e.g., data and GPU hours required for adaptation). To this end, we develop LingBot-VLA with around 20,000 hours of real-world data from 9 popular dual-arm robot configurations. Through a systematic assessment on 3 robotic platforms, each completing 100 tasks with 130 post-training episodes per task, our model achieves clear superiority over competitors, showcasing its strong performance and broad generalizability. We have also built an efficient codebase, which delivers a throughput of 261 samples per second per GPU with an 8-GPU training setup, representing a 1.5~2.8$\\times$ (depending on the relied VLM base model) speedup over existing VLA-oriented codebases. The above features ensure that our model is well-suited for real-world deployment. To advance the field of robot learning, we provide open access to the code, base model, and benchmark data, with a focus on enabling more challenging tasks and promoting sound evaluation standards.", "AI": {"tldr": "LingBot-VLA is a Vision-Language-Action foundation model trained on 20K hours of real-world dual-arm robot data that achieves superior performance across 3 platforms and 100 tasks while being computationally efficient with 1.5-2.8\u00d7 speedup over existing codebases.", "motivation": "To develop a capable VLA foundation model that faithfully generalizes across tasks and platforms while ensuring cost efficiency in terms of data and GPU hours required for adaptation.", "method": "Trained LingBot-VLA on around 20,000 hours of real-world data from 9 popular dual-arm robot configurations, with systematic assessment on 3 robotic platforms completing 100 tasks with 130 post-training episodes per task. Built an efficient codebase delivering 261 samples/second/GPU with 8-GPU setup.", "result": "Achieves clear superiority over competitors, showcasing strong performance and broad generalizability. Codebase delivers 1.5-2.8\u00d7 speedup over existing VLA-oriented codebases depending on the VLM base model used.", "conclusion": "The model is well-suited for real-world deployment. Authors provide open access to code, base model, and benchmark data to advance robot learning, enable more challenging tasks, and promote sound evaluation standards."}}
{"id": "2601.17151", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17151", "abs": "https://arxiv.org/abs/2601.17151", "authors": ["Qianchu Liu", "Sheng Zhang", "Guanghui Qin", "Yu Gu", "Ying Jin", "Sam Preston", "Yanbo Xu", "Sid Kiblawi", "Wen-wai Yim", "Tim Ossowski", "Tristan Naumann", "Mu Wei", "Hoifung Poon"], "title": "Scaling medical imaging report generation with multimodal reinforcement learning", "comment": null, "summary": "Frontier models have demonstrated remarkable capabilities in understanding and reasoning with natural-language text, but they still exhibit major competency gaps in multimodal understanding and reasoning especially in high-value verticals such as biomedicine. Medical imaging report generation is a prominent example. Supervised fine-tuning can substantially improve performance, but they are prone to overfitting to superficial boilerplate patterns. In this paper, we introduce Universal Report Generation (UniRG) as a general framework for medical imaging report generation. By leveraging reinforcement learning as a unifying mechanism to directly optimize for evaluation metrics designed for end applications, UniRG can significantly improve upon supervised fine-tuning and attain durable generalization across diverse institutions and clinical practices. We trained UniRG-CXR on publicly available chest X-ray (CXR) data and conducted a thorough evaluation in CXR report generation with rigorous evaluation scenarios. On the authoritative ReXrank benchmark, UniRG-CXR sets new overall SOTA, outperforming prior state of the art by a wide margin.", "AI": {"tldr": "UniRG is a reinforcement learning framework for medical imaging report generation that outperforms supervised fine-tuning by directly optimizing evaluation metrics, achieving new SOTA on chest X-ray report generation.", "motivation": "Frontier models have competency gaps in multimodal understanding, especially in biomedicine. Medical imaging report generation is a key example where supervised fine-tuning tends to overfit to superficial patterns rather than learning robust medical reasoning.", "method": "Universal Report Generation (UniRG) uses reinforcement learning as a unifying mechanism to directly optimize for evaluation metrics designed for end applications. The framework was trained on publicly available chest X-ray data (UniRG-CXR).", "result": "UniRG-CXR sets new overall state-of-the-art on the authoritative ReXrank benchmark, outperforming prior SOTA by a wide margin. It demonstrates durable generalization across diverse institutions and clinical practices.", "conclusion": "Reinforcement learning provides a superior approach to supervised fine-tuning for medical imaging report generation, enabling direct optimization of application-specific metrics and achieving better generalization across different healthcare settings."}}
{"id": "2601.17814", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17814", "abs": "https://arxiv.org/abs/2601.17814", "authors": ["Haoxuan Ma", "Guannan Lai", "Han-Jia Ye"], "title": "MMR-Bench: A Comprehensive Benchmark for Multimodal LLM Routing", "comment": null, "summary": "Multimodal large language models (MLLMs) have advanced rapidly, yet heterogeneity in architecture, alignment strategies, and efficiency means that no single model is uniformly superior across tasks. In practical deployments, workloads span lightweight OCR to complex multimodal reasoning; using one MLLM for all queries either over-provisions compute on easy instances or sacrifices accuracy on hard ones. Query-level model selection (routing) addresses this tension, but extending routing from text-only LLMs to MLLMs is nontrivial due to modality fusion, wide variation in computational cost across models, and the absence of a standardized, budget-aware evaluation. We present MMR-Bench, a unified benchmark that isolates the multimodal routing problem and enables comparison under fixed candidate sets and cost models. MMR-Bench provides (i) a controlled environment with modality-aware inputs and variable compute budgets, (ii) a broad suite of vision-language tasks covering OCR, general VQA, and multimodal math reasoning, and (iii) strong single-model reference, oracle upper bounds, and representative routing policies. Using MMR-Bench, we show that incorporating multimodal signals improves routing quality. Empirically, these cues improve the cost-accuracy frontier and enable the routed system to exceed the strongest single model's accuracy at roughly 33% of its cost. Furthermore, policies trained on a subset of models and tasks generalize zero-shot to new datasets and text-only benchmarks without retuning, establishing MMR-Bench as a foundation for studying adaptive multimodal model selection and efficient MLLM deployment. The code will be available at: https://github.com/Hunter-Wrynn/MMR-Bench.", "AI": {"tldr": "MMR-Bench is a benchmark for multimodal routing that enables efficient selection of MLLMs based on query difficulty and compute budget, improving cost-accuracy tradeoffs.", "motivation": "Current MLLMs have heterogeneous architectures and varying efficiency, but no single model excels across all tasks. Using one model for all queries leads to either over-provisioning compute on easy tasks or sacrificing accuracy on hard ones, creating a need for intelligent query-level model selection.", "method": "Developed MMR-Bench benchmark with: (1) controlled environment with modality-aware inputs and variable compute budgets, (2) broad suite of vision-language tasks (OCR, VQA, multimodal math reasoning), (3) strong baselines including single-model references, oracle upper bounds, and representative routing policies.", "result": "Incorporating multimodal signals improves routing quality, enabling routed systems to exceed the strongest single model's accuracy at roughly 33% of its cost. Policies trained on subset of models/tasks generalize zero-shot to new datasets and text-only benchmarks without retuning.", "conclusion": "MMR-Bench establishes a foundation for studying adaptive multimodal model selection and efficient MLLM deployment, demonstrating that intelligent routing can significantly improve cost-accuracy tradeoffs in practical multimodal applications."}}
{"id": "2601.18723", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.18723", "abs": "https://arxiv.org/abs/2601.18723", "authors": ["Mengyuan Liu", "Juyi Sheng", "Peiming Li", "Ziyi Wang", "Tianming Xu", "Tiantian Xu", "Hong Liu"], "title": "Trustworthy Evaluation of Robotic Manipulation: A New Benchmark and AutoEval Methods", "comment": null, "summary": "Driven by the rapid evolution of Vision-Action and Vision-Language-Action models, imitation learning has significantly advanced robotic manipulation capabilities. However, evaluation methodologies have lagged behind, hindering the establishment of Trustworthy Evaluation for these behaviors. Current paradigms rely on binary success rates, failing to address the critical dimensions of trust: Source Authenticity (i.e., distinguishing genuine policy behaviors from human teleoperation) and Execution Quality (e.g., smoothness and safety). To bridge these gaps, we propose a solution that combines the Eval-Actions benchmark and the AutoEval architecture. First, we construct the Eval-Actions benchmark to support trustworthiness analysis. Distinct from existing datasets restricted to successful human demonstrations, Eval-Actions integrates VA and VLA policy execution trajectories alongside human teleoperation data, explicitly including failure scenarios. This dataset is structured around three core supervision signals: Expert Grading (EG), Rank-Guided preferences (RG), and Chain-of-Thought (CoT). Building on this, we propose the AutoEval architecture: AutoEval leverages Spatio-Temporal Aggregation for semantic assessment, augmented by an auxiliary Kinematic Calibration Signal to refine motion smoothness; AutoEval Plus (AutoEval-P) incorporates the Group Relative Policy Optimization (GRPO) paradigm to enhance logical reasoning capabilities. Experiments show AutoEval achieves Spearman's Rank Correlation Coefficients (SRCC) of 0.81 and 0.84 under the EG and RG protocols, respectively. Crucially, the framework possesses robust source discrimination capabilities, distinguishing between policy-generated and teleoperated videos with 99.6% accuracy, thereby establishing a rigorous standard for trustworthy robotic evaluation. Our project and code are available at https://term-bench.github.io/.", "AI": {"tldr": "Proposes Eval-Actions benchmark and AutoEval architecture for trustworthy evaluation of robotic imitation learning, addressing source authenticity and execution quality beyond binary success rates.", "motivation": "Current evaluation methods for Vision-Action and Vision-Language-Action models rely on binary success rates, failing to address critical trust dimensions like source authenticity (distinguishing policy behaviors from human teleoperation) and execution quality (smoothness, safety).", "method": "Two-part solution: 1) Eval-Actions benchmark integrating VA/VLA policy trajectories with human teleoperation data including failures, structured around Expert Grading, Rank-Guided preferences, and Chain-of-Thought supervision; 2) AutoEval architecture with Spatio-Temporal Aggregation and Kinematic Calibration Signal, plus AutoEval-P with Group Relative Policy Optimization for enhanced reasoning.", "result": "AutoEval achieves Spearman's Rank Correlation Coefficients of 0.81 and 0.84 under Expert Grading and Rank-Guided protocols, with 99.6% accuracy in distinguishing policy-generated from teleoperated videos.", "conclusion": "The proposed framework establishes rigorous standards for trustworthy robotic evaluation by addressing source discrimination and execution quality, advancing beyond traditional binary success metrics."}}
{"id": "2601.17185", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17185", "abs": "https://arxiv.org/abs/2601.17185", "authors": ["Shima Salehi", "Atharva Agashe", "Andrew J. McFarland", "Joshua Peeples"], "title": "LGDWT-GS: Local and Global Discrete Wavelet-Regularized 3D Gaussian Splatting for Sparse-View Scene Reconstruction", "comment": null, "summary": "We propose a new method for few-shot 3D reconstruction that integrates global and local frequency regularization to stabilize geometry and preserve fine details under sparse-view conditions, addressing a key limitation of existing 3D Gaussian Splatting (3DGS) models. We also introduce a new multispectral greenhouse dataset containing four spectral bands captured from diverse plant species under controlled conditions. Alongside the dataset, we release an open-source benchmarking package that defines standardized few-shot reconstruction protocols for evaluating 3DGS-based methods. Experiments on our multispectral dataset, as well as standard benchmarks, demonstrate that the proposed method achieves sharper, more stable, and spectrally consistent reconstructions than existing baselines. The dataset and code for this work are publicly available", "AI": {"tldr": "New method for few-shot 3D reconstruction using global+local frequency regularization to stabilize geometry and preserve details in sparse-view conditions, plus new multispectral greenhouse dataset and benchmarking package.", "motivation": "Address limitations of existing 3D Gaussian Splatting (3DGS) models in few-shot 3D reconstruction, particularly instability and loss of fine details under sparse-view conditions.", "method": "Integrates global and local frequency regularization to stabilize geometry while preserving fine details. Also introduces a new multispectral greenhouse dataset with four spectral bands and an open-source benchmarking package with standardized few-shot reconstruction protocols.", "result": "Achieves sharper, more stable, and spectrally consistent reconstructions than existing baselines on both the new multispectral dataset and standard benchmarks.", "conclusion": "The proposed frequency regularization approach effectively improves few-shot 3D reconstruction quality, and the released dataset and benchmarking package provide valuable resources for the research community."}}
{"id": "2601.17826", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17826", "abs": "https://arxiv.org/abs/2601.17826", "authors": ["Siyuan Yang", "Xihan Bian", "Jiayin Tang"], "title": "RegGuard: AI-Powered Retrieval-Enhanced Assistant for Pharmaceutical Regulatory Compliance", "comment": null, "summary": "The increasing frequency and complexity of regulatory updates present a significant burden for multinational pharmaceutical companies. Compliance teams must interpret evolving rules across jurisdictions, formats, and agencies, often manually, at high cost and risk of error. We introduce RegGuard, an industrial-scale AI assistant designed to automate the interpretation of heterogeneous regulatory texts and align them with internal corporate policies. The system ingests heterogeneous document sources through a secure pipeline and enhances retrieval and generation quality with two novel components: HiSACC (Hierarchical Semantic Aggregation for Contextual Chunking) semantically segments long documents into coherent units while maintaining consistency across non-contiguous sections. ReLACE (Regulatory Listwise Adaptive Cross-Encoder for Reranking), a domain-adapted cross-encoder built on an open-source model, jointly models user queries and retrieved candidates to improve ranking relevance. Evaluations in enterprise settings demonstrate that RegGuard improves answer quality specifically in terms of relevance, groundedness, and contextual focus, while significantly mitigating hallucination risk. The system architecture is built for auditability and traceability, featuring provenance tracking, access control, and incremental indexing, making it highly responsive to evolving document sources and relevant for any domain with stringent compliance demands.", "AI": {"tldr": "RegGuard is an AI assistant that automates interpretation of regulatory texts for pharmaceutical compliance, using novel document segmentation (HiSACC) and reranking (ReLACE) components to improve answer quality while reducing hallucination risks.", "motivation": "Multinational pharmaceutical companies face significant burdens from frequent and complex regulatory updates across jurisdictions. Manual interpretation by compliance teams is costly, error-prone, and struggles with heterogeneous document formats and agency requirements.", "method": "RegGuard uses a secure pipeline to ingest heterogeneous documents and features two novel components: HiSACC (Hierarchical Semantic Aggregation for Contextual Chunking) for semantic segmentation of long documents, and ReLACE (Regulatory Listwise Adaptive Cross-Encoder for Reranking), a domain-adapted cross-encoder that jointly models user queries and retrieved candidates to improve ranking relevance.", "result": "Enterprise evaluations show RegGuard improves answer quality in relevance, groundedness, and contextual focus while significantly mitigating hallucination risk. The system architecture supports auditability, traceability, provenance tracking, access control, and incremental indexing.", "conclusion": "RegGuard provides an industrial-scale AI solution for regulatory compliance that automates interpretation of heterogeneous texts, aligns them with corporate policies, and is designed for auditability and responsiveness to evolving document sources, making it suitable for any domain with stringent compliance demands."}}
{"id": "2601.18733", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18733", "abs": "https://arxiv.org/abs/2601.18733", "authors": ["Li Kang", "Heng Zhou", "Xiufeng Song", "Rui Li", "Bruno N. Y. Chen", "Ziye Wang", "Ximeng Meng", "Stone Tao", "Yiran Qin", "Xiaohong Liu", "Ruimao Zhang", "Lei Bai", "Yilun Du", "Hao Su", "Philip Torr", "Zhenfei Yin", "Ruihao Gong", "Yejun Zeng", "Fengjun Zhong", "Shenghao Jin", "Jinyang Guo", "Xianglong Liu", "Xiaojun Jia", "Tianqi Shan", "Wenqi Ren", "Simeng Qin", "Jialing Yang", "Xiaoyu Ma", "Tianxing Chen", "Zixuan Li", "Zijian Cai", "Yan Qin", "Yusen Qin", "Qiangyu Chen", "Kaixuan Wang", "Zhaoming Han", "Yao Mu", "Ping Luo", "Yuanqi Yao", "Haoming Song", "Jan-Nico Zaech", "Fabien Despinoy", "Danda Pani Paudel", "Luc Van Gool"], "title": "Advances and Innovations in the Multi-Agent Robotic System (MARS) Challenge", "comment": "MARS Challenge @ NeurIPS 2025 Workshop on Space in Vision, Language, and Embodied AI. Challenge page: https://mars-eai.github.io/MARS-Challenge-Webpage/", "summary": "Recent advancements in multimodal large language models and vision-languageaction models have significantly driven progress in Embodied AI. As the field transitions toward more complex task scenarios, multi-agent system frameworks are becoming essential for achieving scalable, efficient, and collaborative solutions. This shift is fueled by three primary factors: increasing agent capabilities, enhancing system efficiency through task delegation, and enabling advanced human-agent interactions. To address the challenges posed by multi-agent collaboration, we propose the Multi-Agent Robotic System (MARS) Challenge, held at the NeurIPS 2025 Workshop on SpaVLE. The competition focuses on two critical areas: planning and control, where participants explore multi-agent embodied planning using vision-language models (VLMs) to coordinate tasks and policy execution to perform robotic manipulation in dynamic environments. By evaluating solutions submitted by participants, the challenge provides valuable insights into the design and coordination of embodied multi-agent systems, contributing to the future development of advanced collaborative AI systems.", "AI": {"tldr": "The MARS Challenge at NeurIPS 2025 focuses on multi-agent robotic systems for Embodied AI, addressing planning and control through vision-language models and robotic manipulation in dynamic environments.", "motivation": "The field of Embodied AI is transitioning to complex task scenarios requiring multi-agent systems for scalable, efficient, and collaborative solutions. This shift is driven by increasing agent capabilities, task delegation for system efficiency, and advanced human-agent interactions.", "method": "The paper proposes the Multi-Agent Robotic System (MARS) Challenge as a competition framework. It focuses on two critical areas: 1) multi-agent embodied planning using vision-language models (VLMs) to coordinate tasks, and 2) policy execution for robotic manipulation in dynamic environments.", "result": "The challenge provides valuable insights into the design and coordination of embodied multi-agent systems by evaluating solutions submitted by participants at the NeurIPS 2025 Workshop on SpaVLE.", "conclusion": "The MARS Challenge contributes to the future development of advanced collaborative AI systems by addressing the challenges of multi-agent collaboration in Embodied AI through structured competition and evaluation frameworks."}}
{"id": "2601.17194", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17194", "abs": "https://arxiv.org/abs/2601.17194", "authors": ["Cheyu Lin", "Katherine A. Flanigan", "Sirajum Munir"], "title": "Decoding Psychological States Through Movement: Inferring Human Kinesic Functions with Application to Built Environments", "comment": null, "summary": "Social infrastructure and other built environments are increasingly expected to support well-being and community resilience by enabling social interaction. Yet in civil and built-environment research, there is no consistent and privacy-preserving way to represent and measure socially meaningful interaction in these spaces, leaving studies to operationalize \"interaction\" differently across contexts and limiting practitioners' ability to evaluate whether design interventions are changing the forms of interaction that social capital theory predicts should matter. To address this field-level and methodological gap, we introduce the Dyadic User Engagement DataseT (DUET) dataset and an embedded kinesics recognition framework that operationalize Ekman and Friesen's kinesics taxonomy as a function-level interaction vocabulary aligned with social capital-relevant behaviors (e.g., reciprocity and attention coordination). DUET captures 12 dyadic interactions spanning all five kinesic functions-emblems, illustrators, affect displays, adaptors, and regulators-across four sensing modalities and three built-environment contexts, enabling privacy-preserving analysis of communicative intent through movement. Benchmarking six open-source, state-of-the-art human activity recognition models quantifies the difficulty of communicative-function recognition on DUET and highlights the limitations of ubiquitous monadic, action-level recognition when extended to dyadic, socially grounded interaction measurement. Building on DUET, our recognition framework infers communicative function directly from privacy-preserving skeletal motion without handcrafted action-to-function dictionaries; using a transfer-learning architecture, it reveals structured clustering of kinesic functions and a strong association between representation quality and classification performance while generalizing across subjects and contexts.", "AI": {"tldr": "DUET dataset and kinesics recognition framework for privacy-preserving measurement of socially meaningful interactions in built environments, enabling analysis of social capital-relevant behaviors through movement patterns.", "motivation": "There's no consistent, privacy-preserving way to measure socially meaningful interactions in built environments, limiting research and evaluation of design interventions on social capital-relevant behaviors.", "method": "Developed DUET dataset capturing 12 dyadic interactions across 5 kinesic functions using 4 sensing modalities in 3 built-environment contexts, plus a recognition framework that infers communicative function from skeletal motion using transfer learning.", "result": "Benchmarked 6 state-of-the-art HAR models showing difficulty of communicative-function recognition, revealed structured clustering of kinesic functions, strong association between representation quality and classification performance, and generalization across subjects/contexts.", "conclusion": "DUET and the kinesics recognition framework provide a privacy-preserving, standardized approach to measure socially meaningful interactions in built environments, addressing methodological gaps in evaluating design interventions for social capital development."}}
{"id": "2601.17828", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17828", "abs": "https://arxiv.org/abs/2601.17828", "authors": ["Tanvi Verma", "Yang Zhou", "Rick Siow Mong Goh", "Yong Liu"], "title": "Aligning Medical Conversational AI through Online Reinforcement Learning with Information-Theoretic Rewards", "comment": null, "summary": "We present Information Gain Fine-Tuning (IGFT), a novel approach for training medical conversational AI to conduct effective patient interviews and generate comprehensive History of Present Illness (HPI) without requiring pre-collected human conversations. IGFT combines online Group Relative Policy Optimization (GRPO) with information-theoretic rewards, enabling models to learn from self-generated conversations with simulated patients. Unlike existing approaches that rely on expensive expert-annotated conversations or static datasets, our online RL framework allows models to discover effective questioning strategies through exploration. Our key innovation is an information gain reward function that tracks which clinical entities such as symptoms, temporal patterns, and medical history, are revealed during conversation. Each question's reward is computed based on its expected information gain combined with GPT-4o-mini quality assessments across dimensions including clinical relevance, patient engagement, and specificity. This hybrid approach ensures models learn to ask targeted, clinically appropriate questions that efficiently gather diagnostic information. We fine-tune two models using LoRA: Llama-3.1-8B-Instruct and DeepSeek-R1-Distill-Qwen-7B (a reasoning-optimized model). Training exclusively on Avey data containing concise HPIs, we evaluate generalization to MIMIC data with longer, more elaborate HPIs. DeepSeek-R1-Distill-Qwen-7B (IGFT) achieves F1 scores of 0.408 on Avey (10.9% improvement over base) and 0.289 on MIMIC (12.9% improvement), while Llama-3.1-8B-Instruct (IGFT) reaches 0.384 and 0.336 respectively. Both models outperform OpenAI's model on MIMIC and surpass medical domain-specific baselines like HuatuoGPT and UltraMedical, which were optimized for single-turn medical QA rather than multi-turn conversations.", "AI": {"tldr": "IGFT is a novel RL-based fine-tuning method for medical conversational AI that uses information gain rewards and online self-play with simulated patients to train models for effective patient interviews without human conversation data.", "motivation": "Existing approaches for training medical conversational AI require expensive expert-annotated conversations or static datasets, limiting scalability and adaptability. There's a need for methods that can learn effective questioning strategies autonomously without pre-collected human conversations.", "method": "IGFT combines online Group Relative Policy Optimization (GRPO) with information-theoretic rewards. Models learn from self-generated conversations with simulated patients using an information gain reward function that tracks clinical entity revelation. Each question's reward combines expected information gain with GPT-4o-mini quality assessments across clinical relevance, patient engagement, and specificity. The approach fine-tunes models using LoRA.", "result": "DeepSeek-R1-Distill-Qwen-7B (IGFT) achieved F1 scores of 0.408 on Avey (10.9% improvement) and 0.289 on MIMIC (12.9% improvement). Llama-3.1-8B-Instruct (IGFT) reached 0.384 and 0.336 respectively. Both models outperformed OpenAI's model on MIMIC and surpassed medical domain-specific baselines like HuatuoGPT and UltraMedical.", "conclusion": "IGFT enables effective training of medical conversational AI without human conversation data, demonstrating strong generalization across datasets and outperforming existing approaches. The information gain reward mechanism successfully guides models to ask targeted, clinically appropriate questions for comprehensive HPI generation."}}
{"id": "2601.18765", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.18765", "abs": "https://arxiv.org/abs/2601.18765", "authors": ["Shutong Chen", "Adnan Aijaz", "Yansha Deng"], "title": "Goal-oriented Communication for Fast and Robust Robotic Fault Detection and Recovery", "comment": "Submit to IEEE for potential publication", "summary": "Autonomous robotic systems are widely deployed in smart factories and operate in dynamic, uncertain, and human-involved environments that require low-latency and robust fault detection and recovery (FDR). However, existing FDR frameworks exhibit various limitations, such as significant delays in communication and computation, and unreliability in robot motion/trajectory generation, mainly because the communication-computation-control (3C) loop is designed without considering the downstream FDR goal. To address this, we propose a novel Goal-oriented Communication (GoC) framework that jointly designs the 3C loop tailored for fast and robust robotic FDR, with the goal of minimising the FDR time while maximising the robotic task (e.g., workpiece sorting) success rate. For fault detection, our GoC framework innovatively defines and extracts the 3D scene graph (3D-SG) as the semantic representation via our designed representation extractor, and detects faults by monitoring spatial relationship changes in the 3D-SG. For fault recovery, we fine-tune a small language model (SLM) via Low-Rank Adaptation (LoRA) and enhance its reasoning and generalization capabilities via knowledge distillation to generate recovery motions for robots. We also design a lightweight goal-oriented digital twin reconstruction module to refine the recovery motions generated by the SLM when fine-grained robotic control is required, using only task-relevant object contours for digital twin reconstruction. Extensive simulations demonstrate that our GoC framework reduces the FDR time by up to 82.6% and improves the task success rate by up to 76%, compared to the state-of-the-art frameworks that rely on vision language models for fault detection and large language models for fault recovery.", "AI": {"tldr": "A Goal-oriented Communication framework that jointly optimizes communication, computation, and control for faster, more reliable robotic fault detection and recovery in smart factories.", "motivation": "Existing FDR frameworks have significant delays and unreliability due to communication-computation-control loops not being designed with downstream FDR goals in mind, which is critical for robots operating in dynamic, human-involved factory environments.", "method": "Uses 3D scene graphs for semantic fault detection by monitoring spatial relationship changes, fine-tunes small language models with LoRA and knowledge distillation for recovery motion generation, and employs goal-oriented digital twin reconstruction for motion refinement using only task-relevant object contours.", "result": "Reduces FDR time by up to 82.6% and improves task success rate by up to 76% compared to state-of-the-art frameworks using vision language models for detection and large language models for recovery.", "conclusion": "The proposed Goal-oriented Communication framework effectively addresses limitations of existing FDR systems by jointly optimizing the 3C loop, enabling faster and more reliable robotic fault handling in dynamic factory environments."}}
{"id": "2601.17211", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17211", "abs": "https://arxiv.org/abs/2601.17211", "authors": ["Anzhe Cheng", "Italo Ivo Lima Dias Pinto", "Paul Bogdan"], "title": "Structural Complexity of Brain MRI reveals age-associated patterns", "comment": "accepted by icassp2026", "summary": "We adapt structural complexity analysis to three-dimensional signals, with an emphasis on brain magnetic resonance imaging (MRI). This framework captures the multiscale organization of volumetric data by coarse-graining the signal at progressively larger spatial scales and quantifying the information lost between successive resolutions. While the traditional block-based approach can become unstable at coarse resolutions due to limited sampling, we introduce a sliding-window coarse-graining scheme that provides smoother estimates and improved robustness at large scales. Using this refined method, we analyze large structural MRI datasets spanning mid- to late adulthood and find that structural complexity decreases systematically with age, with the strongest effects emerging at coarser scales. These findings highlight structural complexity as a reliable signal processing tool for multiscale analysis of 3D imaging data, while also demonstrating its utility in predicting biological age from brain MRI.", "AI": {"tldr": "Structural complexity analysis adapted to 3D brain MRI using multiscale coarse-graining reveals age-related complexity decreases, strongest at coarse scales.", "motivation": "To develop a robust structural complexity framework for 3D brain MRI that captures multiscale organization and addresses limitations of traditional block-based approaches.", "method": "Introduced sliding-window coarse-graining scheme to replace unstable block-based approach, enabling smoother estimates at large scales. Applied to large structural MRI datasets across adulthood.", "result": "Structural complexity systematically decreases with age, with strongest effects at coarser spatial scales. The method reliably predicts biological age from brain MRI.", "conclusion": "Structural complexity is a reliable multiscale analysis tool for 3D imaging data with demonstrated utility in aging research and biological age prediction from brain MRI."}}
{"id": "2601.17887", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17887", "abs": "https://arxiv.org/abs/2601.17887", "authors": ["Jiahe Guo", "Xiangran Guo", "Yulin Hu", "Zimo Long", "Xingyu Sui", "Xuda Zhi", "Yongbo Huang", "Hao He", "Weixiang Zhao", "Yanyan Zhao", "Bing Qin"], "title": "When Personalization Legitimizes Risks: Uncovering Safety Vulnerabilities in Personalized Dialogue Agents", "comment": null, "summary": "Long-term memory enables large language model (LLM) agents to support personalized and sustained interactions. However, most work on personalized agents prioritizes utility and user experience, treating memory as a neutral component and largely overlooking its safety implications. In this paper, we reveal intent legitimation, a previously underexplored safety failure in personalized agents, where benign personal memories bias intent inference and cause models to legitimize inherently harmful queries. To study this phenomenon, we introduce PS-Bench, a benchmark designed to identify and quantify intent legitimation in personalized interactions. Across multiple memory-augmented agent frameworks and base LLMs, personalization increases attack success rates by 15.8%-243.7% relative to stateless baselines. We further provide mechanistic evidence for intent legitimation from internal representations space, and propose a lightweight detection-reflection method that effectively reduces safety degradation. Overall, our work provides the first systematic exploration and evaluation of intent legitimation as a safety failure mode that naturally arises from benign, real-world personalization, highlighting the importance of assessing safety under long-term personal context. WARNING: This paper may contain harmful content.", "AI": {"tldr": "Personalized LLM agents with long-term memory can legitimize harmful queries through benign personal memories, increasing attack success rates by 15.8%-243.7%.", "motivation": "Most personalized agent research prioritizes utility and user experience while overlooking safety implications of memory systems, particularly how benign personal memories can bias intent inference to legitimize harmful queries.", "method": "Introduced PS-Bench benchmark to identify and quantify intent legitimation; studied multiple memory-augmented agent frameworks and base LLMs; provided mechanistic evidence from internal representations; proposed lightweight detection-reflection method.", "result": "Personalization increases attack success rates by 15.8%-243.7% relative to stateless baselines; intent legitimation occurs across multiple frameworks and LLMs; detection-reflection method effectively reduces safety degradation.", "conclusion": "First systematic exploration of intent legitimation as a safety failure mode arising from benign personalization, highlighting the importance of assessing safety under long-term personal context in LLM agents."}}
{"id": "2601.17216", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2601.17216", "abs": "https://arxiv.org/abs/2601.17216", "authors": ["Murat Arda Onsu", "Poonam Lohan", "Burak Kantarci", "Aisha Syed", "Matthew Andrews", "Sean Kennedy"], "title": "Spatiotemporal Semantic V2X Framework for Cooperative Collision Prediction", "comment": "6 pages 5 figures, accepted to IEEE ICC 2026", "summary": "Intelligent Transportation Systems (ITS) demand real-time collision prediction to ensure road safety and reduce accident severity. Conventional approaches rely on transmitting raw video or high-dimensional sensory data from roadside units (RSUs) to vehicles, which is impractical under vehicular communication bandwidth and latency constraints. In this work, we propose a semantic V2X framework in which RSU-mounted cameras generate spatiotemporal semantic embeddings of future frames using the Video Joint Embedding Predictive Architecture (V-JEPA). To evaluate the system, we construct a digital twin of an urban traffic environment enabling the generation of d verse traffic scenarios with both safe and collision events. These embeddings of the future frame, extracted from V-JEPA, capture task-relevant traffic dynamics and are transmitted via V2X links to vehicles, where a lightweight attentive probe and classifier decode them to predict imminent collisions. By transmitting only semantic embeddings instead of raw frames, the proposed system significantly reduces communication overhead while maintaining predictive accuracy. Experimental results demonstrate that the framework with an appropriate processing method achieves a 10% F1-score improvement for collision prediction while reducing transmission requirements by four orders of magnitude compared to raw video. This validates the potential of semantic V2X communication to enable cooperative, real-time collision prediction in ITS.", "AI": {"tldr": "Semantic V2X framework uses V-JEPA to generate future frame embeddings from RSU cameras, transmitted to vehicles for collision prediction with 10% F1-score improvement and 10,000x bandwidth reduction.", "motivation": "ITS needs real-time collision prediction but conventional raw video transmission is impractical due to bandwidth/latency constraints in vehicular communications.", "method": "RSU cameras generate spatiotemporal semantic embeddings of future frames using V-JEPA, transmitted via V2X to vehicles where lightweight attentive probe and classifier decode them for collision prediction.", "result": "Framework achieves 10% F1-score improvement for collision prediction while reducing transmission requirements by four orders of magnitude (10,000x) compared to raw video.", "conclusion": "Semantic V2X communication enables cooperative, real-time collision prediction in ITS by significantly reducing communication overhead while maintaining predictive accuracy."}}
{"id": "2601.17897", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17897", "abs": "https://arxiv.org/abs/2601.17897", "authors": ["Jiayu Liu", "Yinhe Long", "Zhenya Huang", "Enhong Chen"], "title": "UniCog: Uncovering Cognitive Abilities of LLMs through Latent Mind Space Analysis", "comment": null, "summary": "A growing body of research suggests that the cognitive processes of large language models (LLMs) differ fundamentally from those of humans. However, existing interpretability methods remain limited in explaining how cognitive abilities are engaged during LLM reasoning. In this paper, we propose UniCog, a unified framework that analyzes LLM cognition via a latent mind space. Formulated as a latent variable model, UniCog encodes diverse abilities from dense model activations into sparse, disentangled latent dimensions. Through extensive analysis on six advanced LLMs, including DeepSeek-V3.2 and GPT-4o, we reveal a Pareto principle of LLM cognition, where a shared reasoning core is complemented by ability-specific signatures. Furthermore, we discover that reasoning failures often manifest as anomalous intensity in latent activations. These findings opens a new paradigm in LLM analysis, providing a cognition grounded view of reasoning dynamics. Finally, leveraging these insights, we introduce a latent-informed candidate prioritization strategy, which improves reasoning performance by up to 7.5% across challenging benchmarks. Our code is available at https://github.com/milksalute/unicog.", "AI": {"tldr": "UniCog is a framework that analyzes LLM cognition through a latent mind space, revealing a Pareto principle where shared reasoning is complemented by ability-specific signatures, and using this to improve reasoning performance.", "motivation": "Existing interpretability methods are limited in explaining how cognitive abilities are engaged during LLM reasoning, despite growing evidence that LLM cognitive processes differ fundamentally from humans.", "method": "Propose UniCog, a unified framework formulated as a latent variable model that encodes diverse abilities from dense model activations into sparse, disentangled latent dimensions.", "result": "Analysis on six advanced LLMs reveals a Pareto principle of LLM cognition (shared reasoning core + ability-specific signatures), shows reasoning failures manifest as anomalous latent activations, and latent-informed candidate prioritization improves reasoning performance by up to 7.5%.", "conclusion": "UniCog opens a new paradigm in LLM analysis by providing a cognition-grounded view of reasoning dynamics and demonstrates practical utility in improving reasoning performance through latent-informed strategies."}}
{"id": "2601.17885", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.17885", "abs": "https://arxiv.org/abs/2601.17885", "authors": ["Qingyu Fan", "Zhaoxiang Li", "Yi Lu", "Wang Chen", "Qiu Shen", "Xiao-xiao Long", "Yinghao Cai", "Tao Lu", "Shuo Wang", "Xun Cao"], "title": "PEAfowl: Perception-Enhanced Multi-View Vision-Language-Action for Bimanual Manipulation", "comment": null, "summary": "Bimanual manipulation in cluttered scenes requires policies that remain stable under occlusions, viewpoint and scene variations. Existing vision-language-action models often fail to generalize because (i) multi-view features are fused via view-agnostic token concatenation, yielding weak 3D-consistent spatial understanding, and (ii) language is injected as global conditioning, resulting in coarse instruction grounding.\n  In this paper, we introduce PEAfowl, a perception-enhanced multi-view VLA policy for bimanual manipulation. For spatial reasoning, PEAfowl predicts per-token depth distributions, performs differentiable 3D lifting, and aggregates local cross-view neighbors to form geometrically grounded, cross-view consistent representations. For instruction grounding, we propose to replace global conditioning with a Perceiver-style text-aware readout over frozen CLIP visual features, enabling iterative evidence accumulation. To overcome noisy and incomplete commodity depth without adding inference overhead, we apply training-only depth distillation from a pretrained depth teacher to supervise the depth-distribution head, providing perception front-end with geometry-aware priors.\n  On RoboTwin 2.0 under domain-randomized setting, PEAfowl improves the strongest baseline by 23.0 pp in success rate, and real-robot experiments further demonstrate reliable sim-to-real transfer and consistent improvements from depth distillation.\n  Project website: https://peafowlvla.github.io/.", "AI": {"tldr": "PEAfowl improves bimanual manipulation by enhancing spatial reasoning with 3D-consistent multi-view features and better instruction grounding through iterative text-aware readout, achieving 23% success rate improvement.", "motivation": "Existing vision-language-action models fail to generalize for bimanual manipulation in cluttered scenes due to weak 3D-consistent spatial understanding from view-agnostic token concatenation and coarse instruction grounding from global language conditioning.", "method": "PEAfowl predicts per-token depth distributions, performs differentiable 3D lifting, aggregates local cross-view neighbors for geometrically grounded representations, and uses Perceiver-style text-aware readout over frozen CLIP features for iterative evidence accumulation. Training-only depth distillation from a pretrained teacher provides geometry-aware priors without inference overhead.", "result": "On RoboTwin 2.0 under domain-randomized setting, PEAfowl improves the strongest baseline by 23.0 percentage points in success rate. Real-robot experiments demonstrate reliable sim-to-real transfer and consistent improvements from depth distillation.", "conclusion": "PEAfowl's perception-enhanced approach with 3D-consistent spatial reasoning and iterative instruction grounding significantly improves bimanual manipulation performance and enables effective sim-to-real transfer."}}
{"id": "2601.17228", "categories": ["cs.CV", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2601.17228", "abs": "https://arxiv.org/abs/2601.17228", "authors": ["Tengyue Zhang", "Ruiwen Ding", "Luoting Zhuang", "Yuxiao Wu", "Erika F. Rodriguez", "William Hsu"], "title": "Semi-Supervised Domain Adaptation with Latent Diffusion for Pathology Image Classification", "comment": null, "summary": "Deep learning models in computational pathology often fail to generalize across cohorts and institutions due to domain shift. Existing approaches either fail to leverage unlabeled data from the target domain or rely on image-to-image translation, which can distort tissue structures and compromise model accuracy. In this work, we propose a semi-supervised domain adaptation (SSDA) framework that utilizes a latent diffusion model trained on unlabeled data from both the source and target domains to generate morphology-preserving and target-aware synthetic images. By conditioning the diffusion model on foundation model features, cohort identity, and tissue preparation method, we preserve tissue structure in the source domain while introducing target-domain appearance characteristics. The target-aware synthetic images, combined with real, labeled images from the source cohort, are subsequently used to train a downstream classifier, which is then tested on the target cohort. The effectiveness of the proposed SSDA framework is demonstrated on the task of lung adenocarcinoma prognostication. The proposed augmentation yielded substantially better performance on the held-out test set from the target cohort, without degrading source-cohort performance. The approach improved the weighted F1 score on the target-cohort held-out test set from 0.611 to 0.706 and the macro F1 score from 0.641 to 0.716. Our results demonstrate that target-aware diffusion-based synthetic data augmentation provides a promising and effective approach for improving domain generalization in computational pathology.", "AI": {"tldr": "Semi-supervised domain adaptation framework using latent diffusion models to generate target-aware synthetic images that preserve tissue morphology while adapting appearance, improving lung adenocarcinoma prognostication across cohorts.", "motivation": "Deep learning models in computational pathology fail to generalize across cohorts/institutions due to domain shift. Existing approaches either don't leverage unlabeled target data or use image translation that distorts tissue structures.", "method": "Propose SSDA framework using latent diffusion model trained on unlabeled source/target data. Model conditions on foundation features, cohort identity, and tissue prep to preserve structure while introducing target appearance. Synthetic images + real labeled source data train downstream classifier.", "result": "Improved weighted F1 from 0.611 to 0.706 and macro F1 from 0.641 to 0.716 on target-cohort test set. Better performance on target cohort without degrading source-cohort performance.", "conclusion": "Target-aware diffusion-based synthetic data augmentation provides promising approach for improving domain generalization in computational pathology, demonstrated on lung adenocarcinoma prognostication task."}}
{"id": "2601.17915", "categories": ["cs.AI", "cs.LG", "cs.LO"], "pdf": "https://arxiv.org/pdf/2601.17915", "abs": "https://arxiv.org/abs/2601.17915", "authors": ["Saurabh Jha", "Rohan Arora", "Bhavya", "Noah Zheutlin", "Paulina Toro Isaza", "Laura Shwartz", "Yu Deng", "Daby Sow", "Ruchi Mahindru", "Ruchir Puri"], "title": "Think Locally, Explain Globally: Graph-Guided LLM Investigations via Local Reasoning and Belief Propagation", "comment": null, "summary": "LLM agents excel when environments are mostly static and the needed information fits in a model's context window, but they often fail in open-ended investigations where explanations must be constructed by iteratively mining evidence from massive, heterogeneous operational data. These investigations exhibit hidden dependency structure: entities interact, signals co-vary, and the importance of a fact may only become clear after other evidence is discovered. Because the context window is bounded, agents must summarize intermediate findings before their significance is known, increasing the risk of discarding key evidence. ReAct-style agents are especially brittle in this regime. Their retrieve-summarize-reason loop makes conclusions sensitive to exploration order and introduces run-to-run non-determinism, producing a reliability gap where Pass-at-k may be high but Majority-at-k remains low. Simply sampling more rollouts or generating longer reasoning traces does not reliably stabilize results, since hypotheses cannot be autonomously checked as new evidence arrives and there is no explicit mechanism for belief bookkeeping and revision. In addition, ReAct entangles semantic reasoning with controller duties such as tool orchestration and state tracking, so execution errors and plan drift degrade reasoning while consuming scarce context.\n  We address these issues by formulating investigation as abductive reasoning over a dependency graph and proposing EoG (Explanations over Graphs), a disaggregated framework in which an LLM performs bounded local evidence mining and labeling (cause vs symptom) while a deterministic controller manages traversal, state, and belief propagation to compute a minimal explanatory frontier. On a representative ITBench diagnostics task, EoG improves both accuracy and run-to-run consistency over ReAct baselines, including a 7x average gain in Majority-at-k entity F1.", "AI": {"tldr": "EoG framework improves LLM agent reliability for open-ended investigations by separating reasoning from control and using deterministic graph-based belief propagation instead of ReAct's brittle retrieve-summarize-reason loop.", "motivation": "Current LLM agents fail in open-ended investigations with massive heterogeneous data due to context window limitations, hidden dependencies, and ReAct's brittleness where conclusions are sensitive to exploration order and lack belief revision mechanisms.", "method": "EoG formulates investigation as abductive reasoning over a dependency graph, with LLM performing local evidence mining/labeling (cause vs symptom) while a deterministic controller manages traversal, state, and belief propagation to compute minimal explanatory frontier.", "result": "On ITBench diagnostics task, EoG improves both accuracy and run-to-run consistency over ReAct baselines, including 7x average gain in Majority-at-k entity F1.", "conclusion": "Disaggregating reasoning from control with deterministic graph-based belief propagation addresses reliability gaps in LLM agents for open-ended investigations, overcoming ReAct's brittleness and context window limitations."}}
{"id": "2601.17895", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.17895", "abs": "https://arxiv.org/abs/2601.17895", "authors": ["Bin Tan", "Changjiang Sun", "Xiage Qin", "Hanat Adai", "Zelin Fu", "Tianxiang Zhou", "Han Zhang", "Yinghao Xu", "Xing Zhu", "Yujun Shen", "Nan Xue"], "title": "Masked Depth Modeling for Spatial Perception", "comment": "Tech report, 19 pages, 15 figures and 4 tables", "summary": "Spatial visual perception is a fundamental requirement in physical-world applications like autonomous driving and robotic manipulation, driven by the need to interact with 3D environments. Capturing pixel-aligned metric depth using RGB-D cameras would be the most viable way, yet it usually faces obstacles posed by hardware limitations and challenging imaging conditions, especially in the presence of specular or texture-less surfaces. In this work, we argue that the inaccuracies from depth sensors can be viewed as \"masked\" signals that inherently reflect underlying geometric ambiguities. Building on this motivation, we present LingBot-Depth, a depth completion model which leverages visual context to refine depth maps through masked depth modeling and incorporates an automated data curation pipeline for scalable training. It is encouraging to see that our model outperforms top-tier RGB-D cameras in terms of both depth precision and pixel coverage. Experimental results on a range of downstream tasks further suggest that LingBot-Depth offers an aligned latent representation across RGB and depth modalities. We release the code, checkpoint, and 3M RGB-depth pairs (including 2M real data and 1M simulated data) to the community of spatial perception.", "AI": {"tldr": "LingBot-Depth is a depth completion model that refines inaccurate depth maps from RGB-D cameras using visual context and masked depth modeling, outperforming commercial sensors in precision and coverage.", "motivation": "RGB-D cameras face hardware limitations and imaging challenges (specular/texture-less surfaces) that produce inaccurate depth measurements, which can be viewed as \"masked\" signals reflecting geometric ambiguities.", "method": "LingBot-Depth uses masked depth modeling to refine depth maps through visual context, combined with an automated data curation pipeline for scalable training on 3M RGB-depth pairs (2M real + 1M simulated).", "result": "The model outperforms top-tier RGB-D cameras in both depth precision and pixel coverage, and provides aligned latent representations across RGB and depth modalities for downstream tasks.", "conclusion": "LingBot-Depth offers a practical solution to depth sensor limitations, with released code, checkpoints, and dataset to advance spatial perception research."}}
{"id": "2601.17237", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17237", "abs": "https://arxiv.org/abs/2601.17237", "authors": ["Mike Ranzinger", "Greg Heinrich", "Collin McCarthy", "Jan Kautz", "Andrew Tao", "Bryan Catanzaro", "Pavlo Molchanov"], "title": "C-RADIOv4 (Tech Report)", "comment": null, "summary": "By leveraging multi-teacher distillation, agglomerative vision backbones provide a unified student model that retains and improves the distinct capabilities of multiple teachers. In this tech report, we describe the most recent release of the C-RADIO family of models, C-RADIOv4, which builds upon AM-RADIO/RADIOv2.5 in design, offering strong improvements on key downstream tasks at the same computational complexity. We release -SO400M (412M params), and -H (631M) model variants, both trained with an updated set of teachers: SigLIP2, DINOv3, and SAM3. In addition to improvements on core metrics and new capabilities from imitating SAM3, the C-RADIOv4 model family further improves any-resolution support, brings back the ViTDet option for drastically enhanced efficiency at high-resolution, and comes with a permissive license.", "AI": {"tldr": "C-RADIOv4 is the latest release in the C-RADIO model family, using multi-teacher distillation to create unified student models that improve upon multiple specialized teacher models while maintaining computational efficiency.", "motivation": "To develop a unified vision backbone that combines and improves upon the distinct capabilities of multiple specialized teacher models (SigLIP2, DINOv3, and SAM3) while maintaining computational efficiency and offering permissive licensing.", "method": "Multi-teacher distillation approach building upon AM-RADIO/RADIOv2.5 design, training with updated teacher set (SigLIP2, DINOv3, SAM3), offering two model variants (412M and 631M parameters), with enhanced any-resolution support and ViTDet option for high-resolution efficiency.", "result": "Strong improvements on key downstream tasks at same computational complexity, new capabilities from imitating SAM3, improved any-resolution support, drastically enhanced efficiency at high-resolution via ViTDet option, and permissive licensing.", "conclusion": "C-RADIOv4 successfully advances the agglomerative vision backbone approach, delivering improved performance and capabilities while maintaining efficiency, making it a practical and accessible solution for vision tasks."}}
{"id": "2601.17920", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17920", "abs": "https://arxiv.org/abs/2601.17920", "authors": ["Xuanzhou Chen", "Audrey Wang", "Stanley Yin", "Hanyang Jiang", "Dong Zhang"], "title": "Agentic AI for Self-Driving Laboratories in Soft Matter: Taxonomy, Benchmarks,and Open Challenges", "comment": null, "summary": "Self-driving laboratories (SDLs) close the loop between experiment design, automated execution, and data-driven decision making, and they provide a demanding testbed for agentic AI under expensive actions, noisy and delayed feedback, strict feasibility and safety constraints, and non-stationarity. This survey uses soft matter as a representative setting but focuses on the AI questions that arise in real laboratories. We frame SDL autonomy as an agent environment interaction problem with explicit observations, actions, costs, and constraints, and we use this formulation to connect common SDL pipelines to established AI principles. We review the main method families that enable closed loop experimentation, including Bayesian optimization and active learning for sample efficient experiment selection, planning and reinforcement learning for long horizon protocol optimization, and tool using agents that orchestrate heterogeneous instruments and software. We emphasize verifiable and provenance aware policies that support debugging, reproducibility, and safe operation. We then propose a capability driven taxonomy that organizes systems by decision horizon, uncertainty modeling, action parameterization, constraint handling, failure recovery, and human involvement. To enable meaningful comparison, we synthesize benchmark task templates and evaluation metrics that prioritize cost aware performance, robustness to drift, constraint violation behavior, and reproducibility. Finally, we distill lessons from deployed SDLs and outline open challenges in multi-modal representation, calibrated uncertainty, safe exploration, and shared benchmark infrastructure.", "AI": {"tldr": "Survey of self-driving laboratories (SDLs) as testbeds for agentic AI, focusing on AI methods for closed-loop experimentation, proposing taxonomy and benchmarks, and outlining open challenges.", "motivation": "SDL autonomy presents demanding AI challenges including expensive actions, noisy/delayed feedback, strict constraints, and non-stationarity, making it an ideal testbed for agentic AI systems.", "method": "Frames SDL autonomy as agent-environment interaction; reviews Bayesian optimization, active learning, planning, RL, and tool-using agents; proposes capability taxonomy; synthesizes benchmark templates and metrics.", "result": "Comprehensive framework connecting SDL pipelines to AI principles, taxonomy for system comparison, benchmark tasks prioritizing cost-aware performance, robustness, constraint handling, and reproducibility.", "conclusion": "SDL research advances agentic AI in real-world settings; key open challenges include multi-modal representation, calibrated uncertainty, safe exploration, and shared benchmark infrastructure."}}
{"id": "2601.18714", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.18714", "abs": "https://arxiv.org/abs/2601.18714", "authors": ["Judith Vilella-Cantos", "Mauro Martini", "Marcello Chiaberge", "M\u00f3nica Ballesta", "David Valiente"], "title": "Low Cost, High Efficiency: LiDAR Place Recognition in Vineyards with Matryoshka Representation Learning", "comment": null, "summary": "Localization in agricultural environments is challenging due to their unstructured nature and lack of distinctive landmarks. Although agricultural settings have been studied in the context of object classification and segmentation, the place recognition task for mobile robots is not trivial in the current state of the art. In this study, we propose MinkUNeXt-VINE, a lightweight, deep-learning-based method that surpasses state-of-the-art methods in vineyard environments thanks to its pre-processing and Matryoshka Representation Learning multi-loss approach. Our method prioritizes enhanced performance with low-cost, sparse LiDAR inputs and lower-dimensionality outputs to ensure high efficiency in real-time scenarios. Additionally, we present a comprehensive ablation study of the results on various evaluation cases and two extensive long-term vineyard datasets employing different LiDAR sensors. The results demonstrate the efficiency of the trade-off output produced by this approach, as well as its robust performance on low-cost and low-resolution input data. The code is publicly available for reproduction.", "AI": {"tldr": "MinkUNeXt-VINE is a lightweight deep learning method for place recognition in vineyards using sparse LiDAR data, achieving state-of-the-art performance with efficient real-time operation.", "motivation": "Agricultural environments like vineyards are challenging for robot localization due to their unstructured nature and lack of distinctive landmarks. Current place recognition methods struggle in these settings, creating a need for specialized solutions.", "method": "Proposes MinkUNeXt-VINE with pre-processing and Matryoshka Representation Learning multi-loss approach. Designed for low-cost, sparse LiDAR inputs and lower-dimensional outputs to ensure real-time efficiency.", "result": "Surpasses state-of-the-art methods in vineyard environments, demonstrates efficient trade-off output, and shows robust performance with low-cost, low-resolution LiDAR data across two extensive long-term vineyard datasets.", "conclusion": "The method provides an effective solution for agricultural robot localization, balancing performance and efficiency for real-time operation in challenging vineyard environments."}}
{"id": "2601.17254", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17254", "abs": "https://arxiv.org/abs/2601.17254", "authors": ["Takato Yasuno"], "title": "Multi-stage Bridge Inspection System: Integrating Foundation Models with Location Anonymization", "comment": "8 pages, 5 figures, 2 tables", "summary": "In Japan, civil infrastructure condition monitoring is mandated through visual inspection every five years. Field-captured damage images frequently contain concrete cracks and rebar exposure, often accompanied by construction signs revealing regional information. To enable safe infrastructure use without causing public anxiety, it is essential to protect regional information while accurately extracting damage features and visualizing key indicators for repair decision-making. This paper presents an open-source bridge damage detection system with regional privacy protection capabilities. We employ Segment Anything Model (SAM) 3 for rebar corrosion detection and utilize DBSCAN for automatic completion of missed regions. Construction sign regions are detected and protected through Gaussian blur. Four preprocessing methods improve OCR accuracy, and GPU optimization enables 1.7-second processing per image. The technology stack includes SAM3, PyTorch, OpenCV, pytesseract, and scikit-learn, achieving efficient bridge inspection with regional information protection.", "AI": {"tldr": "Open-source bridge damage detection system with regional privacy protection using SAM3 for rebar corrosion detection and DBSCAN for completion, with construction sign blurring for privacy.", "motivation": "Japan mandates visual infrastructure inspections every 5 years, but field images contain sensitive regional information in construction signs alongside damage features. Need to protect regional privacy while accurately extracting damage for repair decisions without causing public anxiety.", "method": "Uses Segment Anything Model (SAM) 3 for rebar corrosion detection, DBSCAN for automatic completion of missed regions. Construction signs are detected and protected via Gaussian blur. Four preprocessing methods improve OCR accuracy. GPU optimization enables fast processing (1.7 seconds per image). Technology stack includes SAM3, PyTorch, OpenCV, pytesseract, and scikit-learn.", "result": "Achieves efficient bridge inspection with regional information protection. System processes images in 1.7 seconds each with GPU optimization. Successfully detects and protects construction sign regions while accurately extracting damage features for repair decision-making.", "conclusion": "Presents a practical open-source system that balances accurate damage detection with regional privacy protection, enabling safe infrastructure monitoring without compromising public confidence through exposure of sensitive regional information."}}
{"id": "2601.17923", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17923", "abs": "https://arxiv.org/abs/2601.17923", "authors": ["Ali Najar"], "title": "Learning Transferable Skills in Action RPGs via Directed Skill Graphs and Selective Adaptation", "comment": "5 pages", "summary": "Lifelong agents should expand their competence over time without retraining from scratch or overwriting previously learned behaviors. We investigate this in a challenging real-time control setting (Dark Souls III) by representing combat as a directed skill graph and training its components in a hierarchical curriculum. The resulting agent decomposes control into five reusable skills: camera control, target lock-on, movement, dodging, and a heal-attack decision policy, each optimized for a narrow responsibility. This factorization improves sample efficiency by reducing the burden on any single policy and supports selective post-training: when the environment shifts from Phase 1 to Phase 2, only a subset of skills must be adapted, while upstream skills remain transferable. Empirically, we find that targeted fine-tuning of just two skills rapidly recovers performance under a limited interaction budget, suggesting that skill-graph curricula together with selective fine-tuning offer a practical pathway toward evolving, continually learning agents in complex real-time environments.", "AI": {"tldr": "Hierarchical skill graph approach enables lifelong learning in complex real-time games by decomposing combat into reusable skills that can be selectively fine-tuned when environment changes.", "motivation": "Lifelong agents need to expand competence over time without retraining from scratch or overwriting previously learned behaviors, especially in challenging real-time control settings like video games.", "method": "Represent combat as a directed skill graph with five reusable skills (camera control, target lock-on, movement, dodging, heal-attack decision), train components in hierarchical curriculum, and enable selective post-training of only affected skills when environment changes.", "result": "Targeted fine-tuning of just two skills rapidly recovers performance under limited interaction budget when environment shifts from Phase 1 to Phase 2, demonstrating efficient adaptation.", "conclusion": "Skill-graph curricula with selective fine-tuning offer practical pathway for evolving, continually learning agents in complex real-time environments by improving sample efficiency and enabling transferable skill reuse."}}
{"id": "2601.17258", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17258", "abs": "https://arxiv.org/abs/2601.17258", "authors": ["Jo\u00e3o Pereira", "Vasco Lopes", "Jo\u00e3o Neves", "David Semedo"], "title": "FineVAU: A Novel Human-Aligned Benchmark for Fine-Grained Video Anomaly Understanding", "comment": null, "summary": "Video Anomaly Understanding (VAU) is a novel task focused on describing unusual occurrences in videos. Despite growing interest, the evaluation of VAU remains an open challenge. Existing benchmarks rely on n-gram-based metrics (e.g., BLEU, ROUGE-L) or LLM-based evaluation. The first fails to capture the rich, free-form, and visually grounded nature of LVLM responses, while the latter focuses on assessing language quality over factual relevance, often resulting in subjective judgments that are misaligned with human perception. In this work, we address this issue by proposing FineVAU, a new benchmark for VAU that shifts the focus towards rich, fine-grained and domain-specific understanding of anomalous videos. We formulate VAU as a three-fold problem, with the goal of comprehensively understanding key descriptive elements of anomalies in video: events (What), participating entities (Who) and location (Where). Our benchmark introduces a) FVScore, a novel, human-aligned evaluation metric that assesses the presence of critical visual elements in LVLM answers, providing interpretable, fine-grained feedback; and b) FineW3, a novel, comprehensive dataset curated through a structured and fully automatic procedure that augments existing human annotations with high quality, fine-grained visual information. Human evaluation reveals that our proposed metric has a superior alignment with human perception of anomalies in comparison to current approaches. Detailed experiments on FineVAU unveil critical limitations in LVLM's ability to perceive anomalous events that require spatial and fine-grained temporal understanding, despite strong performance on coarse grain, static information, and events with strong visual cues.", "AI": {"tldr": "FineVAU is a new benchmark for Video Anomaly Understanding that introduces FVScore metric and FineW3 dataset to evaluate LVLMs on fine-grained anomaly description across What, Who, Where elements.", "motivation": "Existing VAU evaluation methods are inadequate - n-gram metrics fail to capture rich LVLM responses, while LLM-based evaluation focuses on language quality over factual relevance and produces subjective judgments misaligned with human perception.", "method": "Proposes FineVAU benchmark with: 1) FVScore metric that assesses presence of critical visual elements in LVLM answers, providing interpretable fine-grained feedback; 2) FineW3 dataset curated through structured automatic procedure that augments existing annotations with high-quality fine-grained visual information.", "result": "Human evaluation shows FVScore has superior alignment with human perception compared to current approaches. Experiments reveal LVLMs have critical limitations in perceiving anomalous events requiring spatial and fine-grained temporal understanding, despite strong performance on coarse, static information and events with strong visual cues.", "conclusion": "FineVAU addresses the evaluation gap in Video Anomaly Understanding by shifting focus to rich, fine-grained, domain-specific understanding, providing a comprehensive benchmark that better aligns with human perception of anomalies."}}
{"id": "2601.17942", "categories": ["cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2601.17942", "abs": "https://arxiv.org/abs/2601.17942", "authors": ["Yu-Jie Yang", "Hung-Fu Chang", "Po-An Chen"], "title": "LLM-Based SQL Generation: Prompting, Self-Refinement, and Adaptive Weighted Majority Voting", "comment": "29 pages, 22 figures", "summary": "Text-to-SQL has emerged as a prominent research area, particularly with the rapid advancement of large language models (LLMs). By enabling users to query databases through natural language rather than SQL, this technology significantly lowers the barrier to data analysis. However, generating accurate SQL from natural language remains challenging due to ambiguity in user queries, the complexity of schema linking, limited generalization across SQL dialects, and the need for domain-specific understanding. In this study, we propose a Single-Agent Self-Refinement with Ensemble Voting (SSEV) pipeline built on PET-SQL that operates without ground-truth data, integrating self-refinement with Weighted Majority Voting (WMV) and its randomized variant (RWMA). Experimental results show that the SSEV achieves competitive performance across multiple benchmarks, attaining execution accuracies of 85.5% on Spider 1.0-Dev, 86.4% on Spider 1.0-Test, and 66.3% on BIRD-Dev. Building on insights from the SSEV pipeline, we further propose ReCAPAgent-SQL (Refinement-Critique-Act-Plan agent-based SQL framework) to address the growing complexity of enterprise databases and real-world Text-to-SQL tasks. The framework integrates multiple specialized agents for planning, external knowledge retrieval, critique, action generation, self-refinement, schema linking, and result validation, enabling iterative refinement of SQL predictions through agent collaboration. ReCAPAgent-SQL's WMA results achieve 31% execution accuracy on the first 100 queries of Spider 2.0-Lite, demonstrating significant improvements in handling real-world enterprise scenarios. Overall, our work facilitates the deployment of scalable Text-to-SQL systems in practical settings, supporting better data-driven decision-making at lower cost and with greater efficiency.", "AI": {"tldr": "Proposes SSEV pipeline for Text-to-SQL without ground-truth data using self-refinement with weighted voting, and ReCAPAgent-SQL framework with specialized agents for complex enterprise databases.", "motivation": "Text-to-SQL lowers data analysis barriers but faces challenges with query ambiguity, schema linking complexity, limited SQL dialect generalization, and domain-specific understanding needs.", "method": "SSEV pipeline: Single-Agent Self-Refinement with Ensemble Voting built on PET-SQL, using Weighted Majority Voting (WMV) and randomized variant (RWMA). ReCAPAgent-SQL: Multi-agent framework with specialized agents for planning, knowledge retrieval, critique, action generation, self-refinement, schema linking, and validation.", "result": "SSEV achieves 85.5% on Spider 1.0-Dev, 86.4% on Spider 1.0-Test, 66.3% on BIRD-Dev. ReCAPAgent-SQL achieves 31% execution accuracy on first 100 queries of Spider 2.0-Lite, showing significant improvements for enterprise scenarios.", "conclusion": "The work facilitates deployment of scalable Text-to-SQL systems in practical settings, supporting better data-driven decision-making at lower cost and greater efficiency."}}
{"id": "2601.17259", "categories": ["cs.CV", "cs.GR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17259", "abs": "https://arxiv.org/abs/2601.17259", "authors": ["Angad Singh Ahuja", "Aarush Ram Anandh"], "title": "Inference-Time Loss-Guided Colour Preservation in Diffusion Sampling", "comment": "25 Pages, 12 Figures, 3 Tables, 5 Appendices, 8 Algorithms", "summary": "Precise color control remains a persistent failure mode in text-to-image diffusion systems, particularly in design-oriented workflows where outputs must satisfy explicit, user-specified color targets. We present an inference-time, region-constrained color preservation method that steers a pretrained diffusion model without any additional training. Our approach combines (i) ROI-based inpainting for spatial selectivity, (ii) background-latent re-imposition to prevent color drift outside the ROI, and (iii) latent nudging via gradient guidance using a composite loss defined in CIE Lab and linear RGB. The loss is constructed to control not only the mean ROI color but also the tail of the pixelwise error distribution through CVaR-style and soft-maximum penalties, with a late-start gate and a time-dependent schedule to stabilize guidance across denoising steps. We show that mean-only baselines can satisfy average color constraints while producing perceptually salient local failures, motivating our distribution-aware objective. The resulting method provides a practical, training-free mechanism for targeted color adherence that can be integrated into standard Stable Diffusion inpainting pipelines.", "AI": {"tldr": "Training-free method for precise color control in text-to-image diffusion models using ROI-based inpainting with distribution-aware loss functions in CIE Lab/RGB space.", "motivation": "Current text-to-image diffusion systems struggle with precise color control, especially for design workflows requiring exact color targets. Mean-only color constraints often fail perceptually despite meeting average targets.", "method": "Inference-time, region-constrained color preservation combining: (1) ROI-based inpainting for spatial selectivity, (2) background-latent re-imposition to prevent color drift, (3) latent nudging via gradient guidance with composite loss in CIE Lab/linear RGB. Uses CVaR-style and soft-maximum penalties to control error distribution tails, with late-start gate and time-dependent scheduling.", "result": "Method provides practical, training-free mechanism for targeted color adherence that integrates into standard Stable Diffusion inpainting pipelines, addressing perceptual failures of mean-only approaches.", "conclusion": "Distribution-aware color control is necessary for perceptually successful color preservation in diffusion models, achievable through inference-time guidance without additional training."}}
{"id": "2601.18027", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18027", "abs": "https://arxiv.org/abs/2601.18027", "authors": ["Chiyuan Fu", "Lyuhao Chen", "Yunze Xiao", "Weihao Xuan", "Carlos Busso", "Mona Diab"], "title": "Sentipolis: Emotion-Aware Agents for Social Simulations", "comment": null, "summary": "LLM agents are increasingly used for social simulation, yet emotion is often treated as a transient cue, causing emotional amnesia and weak long-horizon continuity. We present Sentipolis, a framework for emotionally stateful agents that integrates continuous Pleasure-Arousal-Dominance (PAD) representation, dual-speed emotion dynamics, and emotion--memory coupling. Across thousands of interactions over multiple base models and evaluators, Sentipolis improves emotionally grounded behavior, boosting communication, and emotional continuity. Gains are model-dependent: believability increases for higher-capacity models but can drop for smaller ones, and emotion-awareness can mildly reduce adherence to social norms, reflecting a human-like tension between emotion-driven behavior and rule compliance in social simulation. Network-level diagnostics show reciprocal, moderately clustered, and temporally stable relationship structures, supporting the study of cumulative social dynamics such as alliance formation and gradual relationship change.", "AI": {"tldr": "Sentipolis is a framework for emotionally stateful LLM agents that addresses emotional amnesia by integrating continuous PAD emotion representation, dual-speed emotion dynamics, and emotion-memory coupling to improve emotional continuity and grounded behavior in social simulations.", "motivation": "Current LLM agents for social simulation treat emotion as transient cues, leading to emotional amnesia and weak long-horizon emotional continuity, limiting their ability to simulate realistic social dynamics.", "method": "Sentipolis integrates three key components: 1) continuous Pleasure-Arousal-Dominance (PAD) emotion representation, 2) dual-speed emotion dynamics (fast/slow emotional responses), and 3) emotion-memory coupling to maintain emotional state across interactions.", "result": "Across thousands of interactions with multiple base models and evaluators, Sentipolis improves emotionally grounded behavior, boosts communication, and enhances emotional continuity. However, gains are model-dependent: believability increases for higher-capacity models but can drop for smaller ones, and emotion-awareness can mildly reduce adherence to social norms.", "conclusion": "Sentipolis enables more realistic social simulations with emotionally continuous agents, supporting study of cumulative social dynamics like alliance formation and relationship change, while revealing a human-like tension between emotion-driven behavior and rule compliance."}}
{"id": "2601.17271", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17271", "abs": "https://arxiv.org/abs/2601.17271", "authors": ["Kun Huang", "Fang-Lue Zhang", "Neil Dodgson"], "title": "Cross360: 360\u00b0 Monocular Depth Estimation via Cross Projections Across Scales", "comment": "TIP, 12 pages", "summary": "360\u00b0 depth estimation is a challenging research problem due to the difficulty of finding a representation that both preserves global continuity and avoids distortion in spherical images. Existing methods attempt to leverage complementary information from multiple projections, but struggle with balancing global and local consistency. Their local patch features have limited global perception, and the combined global representation does not address discrepancies in feature extraction at the boundaries between patches. To address these issues, we propose Cross360, a novel cross-attention-based architecture integrating local and global information using less-distorted tangent patches along with equirectangular features. Our Cross Projection Feature Alignment module employs cross-attention to align local tangent projection features with the equirectangular projection's 360\u00b0 field of view, ensuring each tangent projection patch is aware of the global context. Additionally, our Progressive Feature Aggregation with Attention module refines multi-scaled features progressively, enhancing depth estimation accuracy. Cross360 significantly outperforms existing methods across most benchmark datasets, especially those in which the entire 360\u00b0 image is available, demonstrating its effectiveness in accurate and globally consistent depth estimation. The code and model are available at https://github.com/huangkun101230/Cross360.", "AI": {"tldr": "Cross360 is a novel cross-attention-based architecture for 360\u00b0 depth estimation that integrates local tangent patches with global equirectangular features to achieve accurate and globally consistent depth prediction.", "motivation": "Existing 360\u00b0 depth estimation methods struggle to balance global and local consistency - local patch features have limited global perception, and combined global representations don't address boundary discrepancies between patches.", "method": "Proposes Cross360 with two key modules: 1) Cross Projection Feature Alignment using cross-attention to align local tangent projection features with equirectangular projection's 360\u00b0 field of view, and 2) Progressive Feature Aggregation with Attention that refines multi-scaled features progressively.", "result": "Cross360 significantly outperforms existing methods across most benchmark datasets, especially when the entire 360\u00b0 image is available, demonstrating effectiveness in accurate and globally consistent depth estimation.", "conclusion": "The proposed cross-attention-based architecture successfully addresses the challenge of balancing global and local consistency in 360\u00b0 depth estimation by integrating local tangent patches with global equirectangular features."}}
{"id": "2601.18061", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.18061", "abs": "https://arxiv.org/abs/2601.18061", "authors": ["Kiana Jafari", "Paul Ulrich Nikolaus Rust", "Duncan Eddy", "Robbie Fraser", "Nina Vasan", "Darja Djordjevic", "Akanksha Dadlani", "Max Lamparth", "Eugenia Kim", "Mykel Kochenderfer"], "title": "Expert Evaluation and the Limits of Human Feedback in Mental Health AI Safety Testing", "comment": "17 pages, 7 pages of appendix, 21 tables", "summary": "Learning from human feedback~(LHF) assumes that expert judgments, appropriately aggregated, yield valid ground truth for training and evaluating AI systems. We tested this assumption in mental health, where high safety stakes make expert consensus essential. Three certified psychiatrists independently evaluated LLM-generated responses using a calibrated rubric. Despite similar training and shared instructions, inter-rater reliability was consistently poor ($ICC$ $0.087$--$0.295$), falling below thresholds considered acceptable for consequential assessment. Disagreement was highest on the most safety-critical items. Suicide and self-harm responses produced greater divergence than any other category, and was systematic rather than random. One factor yielded negative reliability (Krippendorff's $\u03b1= -0.203$), indicating structured disagreement worse than chance. Qualitative interviews revealed that disagreement reflects coherent but incompatible individual clinical frameworks, safety-first, engagement-centered, and culturally-informed orientations, rather than measurement error. By demonstrating that experts rely on holistic risk heuristics rather than granular factor discrimination, these findings suggest that aggregated labels function as arithmetic compromises that effectively erase grounded professional philosophies. Our results characterize expert disagreement in safety-critical AI as a sociotechnical phenomenon where professional experience introduces sophisticated layers of principled divergence. We discuss implications for reward modeling, safety classification, and evaluation benchmarks, recommending that practitioners shift from consensus-based aggregation to alignment methods that preserve and learn from expert disagreement.", "AI": {"tldr": "Expert psychiatrists show poor agreement when evaluating LLM mental health responses, with highest disagreement on safety-critical suicide/self-harm content, revealing systematic professional disagreements rather than measurement error.", "motivation": "To test the assumption that aggregated expert judgments provide valid ground truth for AI training/evaluation in high-stakes mental health contexts where expert consensus is essential for safety.", "method": "Three certified psychiatrists independently evaluated LLM-generated mental health responses using a calibrated rubric, with inter-rater reliability analysis (ICC, Krippendorff's \u03b1) and qualitative interviews to understand disagreement sources.", "result": "Poor inter-rater reliability (ICC 0.087-0.295), below acceptable thresholds; highest disagreement on suicide/self-harm responses; systematic rather than random disagreement; negative reliability on some factors; disagreement reflects coherent but incompatible clinical frameworks (safety-first, engagement-centered, culturally-informed).", "conclusion": "Expert disagreement in safety-critical AI evaluation is a sociotechnical phenomenon where aggregated labels erase professional philosophies; recommends shifting from consensus-based aggregation to methods that preserve and learn from expert disagreement."}}
{"id": "2601.17288", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17288", "abs": "https://arxiv.org/abs/2601.17288", "authors": ["Jin Bai", "Huiyao Zhang", "Qi Wen", "Shengyang Li", "Xiaolin Tian", "Atta ur Rahman"], "title": "Fluxamba: Topology-Aware Anisotropic State Space Models for Geological Lineament Segmentation in Multi-Source Remote Sensing", "comment": null, "summary": "The precise segmentation of geological linear features, spanning from planetary lineaments to terrestrial fractures, demands capturing long-range dependencies across complex anisotropic topologies. Although State Space Models (SSMs) offer near-linear computational complexity, their dependence on rigid, axis-aligned scanning trajectories induces a fundamental topological mismatch with curvilinear targets, resulting in fragmented context and feature erosion. To bridge this gap, we propose Fluxamba, a lightweight architecture that introduces a topology-aware feature rectification framework. Central to our design is the Structural Flux Block (SFB), which orchestrates an anisotropic information flux by integrating an Anisotropic Structural Gate (ASG) with a Prior-Modulated Flow (PMF). This mechanism decouples feature orientation from spatial location, dynamically gating context aggregation along the target's intrinsic geometry rather than rigid paths. Furthermore, to mitigate serialization-induced noise in low-contrast environments, we incorporate a Hierarchical Spatial Regulator (HSR) for multi-scale semantic alignment and a High-Fidelity Focus Unit (HFFU) to explicitly maximize the signal-to-noise ratio of faint features. Extensive experiments on diverse geological benchmarks (LROC-Lineament, LineaMapper, and GeoCrack) demonstrate that Fluxamba establishes a new state-of-the-art. Notably, on the challenging LROC-Lineament dataset, it achieves an F1-score of 89.22% and mIoU of 89.87%. Achieving a real-time inference speed of over 24 FPS with only 3.4M parameters and 6.3G FLOPs, Fluxamba reduces computational costs by up to two orders of magnitude compared to heavy-weight baselines, thereby establishing a new Pareto frontier between segmentation fidelity and onboard deployment feasibility.", "AI": {"tldr": "Fluxamba: Lightweight architecture for precise geological linear feature segmentation using topology-aware feature rectification to handle complex anisotropic topologies with near-linear computational efficiency.", "motivation": "Existing State Space Models (SSMs) have rigid, axis-aligned scanning trajectories that create topological mismatch with curvilinear geological features, causing fragmented context and feature erosion in segmentation tasks.", "method": "Proposes Fluxamba with Structural Flux Block (SFB) integrating Anisotropic Structural Gate (ASG) and Prior-Modulated Flow (PMF) to decouple feature orientation from spatial location. Includes Hierarchical Spatial Regulator (HSR) for multi-scale alignment and High-Fidelity Focus Unit (HFFU) to maximize signal-to-noise ratio for faint features.", "result": "Achieves SOTA on geological benchmarks: 89.22% F1-score and 89.87% mIoU on LROC-Lineament. Real-time inference at 24 FPS with only 3.4M parameters and 6.3G FLOPs, reducing computational costs by up to two orders of magnitude.", "conclusion": "Fluxamba establishes new Pareto frontier between segmentation fidelity and deployment feasibility, enabling precise geological linear feature segmentation with lightweight architecture suitable for onboard deployment."}}
{"id": "2601.18067", "categories": ["cs.AI", "cs.NE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2601.18067", "abs": "https://arxiv.org/abs/2601.18067", "authors": ["Wei-Po Hsin", "Ren-Hao Deng", "Yao-Ting Hsieh", "En-Ming Huang", "Shih-Hao Hung"], "title": "EvolVE: Evolutionary Search for LLM-based Verilog Generation and Optimization", "comment": "17 pages, 6 figures, 8 tables", "summary": "Verilog's design cycle is inherently labor-intensive and necessitates extensive domain expertise. Although Large Language Models (LLMs) offer a promising pathway toward automation, their limited training data and intrinsic sequential reasoning fail to capture the strict formal logic and concurrency inherent in hardware systems. To overcome these barriers, we present EvolVE, the first framework to analyze multiple evolution strategies on chip design tasks, revealing that Monte Carlo Tree Search (MCTS) excels at maximizing functional correctness, while Idea-Guided Refinement (IGR) proves superior for optimization. We further leverage Structured Testbench Generation (STG) to accelerate the evolutionary process. To address the lack of complex optimization benchmarks, we introduce IC-RTL, targeting industry-scale problems derived from the National Integrated Circuit Contest. Evaluations establish EvolVE as the new state-of-the-art, achieving 98.1% on VerilogEval v2 and 92% on RTLLM v2. Furthermore, on the industry-scale IC-RTL suite, our framework surpasses reference implementations authored by contest participants, reducing the Power, Performance, Area (PPA) product by up to 66% in Huffman Coding and 17% in the geometric mean across all problems. The source code of the IC-RTL benchmark is available at https://github.com/weiber2002/ICRTL.", "AI": {"tldr": "EvolVE is a framework that uses evolution strategies (MCTS for correctness, IGR for optimization) with structured testbench generation to automate Verilog design, achieving SOTA results on benchmarks and outperforming human contest implementations on industry-scale PPA metrics.", "motivation": "Verilog design is labor-intensive and requires domain expertise. LLMs struggle with hardware's formal logic and concurrency due to limited training data and sequential reasoning limitations.", "method": "EvolVE analyzes multiple evolution strategies, finding MCTS best for functional correctness and Idea-Guided Refinement (IGR) superior for optimization. Uses Structured Testbench Generation (STG) to accelerate evolution. Introduces IC-RTL benchmark for complex optimization tasks.", "result": "Achieves 98.1% on VerilogEval v2 and 92% on RTLLM v2. On industry-scale IC-RTL suite, surpasses contest participants' implementations, reducing PPA product by up to 66% in Huffman Coding and 17% geometric mean across all problems.", "conclusion": "EvolVE establishes new state-of-the-art for automated Verilog design, effectively addressing LLM limitations through evolution strategies and structured testbench generation, demonstrating practical value on industry-scale optimization problems."}}
{"id": "2601.17290", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17290", "abs": "https://arxiv.org/abs/2601.17290", "authors": ["Weloday Fikadu Moges", "Jianmei Su", "Amin Waqas"], "title": "Dynamic Meta-Ensemble Framework for Efficient and Accurate Deep Learning in Plant Leaf Disease Detection on Resource-Constrained Edge Devices", "comment": null, "summary": "Deploying deep learning models for plant disease detection on edge devices such as IoT sensors, smartphones, and embedded systems is severely constrained by limited computational resources and energy budgets. To address this challenge, we introduce a novel Dynamic Meta-Ensemble Framework (DMEF) for high-accuracy plant disease diagnosis under resource constraints. DMEF employs an adaptive weighting mechanism that dynamically combines the predictions of three lightweight convolutional neural networks (MobileNetV2, NASNetMobile, and InceptionV3) by optimizing a trade-off between accuracy improvements (DeltaAcc) and computational efficiency (model size). During training, the ensemble weights are updated iteratively, favoring models exhibiting high performance and low complexity. Extensive experiments on benchmark datasets for potato and maize diseases demonstrate state-of-the-art classification accuracies of 99.53% and 96.61%, respectively, surpassing standalone models and static ensembles by 2.1% and 6.3%. With computationally efficient inference latency (<75ms) and a compact footprint (<1 million parameters), DMEF shows strong potential for edge-based agricultural monitoring, suggesting viability for scalable crop disease management. This bridges the gap between high-accuracy AI and practical field applications.", "AI": {"tldr": "Dynamic Meta-Ensemble Framework (DMEF) combines lightweight CNNs with adaptive weighting to achieve high-accuracy plant disease detection on edge devices with limited resources.", "motivation": "Edge devices for plant disease detection (IoT sensors, smartphones, embedded systems) face severe computational and energy constraints, creating a gap between high-accuracy AI models and practical field applications.", "method": "DMEF uses an adaptive weighting mechanism that dynamically combines predictions from three lightweight CNNs (MobileNetV2, NASNetMobile, InceptionV3) by optimizing trade-off between accuracy improvements (DeltaAcc) and computational efficiency (model size). Ensemble weights are updated iteratively during training to favor models with high performance and low complexity.", "result": "Achieved state-of-the-art classification accuracies of 99.53% for potato diseases and 96.61% for maize diseases, surpassing standalone models and static ensembles by 2.1% and 6.3% respectively. Maintained computationally efficient inference latency (<75ms) and compact footprint (<1 million parameters).", "conclusion": "DMEF bridges the gap between high-accuracy AI and practical field applications, showing strong potential for edge-based agricultural monitoring and scalable crop disease management with its efficient performance on resource-constrained devices."}}
{"id": "2601.18119", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18119", "abs": "https://arxiv.org/abs/2601.18119", "authors": ["Jing Ye", "Yiwen Duan", "Yonghong Yu", "Victor Ma", "Yang Gao", "Xing Chen"], "title": "Beyond Text-to-SQL: Can LLMs Really Debug Enterprise ETL SQL?", "comment": null, "summary": "SQL is central to enterprise data engineering, yet generating fully correct SQL code in a single attempt remains difficult, even for experienced developers and advanced text-to-SQL LLMs, often requiring multiple debugging iterations. We introduce OurBench, the first benchmark for enterprise-level SQL reasoning and debugging. Our benchmark is built on two key innovations: (1) an automated construction workflow that uses reverse engineering to systematically inject realistic bugs into large-scale SQL code, enabling scalable and diverse benchmark generation; and (2) an execution-free evaluation framework tailored to enterprise settings, providing fast, accurate, and resource-efficient assessment.\n  OurBench comprises 469 OurBenchSyn queries featuring syntax errors with explicit error messages, and 516 OurBenchSem queries targeting semantic errors in which the code fails to meet user intent. The queries are highly complex, averaging over 140 lines and featuring deep and wide abstract syntax trees.\n  Evaluation of nearly 30 LLMs reveals a substantial performance gap: the best-performing model, Claude-4-Sonnet, achieves only 36.46 percent accuracy on OurBenchSyn and 32.17 percent on OurBenchSem, while most models score below 20 percent. We further explore four solution strategies, identify key challenges, and outline promising directions for enterprise SQL debugging with LLMs.", "AI": {"tldr": "OurBench is the first benchmark for enterprise SQL debugging, featuring 985 complex queries with injected realistic bugs, revealing LLMs struggle with SQL debugging (best model <37% accuracy).", "motivation": "SQL is central to enterprise data engineering but generating fully correct SQL in one attempt is difficult even for experienced developers and advanced LLMs, requiring multiple debugging iterations. There's a need for a benchmark to evaluate SQL reasoning and debugging capabilities in enterprise settings.", "method": "1) Automated construction workflow using reverse engineering to systematically inject realistic bugs into large-scale SQL code for scalable benchmark generation. 2) Execution-free evaluation framework tailored to enterprise settings for fast, accurate, resource-efficient assessment. The benchmark includes 469 syntax error queries and 516 semantic error queries, averaging over 140 lines each with deep/wide ASTs.", "result": "Evaluation of nearly 30 LLMs shows substantial performance gap: best model (Claude-4-Sonnet) achieves only 36.46% accuracy on syntax errors and 32.17% on semantic errors, with most models scoring below 20%. The study explores four solution strategies and identifies key challenges.", "conclusion": "OurBench provides the first comprehensive benchmark for enterprise SQL debugging, revealing current LLMs' limitations in SQL debugging tasks and outlining promising directions for improving enterprise SQL debugging with LLMs."}}
{"id": "2601.17315", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17315", "abs": "https://arxiv.org/abs/2601.17315", "authors": ["Xiaoyang Li", "Runni Zhou"], "title": "ClinNet: Evidential Ordinal Regression with Bilateral Asymmetry and Prototype Memory for Knee Osteoarthritis Grading", "comment": "12 pages", "summary": "Knee osteoarthritis (KOA) grading based on radiographic images is a critical yet challenging task due to subtle inter-grade differences, annotation uncertainty, and the inherently ordinal nature of disease progression. Conventional deep learning approaches typically formulate this problem as deterministic multi-class classification, ignoring both the continuous progression of degeneration and the uncertainty in expert annotations. In this work, we propose ClinNet, a novel trustworthy framework that addresses KOA grading as an evidential ordinal regression problem. The proposed method integrates three key components: (1) a Bilateral Asymmetry Encoder (BAE) that explicitly models medial-lateral structural discrepancies; (2) a Diagnostic Memory Bank that maintains class-wise prototypes to stabilize feature representations; and (3) an Evidential Ordinal Head based on the Normal-Inverse-Gamma (NIG) distribution to jointly estimate continuous KL grades and epistemic uncertainty. Extensive experiments demonstrate that ClinNet achieves a Quadratic Weighted Kappa of 0.892 and Accuracy of 0.768, statistically outperforming state-of-the-art baselines (p < 0.001). Crucially, we demonstrate that the model's uncertainty estimates successfully flag out-of-distribution samples and potential misdiagnoses, paving the way for safe clinical deployment.", "AI": {"tldr": "ClinNet: A trustworthy evidential ordinal regression framework for knee osteoarthritis grading that models bilateral asymmetry, uses diagnostic memory banks, and estimates uncertainty via Normal-Inverse-Gamma distribution.", "motivation": "Knee osteoarthritis grading is challenging due to subtle inter-grade differences, annotation uncertainty, and the ordinal nature of disease progression. Conventional deep learning approaches treat it as deterministic multi-class classification, ignoring both continuous progression and annotation uncertainty.", "method": "ClinNet integrates three components: (1) Bilateral Asymmetry Encoder (BAE) to model medial-lateral structural discrepancies, (2) Diagnostic Memory Bank to maintain class-wise prototypes and stabilize feature representations, and (3) Evidential Ordinal Head based on Normal-Inverse-Gamma distribution to jointly estimate continuous KL grades and epistemic uncertainty.", "result": "Achieves Quadratic Weighted Kappa of 0.892 and Accuracy of 0.768, statistically outperforming state-of-the-art baselines (p < 0.001). The model's uncertainty estimates successfully flag out-of-distribution samples and potential misdiagnoses.", "conclusion": "ClinNet provides a trustworthy framework for KOA grading that addresses both the ordinal nature of disease progression and annotation uncertainty, with uncertainty estimates that enable safe clinical deployment by identifying potential misdiagnoses."}}
{"id": "2601.18123", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18123", "abs": "https://arxiv.org/abs/2601.18123", "authors": ["Muhammad Ibrahim Khan", "Bivin Pradeep", "James Brusey"], "title": "Deadline-Aware, Energy-Efficient Control of Domestic Immersion Hot Water Heaters", "comment": "Accepted at AAAI 2026", "summary": "Typical domestic immersion water heater systems are often operated continuously during winter, heating quickly rather than efficiently and ignoring predictable demand windows and ambient losses. We study deadline-aware control, where the aim is to reach a target temperature at a specified time while minimising energy consumption. We introduce an efficient Gymnasium environment that models an immersion hot water heater with first-order thermal losses and discrete on and off actions of 0 W and 6000 W applied every 120 seconds. Methods include a time-optimal bang-bang baseline, a zero-shot Monte Carlo Tree Search planner, and a Proximal Policy Optimisation policy. We report total energy consumption in watt-hours under identical physical dynamics. Across sweeps of initial temperature from 10 to 30 degrees Celsius, deadline from 30 to 90 steps, and target temperature from 40 to 80 degrees Celsius, PPO achieves the most energy-efficient performance at a 60-step horizon of 2 hours, using 3.23 kilowatt-hours, compared to 4.37 to 10.45 kilowatt-hours for bang-bang control and 4.18 to 6.46 kilowatt-hours for MCTS. This corresponds to energy savings of 26 percent at 30 steps and 69 percent at 90 steps. In a representative trajectory with a 50 kg water mass, 20 degrees Celsius ambient temperature, and a 60 degrees Celsius target, PPO consumes 54 percent less energy than bang-bang control and 33 percent less than MCTS. These results show that learned deadline-aware control reduces energy consumption under identical physical assumptions, while planners provide partial savings without training and learned policies offer near-zero inference cost once trained.", "AI": {"tldr": "PPO-based deadline-aware control for immersion water heaters reduces energy consumption by 26-69% compared to traditional methods, achieving significant efficiency gains through learned scheduling.", "motivation": "Traditional immersion water heaters operate inefficiently by heating continuously during winter, ignoring predictable demand windows and ambient heat losses, leading to unnecessary energy consumption.", "method": "Developed a Gymnasium environment modeling immersion heater with thermal losses, comparing time-optimal bang-bang control, zero-shot Monte Carlo Tree Search planner, and Proximal Policy Optimization (PPO) policy with discrete on/off actions every 120 seconds.", "result": "PPO achieved most energy-efficient performance (3.23 kWh at 2-hour horizon), saving 26% energy at 30 steps and 69% at 90 steps compared to bang-bang control, and 33-54% less energy in representative scenarios.", "conclusion": "Learned deadline-aware control significantly reduces energy consumption while planners offer partial savings without training, and trained policies provide near-zero inference cost for practical deployment."}}
{"id": "2601.17323", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17323", "abs": "https://arxiv.org/abs/2601.17323", "authors": ["Debang Li", "Zhengcong Fei", "Tuanhui Li", "Yikun Dou", "Zheng Chen", "Jiangping Yang", "Mingyuan Fan", "Jingtao Xu", "Jiahua Wang", "Baoxuan Gu", "Mingshan Chang", "Yuqiang Xie", "Binjie Mao", "Youqiang Zhang", "Nuo Pang", "Hao Zhang", "Yuzhe Jin", "Zhiheng Xu", "Dixuan Lin", "Guibin Chen", "Yahui Zhou"], "title": "SkyReels-V3 Technique Report", "comment": null, "summary": "Video generation serves as a cornerstone for building world models, where multimodal contextual inference stands as the defining test of capability. In this end, we present SkyReels-V3, a conditional video generation model, built upon a unified multimodal in-context learning framework with diffusion Transformers. SkyReels-V3 model supports three core generative paradigms within a single architecture: reference images-to-video synthesis, video-to-video extension and audio-guided video generation. (i) reference images-to-video model is designed to produce high-fidelity videos with strong subject identity preservation, temporal coherence, and narrative consistency. To enhance reference adherence and compositional stability, we design a comprehensive data processing pipeline that leverages cross frame pairing, image editing, and semantic rewriting, effectively mitigating copy paste artifacts. During training, an image video hybrid strategy combined with multi-resolution joint optimization is employed to improve generalization and robustness across diverse scenarios. (ii) video extension model integrates spatio-temporal consistency modeling with large-scale video understanding, enabling both seamless single-shot continuation and intelligent multi-shot switching with professional cinematographic patterns. (iii) Talking avatar model supports minute-level audio-conditioned video generation by training first-and-last frame insertion patterns and reconstructing key-frame inference paradigms. On the basis of ensuring visual quality, synchronization of audio and videos has been optimized.\n  Extensive evaluations demonstrate that SkyReels-V3 achieves state-of-the-art or near state-of-the-art performance on key metrics including visual quality, instruction following, and specific aspect metrics, approaching leading closed-source systems. Github: https://github.com/SkyworkAI/SkyReels-V3.", "AI": {"tldr": "SkyReels-V3 is a unified multimodal video generation model using diffusion Transformers that supports three paradigms: reference image-to-video synthesis, video extension, and audio-guided generation with strong subject preservation and temporal coherence.", "motivation": "Video generation is fundamental for building world models, and multimodal contextual inference represents the key capability test. The authors aim to create a unified framework that can handle multiple video generation tasks within a single architecture.", "method": "Built on diffusion Transformers with unified multimodal in-context learning. Uses comprehensive data processing with cross-frame pairing, image editing, and semantic rewriting to reduce artifacts. Employs image-video hybrid training with multi-resolution joint optimization. Video extension integrates spatio-temporal consistency with large-scale video understanding. Talking avatar uses first-and-last frame insertion patterns and key-frame inference paradigms.", "result": "Achieves state-of-the-art or near state-of-the-art performance on key metrics including visual quality, instruction following, and specific aspect metrics, approaching leading closed-source systems.", "conclusion": "SkyReels-V3 demonstrates that a single unified architecture can effectively support multiple video generation paradigms while maintaining high quality and coherence, representing significant progress toward building comprehensive world models through multimodal contextual inference."}}
{"id": "2601.18130", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18130", "abs": "https://arxiv.org/abs/2601.18130", "authors": ["Jize Wang", "Han Wu", "Zhiyuan You", "Yiming Song", "Yijun Wang", "Zifei Shan", "Yining Li", "Songyang Zhang", "Xinyi Le", "Cailian Chen", "Xinping Guan", "Dacheng Tao"], "title": "RouteMoA: Dynamic Routing without Pre-Inference Boosts Efficient Mixture-of-Agents", "comment": null, "summary": "Mixture-of-Agents (MoA) improves LLM performance through layered collaboration, but its dense topology raises costs and latency. Existing methods employ LLM judges to filter responses, yet still require all models to perform inference before judging, failing to cut costs effectively. They also lack model selection criteria and struggle with large model pools, where full inference is costly and can exceed context limits. To address this, we propose RouteMoA, an efficient mixture-of-agents framework with dynamic routing. It employs a lightweight scorer to perform initial screening by predicting coarse-grained performance from the query, narrowing candidates to a high-potential subset without inference. A mixture of judges then refines these scores through lightweight self- and cross-assessment based on existing model outputs, providing posterior correction without additional inference. Finally, a model ranking mechanism selects models by balancing performance, cost, and latency. RouteMoA outperforms MoA across varying tasks and model pool sizes, reducing cost by 89.8% and latency by 63.6% in the large-scale model pool.", "AI": {"tldr": "RouteMoA is an efficient mixture-of-agents framework that uses dynamic routing to reduce costs and latency by employing a lightweight scorer for initial screening and mixture of judges for refinement, achieving 89.8% cost reduction and 63.6% latency reduction.", "motivation": "Current Mixture-of-Agents (MoA) approaches have dense topology that increases costs and latency. Existing methods using LLM judges still require all models to perform inference before filtering, failing to effectively reduce costs. They also lack proper model selection criteria and struggle with large model pools where full inference is costly and can exceed context limits.", "method": "RouteMoA uses dynamic routing with three components: 1) a lightweight scorer for initial screening that predicts coarse-grained performance from queries to narrow candidates without inference, 2) a mixture of judges that refines scores through lightweight self- and cross-assessment using existing model outputs, and 3) a model ranking mechanism that balances performance, cost, and latency for final model selection.", "result": "RouteMoA outperforms standard MoA across varying tasks and model pool sizes. In large-scale model pools, it achieves 89.8% cost reduction and 63.6% latency reduction while maintaining performance.", "conclusion": "RouteMoA provides an efficient alternative to dense MoA frameworks by introducing dynamic routing that significantly reduces computational costs and latency while maintaining or improving performance, making large-scale model collaboration more practical."}}
{"id": "2601.17326", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.17326", "abs": "https://arxiv.org/abs/2601.17326", "authors": ["Jasmine Lesner", "Michael Beyeler"], "title": "SymbolSight: Minimizing Inter-Symbol Interference for Reading with Prosthetic Vision", "comment": "Submitted to IEEE EMBC 2026. 7 pages, 6 figures, 2 tables", "summary": "Retinal prostheses restore limited visual perception, but low spatial resolution and temporal persistence make reading difficult. In sequential letter presentation, the afterimage of one symbol can interfere with perception of the next, leading to systematic recognition errors. Rather than relying on future hardware improvements, we investigate whether optimizing the visual symbols themselves can mitigate this temporal interference. We present SymbolSight, a computational framework that selects symbol-to-letter mappings to minimize confusion among frequently adjacent letters. Using simulated prosthetic vision (SPV) and a neural proxy observer, we estimate pairwise symbol confusability and optimize assignments using language-specific bigram statistics. Across simulations in Arabic, Bulgarian, and English, the resulting heterogeneous symbol sets reduced predicted confusion by a median factor of 22 relative to native alphabets. These results suggest that standard typography is poorly matched to serial, low-bandwidth prosthetic vision and demonstrate how computational modeling can efficiently narrow the design space of visual encodings to generate high-potential candidates for future psychophysical and clinical evaluation.", "AI": {"tldr": "SymbolSight framework optimizes visual symbols for retinal prostheses to reduce letter confusion in sequential reading, achieving 22x less predicted confusion than native alphabets.", "motivation": "Retinal prostheses have low spatial resolution and temporal persistence, causing afterimages to interfere with sequential letter recognition. Instead of waiting for hardware improvements, the authors explore optimizing visual symbols themselves to mitigate temporal interference.", "method": "Developed SymbolSight computational framework that selects symbol-to-letter mappings to minimize confusion among frequently adjacent letters. Used simulated prosthetic vision (SPV) with neural proxy observer to estimate pairwise symbol confusability, then optimized assignments using language-specific bigram statistics.", "result": "Across simulations in Arabic, Bulgarian, and English, the optimized heterogeneous symbol sets reduced predicted confusion by a median factor of 22 relative to native alphabets.", "conclusion": "Standard typography is poorly matched to serial, low-bandwidth prosthetic vision. Computational modeling can efficiently narrow design space of visual encodings to generate high-potential candidates for future psychophysical and clinical evaluation."}}
{"id": "2601.18132", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18132", "abs": "https://arxiv.org/abs/2601.18132", "authors": ["Xi Chen", "Hongru Zhou", "Huahui Yi", "Shiyu Feng", "Hanyu Zhou", "Tiancheng He", "Mingke You", "Li Wang", "Qiankun Li", "Kun Wang", "Weili Fu", "Kang Li", "Jian Li"], "title": "RareAlert: Aligning heterogeneous large language model reasoning for early rare disease risk screening", "comment": "28 page, 3 figures", "summary": "Missed and delayed diagnosis remains a major challenge in rare disease care. At the initial clinical encounters, physicians assess rare disease risk using only limited information under high uncertainty. When high-risk patients are not recognised at this stage, targeted diagnostic testing is often not initiated, resulting in missed diagnosis. Existing primary care triage processes are structurally insufficient to reliably identify patients with rare diseases at initial clinical presentation and universal screening is needed to reduce diagnostic delay. Here we present RareAlert, an early screening system which predict patient-level rare disease risk from routinely available primary-visit information. RareAlert integrates reasoning generated by ten LLMs, calibrates and weights these signals using machine learning, and distils the aligned reasoning into a single locally deployable model. To develop and evaluate RareAlert, we curated RareBench, a real-world dataset of 158,666 cases covering 33 Orphanet disease categories and more than 7,000 rare conditions, including both rare and non-rare presentations. The results showed that rare disease identification can be reconceptualised as a universal uncertainty resolution process applied to the general patient population. On an independent test set, RareAlert, a Qwen3-4B based model trained with calibrated reasoning signals, achieved an AUC of 0.917, outperforming the best machine learning ensemble and all evaluated LLMs, including GPT-5, DeepSeek-R1, Claude-3.7-Sonnet, o3-mini, Gemini-2.5-Pro, and Qwen3-235B. These findings demonstrate the diversity in LLM medical reasoning and the effectiveness of aligning such reasoning in highly uncertain clinical tasks. By incorporating calibrated reasoning into a single model, RareAlert enables accurate, privacy-preserving, and scalable rare disease risk screening suitable for large-scale local deployment.", "AI": {"tldr": "RareAlert is an early screening system that predicts rare disease risk from primary care data by integrating and calibrating reasoning from multiple LLMs into a single deployable model, achieving state-of-the-art performance on real-world rare disease data.", "motivation": "Missed and delayed diagnosis remains a major challenge in rare disease care. Existing primary care triage processes are structurally insufficient to reliably identify patients with rare diseases at initial clinical presentation, and universal screening is needed to reduce diagnostic delay.", "method": "RareAlert integrates reasoning generated by ten LLMs, calibrates and weights these signals using machine learning, and distills the aligned reasoning into a single locally deployable model (Qwen3-4B based). The system was developed and evaluated using RareBench, a real-world dataset of 158,666 cases covering 33 Orphanet disease categories and more than 7,000 rare conditions.", "result": "RareAlert achieved an AUC of 0.917 on an independent test set, outperforming the best machine learning ensemble and all evaluated LLMs including GPT-5, DeepSeek-R1, Claude-3.7-Sonnet, o3-mini, Gemini-2.5-Pro, and Qwen3-235B.", "conclusion": "The findings demonstrate the diversity in LLM medical reasoning and the effectiveness of aligning such reasoning in highly uncertain clinical tasks. RareAlert enables accurate, privacy-preserving, and scalable rare disease risk screening suitable for large-scale local deployment by incorporating calibrated reasoning into a single model."}}
{"id": "2601.17331", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17331", "abs": "https://arxiv.org/abs/2601.17331", "authors": ["Fabian Vazquez", "Jose A. Nu\u00f1ez", "Diego Adame", "Alissen Moreno", "Augustin Zhan", "Huimin Li", "Jinghao Yang", "Haoteng Tang", "Bin Fu", "Pengfei Gu"], "title": "Learning with Geometric Priors in U-Net Variants for Polyp Segmentation", "comment": null, "summary": "Accurate and robust polyp segmentation is essential for early colorectal cancer detection and for computer-aided diagnosis. While convolutional neural network-, Transformer-, and Mamba-based U-Net variants have achieved strong performance, they still struggle to capture geometric and structural cues, especially in low-contrast or cluttered colonoscopy scenes. To address this challenge, we propose a novel Geometric Prior-guided Module (GPM) that injects explicit geometric priors into U-Net-based architectures for polyp segmentation. Specifically, we fine-tune the Visual Geometry Grounded Transformer (VGGT) on a simulated ColonDepth dataset to estimate depth maps of polyp images tailored to the endoscopic domain. These depth maps are then processed by GPM to encode geometric priors into the encoder's feature maps, where they are further refined using spatial and channel attention mechanisms that emphasize both local spatial and global channel information. GPM is plug-and-play and can be seamlessly integrated into diverse U-Net variants. Extensive experiments on five public polyp segmentation datasets demonstrate consistent gains over three strong baselines. Code and the generated depth maps are available at: https://github.com/fvazqu/GPM-PolypSeg", "AI": {"tldr": "Proposes Geometric Prior-guided Module (GPM) that injects explicit geometric priors into U-Net architectures for improved polyp segmentation in colonoscopy images.", "motivation": "Existing CNN-, Transformer-, and Mamba-based U-Net variants struggle to capture geometric and structural cues in low-contrast or cluttered colonoscopy scenes, limiting polyp segmentation accuracy.", "method": "Fine-tunes Visual Geometry Grounded Transformer (VGGT) on simulated ColonDepth dataset to estimate depth maps, then uses GPM to encode geometric priors into encoder feature maps with spatial and channel attention mechanisms.", "result": "Extensive experiments on five public polyp segmentation datasets show consistent gains over three strong baselines.", "conclusion": "GPM is an effective plug-and-play module that enhances polyp segmentation by incorporating explicit geometric priors, with code and generated depth maps publicly available."}}
{"id": "2601.18137", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18137", "abs": "https://arxiv.org/abs/2601.18137", "authors": ["Yinger Zhang", "Shutong Jiang", "Renhao Li", "Jianhong Tu", "Yang Su", "Lianghao Deng", "Xudong Guo", "Chenxu Lv", "Junyang Lin"], "title": "DeepPlanning: Benchmarking Long-Horizon Agentic Planning with Verifiable Constraints", "comment": null, "summary": "While agent evaluation has shifted toward long-horizon tasks, most benchmarks still emphasize local, step-level reasoning rather than the global constrained optimization (e.g., time and financial budgets) that demands genuine planning ability. Meanwhile, existing LLM planning benchmarks underrepresent the active information gathering and fine-grained local constraints typical of real-world settings. To address this, we introduce DeepPlanning, a challenging benchmark for practical long-horizon agent planning. It features multi-day travel planning and multi-product shopping tasks that require proactive information acquisition, local constrained reasoning, and global constrained optimization. Evaluations on DeepPlanning show that even frontier agentic LLMs struggle with these problems, highlighting the importance of reliable explicit reasoning patterns and parallel tool use for achieving better effectiveness-efficiency trade-offs. Error analysis further points to promising directions for improving agentic LLMs over long planning horizons. We open-source the code and data to support future research.", "AI": {"tldr": "DeepPlanning is a new benchmark for evaluating long-horizon agent planning that focuses on global constrained optimization, proactive information gathering, and fine-grained local constraints in practical scenarios like travel planning and shopping.", "motivation": "Current agent evaluation benchmarks emphasize local, step-level reasoning rather than global constrained optimization (time/financial budgets) that requires genuine planning ability. Existing LLM planning benchmarks also underrepresent active information gathering and fine-grained local constraints typical of real-world settings.", "method": "Introduce DeepPlanning benchmark featuring multi-day travel planning and multi-product shopping tasks that require proactive information acquisition, local constrained reasoning, and global constrained optimization.", "result": "Evaluations show even frontier agentic LLMs struggle with these problems, highlighting the importance of reliable explicit reasoning patterns and parallel tool use for better effectiveness-efficiency trade-offs.", "conclusion": "DeepPlanning reveals limitations in current agentic LLMs for long-horizon planning and points to promising directions for improvement. The code and data are open-sourced to support future research."}}
{"id": "2601.17336", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17336", "abs": "https://arxiv.org/abs/2601.17336", "authors": ["Xiaoyang Li", "Runni Zhou"], "title": "AGE-Net: Spectral--Spatial Fusion and Anatomical Graph Reasoning with Evidential Ordinal Regression for Knee Osteoarthritis Grading", "comment": "22 pages", "summary": "Automated Kellgren--Lawrence (KL) grading from knee radiographs is challenging due to subtle structural changes, long-range anatomical dependencies, and ambiguity near grade boundaries. We propose AGE-Net, a ConvNeXt-based framework that integrates Spectral--Spatial Fusion (SSF), Anatomical Graph Reasoning (AGR), and Differential Refinement (DFR). To capture predictive uncertainty and preserve label ordinality, AGE-Net employs a Normal-Inverse-Gamma (NIG) evidential regression head and a pairwise ordinal ranking constraint. On a knee KL dataset, AGE-Net achieves a quadratic weighted kappa (QWK) of 0.9017 +/- 0.0045 and a mean squared error (MSE) of 0.2349 +/- 0.0028 over three random seeds, outperforming strong CNN baselines and showing consistent gains in ablation studies. We further outline evaluations of uncertainty quality, robustness, and explainability, with additional experimental figures to be included in the full manuscript.", "AI": {"tldr": "AGE-Net: A ConvNeXt-based framework for automated KL grading from knee radiographs using spectral-spatial fusion, anatomical graph reasoning, and differential refinement with evidential uncertainty estimation.", "motivation": "Automated KL grading is challenging due to subtle structural changes, long-range anatomical dependencies, and ambiguity near grade boundaries in knee radiographs.", "method": "Proposes AGE-Net with Spectral-Spatial Fusion (SSF), Anatomical Graph Reasoning (AGR), and Differential Refinement (DFR). Uses Normal-Inverse-Gamma evidential regression head and pairwise ordinal ranking constraint for uncertainty estimation and label ordinality.", "result": "Achieves quadratic weighted kappa (QWK) of 0.9017 \u00b1 0.0045 and MSE of 0.2349 \u00b1 0.0028, outperforming strong CNN baselines with consistent gains in ablation studies.", "conclusion": "AGE-Net effectively addresses KL grading challenges through integrated architectural components and uncertainty-aware learning, demonstrating superior performance and robustness."}}
{"id": "2601.18175", "categories": ["cs.AI", "cs.LG", "eess.SY", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.18175", "abs": "https://arxiv.org/abs/2601.18175", "authors": ["Daniel Russo"], "title": "Success Conditioning as Policy Improvement: The Optimization Problem Solved by Imitating Success", "comment": null, "summary": "A widely used technique for improving policies is success conditioning, in which one collects trajectories, identifies those that achieve a desired outcome, and updates the policy to imitate the actions taken along successful trajectories. This principle appears under many names -- rejection sampling with SFT, goal-conditioned RL, Decision Transformers -- yet what optimization problem it solves, if any, has remained unclear. We prove that success conditioning exactly solves a trust-region optimization problem, maximizing policy improvement subject to a $\u03c7^2$ divergence constraint whose radius is determined automatically by the data. This yields an identity: relative policy improvement, the magnitude of policy change, and a quantity we call action-influence -- measuring how random variation in action choices affects success rates -- are exactly equal at every state. Success conditioning thus emerges as a conservative improvement operator. Exact success conditioning cannot degrade performance or induce dangerous distribution shift, but when it fails, it does so observably, by hardly changing the policy at all. We apply our theory to the common practice of return thresholding, showing this can amplify improvement, but at the cost of potential misalignment with the true objective.", "AI": {"tldr": "Success conditioning (rejection sampling, goal-conditioned RL, Decision Transformers) solves a trust-region optimization problem with automatic \u03c7\u00b2 divergence constraint, making it a conservative improvement operator that cannot degrade performance.", "motivation": "Success conditioning is widely used but its underlying optimization problem has remained unclear. The paper aims to formalize what mathematical problem success conditioning actually solves.", "method": "Proves that success conditioning exactly solves a trust-region optimization problem maximizing policy improvement subject to an automatic \u03c7\u00b2 divergence constraint. Establishes an identity relating relative policy improvement, policy change magnitude, and action-influence.", "result": "Success conditioning emerges as a conservative improvement operator that cannot degrade performance or induce dangerous distribution shift. When it fails, it does so observably by hardly changing the policy. Return thresholding can amplify improvement but risks misalignment.", "conclusion": "Success conditioning provides a theoretically grounded, safe policy improvement method that solves a specific trust-region optimization problem, offering predictable behavior and observable failure modes."}}
{"id": "2601.17340", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17340", "abs": "https://arxiv.org/abs/2601.17340", "authors": ["Haodong He", "Xin Zhan", "Yancheng Bai", "Rui Lan", "Lei Sun", "Xiangxiang Chu"], "title": "TEXTS-Diff: TEXTS-Aware Diffusion Model for Real-World Text Image Super-Resolution", "comment": "Accepted by ICASSP 2026", "summary": "Real-world text image super-resolution aims to restore overall visual quality and text legibility in images suffering from diverse degradations and text distortions. However, the scarcity of text image data in existing datasets results in poor performance on text regions. In addition, datasets consisting of isolated text samples limit the quality of background reconstruction. To address these limitations, we construct Real-Texts, a large-scale, high-quality dataset collected from real-world images, which covers diverse scenarios and contains natural text instances in both Chinese and English. Additionally, we propose the TEXTS-Aware Diffusion Model (TEXTS-Diff) to achieve high-quality generation in both background and textual regions. This approach leverages abstract concepts to improve the understanding of textual elements within visual scenes and concrete text regions to enhance textual details. It mitigates distortions and hallucination artifacts commonly observed in text regions, while preserving high-quality visual scene fidelity. Extensive experiments demonstrate that our method achieves state-of-the-art performance across multiple evaluation metrics, exhibiting superior generalization ability and text restoration accuracy in complex scenarios. All the code, model, and dataset will be released.", "AI": {"tldr": "Real-Texts dataset and TEXTS-Diff model for text image super-resolution that improves both background and text region quality in real-world degraded images.", "motivation": "Existing datasets lack sufficient text image data, leading to poor performance on text regions, and isolated text samples limit background reconstruction quality in real-world text image super-resolution.", "method": "Construct Real-Texts dataset from real-world images with diverse scenarios and natural text instances in Chinese/English, and propose TEXTS-Diff model that leverages abstract concepts for text understanding and concrete text regions for detail enhancement.", "result": "Achieves state-of-the-art performance across multiple evaluation metrics with superior generalization ability and text restoration accuracy in complex scenarios.", "conclusion": "The proposed dataset and model effectively address text region distortions and hallucination artifacts while preserving visual scene fidelity, with code, model, and dataset to be released."}}
{"id": "2601.18197", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18197", "abs": "https://arxiv.org/abs/2601.18197", "authors": ["Shaokang Wang", "Pei Fu", "Ruoceng Zhang", "Shaojie Zhang", "Xiuwen Xi", "Jiahui Yang", "Bin Qin", "Ying Huang", "Zhenbo Luo", "Jian Luan"], "title": "GAIA: A Data Flywheel System for Training GUI Test-Time Scaling Critic Models", "comment": null, "summary": "While Large Vision-Language Models (LVLMs) have significantly advanced GUI agents' capabilities in parsing textual instructions, interpreting screen content, and executing tasks, a critical challenge persists: the irreversibility of agent operations, where a single erroneous action can trigger catastrophic deviations. To address this, we propose the GUI Action Critic's Data Flywheel System (GAIA), a training framework that enables the models to have iterative critic capabilities, which are used to improve the Test-Time Scaling (TTS) of basic GUI agents' performance. Specifically, we train an Intuitive Critic Model (ICM) using positive and negative action examples from a base agent first. This critic evaluates the immediate correctness of the agent's intended actions, thereby selecting operations with higher success probability. Then, the initial critic guides agent actions to collect refined positive/negative samples, initiating the self-improving cycle. The augmented data then trains a second-round critic with enhanced discernment capability. We conduct experiments on various datasets and demonstrate that the proposed ICM can improve the test-time performance of various closed-source and open-source models, and the performance can be gradually improved as the data is recycled. The code and dataset will be publicly released.", "AI": {"tldr": "GAIA is a training framework that adds iterative critic capabilities to GUI agents, enabling self-improvement through a data flywheel system that cycles between action evaluation and data collection.", "motivation": "Current LVLM-based GUI agents suffer from irreversibility of operations where single erroneous actions cause catastrophic deviations, limiting their reliability in real-world GUI task execution.", "method": "Train an Intuitive Critic Model (ICM) using positive/negative examples from a base agent, then use it to guide actions and collect refined samples, creating a self-improving cycle that trains enhanced critics with better discernment.", "result": "Experiments show ICM improves test-time performance of various closed-source and open-source models, with performance gradually improving as data is recycled through the flywheel system.", "conclusion": "GAIA's data flywheel system enables GUI agents to develop iterative critic capabilities that enhance reliability through self-improvement cycles, addressing the critical challenge of irreversible errors in GUI automation."}}
{"id": "2601.17342", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17342", "abs": "https://arxiv.org/abs/2601.17342", "authors": ["Tong Wang", "Xiaodong Zhang", "Guanzhou Chen", "Jiaqi Wang", "Chenxi Liu", "Xiaoliang Tan", "Wenchao Guo", "Xuyang Li", "Xuanrui Wang", "Zifan Wang"], "title": "STARS: Shared-specific Translation and Alignment for missing-modality Remote Sensing Semantic Segmentation", "comment": null, "summary": "Multimodal remote sensing technology significantly enhances the understanding of surface semantics by integrating heterogeneous data such as optical images, Synthetic Aperture Radar (SAR), and Digital Surface Models (DSM). However, in practical applications, the missing of modality data (e.g., optical or DSM) is a common and severe challenge, which leads to performance decline in traditional multimodal fusion models. Existing methods for addressing missing modalities still face limitations, including feature collapse and overly generalized recovered features. To address these issues, we propose \\textbf{STARS} (\\textbf{S}hared-specific \\textbf{T}ranslation and \\textbf{A}lignment for missing-modality \\textbf{R}emote \\textbf{S}ensing), a robust semantic segmentation framework for incomplete multimodal inputs. STARS is built on two key designs. First, we introduce an asymmetric alignment mechanism with bidirectional translation and stop-gradient, which effectively prevents feature collapse and reduces sensitivity to hyperparameters. Second, we propose a Pixel-level Semantic sampling Alignment (PSA) strategy that combines class-balanced pixel sampling with cross-modality semantic alignment loss, to mitigate alignment failures caused by severe class imbalance and improve minority-class recognition.", "AI": {"tldr": "STARS is a robust semantic segmentation framework for incomplete multimodal remote sensing data that addresses missing modalities through asymmetric alignment and pixel-level semantic sampling to prevent feature collapse and improve minority-class recognition.", "motivation": "Missing modality data (optical, SAR, DSM) is common in practical remote sensing applications, causing performance decline in traditional multimodal fusion models. Existing methods face limitations like feature collapse and overly generalized recovered features.", "method": "STARS uses two key designs: 1) asymmetric alignment mechanism with bidirectional translation and stop-gradient to prevent feature collapse, and 2) Pixel-level Semantic sampling Alignment (PSA) combining class-balanced pixel sampling with cross-modality semantic alignment loss to address severe class imbalance.", "result": "The framework effectively handles incomplete multimodal inputs, reduces sensitivity to hyperparameters, prevents feature collapse, and improves minority-class recognition through semantic alignment.", "conclusion": "STARS provides a robust solution for semantic segmentation with missing multimodal remote sensing data by addressing key limitations of existing methods through innovative alignment and sampling strategies."}}
{"id": "2601.18202", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18202", "abs": "https://arxiv.org/abs/2601.18202", "authors": ["Fangyuan Xu", "Rujun Han", "Yanfei Chen", "Zifeng Wang", "I-Hung Hsu", "Jun Yan", "Vishy Tirumalashetty", "Eunsol Choi", "Tomas Pfister", "Chen-Yu Lee"], "title": "SAGE: Steerable Agentic Data Generation for Deep Search with Execution Feedback", "comment": null, "summary": "Deep search agents, which aim to answer complex questions requiring reasoning across multiple documents, can significantly speed up the information-seeking process. Collecting human annotations for this application is prohibitively expensive due to long and complex exploration trajectories. We propose an agentic pipeline that automatically generates high quality, difficulty-controlled deep search question-answer pairs for a given corpus and a target difficulty level. Our pipeline, SAGE, consists of a data generator which proposes QA pairs and a search agent which attempts to solve the generated question and provide execution feedback for the data generator. The two components interact over multiple rounds to iteratively refine the question-answer pairs until they satisfy the target difficulty level. Our intrinsic evaluation shows SAGE generates questions that require diverse reasoning strategies, while significantly increases the correctness and difficulty of the generated data. Our extrinsic evaluation demonstrates up to 23% relative performance gain on popular deep search benchmarks by training deep search agents with our synthetic data. Additional experiments show that agents trained on our data can adapt from fixed-corpus retrieval to Google Search at inference time, without further training.", "AI": {"tldr": "SAGE: An agentic pipeline that automatically generates high-quality, difficulty-controlled deep search QA pairs using iterative refinement between a data generator and search agent.", "motivation": "Collecting human annotations for deep search questions is prohibitively expensive due to long and complex exploration trajectories, creating a need for automated QA pair generation.", "method": "SAGE pipeline with two components: data generator proposes QA pairs, and search agent attempts to solve them while providing execution feedback. They interact over multiple rounds to iteratively refine QA pairs until reaching target difficulty level.", "result": "Generates questions requiring diverse reasoning strategies with increased correctness and difficulty. Achieves up to 23% relative performance gain on deep search benchmarks when training agents with synthetic data. Agents can adapt from fixed-corpus retrieval to Google Search at inference time without further training.", "conclusion": "SAGE provides an effective automated solution for generating high-quality, difficulty-controlled deep search QA pairs, enabling better training of search agents without expensive human annotation."}}
{"id": "2601.17349", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17349", "abs": "https://arxiv.org/abs/2601.17349", "authors": ["Hailong Yan", "Shice Liu", "Xiangtao Zhang", "Lujian Yao", "Fengxiang Yang", "Jinwei Chen", "Bo Li"], "title": "Revisiting Lightweight Low-Light Image Enhancement: From a YUV Color Space Perspective", "comment": "Tech report", "summary": "In the current era of mobile internet, Lightweight Low-Light Image Enhancement (L3IE) is critical for mobile devices, which faces a persistent trade-off between visual quality and model compactness. While recent methods employ disentangling strategies to simplify lightweight architectural design, such as Retinex theory and YUV color space transformations, their performance is fundamentally limited by overlooking channel-specific degradation patterns and cross-channel interactions. To address this gap, we perform a frequency-domain analysis that confirms the superiority of the YUV color space for L3IE. We identify a key insight: the Y channel primarily loses low-frequency content, while the UV channels are corrupted by high-frequency noise. Leveraging this finding, we propose a novel YUV-based paradigm that strategically restores channels using a Dual-Stream Global-Local Attention module for the Y channel, a Y-guided Local-Aware Frequency Attention module for the UV channels, and a Guided Interaction module for final feature fusion. Extensive experiments validate that our model establishes a new state-of-the-art on multiple benchmarks, delivering superior visual quality with a significantly lower parameter count.", "AI": {"tldr": "A novel YUV-based lightweight low-light image enhancement method using frequency-domain analysis and specialized attention modules for different channels achieves state-of-the-art performance with fewer parameters.", "motivation": "Mobile devices need lightweight low-light image enhancement (L3IE) but face trade-offs between visual quality and model compactness. Current methods using disentangling strategies (Retinex theory, YUV transformations) overlook channel-specific degradation patterns and cross-channel interactions.", "method": "Frequency-domain analysis reveals Y channel loses low-frequency content while UV channels have high-frequency noise. Proposed YUV-based paradigm uses: 1) Dual-Stream Global-Local Attention for Y channel, 2) Y-guided Local-Aware Frequency Attention for UV channels, and 3) Guided Interaction module for feature fusion.", "result": "Extensive experiments show the model establishes new state-of-the-art on multiple benchmarks, delivering superior visual quality with significantly lower parameter count.", "conclusion": "The proposed YUV-based paradigm with specialized attention modules for different channels effectively addresses channel-specific degradation patterns in low-light images, achieving optimal balance between enhancement quality and model efficiency for mobile applications."}}
{"id": "2601.18217", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18217", "abs": "https://arxiv.org/abs/2601.18217", "authors": ["Zhihan Liu", "Lin Guan", "Yixin Nie", "Kai Zhang", "Zhuoqun Hao", "Lin Chen", "Asli Celikyilmaz", "Zhaoran Wang", "Na Zhang"], "title": "Paying Less Generalization Tax: A Cross-Domain Generalization Study of RL Training for LLM Agents", "comment": null, "summary": "Generalist LLM agents are often post-trained on a narrow set of environments but deployed across far broader, unseen domains. In this work, we investigate the challenge of agentic post-training when the eventual test domains are unknown. Specifically, we analyze which properties of reinforcement learning (RL) environments and modeling choices have the greatest influence on out-of-domain performance. First, we identify two environment axes that strongly correlate with cross-domain generalization: (i) state information richness, i.e., the amount of information for the agent to process from the state, and (ii) planning complexity, estimated via goal reachability and trajectory length under a base policy. Notably, domain realism and text-level similarity are not the primary factors; for instance, the simple grid-world domain Sokoban leads to even stronger generalization in SciWorld than the more realistic ALFWorld. Motivated by these findings, we further show that increasing state information richness alone can already effectively improve cross-domain robustness. We propose a randomization technique, which is low-overhead and broadly applicable: add small amounts of distractive goal-irrelevant features to the state to make it richer without altering the task. Beyond environment-side properties, we also examine several modeling choices: (a) SFT warmup or mid-training helps prevent catastrophic forgetting during RL but undermines generalization to domains that are not included in the mid-training datamix; and (b) turning on step-by-step thinking during RL, while not always improving in-domain performance, plays a crucial role in preserving generalization.", "AI": {"tldr": "Post-training LLM agents for unknown test domains: State information richness and planning complexity are key for cross-domain generalization, not realism. Randomization with distractive features improves robustness, while SFT warmup helps but can hurt generalization to excluded domains.", "motivation": "Generalist LLM agents are typically post-trained on narrow environments but deployed across broader unseen domains. The paper investigates how to optimize agentic post-training when test domains are unknown, focusing on what environment properties and modeling choices influence out-of-domain performance.", "method": "1) Identified two key environment axes: state information richness (amount of information to process) and planning complexity (goal reachability and trajectory length). 2) Proposed randomization technique: adding small amounts of distractive goal-irrelevant features to states to increase richness without altering tasks. 3) Examined modeling choices: SFT warmup/mid-training effects and step-by-step thinking during RL.", "result": "1) State information richness and planning complexity strongly correlate with cross-domain generalization, while domain realism and text-level similarity are not primary factors. 2) Simple Sokoban leads to stronger generalization in SciWorld than more realistic ALFWorld. 3) Increasing state information richness alone effectively improves cross-domain robustness. 4) SFT warmup prevents catastrophic forgetting but undermines generalization to domains not in mid-training datamix. 5) Step-by-step thinking during RL preserves generalization even when not improving in-domain performance.", "conclusion": "For post-training LLM agents for unknown domains, focus on state information richness and planning complexity rather than realism. Simple randomization with distractive features can improve cross-domain robustness. SFT warmup helps stability but may limit generalization to excluded domains, while step-by-step thinking is crucial for maintaining generalization capabilities."}}
{"id": "2601.17350", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17350", "abs": "https://arxiv.org/abs/2601.17350", "authors": ["Xianliang Huang", "Zhizhou Zhong", "Shuhang Chen", "Yi Xu", "Juhong Guan", "Shuigeng Zhou"], "title": "NeRF-MIR: Towards High-Quality Restoration of Masked Images with Neural Radiance Fields", "comment": "14 pages, 15 figures", "summary": "Neural Radiance Fields (NeRF) have demonstrated remarkable performance in novel view synthesis. However, there is much improvement room on restoring 3D scenes based on NeRF from corrupted images, which are common in natural scene captures and can significantly impact the effectiveness of NeRF. This paper introduces NeRF-MIR, a novel neural rendering approach specifically proposed for the restoration of masked images, demonstrating the potential of NeRF in this domain. Recognizing that randomly emitting rays to pixels in NeRF may not effectively learn intricate image textures, we propose a \\textbf{P}atch-based \\textbf{E}ntropy for \\textbf{R}ay \\textbf{E}mitting (\\textbf{PERE}) strategy to distribute emitted rays properly. This enables NeRF-MIR to fuse comprehensive information from images of different views. Additionally, we introduce a \\textbf{P}rogressively \\textbf{I}terative \\textbf{RE}storation (\\textbf{PIRE}) mechanism to restore the masked regions in a self-training process. Furthermore, we design a dynamically-weighted loss function that automatically recalibrates the loss weights for masked regions. As existing datasets do not support NeRF-based masked image restoration, we construct three masked datasets to simulate corrupted scenarios. Extensive experiments on real data and constructed datasets demonstrate the superiority of NeRF-MIR over its counterparts in masked image restoration.", "AI": {"tldr": "NeRF-MIR introduces a novel neural rendering approach for restoring masked images using NeRF, featuring PERE strategy for ray emitting, PIRE mechanism for progressive restoration, and dynamic loss weighting.", "motivation": "NeRF shows remarkable performance in novel view synthesis but has room for improvement in restoring 3D scenes from corrupted images, which are common in natural scene captures and can significantly impact NeRF effectiveness.", "method": "Proposes NeRF-MIR with three key components: 1) PERE (Patch-based Entropy for Ray Emitting) strategy to properly distribute emitted rays for learning intricate textures, 2) PIRE (Progressively Iterative REstoration) mechanism for self-training restoration of masked regions, and 3) dynamically-weighted loss function that automatically recalibrates loss weights for masked regions.", "result": "Extensive experiments on real data and three newly constructed masked datasets demonstrate superiority of NeRF-MIR over counterparts in masked image restoration.", "conclusion": "NeRF-MIR demonstrates the potential of NeRF in masked image restoration domain, showing that proper ray emitting strategies and progressive restoration mechanisms can effectively restore corrupted images while leveraging NeRF's 3D scene understanding capabilities."}}
{"id": "2601.18225", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18225", "abs": "https://arxiv.org/abs/2601.18225", "authors": ["Pei Wang", "Yanan Wu", "Xiaoshuai Song", "Weixun Wang", "Gengru Chen", "Zhongwen Li", "Kezhong Yan", "Ken Deng", "Qi Liu", "Shuaibing Zhao", "Shaopan Xiong", "Xuepeng Liu", "Xuefeng Chen", "Wanxi Deng", "Wenbo Su", "Bo Zheng"], "title": "ShopSimulator: Evaluating and Exploring RL-Driven LLM Agent for Shopping Assistants", "comment": null, "summary": "Large language model (LLM)-based agents are increasingly deployed in e-commerce shopping. To perform thorough, user-tailored product searches, agents should interpret personal preferences, engage in multi-turn dialogues, and ultimately retrieve and discriminate among highly similar products. However, existing research has yet to provide a unified simulation environment that consistently captures all of these aspects, and always focuses solely on evaluation benchmarks without training support. In this paper, we introduce ShopSimulator, a large-scale and challenging Chinese shopping environment. Leveraging ShopSimulator, we evaluate LLMs across diverse scenarios, finding that even the best-performing models achieve less than 40% full-success rate. Error analysis reveals that agents struggle with deep search and product selection in long trajectories, fail to balance the use of personalization cues, and to effectively engage with users. Further training exploration provides practical guidance for overcoming these weaknesses, with the combination of supervised fine-tuning (SFT) and reinforcement learning (RL) yielding significant performance improvements. Code and data will be released at https://github.com/ShopAgent-Team/ShopSimulator.", "AI": {"tldr": "ShopSimulator is a Chinese e-commerce simulation environment that reveals LLM agents struggle with complex shopping tasks (<40% success), and shows SFT+RL training significantly improves performance.", "motivation": "Existing research lacks a unified simulation environment that captures all aspects of e-commerce shopping (personal preference interpretation, multi-turn dialogues, product discrimination), and focuses only on evaluation without training support.", "method": "Introduce ShopSimulator, a large-scale Chinese shopping environment, then evaluate LLMs across diverse scenarios and explore training methods including supervised fine-tuning (SFT) and reinforcement learning (RL).", "result": "Even best-performing LLMs achieve less than 40% full-success rate; agents struggle with deep search, product selection in long trajectories, balancing personalization cues, and user engagement. SFT+RL yields significant performance improvements.", "conclusion": "ShopSimulator provides a challenging benchmark for e-commerce agents, reveals key weaknesses in current LLMs for shopping tasks, and demonstrates practical training approaches (SFT+RL) to overcome these limitations."}}
{"id": "2601.17352", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17352", "abs": "https://arxiv.org/abs/2601.17352", "authors": ["M. L. Mamud", "Piyoosh Jaysaval", "Frederick D Day-Lewis", "M. K. Mudunuru"], "title": "HyDeMiC: A Deep Learning-based Mineral Classifier using Hyperspectral Data", "comment": null, "summary": "Hyperspectral imaging (HSI) has emerged as a powerful remote sensing tool for mineral exploration, capitalizing on unique spectral signatures of minerals. However, traditional classification methods such as discriminant analysis, logistic regression, and support vector machines often struggle with environmental noise in data, sensor limitations, and the computational complexity of analyzing high-dimensional HSI data. This study presents HyDeMiC (Hyperspectral Deep Learning-based Mineral Classifier), a convolutional neural network (CNN) model designed for robust mineral classification under noisy data. To train HyDeMiC, laboratory-measured hyperspectral data for 115 minerals spanning various mineral groups were used from the United States Geological Survey (USGS) library. The training dataset was generated by convolving reference mineral spectra with an HSI sensor response function. These datasets contained three copper-bearing minerals, Cuprite, Malachite, and Chalcopyrite, used as case studies for performance demonstration. The trained CNN model was evaluated on several synthetic 2D hyperspectral datasets with noise levels of 1%, 2%, 5%, and 10%. Our noisy data analysis aims to replicate realistic field conditions. The HyDeMiC's performance was assessed using the Matthews Correlation Coefficient (MCC), providing a comprehensive measure across different noise regimes. Results demonstrate that HyDeMiC achieved near-perfect classification accuracy (MCC = 1.00) on clean and low-noise datasets and maintained strong performance under moderate noise conditions. These findings emphasize HyDeMiC's robustness in the presence of moderate noise, highlighting its potential for real-world applications in hyperspectral imaging, where noise is often a significant challenge.", "AI": {"tldr": "HyDeMiC is a CNN-based mineral classifier that achieves near-perfect accuracy on clean/low-noise hyperspectral data and maintains strong performance under moderate noise conditions, demonstrating robustness for real-world mineral exploration applications.", "motivation": "Traditional mineral classification methods struggle with environmental noise, sensor limitations, and computational complexity in high-dimensional hyperspectral imaging data, creating a need for more robust solutions.", "method": "Developed HyDeMiC, a convolutional neural network trained on 115 USGS mineral spectra convolved with HSI sensor response. Evaluated on synthetic 2D datasets with 1%, 2%, 5%, and 10% noise levels using Matthews Correlation Coefficient.", "result": "Achieved near-perfect classification (MCC = 1.00) on clean and low-noise datasets, with strong performance maintained under moderate noise conditions, demonstrating robustness in noisy environments.", "conclusion": "HyDeMiC shows strong potential for real-world hyperspectral imaging applications where noise is a significant challenge, offering robust mineral classification capabilities under realistic field conditions."}}
{"id": "2601.18226", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18226", "abs": "https://arxiv.org/abs/2601.18226", "authors": ["Haotian Li", "Shijun Yang", "Weizhen Qi", "Silei Zhao", "Rui Hua", "Mingzhu Song", "Xiaojian Yang", "Chao Peng"], "title": "Yunjue Agent Tech Report: A Fully Reproducible, Zero-Start In-Situ Self-Evolving Agent System for Open-Ended Tasks", "comment": null, "summary": "Conventional agent systems often struggle in open-ended environments where task distributions continuously drift and external supervision is scarce. Their reliance on static toolsets or offline training lags behind these dynamics, leaving the system's capability boundaries rigid and unknown. To address this, we propose the In-Situ Self-Evolving paradigm. This approach treats sequential task interactions as a continuous stream of experience, enabling the system to distill short-term execution feedback into long-term, reusable capabilities without access to ground-truth labels. Within this framework, we identify tool evolution as the critical pathway for capability expansion, which provides verifiable, binary feedback signals. Within this framework, we develop Yunjue Agent, a system that iteratively synthesizes, optimizes, and reuses tools to navigate emerging challenges. To optimize evolutionary efficiency, we further introduce a Parallel Batch Evolution strategy. Empirical evaluations across five diverse benchmarks under a zero-start setting demonstrate significant performance gains over proprietary baselines. Additionally, complementary warm-start evaluations confirm that the accumulated general knowledge can be seamlessly transferred to novel domains. Finally, we propose a novel metric to monitor evolution convergence, serving as a function analogous to training loss in conventional optimization. We open-source our codebase, system traces, and evolved tools to facilitate future research in resilient, self-evolving intelligence.", "AI": {"tldr": "Yunjue Agent introduces an In-Situ Self-Evolving paradigm that enables AI agents to continuously evolve tools from task experience without ground-truth labels, achieving significant performance gains in open-ended environments.", "motivation": "Conventional agent systems struggle with continuously drifting task distributions and scarce supervision in open-ended environments, relying on static toolsets that create rigid capability boundaries.", "method": "Proposes In-Situ Self-Evolving paradigm treating sequential tasks as experience streams, with tool evolution as the critical pathway. Yunjue Agent synthesizes, optimizes, and reuses tools, using Parallel Batch Evolution strategy for efficiency.", "result": "Significant performance gains over proprietary baselines across five diverse benchmarks in zero-start setting; accumulated knowledge transfers seamlessly to novel domains in warm-start evaluations.", "conclusion": "The approach enables resilient, self-evolving intelligence with verifiable evolution convergence, demonstrated through open-sourced codebase, system traces, and evolved tools for future research."}}
{"id": "2601.17354", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2601.17354", "abs": "https://arxiv.org/abs/2601.17354", "authors": ["Wenzhi Guo", "Guangchi Fang", "Shu Yang", "Bing Wang"], "title": "PocketGS: On-Device Training of 3D Gaussian Splatting for High Perceptual Modeling", "comment": null, "summary": "Efficient and high-fidelity 3D scene modeling is a long-standing pursuit in computer graphics. While recent 3D Gaussian Splatting (3DGS) methods achieve impressive real-time modeling performance, they rely on resource-unconstrained training assumptions that fail on mobile devices, which are limited by minute-scale training budgets and hardware-available peak-memory. We present PocketGS, a mobile scene modeling paradigm that enables on-device 3DGS training under these tightly coupled constraints while preserving high perceptual fidelity. Our method resolves the fundamental contradictions of standard 3DGS through three co-designed operators: G builds geometry-faithful point-cloud priors; I injects local surface statistics to seed anisotropic Gaussians, thereby reducing early conditioning gaps; and T unrolls alpha compositing with cached intermediates and index-mapped gradient scattering for stable mobile backpropagation. Collectively, these operators satisfy the competing requirements of training efficiency, memory compactness, and modeling fidelity. Extensive experiments demonstrate that PocketGS is able to outperform the powerful mainstream workstation 3DGS baseline to deliver high-quality reconstructions, enabling a fully on-device, practical capture-to-rendering workflow.", "AI": {"tldr": "PocketGS enables efficient 3D Gaussian Splatting training on mobile devices with limited memory and training time, achieving high-fidelity scene modeling through co-designed operators that optimize geometry priors, surface statistics, and backpropagation.", "motivation": "Current 3D Gaussian Splatting methods require substantial computational resources and fail on mobile devices due to minute-scale training budgets and hardware memory limitations, creating a need for efficient on-device 3D scene modeling solutions.", "method": "Three co-designed operators: G builds geometry-faithful point-cloud priors; I injects local surface statistics to seed anisotropic Gaussians for reduced early conditioning gaps; and T unrolls alpha compositing with cached intermediates and index-mapped gradient scattering for stable mobile backpropagation.", "result": "PocketGS outperforms mainstream workstation 3DGS baselines, delivers high-quality reconstructions, and enables a fully on-device capture-to-rendering workflow while satisfying competing requirements of training efficiency, memory compactness, and modeling fidelity.", "conclusion": "PocketGS successfully enables practical on-device 3D Gaussian Splatting training under mobile constraints while preserving high perceptual fidelity, resolving fundamental contradictions in standard 3DGS through its co-designed operator approach."}}
{"id": "2601.18282", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18282", "abs": "https://arxiv.org/abs/2601.18282", "authors": ["Lei Wei", "Jinpeng Ou", "Xiao Peng", "Bin Wang"], "title": "Think-Augmented Function Calling: Improving LLM Parameter Accuracy Through Embedded Reasoning", "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in function calling for autonomous agents, yet current mechanisms lack explicit reasoning transparency during parameter generation, particularly for complex functions with interdependent parameters. While existing approaches like chain-of-thought prompting operate at the agent level, they fail to provide fine-grained reasoning guidance for individual function parameters. To address these limitations, we propose Think-Augmented Function Calling (TAFC), a novel framework that enhances function calling accuracy through explicit reasoning at both function and parameter levels. Our method introduces a universal \"think\" parameter augmentation that enables models to articulate their decision-making process, with dynamic optimization for parameter descriptions to improve reasoning quality. For complex parameters, TAFC automatically triggers granular reasoning based on complexity scoring, ensuring appropriate justification for critical decisions. Additionally, we propose reasoning-guided optimization to align generated reasoning with human expectations. TAFC requires no architectural modifications to existing LLMs while maintaining full API compatibility. Evaluation on ToolBench across proprietary and open-source models demonstrates significant improvements in parameter generation accuracy and reasoning coherence for multi-parameter functions, while providing enhanced interpretability for debugging AI agent behaviors.", "AI": {"tldr": "TAFC enhances LLM function calling by adding explicit reasoning at function and parameter levels through \"think\" parameter augmentation, improving accuracy and interpretability without model modifications.", "motivation": "Current LLM function calling lacks explicit reasoning transparency for parameter generation, especially for complex functions with interdependent parameters. Existing approaches like chain-of-thought operate at agent level but fail to provide fine-grained reasoning guidance for individual parameters.", "method": "Proposes Think-Augmented Function Calling (TAFC) framework with universal \"think\" parameter augmentation for explicit decision-making articulation. Includes dynamic optimization for parameter descriptions, automatic granular reasoning triggering based on complexity scoring for complex parameters, and reasoning-guided optimization to align reasoning with human expectations.", "result": "Evaluation on ToolBench across proprietary and open-source models shows significant improvements in parameter generation accuracy and reasoning coherence for multi-parameter functions, while providing enhanced interpretability for debugging AI agent behaviors.", "conclusion": "TAFC successfully addresses reasoning transparency limitations in LLM function calling by enabling explicit reasoning at both function and parameter levels without requiring architectural modifications to existing LLMs, maintaining full API compatibility while improving accuracy and interpretability."}}
{"id": "2601.17366", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17366", "abs": "https://arxiv.org/abs/2601.17366", "authors": ["Chengbo Ding", "Fenghe Tang", "Shaohua Kevin Zhou"], "title": "UCAD: Uncertainty-guided Contour-aware Displacement for semi-supervised medical image segmentation", "comment": "Accepted by ISBI 2026", "summary": "Existing displacement strategies in semi-supervised segmentation only operate on rectangular regions, ignoring anatomical structures and resulting in boundary distortions and semantic inconsistency. To address these issues, we propose UCAD, an Uncertainty-Guided Contour-Aware Displacement framework for semi-supervised medical image segmentation that preserves contour-aware semantics while enhancing consistency learning. Our UCAD leverages superpixels to generate anatomically coherent regions aligned with anatomy boundaries, and an uncertainty-guided selection mechanism to selectively displace challenging regions for better consistency learning. We further propose a dynamic uncertainty-weighted consistency loss, which adaptively stabilizes training and effectively regularizes the model on unlabeled regions. Extensive experiments demonstrate that UCAD consistently outperforms state-of-the-art semi-supervised segmentation methods, achieving superior segmentation accuracy under limited annotation. The code is available at:https://github.com/dcb937/UCAD.", "AI": {"tldr": "UCAD is a semi-supervised medical image segmentation framework that uses uncertainty-guided contour-aware displacement to preserve anatomical structures and improve consistency learning.", "motivation": "Existing displacement strategies in semi-supervised segmentation only operate on rectangular regions, ignoring anatomical structures and resulting in boundary distortions and semantic inconsistency.", "method": "UCAD uses superpixels to generate anatomically coherent regions aligned with anatomy boundaries, an uncertainty-guided selection mechanism to selectively displace challenging regions, and a dynamic uncertainty-weighted consistency loss to adaptively stabilize training.", "result": "Extensive experiments demonstrate that UCAD consistently outperforms state-of-the-art semi-supervised segmentation methods, achieving superior segmentation accuracy under limited annotation.", "conclusion": "UCAD effectively addresses boundary distortions and semantic inconsistency in semi-supervised medical image segmentation by preserving contour-aware semantics while enhancing consistency learning."}}
{"id": "2601.18308", "categories": ["cs.AI", "cs.SI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.18308", "abs": "https://arxiv.org/abs/2601.18308", "authors": ["Geunsik Lim"], "title": "A Generative AI-Driven Reliability Layer for Action-Oriented Disaster Resilience", "comment": "19 pages", "summary": "As climate-related hazards intensify, conventional early warning systems (EWS) disseminate alerts rapidly but often fail to trigger timely protective actions, leading to preventable losses and inequities. We introduce Climate RADAR (Risk-Aware, Dynamic, and Action Recommendation system), a generative AI-based reliability layer that reframes disaster communication from alerts delivered to actions executed. It integrates meteorological, hydrological, vulnerability, and social data into a composite risk index and employs guardrail-embedded large language models (LLMs) to deliver personalized recommendations across citizen, volunteer, and municipal interfaces. Evaluation through simulations, user studies, and a municipal pilot shows improved outcomes, including higher protective action execution, reduced response latency, and increased usability and trust. By combining predictive analytics, behavioral science, and responsible AI, Climate RADAR advances people-centered, transparent, and equitable early warning systems, offering practical pathways toward compliance-ready disaster resilience infrastructures.", "AI": {"tldr": "Climate RADAR is a generative AI system that transforms disaster alerts into personalized action recommendations to improve protective responses and reduce inequities in early warning systems.", "motivation": "Conventional early warning systems disseminate alerts quickly but often fail to trigger timely protective actions, leading to preventable losses and inequities in disaster response.", "method": "Integrates meteorological, hydrological, vulnerability, and social data into a composite risk index, then uses guardrail-embedded LLMs to generate personalized recommendations across citizen, volunteer, and municipal interfaces.", "result": "Evaluation shows improved outcomes including higher protective action execution, reduced response latency, and increased usability and trust through simulations, user studies, and a municipal pilot.", "conclusion": "Climate RADAR advances people-centered, transparent, and equitable early warning systems by combining predictive analytics, behavioral science, and responsible AI, offering practical pathways toward compliance-ready disaster resilience infrastructures."}}
{"id": "2601.17383", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17383", "abs": "https://arxiv.org/abs/2601.17383", "authors": ["Chen Ling", "Kai Hu", "Hangcheng Liu", "Xingshuo Han", "Tianwei Zhang", "Changhai Ou"], "title": "Physical Prompt Injection Attacks on Large Vision-Language Models", "comment": null, "summary": "Large Vision-Language Models (LVLMs) are increasingly deployed in real-world intelligent systems for perception and reasoning in open physical environments. While LVLMs are known to be vulnerable to prompt injection attacks, existing methods either require access to input channels or depend on knowledge of user queries, assumptions that rarely hold in practical deployments. We propose the first Physical Prompt Injection Attack (PPIA), a black-box, query-agnostic attack that embeds malicious typographic instructions into physical objects perceivable by the LVLM. PPIA requires no access to the model, its inputs, or internal pipeline, and operates solely through visual observation. It combines offline selection of highly recognizable and semantically effective visual prompts with strategic environment-aware placement guided by spatiotemporal attention, ensuring that the injected prompts are both perceivable and influential on model behavior. We evaluate PPIA across 10 state-of-the-art LVLMs in both simulated and real-world settings on tasks including visual question answering, planning, and navigation, PPIA achieves attack success rates up to 98%, with strong robustness under varying physical conditions such as distance, viewpoint, and illumination. Our code is publicly available at https://github.com/2023cghacker/Physical-Prompt-Injection-Attack.", "AI": {"tldr": "PPIA is the first black-box, query-agnostic physical prompt injection attack that embeds malicious typographic instructions into physical objects to manipulate LVLMs without access to model internals or user queries.", "motivation": "Existing prompt injection attacks on LVLMs require access to input channels or knowledge of user queries, which are impractical assumptions for real-world deployments where models operate in open physical environments.", "method": "Combines offline selection of highly recognizable visual prompts with environment-aware placement guided by spatiotemporal attention to ensure injected prompts are perceivable and influential on model behavior.", "result": "Achieves attack success rates up to 98% across 10 state-of-the-art LVLMs in simulated and real-world settings, with strong robustness under varying physical conditions (distance, viewpoint, illumination).", "conclusion": "PPIA demonstrates severe security vulnerabilities in LVLMs deployed in physical environments, highlighting the need for robust defenses against visual prompt injection attacks."}}
{"id": "2601.18353", "categories": ["cs.AI", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.18353", "abs": "https://arxiv.org/abs/2601.18353", "authors": ["Tuhin Chakrabarty", "Paramveer S. Dhillon"], "title": "Can Good Writing Be Generative? Expert-Level AI Writing Emerges through Fine-Tuning on High-Quality Books", "comment": "Proceedings of CHI 2026 Conference (To Appear)", "summary": "Creative writing has long been considered a uniquely human endeavor, requiring voice and style that machines could not replicate. This assumption is challenged by Generative AI that can emulate thousands of author styles in seconds with negligible marginal labor. To understand this better, we conducted a behavioral experiment where 28 MFA writers (experts) competed against three LLMs in emulating 50 critically acclaimed authors. Based on blind pairwise comparisons by 28 expert judges and 131 lay judges, we find that experts preferred human writing in 82.7% of cases under the in-context prompting condition but this reversed to 62% preference for AI after fine-tuning on authors' complete works. Lay judges, however, consistently preferred AI writing. Debrief interviews with expert writers revealed that their preference for AI writing triggered an identity crisis, eroding aesthetic confidence and questioning what constitutes \"good writing.\" These findings challenge discourse about AI's creative limitations and raise fundamental questions about the future of creative labor.", "AI": {"tldr": "AI can now emulate acclaimed author styles better than human experts after fine-tuning, with lay judges consistently preferring AI writing, causing identity crises among human writers.", "motivation": "To challenge the assumption that creative writing is uniquely human by testing whether AI can emulate acclaimed author styles better than expert human writers.", "method": "Behavioral experiment with 28 MFA writers competing against 3 LLMs to emulate 50 acclaimed authors; blind pairwise comparisons by 28 expert judges and 131 lay judges under in-context prompting vs. fine-tuning conditions.", "result": "Experts preferred human writing 82.7% with in-context prompting but reversed to 62% preference for AI after fine-tuning; lay judges consistently preferred AI writing; expert writers experienced identity crisis and eroded aesthetic confidence.", "conclusion": "AI challenges assumptions about creative limitations and raises fundamental questions about the future of creative labor, as AI can surpass human experts in emulating acclaimed writing styles."}}
{"id": "2601.17388", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17388", "abs": "https://arxiv.org/abs/2601.17388", "authors": ["Xuan Ding", "Xiu Yan", "Chuanlong Xie", "Yao Zhu"], "title": "ONRW: Optimizing inversion noise for high-quality and robust watermark", "comment": "Preprint. Under review", "summary": "Watermarking methods have always been effective means of protecting intellectual property, yet they face significant challenges. Although existing deep learning-based watermarking systems can hide watermarks in images with minimal impact on image quality, they often lack robustness when encountering image corruptions during transmission, which undermines their practical application value. To this end, we propose a high-quality and robust watermark framework based on the diffusion model. Our method first converts the clean image into inversion noise through a null-text optimization process, and after optimizing the inversion noise in the latent space, it produces a high-quality watermarked image through an iterative denoising process of the diffusion model. The iterative denoising process serves as a powerful purification mechanism, ensuring both the visual quality of the watermarked image and enhancing the robustness of the watermark against various corruptions. To prevent the optimizing of inversion noise from distorting the original semantics of the image, we specifically introduced self-attention constraints and pseudo-mask strategies. Extensive experimental results demonstrate the superior performance of our method against various image corruptions. In particular, our method outperforms the stable signature method by an average of 10\\% across 12 different image transformations on COCO datasets. Our codes are available at https://github.com/920927/ONRW.", "AI": {"tldr": "A diffusion model-based watermarking framework that uses null-text optimization and iterative denoising to create robust watermarks that survive image corruptions while maintaining visual quality.", "motivation": "Existing deep learning watermarking systems lack robustness against image corruptions during transmission, limiting their practical application value despite good visual quality.", "method": "Uses null-text optimization to convert clean images to inversion noise, optimizes noise in latent space, then applies iterative denoising via diffusion model. Includes self-attention constraints and pseudo-mask strategies to preserve original image semantics.", "result": "Outperforms stable signature method by average 10% across 12 different image transformations on COCO datasets, demonstrating superior robustness against various corruptions.", "conclusion": "The proposed diffusion-based framework provides both high visual quality and enhanced robustness for practical watermarking applications, addressing key limitations of existing methods."}}
{"id": "2601.18381", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.18381", "abs": "https://arxiv.org/abs/2601.18381", "authors": ["Yinghan Hou", "Zongyou Yang"], "title": "AI Agent for Reverse-Engineering Legacy Finite-Difference Code and Translating to Devito", "comment": "14 pages, 7 figures", "summary": "To facilitate the transformation of legacy finite difference implementations into the Devito environment, this study develops an integrated AI agent framework. Retrieval-Augmented Generation (RAG) and open-source Large Language Models are combined through multi-stage iterative workflows in the system's hybrid LangGraph architecture. The agent constructs an extensive Devito knowledge graph through document parsing, structure-aware segmentation, extraction of entity relationships, and Leiden-based community detection. GraphRAG optimisation enhances query performance across semantic communities that include seismic wave simulation, computational fluid dynamics, and performance tuning libraries. A reverse engineering component derives three-level query strategies for RAG retrieval through static analysis of Fortran source code. To deliver precise contextual information for language model guidance, the multi-stage retrieval pipeline performs parallel searching, concept expansion, community-scale retrieval, and semantic similarity analysis. Code synthesis is governed by Pydantic-based constraints to guarantee structured outputs and reliability. A comprehensive validation framework integrates conventional static analysis with the G-Eval approach, covering execution correctness, structural soundness, mathematical consistency, and API compliance. The overall agent workflow is implemented on the LangGraph framework and adopts concurrent processing to support quality-based iterative refinement and state-aware dynamic routing. The principal contribution lies in the incorporation of feedback mechanisms motivated by reinforcement learning, enabling a transition from static code translation toward dynamic and adaptive analytical behavior.", "AI": {"tldr": "AI agent framework for transforming legacy Fortran finite difference code to Devito using RAG, knowledge graphs, and multi-stage retrieval with reinforcement learning feedback.", "motivation": "To facilitate the transformation of legacy finite difference implementations into the Devito environment, addressing the challenge of migrating traditional computational science code to modern frameworks.", "method": "Hybrid LangGraph architecture combining RAG and LLMs with multi-stage iterative workflows. Includes Devito knowledge graph construction via document parsing, entity extraction, and community detection; GraphRAG optimization; reverse engineering of Fortran code for query strategies; parallel multi-stage retrieval; Pydantic-constrained code synthesis; and comprehensive validation with static analysis and G-Eval.", "result": "The framework enables precise code transformation through dynamic analytical behavior rather than static translation, with enhanced query performance across semantic communities (seismic wave simulation, CFD, performance tuning).", "conclusion": "The principal contribution is the incorporation of reinforcement learning-inspired feedback mechanisms, enabling transition from static code translation to dynamic, adaptive analytical behavior for legacy code migration to Devito."}}
{"id": "2601.17391", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17391", "abs": "https://arxiv.org/abs/2601.17391", "authors": ["Rui Fan", "Weidong Hao"], "title": "SMV-EAR: Bring Spatiotemporal Multi-View Representation Learning into Efficient Event-Based Action Recognition", "comment": null, "summary": "Event cameras action recognition (EAR) offers compelling privacy-protecting and efficiency advantages, where temporal motion dynamics is of great importance. Existing spatiotemporal multi-view representation learning (SMVRL) methods for event-based object recognition (EOR) offer promising solutions by projecting H-W-T events along spatial axis H and W, yet are limited by its translation-variant spatial binning representation and naive early concatenation fusion architecture. This paper reexamines the key SMVRL design stages for EAR and propose: (i) a principled spatiotemporal multi-view representation through translation-invariant dense conversion of sparse events, (ii) a dual-branch, dynamic fusion architecture that models sample-wise complementarity between motion features from different views, and (iii) a bio-inspired temporal warping augmentation that mimics speed variability of real-world human actions. On three challenging EAR datasets of HARDVS, DailyDVS-200 and THU-EACT-50-CHL, we show +7.0%, +10.7%, and +10.2% Top-1 accuracy gains over existing SMVRL EOR method with surprising 30.1% reduced parameters and 35.7% lower computations, establishing our framework as a novel and powerful EAR paradigm.", "AI": {"tldr": "A new event camera action recognition framework with translation-invariant dense conversion, dual-branch dynamic fusion, and bio-inspired temporal warping achieves significant accuracy gains with reduced parameters and computations.", "motivation": "Existing spatiotemporal multi-view representation learning methods for event-based object recognition have limitations: translation-variant spatial binning representation and naive early concatenation fusion architecture. Event cameras offer privacy-protecting and efficiency advantages for action recognition where temporal motion dynamics is crucial.", "method": "Three key innovations: (1) principled spatiotemporal multi-view representation through translation-invariant dense conversion of sparse events, (2) dual-branch dynamic fusion architecture modeling sample-wise complementarity between motion features from different views, and (3) bio-inspired temporal warping augmentation mimicking real-world human action speed variability.", "result": "Achieves +7.0%, +10.7%, and +10.2% Top-1 accuracy gains on HARDVS, DailyDVS-200, and THU-EACT-50-CHL datasets respectively, with 30.1% reduced parameters and 35.7% lower computations compared to existing SMVRL methods.", "conclusion": "The proposed framework establishes a novel and powerful paradigm for event camera action recognition, addressing key limitations of existing methods while achieving superior performance with improved efficiency."}}
{"id": "2601.18383", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18383", "abs": "https://arxiv.org/abs/2601.18383", "authors": ["Zhenyuan Guo", "Tong Chen", "Wenlong Meng", "Chen Gong", "Xin Yu", "Chengkun Wei", "Wenzhi Chen"], "title": "Dynamic Thinking-Token Selection for Efficient Reasoning in Large Reasoning Models", "comment": null, "summary": "Large Reasoning Models (LRMs) excel at solving complex problems by explicitly generating a reasoning trace before deriving the final answer. However, these extended generations incur substantial memory footprint and computational overhead, bottlenecking LRMs' efficiency. This work uses attention maps to analyze the influence of reasoning traces and uncover an interesting phenomenon: only some decision-critical tokens in a reasoning trace steer the model toward the final answer, while the remaining tokens contribute negligibly. Building on this observation, we propose Dynamic Thinking-Token Selection (DynTS). This method identifies decision-critical tokens and retains only their associated Key-Value (KV) cache states during inference, evicting the remaining redundant entries to optimize efficiency.", "AI": {"tldr": "DynTS identifies and retains only decision-critical tokens in reasoning traces to reduce KV cache memory overhead during inference.", "motivation": "Large Reasoning Models generate extensive reasoning traces that cause substantial memory and computational overhead, bottlenecking efficiency despite their problem-solving capabilities.", "method": "Uses attention maps to analyze reasoning trace influence, identifies decision-critical tokens, and proposes Dynamic Thinking-Token Selection (DynTS) to retain only critical tokens' KV cache states while evicting redundant entries.", "result": "Analysis reveals that only some decision-critical tokens in reasoning traces steer models toward final answers, while remaining tokens contribute negligibly, enabling selective KV cache retention.", "conclusion": "DynTS optimizes LRM efficiency by dynamically selecting and preserving only essential reasoning tokens, reducing memory footprint and computational overhead without compromising reasoning quality."}}
{"id": "2601.17399", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17399", "abs": "https://arxiv.org/abs/2601.17399", "authors": ["Rui Fang", "Jian Li", "Wei Chen", "Bin Hu", "Ying-Cong Chen", "Xin Tang", "Liang Diao"], "title": "ReLE: A Scalable System and Structured Benchmark for Diagnosing Capability Anisotropy in Chinese LLMs", "comment": null, "summary": "Large Language Models (LLMs) have achieved rapid progress in Chinese language understanding, yet accurately evaluating their capabilities remains challenged by benchmark saturation and prohibitive computational costs. While static leaderboards provide snapshot rankings, they often mask the structural trade-offs between capabilities. In this work, we present ReLE (Robust Efficient Live Evaluation), a scalable system designed to diagnose Capability Anisotropy, the non-uniformity of model performance across domains. Using ReLE, we evaluate 304 models (189 commercial, 115 open-source) across a Domain $\\times$ Capability orthogonal matrix comprising 207,843 samples. We introduce two methodological contributions to address current evaluation pitfalls: (1) A Symbolic-Grounded Hybrid Scoring Mechanism that eliminates embedding-based false positives in reasoning tasks; (2) A Dynamic Variance-Aware Scheduler based on Neyman allocation with noise correction, which reduces compute costs by 70\\% compared to full-pass evaluations while maintaining a ranking correlation of $\u03c1=0.96$. Our analysis reveals that aggregate rankings are highly sensitive to weighting schemes: models exhibit a Rank Stability Amplitude (RSA) of 11.4 in ReLE versus $\\sim$5.0 in traditional benchmarks, confirming that modern models are highly specialized rather than generally superior. We position ReLE not as a replacement for comprehensive static benchmarks, but as a high-frequency diagnostic monitor for the evolving model landscape.", "AI": {"tldr": "ReLE is a scalable evaluation system that diagnoses capability anisotropy in LLMs using hybrid scoring and dynamic scheduling to reduce computational costs by 70% while maintaining high ranking accuracy.", "motivation": "Current LLM evaluation faces challenges with benchmark saturation, high computational costs, and static leaderboards that mask structural trade-offs between capabilities. There's a need for more efficient, dynamic evaluation that reveals non-uniform performance across domains (capability anisotropy).", "method": "ReLE uses: 1) Symbolic-Grounded Hybrid Scoring Mechanism to eliminate embedding-based false positives in reasoning tasks; 2) Dynamic Variance-Aware Scheduler based on Neyman allocation with noise correction for efficient sampling; and evaluates 304 models across a Domain \u00d7 Capability orthogonal matrix with 207,843 samples.", "result": "The system reduces compute costs by 70% compared to full-pass evaluations while maintaining ranking correlation of \u03c1=0.96. Analysis reveals aggregate rankings are highly sensitive to weighting schemes, with models showing Rank Stability Amplitude of 11.4 in ReLE vs ~5.0 in traditional benchmarks, confirming models are specialized rather than generally superior.", "conclusion": "ReLE serves as a high-frequency diagnostic monitor for the evolving LLM landscape, not as a replacement for comprehensive static benchmarks, but as a tool to reveal capability anisotropy and model specialization patterns."}}
{"id": "2601.18467", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18467", "abs": "https://arxiv.org/abs/2601.18467", "authors": ["Yuhang Zhou", "Kai Zheng", "Qiguang Chen", "Mengkang Hu", "Qingfeng Sun", "Can Xu", "Jingjing Chen"], "title": "OffSeeker: Online Reinforcement Learning Is Not All You Need for Deep Research Agents", "comment": null, "summary": "Deep research agents have shown remarkable potential in handling long-horizon tasks. However, state-of-the-art performance typically relies on online reinforcement learning (RL), which is financially expensive due to extensive API calls. While offline training offers a more efficient alternative, its progress is hindered by the scarcity of high-quality research trajectories. In this paper, we demonstrate that expensive online reinforcement learning is not all you need to build powerful research agents. To bridge this gap, we introduce a fully open-source suite designed for effective offline training. Our core contributions include DeepForge, a ready-to-use task synthesis framework that generates large-scale research queries without heavy preprocessing; and a curated collection of 66k QA pairs, 33k SFT trajectories, and 21k DPO pairs. Leveraging these resources, we train OffSeeker (8B), a model developed entirely offline. Extensive evaluations across six benchmarks show that OffSeeker not only leads among similar-sized agents but also remains competitive with 30B-parameter systems trained via heavy online RL.", "AI": {"tldr": "OffSeeker (8B) achieves state-of-the-art research agent performance through fully offline training, eliminating expensive online RL while matching or exceeding larger models trained with heavy API calls.", "motivation": "Current deep research agents rely on expensive online reinforcement learning with extensive API calls, while offline training is hindered by lack of high-quality research trajectories. The paper aims to prove that expensive online RL isn't necessary for building powerful research agents.", "method": "Introduces DeepForge task synthesis framework for generating large-scale research queries without heavy preprocessing, plus curated datasets (66k QA pairs, 33k SFT trajectories, 21k DPO pairs). Uses these resources to train OffSeeker (8B) entirely offline.", "result": "OffSeeker leads among similar-sized agents and remains competitive with 30B-parameter systems trained via heavy online RL across six benchmarks, demonstrating offline training can match expensive online RL performance.", "conclusion": "Expensive online reinforcement learning is not essential for building powerful research agents; fully offline training with proper datasets and frameworks can achieve state-of-the-art performance while being more cost-effective."}}
{"id": "2601.17405", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17405", "abs": "https://arxiv.org/abs/2601.17405", "authors": ["Chunze Yang", "Wenjie Zhao", "Yue Tang", "Junbo Lu", "Jiusong Ge", "Qidong Liu", "Zeyu Gao", "Chen Li"], "title": "HAAF: Hierarchical Adaptation and Alignment of Foundation Models for Few-Shot Pathology Anomaly Detection", "comment": null, "summary": "Precision pathology relies on detecting fine-grained morphological abnormalities within specific Regions of Interest (ROIs), as these local, texture-rich cues - rather than global slide contexts - drive expert diagnostic reasoning. While Vision-Language (V-L) models promise data efficiency by leveraging semantic priors, adapting them faces a critical Granularity Mismatch, where generic representations fail to resolve such subtle defects. Current adaptation methods often treat modalities as independent streams, failing to ground semantic prompts in ROI-specific visual contexts. To bridge this gap, we propose the Hierarchical Adaptation and Alignment Framework (HAAF). At its core is a novel Cross-Level Scaled Alignment (CLSA) mechanism that enforces a sequential calibration order: visual features first inject context into text prompts to generate content-adaptive descriptors, which then spatially guide the visual encoder to spotlight anomalies. Additionally, a dual-branch inference strategy integrates semantic scores with geometric prototypes to ensure stability in few-shot settings. Experiments on four benchmarks show HAAF significantly outperforms state-of-the-art methods and effectively scales with domain-specific backbones (e.g., CONCH) in low-resource scenarios.", "AI": {"tldr": "HAAF framework addresses granularity mismatch in pathology by using cross-level alignment to ground semantic prompts in ROI-specific visual contexts, outperforming SOTA methods in few-shot settings.", "motivation": "Precision pathology requires detecting fine-grained abnormalities in specific ROIs, but current V-L models suffer from granularity mismatch where generic representations fail to resolve subtle defects, and existing methods don't properly ground semantic prompts in ROI-specific visual contexts.", "method": "Proposes Hierarchical Adaptation and Alignment Framework (HAAF) with Cross-Level Scaled Alignment (CLSA) mechanism that sequentially calibrates visual features to inject context into text prompts, then uses these content-adaptive descriptors to spatially guide visual encoder. Includes dual-branch inference integrating semantic scores with geometric prototypes for few-shot stability.", "result": "Experiments on four benchmarks show HAAF significantly outperforms state-of-the-art methods and effectively scales with domain-specific backbones (like CONCH) in low-resource scenarios.", "conclusion": "HAAF successfully bridges the granularity mismatch in pathology by enabling better alignment between visual features and semantic prompts, providing an effective framework for few-shot precision pathology applications."}}
{"id": "2601.18491", "categories": ["cs.AI", "cs.CC", "cs.CL", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18491", "abs": "https://arxiv.org/abs/2601.18491", "authors": ["Dongrui Liu", "Qihan Ren", "Chen Qian", "Shuai Shao", "Yuejin Xie", "Yu Li", "Zhonghao Yang", "Haoyu Luo", "Peng Wang", "Qingyu Liu", "Binxin Hu", "Ling Tang", "Jilin Mei", "Dadi Guo", "Leitao Yuan", "Junyao Yang", "Guanxu Chen", "Qihao Lin", "Yi Yu", "Bo Zhang", "Jiaxuan Guo", "Jie Zhang", "Wenqi Shao", "Huiqi Deng", "Zhiheng Xi", "Wenjie Wang", "Wenxuan Wang", "Wen Shen", "Zhikai Chen", "Haoyu Xie", "Jialing Tao", "Juntao Dai", "Jiaming Ji", "Zhongjie Ba", "Linfeng Zhang", "Yong Liu", "Quanshi Zhang", "Lei Zhu", "Zhihua Wei", "Hui Xue", "Chaochao Lu", "Jing Shao", "Xia Hu"], "title": "AgentDoG: A Diagnostic Guardrail Framework for AI Agent Safety and Security", "comment": "40 pages, 26 figures", "summary": "The rise of AI agents introduces complex safety and security challenges arising from autonomous tool use and environmental interactions. Current guardrail models lack agentic risk awareness and transparency in risk diagnosis. To introduce an agentic guardrail that covers complex and numerous risky behaviors, we first propose a unified three-dimensional taxonomy that orthogonally categorizes agentic risks by their source (where), failure mode (how), and consequence (what). Guided by this structured and hierarchical taxonomy, we introduce a new fine-grained agentic safety benchmark (ATBench) and a Diagnostic Guardrail framework for agent safety and security (AgentDoG). AgentDoG provides fine-grained and contextual monitoring across agent trajectories. More Crucially, AgentDoG can diagnose the root causes of unsafe actions and seemingly safe but unreasonable actions, offering provenance and transparency beyond binary labels to facilitate effective agent alignment. AgentDoG variants are available in three sizes (4B, 7B, and 8B parameters) across Qwen and Llama model families. Extensive experimental results demonstrate that AgentDoG achieves state-of-the-art performance in agentic safety moderation in diverse and complex interactive scenarios. All models and datasets are openly released.", "AI": {"tldr": "AgentDoG is a diagnostic guardrail framework for AI agent safety that provides fine-grained monitoring and root cause diagnosis of risky behaviors, outperforming existing methods on a new benchmark ATBench.", "motivation": "Current AI agent guardrails lack agentic risk awareness and transparency in diagnosing risks from autonomous tool use and environmental interactions, creating safety and security challenges.", "method": "Proposed a three-dimensional taxonomy categorizing agentic risks by source, failure mode, and consequence; created ATBench benchmark; developed AgentDoG framework with fine-grained contextual monitoring across agent trajectories and root cause diagnosis capabilities.", "result": "AgentDoG achieves state-of-the-art performance in agentic safety moderation across diverse interactive scenarios, with variants available in 4B, 7B, and 8B parameter sizes across Qwen and Llama model families.", "conclusion": "AgentDoG provides transparent, diagnostic safety monitoring for AI agents that goes beyond binary labels to offer provenance and root cause analysis, facilitating effective agent alignment through open-sourced models and datasets."}}
{"id": "2601.17408", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17408", "abs": "https://arxiv.org/abs/2601.17408", "authors": ["Harsharaj Pathak", "Vineeth N Balasubramanian"], "title": "Source-Free Domain Adaptation by Optimizing Batch-Wise Cosine Similarity", "comment": null, "summary": "Source-Free Domain Adaptation (SFDA) is an emerging area of research that aims to adapt a model trained on a labeled source domain to an unlabeled target domain without accessing the source data. Most of the successful methods in this area rely on the concept of neighborhood consistency but are prone to errors due to misleading neighborhood information. In this paper, we explore this approach from the point of view of learning more informative clusters and mitigating the effect of noisy neighbors using a concept called neighborhood signature, and demonstrate that adaptation can be achieved using just a single loss term tailored to optimize the similarity and dissimilarity of predictions of samples in the target domain. In particular, our proposed method outperforms existing methods in the challenging VisDA dataset while also yielding competitive results on other benchmark datasets.", "AI": {"tldr": "Proposes a single-loss SFDA method using neighborhood signatures to improve clustering and reduce noisy neighbor effects, achieving SOTA on VisDA with competitive results on other benchmarks.", "motivation": "Existing SFDA methods rely on neighborhood consistency but suffer from misleading neighborhood information and noisy neighbors, limiting adaptation performance.", "method": "Uses neighborhood signatures to learn more informative clusters and mitigate noisy neighbor effects, with adaptation achieved through a single loss term optimizing similarity/dissimilarity of target domain predictions.", "result": "Outperforms existing methods on challenging VisDA dataset and yields competitive results on other benchmark datasets.", "conclusion": "Demonstrates that effective SFDA can be achieved with a simple single-loss approach using neighborhood signatures to address noisy neighbor problems and improve clustering quality."}}
{"id": "2601.18496", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18496", "abs": "https://arxiv.org/abs/2601.18496", "authors": ["Zihan wang", "Hao Wang", "Shi Feng", "Xiaocui Yang", "Daling Wang", "Yiqun Zhang", "Jinghao Lin", "Haihua Yang", "Xiaozhong Ji"], "title": "DEEPMED: Building a Medical DeepResearch Agent via Multi-hop Med-Search Data and Turn-Controlled Agentic Training & Inference", "comment": null, "summary": "Medical reasoning models remain constrained by parametric knowledge and are thus susceptible to forgetting and hallucinations. DeepResearch (DR) models ground outputs in verifiable evidence from tools and perform strongly in general domains, but their direct transfer to medical field yields relatively limited gains. We attribute this to two gaps: task characteristic and tool-use scaling. Medical questions require evidence interpretation in a knowledge-intensive clinical context; while general DR models can retrieve information, they often lack clinical-context reasoning and thus \"find it but fail to use it,\" leaving performance limited by medical abilities. Moreover, in medical scenarios, blindly scaling tool-call can inject noisy context, derailing sensitive medical reasoning and prompting repetitive evidence-seeking along incorrect paths. Therefore, we propose DeepMed. For data, we deploy a multi-hop med-search QA synthesis method supporting the model to apply the DR paradigm in medical contexts. For training, we introduce a difficulty-aware turn-penalty to suppress excessive tool-call growth. For inference, we bring a monitor to help validate hypotheses within a controlled number of steps and avoid context rot. Overall, on seven medical benchmarks, DeepMed improves its base model by 9.79\\% on average and outperforms larger medical reasoning and DR models.", "AI": {"tldr": "DeepMed improves medical reasoning by addressing gaps in task characteristics and tool-use scaling through multi-hop QA synthesis, difficulty-aware training, and inference monitoring.", "motivation": "General deep research models underperform in medical domains due to two gaps: 1) inability to interpret evidence in clinical context despite retrieval capabilities, and 2) tool-call scaling issues that inject noise and derail sensitive medical reasoning.", "method": "Three key components: 1) Multi-hop medical search QA synthesis for data, 2) Difficulty-aware turn-penalty training to suppress excessive tool-calls, and 3) Inference monitor to validate hypotheses within controlled steps and prevent context rot.", "result": "On seven medical benchmarks, DeepMed improves its base model by 9.79% on average and outperforms both larger medical reasoning models and general deep research models.", "conclusion": "DeepMed successfully bridges the gaps in medical reasoning by adapting deep research paradigms to medical contexts through specialized data synthesis, training techniques, and inference controls."}}
{"id": "2601.17414", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17414", "abs": "https://arxiv.org/abs/2601.17414", "authors": ["Abdul Hasib", "A. S. M. Ahsanul Sarkar Akib"], "title": "Cloud-Enabled IoT System for Real-Time Environmental Monitoring and Remote Device Control Using Firebase", "comment": null, "summary": "The proliferation of Internet of Things (IoT) devices has created unprecedented opportunities for remote monitoring and control applications across various domains. Traditional monitoring systems often suffer from limitations in real-time data accessibility, remote controllability, and cloud integration. This paper presents a cloud-enabled IoT system that leverages Google's Firebase Realtime Database for synchronized environmental monitoring and device control. The system utilizes an ESP32 microcontroller to interface with a DHT22 temperature/humidity sensor and an HC-SR04 ultrasonic distance sensor, while enabling remote control of two LED indicators through a cloud-based interface. Real-time sensor data is transmitted to Firebase, providing a synchronized platform accessible from multiple devices simultaneously. Experimental results demonstrate reliable data transmission with 99.2\\% success rate, real-time control latency under 1.5 seconds, and persistent data storage for historical analysis. The system architecture offers a scalable framework for various IoT applications, from smart home automation to industrial monitoring, with a total implementation cost of \\$32.50. The integration of Firebase provides robust cloud capabilities without requiring complex server infrastructure, making advanced IoT applications accessible to developers and researchers with limited resources.", "AI": {"tldr": "Cloud-enabled IoT system using Firebase for synchronized environmental monitoring and device control with ESP32, achieving 99.2% data transmission success and <1.5s control latency.", "motivation": "Traditional IoT monitoring systems lack real-time data accessibility, remote controllability, and cloud integration capabilities needed for modern applications.", "method": "Uses ESP32 microcontroller with DHT22 temperature/humidity sensor and HC-SR04 ultrasonic distance sensor, integrated with Google Firebase Realtime Database for cloud synchronization and remote LED control.", "result": "Achieved 99.2% data transmission success rate, real-time control latency under 1.5 seconds, persistent data storage, with total implementation cost of $32.50.", "conclusion": "The system provides scalable, cost-effective IoT framework accessible to developers with limited resources, suitable for smart home automation to industrial monitoring applications."}}
{"id": "2601.18554", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18554", "abs": "https://arxiv.org/abs/2601.18554", "authors": ["Alberto Purpura", "Li Wang", "Sahil Badyal", "Eugenio Beaufrand", "Adam Faulkner"], "title": "Deconstructing Instruction-Following: A New Benchmark for Granular Evaluation of Large Language Model Instruction Compliance Abilities", "comment": "Paper accepted to EACL 2026", "summary": "Reliably ensuring Large Language Models (LLMs) follow complex instructions is a critical challenge, as existing benchmarks often fail to reflect real-world use or isolate compliance from task success. We introduce MOSAIC (MOdular Synthetic Assessment of Instruction Compliance), a modular framework that uses a dynamically generated dataset with up to 20 application-oriented generation constraints to enable a granular and independent analysis of this capability. Our evaluation of five LLMs from different families based on this new benchmark demonstrates that compliance is not a monolithic capability but varies significantly with constraint type, quantity, and position. The analysis reveals model-specific weaknesses, uncovers synergistic and conflicting interactions between instructions, and identifies distinct positional biases such as primacy and recency effects. These granular insights are critical for diagnosing model failures and developing more reliable LLMs for systems that demand strict adherence to complex instructions.", "AI": {"tldr": "MOSAIC is a modular framework for granular evaluation of LLM instruction compliance using synthetic datasets with up to 20 application-oriented constraints, revealing compliance varies by constraint type/quantity/position and showing model-specific weaknesses and positional biases.", "motivation": "Existing benchmarks fail to reflect real-world use or isolate compliance from task success, making it difficult to reliably ensure LLMs follow complex instructions - a critical challenge for practical applications.", "method": "Introduces MOSAIC framework using dynamically generated datasets with up to 20 application-oriented generation constraints to enable granular, independent analysis of instruction compliance capability across different LLMs.", "result": "Evaluation of five LLMs shows compliance is not monolithic but varies significantly with constraint type, quantity, and position; reveals model-specific weaknesses, synergistic/conflicting instruction interactions, and distinct positional biases (primacy and recency effects).", "conclusion": "Granular insights from MOSAIC are critical for diagnosing model failures and developing more reliable LLMs for systems requiring strict adherence to complex instructions, addressing limitations of existing benchmarks."}}
{"id": "2601.17420", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17420", "abs": "https://arxiv.org/abs/2601.17420", "authors": ["Shiu-hong Kao", "Chak Ho Huang", "Huaiqian Liu", "Yu-Wing Tai", "Chi-Keung Tang"], "title": "CoT-Seg: Rethinking Segmentation with Chain-of-Thought Reasoning and Self-Correction", "comment": "Project page: https://danielshkao.github.io/cot-seg.html", "summary": "Existing works of reasoning segmentation often fall short in complex cases, particularly when addressing complicated queries and out-of-domain images. Inspired by the chain-of-thought reasoning, where harder problems require longer thinking steps/time, this paper aims to explore a system that can think step-by-step, look up information if needed, generate results, self-evaluate its own results, and refine the results, in the same way humans approach harder questions. We introduce CoT-Seg, a training-free framework that rethinks reasoning segmentation by combining chain-of-thought reasoning with self-correction. Instead of fine-tuning, CoT-Seg leverages the inherent reasoning ability of pre-trained MLLMs (GPT-4o) to decompose queries into meta-instructions, extract fine-grained semantics from images, and identify target objects even under implicit or complex prompts. Moreover, CoT-Seg incorporates a self-correction stage: the model evaluates its own segmentation against the original query and reasoning trace, identifies mismatches, and iteratively refines the mask. This tight integration of reasoning and correction significantly improves reliability and robustness, especially in ambiguous or error-prone cases. Furthermore, our CoT-Seg framework allows easy incorporation of retrieval-augmented reasoning, enabling the system to access external knowledge when the input lacks sufficient information. To showcase CoT-Seg's ability to handle very challenging cases ,we introduce a new dataset ReasonSeg-Hard. Our results highlight that combining chain-of-thought reasoning, self-correction, offers a powerful paradigm for vision-language integration driven segmentation.", "AI": {"tldr": "CoT-Seg is a training-free framework that combines chain-of-thought reasoning with self-correction for complex reasoning segmentation tasks, using pre-trained MLLMs to decompose queries, extract semantics, and iteratively refine segmentation masks.", "motivation": "Existing reasoning segmentation methods struggle with complex queries and out-of-domain images. Inspired by how humans approach harder problems with longer thinking steps, the authors aim to create a system that can think step-by-step, look up information if needed, generate results, self-evaluate, and refine results.", "method": "CoT-Seg leverages pre-trained MLLMs (GPT-4o) to decompose queries into meta-instructions, extract fine-grained semantics from images, and identify target objects. It incorporates a self-correction stage where the model evaluates its segmentation against the original query and reasoning trace, identifies mismatches, and iteratively refines the mask. The framework also allows retrieval-augmented reasoning for accessing external knowledge.", "result": "The framework demonstrates improved reliability and robustness, especially in ambiguous or error-prone cases. A new dataset ReasonSeg-Hard is introduced to showcase CoT-Seg's ability to handle very challenging cases.", "conclusion": "Combining chain-of-thought reasoning with self-correction offers a powerful paradigm for vision-language integration driven segmentation, providing a training-free solution that mimics human problem-solving approaches for complex reasoning segmentation tasks."}}
{"id": "2601.18588", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18588", "abs": "https://arxiv.org/abs/2601.18588", "authors": ["Xianzhe Meng", "Qiangsheng Zeng", "Ling Luo", "Qinghan Yang", "Jiarui Hao", "Wenbo Wu", "Qinyu Wang", "Rui Yin", "Lin Qi", "Renzhi Lu"], "title": "Stability as a Liability:Systematic Breakdown of Linguistic Structure in LLMs", "comment": null, "summary": "Training stability is typically regarded as a prerequisite for reliable optimization in large language models. In this work, we analyze how stabilizing training dynamics affects the induced generation distribution. We show that under standard maximum likelihood training, stable parameter trajectories lead stationary solutions to approximately minimize the forward KL divergence to the empirical distribution, while implicitly reducing generative entropy. As a consequence, the learned model can concentrate probability mass on a limited subset of empirical modes, exhibiting systematic degeneration despite smooth loss convergence. We empirically validate this effect using a controlled feedback-based training framework that stabilizes internal generation statistics, observing consistent low-entropy outputs and repetitive behavior across architectures and random seeds. It indicates that optimization stability and generative expressivity are not inherently aligned, and that stability alone is an insufficient indicator of generative quality.", "AI": {"tldr": "Training stability in LLMs can paradoxically lead to low-entropy, repetitive outputs by minimizing forward KL divergence while reducing generative entropy, showing stability \u2260 generative quality.", "motivation": "To analyze how training stability affects the induced generation distribution in large language models, challenging the assumption that stable training dynamics necessarily lead to high-quality generative outputs.", "method": "Theoretical analysis of maximum likelihood training showing stable parameter trajectories lead to stationary solutions minimizing forward KL divergence while reducing generative entropy, plus empirical validation using a controlled feedback-based training framework that stabilizes internal generation statistics.", "result": "Stable training leads to models concentrating probability mass on limited subsets of empirical modes, exhibiting systematic degeneration with low-entropy outputs and repetitive behavior across architectures and random seeds, despite smooth loss convergence.", "conclusion": "Optimization stability and generative expressivity are not inherently aligned, and stability alone is an insufficient indicator of generative quality in language models."}}
{"id": "2601.17429", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17429", "abs": "https://arxiv.org/abs/2601.17429", "authors": ["Mehdi Yousefzadeh", "Siavash Shirzadeh Barough", "Ashkan Fakharifar", "Yashar Tayyarazad", "Narges Eghbali", "Mohaddeseh Mozaffari", "Hoda Taeb", "Negar Sadat Rafiee Tabatabaee", "Parsa Esfahanian", "Ghazaleh Sadeghi Gohar", "Amineh Safavirad", "Saeideh Mazloomzadeh", "Ehsan khalilipur", "Armin Elahifar", "Majid Maleki"], "title": "Coronary Artery Segmentation and Vessel-Type Classification in X-Ray Angiography", "comment": null, "summary": "X-ray coronary angiography (XCA) is the clinical reference standard for assessing coronary artery disease, yet quantitative analysis is limited by the difficulty of robust vessel segmentation in routine data. Low contrast, motion, foreshortening, overlap, and catheter confounding degrade segmentation and contribute to domain shift across centers. Reliable segmentation, together with vessel-type labeling, enables vessel-specific coronary analytics and downstream measurements that depend on anatomical localization. From 670 cine sequences (407 subjects), we select a best frame near peak opacification using a low-intensity histogram criterion and apply joint super-resolution and enhancement. We benchmark classical Meijering, Frangi, and Sato vesselness filters under per-image oracle tuning, a single global mean setting, and per-image parameter prediction via Support Vector Regression (SVR). Neural baselines include U-Net, FPN, and a Swin Transformer, trained with coronary-only and merged coronary+catheter supervision. A second stage assigns vessel identity (LAD, LCX, RCA). External evaluation uses the public DCA1 cohort. SVR per-image tuning improves Dice over global means for all classical filters (e.g., Frangi: 0.759 vs. 0.741). Among deep models, FPN attains 0.914+/-0.007 Dice (coronary-only), and merged coronary+catheter labels further improve to 0.931+/-0.006. On DCA1 as a strict external test, Dice drops to 0.798 (coronary-only) and 0.814 (merged), while light in-domain fine-tuning recovers to 0.881+/-0.014 and 0.882+/-0.015. Vessel-type labeling achieves 98.5% accuracy (Dice 0.844) for RCA, 95.4% (0.786) for LAD, and 96.2% (0.794) for LCX. Learned per-image tuning strengthens classical pipelines, while high-resolution FPN models and merged-label supervision improve stability and external transfer with modest adaptation.", "AI": {"tldr": "XCA vessel segmentation improved via per-image tuning of classical filters and deep learning with merged coronary+catheter supervision, achieving high Dice scores and accurate vessel-type labeling.", "motivation": "X-ray coronary angiography (XCA) is the clinical gold standard but suffers from poor vessel segmentation due to low contrast, motion, foreshortening, overlap, and catheter artifacts, limiting quantitative analysis and causing domain shift across centers.", "method": "Best frame selection near peak opacification using low-intensity histogram criterion, joint super-resolution and enhancement. Benchmark classical vesselness filters (Meijering, Frangi, Sato) with per-image oracle tuning, global mean, and SVR parameter prediction. Neural baselines (U-Net, FPN, Swin Transformer) trained with coronary-only and merged coronary+catheter supervision. Second stage for vessel-type labeling (LAD, LCX, RCA). External evaluation on DCA1 cohort.", "result": "SVR per-image tuning improved Dice over global means for all classical filters (Frangi: 0.759 vs 0.741). FPN achieved 0.914\u00b10.007 Dice (coronary-only), improved to 0.931\u00b10.006 with merged labels. On external DCA1 test, Dice dropped to 0.798/0.814 but recovered to 0.881\u00b10.014/0.882\u00b10.015 with fine-tuning. Vessel-type labeling accuracy: 98.5% (RCA), 95.4% (LAD), 96.2% (LCX).", "conclusion": "Learned per-image tuning strengthens classical pipelines, while high-resolution FPN models with merged-label supervision improve stability and external transfer with modest adaptation, enabling reliable vessel segmentation and anatomical localization for coronary analytics."}}
{"id": "2601.18595", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18595", "abs": "https://arxiv.org/abs/2601.18595", "authors": ["Joseph Cotnareanu", "Didier Chetelat", "Yingxue Zhang", "Mark Coates"], "title": "A Balanced Neuro-Symbolic Approach for Commonsense Abductive Logic", "comment": null, "summary": "Although Large Language Models (LLMs) have demonstrated impressive formal reasoning abilities, they often break down when problems require complex proof planning. One promising approach for improving LLM reasoning abilities involves translating problems into formal logic and using a logic solver. Although off-the-shelf logic solvers are in principle substantially more efficient than LLMs at logical reasoning, they assume that all relevant facts are provided in a question and are unable to deal with missing commonsense relations. In this work, we propose a novel method that uses feedback from the logic solver to augment a logic problem with commonsense relations provided by the LLM, in an iterative manner. This involves a search procedure through potential commonsense assumptions to maximize the chance of finding useful facts while keeping cost tractable. On a collection of pure-logical reasoning datasets, from which some commonsense information has been removed, our method consistently achieves considerable improvements over existing techniques, demonstrating the value in balancing neural and symbolic elements when working in human contexts.", "AI": {"tldr": "LLMs struggle with complex proof planning requiring commonsense reasoning, so the paper proposes an iterative method combining LLM commonsense generation with logic solver feedback to augment logical problems with missing commonsense relations.", "motivation": "LLMs have strong formal reasoning but break down on problems requiring complex proof planning, especially when commonsense relations are missing. Logic solvers are efficient but can't handle missing commonsense information, creating a gap in solving real-world reasoning problems.", "method": "Proposes an iterative method where: 1) Logic solver provides feedback on missing information, 2) LLM generates potential commonsense relations to fill gaps, 3) Uses search procedure to maximize finding useful facts while controlling cost, 4) Iteratively augments logic problems with commonsense relations.", "result": "On logical reasoning datasets with removed commonsense information, the method achieves considerable improvements over existing techniques, demonstrating effective balancing of neural and symbolic elements.", "conclusion": "The hybrid approach combining LLM commonsense generation with logic solver feedback effectively addresses the limitations of both systems, showing value in balancing neural and symbolic reasoning for human-context problems."}}
{"id": "2601.17468", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17468", "abs": "https://arxiv.org/abs/2601.17468", "authors": ["Chia-Ming Lee", "Yu-Fan Lin", "Jing-Hui Jung", "Yu-Jou Hsiao", "Chih-Chung Hsu", "Yu-Lun Liu"], "title": "ReflexSplit: Single Image Reflection Separation via Layer Fusion-Separation", "comment": "Project page: https://wuw2135.github.io/ReflexSplit-ProjectPage/", "summary": "Single Image Reflection Separation (SIRS) disentangles mixed images into transmission and reflection layers. Existing methods suffer from transmission-reflection confusion under nonlinear mixing, particularly in deep decoder layers, due to implicit fusion mechanisms and inadequate multi-scale coordination. We propose ReflexSplit, a dual-stream framework with three key innovations. (1) Cross-scale Gated Fusion (CrGF) adaptively aggregates semantic priors, texture details, and decoder context across hierarchical depths, stabilizing gradient flow and maintaining feature consistency. (2) Layer Fusion-Separation Blocks (LFSB) alternate between fusion for shared structure extraction and differential separation for layer-specific disentanglement. Inspired by Differential Transformer, we extend attention cancellation to dual-stream separation via cross-stream subtraction. (3) Curriculum training progressively strengthens differential separation through depth-dependent initialization and epoch-wise warmup. Extensive experiments on synthetic and real-world benchmarks demonstrate state-of-the-art performance with superior perceptual quality and robust generalization. Our code is available at https://github.com/wuw2135/ReflexSplit.", "AI": {"tldr": "ReflexSplit is a dual-stream framework for single image reflection separation that addresses transmission-reflection confusion through cross-scale gated fusion, layer fusion-separation blocks, and curriculum training.", "motivation": "Existing SIRS methods struggle with transmission-reflection confusion under nonlinear mixing, especially in deep decoder layers, due to implicit fusion mechanisms and inadequate multi-scale coordination.", "method": "Proposes ReflexSplit with three innovations: 1) Cross-scale Gated Fusion for adaptive aggregation across hierarchical depths, 2) Layer Fusion-Separation Blocks alternating between fusion and separation with cross-stream subtraction, 3) Curriculum training with depth-dependent initialization and epoch-wise warmup.", "result": "State-of-the-art performance on synthetic and real-world benchmarks with superior perceptual quality and robust generalization.", "conclusion": "ReflexSplit effectively addresses transmission-reflection confusion through its dual-stream architecture with cross-scale coordination and differential separation mechanisms."}}
{"id": "2601.18608", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18608", "abs": "https://arxiv.org/abs/2601.18608", "authors": ["Fabian Fumagalli", "R. Teal Witter", "Christopher Musco"], "title": "PolySHAP: Extending KernelSHAP with Interaction-Informed Polynomial Regression", "comment": null, "summary": "Shapley values have emerged as a central game-theoretic tool in explainable AI (XAI). However, computing Shapley values exactly requires $2^d$ game evaluations for a model with $d$ features. Lundberg and Lee's KernelSHAP algorithm has emerged as a leading method for avoiding this exponential cost. KernelSHAP approximates Shapley values by approximating the game as a linear function, which is fit using a small number of game evaluations for random feature subsets.\n  In this work, we extend KernelSHAP by approximating the game via higher degree polynomials, which capture non-linear interactions between features. Our resulting PolySHAP method yields empirically better Shapley value estimates for various benchmark datasets, and we prove that these estimates are consistent.\n  Moreover, we connect our approach to paired sampling (antithetic sampling), a ubiquitous modification to KernelSHAP that improves empirical accuracy. We prove that paired sampling outputs exactly the same Shapley value approximations as second-order PolySHAP, without ever fitting a degree 2 polynomial. To the best of our knowledge, this finding provides the first strong theoretical justification for the excellent practical performance of the paired sampling heuristic.", "AI": {"tldr": "PolySHAP extends KernelSHAP by using higher-degree polynomials to capture feature interactions, improving Shapley value estimates and connecting to paired sampling theory.", "motivation": "KernelSHAP approximates Shapley values efficiently but uses linear approximations that miss non-linear feature interactions. There's a need for more accurate approximations that capture these interactions while maintaining computational efficiency.", "method": "Extends KernelSHAP by approximating the game via higher-degree polynomials (PolySHAP) instead of linear functions. This captures non-linear interactions between features while still using a small number of game evaluations.", "result": "PolySHAP yields empirically better Shapley value estimates across various benchmark datasets. The method proves consistency of estimates and reveals that paired sampling (antithetic sampling) is equivalent to second-order PolySHAP without explicitly fitting degree 2 polynomials.", "conclusion": "PolySHAP provides a theoretically grounded extension to KernelSHAP that improves accuracy by capturing feature interactions. The connection to paired sampling offers the first strong theoretical justification for this widely-used heuristic's practical performance."}}
{"id": "2601.17470", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17470", "abs": "https://arxiv.org/abs/2601.17470", "authors": ["Chia-Ming Lee", "Yu-Fan Lin", "Yu-Jou Hsiao", "Jing-Hui Jung", "Yu-Lun Liu", "Chih-Chung Hsu"], "title": "PhaSR: Generalized Image Shadow Removal with Physically Aligned Priors", "comment": "Project Page: https://ming053l.github.io/PhaSR", "summary": "Shadow removal under diverse lighting conditions requires disentangling illumination from intrinsic reflectance, a challenge compounded when physical priors are not properly aligned. We propose PhaSR (Physically Aligned Shadow Removal), addressing this through dual-level prior alignment to enable robust performance from single-light shadows to multi-source ambient lighting. First, Physically Aligned Normalization (PAN) performs closed-form illumination correction via Gray-world normalization, log-domain Retinex decomposition, and dynamic range recombination, suppressing chromatic bias. Second, Geometric-Semantic Rectification Attention (GSRA) extends differential attention to cross-modal alignment, harmonizing depth-derived geometry with DINO-v2 semantic embeddings to resolve modal conflicts under varying illumination. Experiments show competitive performance in shadow removal with lower complexity and generalization to ambient lighting where traditional methods fail under multi-source illumination. Our source code is available at https://github.com/ming053l/PhaSR.", "AI": {"tldr": "PhaSR uses dual-level prior alignment for robust shadow removal under diverse lighting, combining illumination correction with geometric-semantic attention to handle single-light to multi-source ambient conditions.", "motivation": "Shadow removal under diverse lighting conditions is challenging due to difficulty disentangling illumination from intrinsic reflectance, especially when physical priors are misaligned. Traditional methods fail under multi-source illumination.", "method": "PhaSR employs dual-level prior alignment: 1) Physically Aligned Normalization (PAN) for illumination correction via Gray-world normalization, log-domain Retinex decomposition, and dynamic range recombination; 2) Geometric-Semantic Rectification Attention (GSRA) for cross-modal alignment harmonizing depth geometry with DINO-v2 semantic embeddings.", "result": "Competitive performance in shadow removal with lower complexity, and generalization to ambient lighting where traditional methods fail under multi-source illumination.", "conclusion": "PhaSR effectively addresses shadow removal under diverse lighting through physically aligned priors, enabling robust performance from single-light to multi-source ambient conditions."}}
{"id": "2601.18617", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18617", "abs": "https://arxiv.org/abs/2601.18617", "authors": ["Pierre Orhan", "Pablo Diego-Sim\u00f3n", "Emmnanuel Chemla", "Yair Lakretz", "Yves Boubenec", "Jean-R\u00e9mi King"], "title": "Emergence of Phonemic, Syntactic, and Semantic Representations in Artificial Neural Networks", "comment": null, "summary": "During language acquisition, children successively learn to categorize phonemes, identify words, and combine them with syntax to form new meaning. While the development of this behavior is well characterized, we still lack a unifying computational framework to explain its underlying neural representations. Here, we investigate whether and when phonemic, lexical, and syntactic representations emerge in the activations of artificial neural networks during their training. Our results show that both speech- and text-based models follow a sequence of learning stages: during training, their neural activations successively build subspaces, where the geometry of the neural activations represents phonemic, lexical, and syntactic structure. While this developmental trajectory qualitatively relates to children's, it is quantitatively different: These algorithms indeed require two to four orders of magnitude more data for these neural representations to emerge. Together, these results show conditions under which major stages of language acquisition spontaneously emerge, and hence delineate a promising path to understand the computations underpinning language acquisition.", "AI": {"tldr": "Neural networks develop phonemic, lexical, and syntactic representations in stages during training, similar to children's language acquisition but requiring vastly more data.", "motivation": "While children's language development stages are well characterized, we lack a unifying computational framework to explain the underlying neural representations of phoneme categorization, word identification, and syntax combination.", "method": "Investigating whether and when phonemic, lexical, and syntactic representations emerge in artificial neural network activations during training, using both speech- and text-based models.", "result": "Models follow a sequence of learning stages where neural activations successively build subspaces representing phonemic, lexical, and syntactic structure. This trajectory qualitatively resembles children's development but requires 2-4 orders of magnitude more data.", "conclusion": "The study shows conditions under which major language acquisition stages spontaneously emerge, delineating a promising path to understand the computations underpinning language acquisition."}}
{"id": "2601.17504", "categories": ["cs.CV", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2601.17504", "abs": "https://arxiv.org/abs/2601.17504", "authors": ["Yan Zhou", "Zhen Huang", "Yingqiu Li", "Yue Ouyang", "Suncheng Xiang", "Zehua Wang"], "title": "BMDS-Net: A Bayesian Multi-Modal Deep Supervision Network for Robust Brain Tumor Segmentation", "comment": "16 pages, 5 figures. Manuscript prepared for submission to ACM TOMM", "summary": "Accurate brain tumor segmentation from multi-modal magnetic resonance imaging (MRI) is a prerequisite for precise radiotherapy planning and surgical navigation. While recent Transformer-based models such as Swin UNETR have achieved impressive benchmark performance, their clinical utility is often compromised by two critical issues: sensitivity to missing modalities (common in clinical practice) and a lack of confidence calibration. Merely chasing higher Dice scores on idealized data fails to meet the safety requirements of real-world medical deployment. In this work, we propose BMDS-Net, a unified framework that prioritizes clinical robustness and trustworthiness over simple metric maximization. Our contribution is three-fold. First, we construct a robust deterministic backbone by integrating a Zero-Init Multimodal Contextual Fusion (MMCF) module and a Residual-Gated Deep Decoder Supervision (DDS) mechanism, enabling stable feature learning and precise boundary delineation with significantly reduced Hausdorff Distance, even under modality corruption. Second, and most importantly, we introduce a memory-efficient Bayesian fine-tuning strategy that transforms the network into a probabilistic predictor, providing voxel-wise uncertainty maps to highlight potential errors for clinicians. Third, comprehensive experiments on the BraTS 2021 dataset demonstrate that BMDS-Net not only maintains competitive accuracy but, more importantly, exhibits superior stability in missing-modality scenarios where baseline models fail. The source code is publicly available at https://github.com/RyanZhou168/BMDS-Net.", "AI": {"tldr": "BMDS-Net: A unified brain tumor segmentation framework that prioritizes clinical robustness and trustworthiness over simple metric maximization, addressing missing modalities and uncertainty calibration.", "motivation": "Current Transformer-based models like Swin UNETR achieve good benchmark performance but lack clinical robustness - they're sensitive to missing modalities (common in practice) and lack confidence calibration, failing to meet real-world medical safety requirements.", "method": "Three key contributions: 1) Robust deterministic backbone with Zero-Init Multimodal Contextual Fusion (MMCF) module and Residual-Gated Deep Decoder Supervision (DDS) for stable feature learning and boundary delineation; 2) Memory-efficient Bayesian fine-tuning strategy to transform network into probabilistic predictor with voxel-wise uncertainty maps; 3) Comprehensive experiments on BraTS 2021 dataset.", "result": "BMDS-Net maintains competitive accuracy while exhibiting superior stability in missing-modality scenarios where baseline models fail, with significantly reduced Hausdorff Distance even under modality corruption.", "conclusion": "BMDS-Net provides a clinically robust and trustworthy brain tumor segmentation solution that addresses critical real-world issues of missing modalities and uncertainty calibration, prioritizing safety over simple metric maximization."}}
{"id": "2601.18630", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.18630", "abs": "https://arxiv.org/abs/2601.18630", "authors": ["Abeer Badawi", "Md Tahmid Rahman Laskar", "Elahe Rahimi", "Sheri Grach", "Lindsay Bertrand", "Lames Danok", "Frank Rudzicz", "Jimmy Huang", "Elham Dolatabadi"], "title": "Assessing the Quality of Mental Health Support in LLM Responses through Multi-Attribute Human Evaluation", "comment": null, "summary": "The escalating global mental health crisis, marked by persistent treatment gaps, availability, and a shortage of qualified therapists, positions Large Language Models (LLMs) as a promising avenue for scalable support. While LLMs offer potential for accessible emotional assistance, their reliability, therapeutic relevance, and alignment with human standards remain challenging to address. This paper introduces a human-grounded evaluation methodology designed to assess LLM generated responses in therapeutic dialogue. Our approach involved curating a dataset of 500 mental health conversations from datasets with real-world scenario questions and evaluating the responses generated by nine diverse LLMs, including closed source and open source models. More specifically, these responses were evaluated by two psychiatric trained experts, who independently rated each on a 5 point Likert scale across a comprehensive 6 attribute rubric. This rubric captures Cognitive Support and Affective Resonance, providing a multidimensional perspective on therapeutic quality. Our analysis reveals that LLMs provide strong cognitive reliability by producing safe, coherent, and clinically appropriate information, but they demonstrate unstable affective alignment. Although closed source models (e.g., GPT-4o) offer balanced therapeutic responses, open source models show greater variability and emotional flatness. We reveal a persistent cognitive-affective gap and highlight the need for failure aware, clinically grounded evaluation frameworks that prioritize relational sensitivity alongside informational accuracy in mental health oriented LLMs. We advocate for balanced evaluation protocols with human in the loop that center on therapeutic sensitivity and provide a framework to guide the responsible design and clinical oversight of mental health oriented conversational AI.", "AI": {"tldr": "LLMs show strong cognitive reliability but unstable affective alignment in mental health conversations, revealing a persistent cognitive-affective gap that requires failure-aware, clinically-grounded evaluation frameworks.", "motivation": "Addressing the global mental health crisis with treatment gaps and therapist shortages by exploring LLMs as scalable support, while assessing their reliability, therapeutic relevance, and alignment with human standards.", "method": "Human-grounded evaluation methodology using 500 mental health conversations from real-world datasets, evaluated by two psychiatric experts rating nine diverse LLMs on a 5-point Likert scale across 6 attributes capturing Cognitive Support and Affective Resonance.", "result": "LLMs provide strong cognitive reliability (safe, coherent, clinically appropriate information) but demonstrate unstable affective alignment. Closed-source models like GPT-4o offer balanced therapeutic responses, while open-source models show greater variability and emotional flatness.", "conclusion": "Reveals a persistent cognitive-affective gap and advocates for failure-aware, clinically-grounded evaluation frameworks that prioritize relational sensitivity alongside informational accuracy, with balanced protocols and human-in-the-loop approaches for responsible mental health AI design."}}
{"id": "2601.17529", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17529", "abs": "https://arxiv.org/abs/2601.17529", "authors": ["Fengting Zhang", "Yue He", "Qinghao Liu", "Yaonan Wang", "Xiang Chen", "Hang Zhang"], "title": "FMIR, a foundation model-based Image Registration Framework for Robust Image Registration", "comment": null, "summary": "Deep learning has revolutionized medical image registration by achieving unprecedented speeds, yet its clinical application is hindered by a limited ability to generalize beyond the training domain, a critical weakness given the typically small scale of medical datasets. In this paper, we introduce FMIR, a foundation model-based registration framework that overcomes this limitation.Combining a foundation model-based feature encoder for extracting anatomical structures with a general registration head, and trained with a channel regularization strategy on just a single dataset, FMIR achieves state-of-the-art(SOTA) in-domain performance while maintaining robust registration on out-of-domain images.Our approach demonstrates a viable path toward building generalizable medical imaging foundation models with limited resources. The code is available at https://github.com/Monday0328/FMIR.git.", "AI": {"tldr": "FMIR is a foundation model-based medical image registration framework that achieves SOTA in-domain performance and robust out-of-domain generalization using limited training data.", "motivation": "Current deep learning medical image registration methods suffer from poor generalization beyond training domains, which is problematic given typically small medical datasets. There's a need for more generalizable registration models that work with limited training resources.", "method": "FMIR combines a foundation model-based feature encoder for anatomical structure extraction with a general registration head, trained using a channel regularization strategy on just a single dataset.", "result": "FMIR achieves state-of-the-art in-domain performance while maintaining robust registration on out-of-domain images, demonstrating strong generalization capabilities.", "conclusion": "The approach shows a viable path toward building generalizable medical imaging foundation models with limited resources, addressing a key limitation in clinical application of deep learning registration methods."}}
{"id": "2601.18631", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.18631", "abs": "https://arxiv.org/abs/2601.18631", "authors": ["Mingyang Song", "Haoyu Sun", "Jiawei Gu", "Linjie Li", "Luxin Xu", "Ranjay Krishna", "Yu Cheng"], "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning", "comment": "28 pages, 10 figures and 13 tables", "summary": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw.", "AI": {"tldr": "AdaReasoner is a multimodal model family that learns tool use as a general reasoning skill, enabling autonomous tool selection, sequencing, and adaptation to new tools/tasks without explicit supervision.", "motivation": "Humans use tools to solve complex problems, suggesting a promising approach for improving visual reasoning in MLLMs. The key challenge is enabling models to know which tools to use, when to invoke them, and how to compose them across multiple steps, especially with new tools or tasks.", "method": "Three main components: (1) scalable data curation pipeline for long-horizon multi-step tool interactions, (2) Tool-GRPO reinforcement learning algorithm for optimizing tool selection and sequencing based on end-task success, and (3) adaptive learning mechanism that dynamically regulates tool usage.", "result": "AdaReasoner shows strong tool-adaptive and generalization behaviors: autonomously adopts beneficial tools, suppresses irrelevant ones, adjusts usage frequency based on task demands. Achieves state-of-the-art performance across benchmarks, improving 7B base model by +24.9% on average and surpassing proprietary systems like GPT-5 on multiple tasks including VSP and Jigsaw.", "conclusion": "AdaReasoner successfully learns tool use as a general reasoning skill, enabling models to infer tool utility from context and intermediate outcomes, coordinate multiple tools, and generalize to unseen tools without explicit training for these capabilities."}}
{"id": "2601.17535", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17535", "abs": "https://arxiv.org/abs/2601.17535", "authors": ["Kevin Robbins", "Xiaotong Liu", "Yu Wu", "Le Sun", "Grady McPeak", "Abby Stylianou", "Robert Pless"], "title": "Will It Zero-Shot?: Will It Zero-Shot?: Predicting Zero-Shot Classification Performance For Arbitrary Queries", "comment": null, "summary": "Vision-Language Models like CLIP create aligned embedding spaces for text and images, making it possible for anyone to build a visual classifier by simply naming the classes they want to distinguish. However, a model that works well in one domain may fail in another, and non-expert users have no straightforward way to assess whether their chosen VLM will work on their problem. We build on prior work using text-only comparisons to evaluate how well a model works for a given natural language task, and explore approaches that also generate synthetic images relevant to that task to evaluate and refine the prediction of zero-shot accuracy. We show that generated imagery to the baseline text-only scores substantially improves the quality of these predictions. Additionally, it gives a user feedback on the kinds of images that were used to make the assessment. Experiments on standard CLIP benchmark datasets demonstrate that the image-based approach helps users predict, without any labeled examples, whether a VLM will be effective for their application.", "AI": {"tldr": "Using generated images alongside text improves zero-shot accuracy predictions for vision-language models, helping users assess model suitability without labeled data.", "motivation": "Users need ways to evaluate if a vision-language model (VLM) like CLIP will work for their specific domain without requiring labeled examples or expertise.", "method": "Extends text-only evaluation by generating synthetic images relevant to the task, combining visual and textual information to predict zero-shot accuracy.", "result": "Image-based approach substantially improves zero-shot accuracy predictions compared to text-only baselines and provides visual feedback about assessment basis.", "conclusion": "Generated imagery enables users to better predict VLM effectiveness for their applications without requiring labeled data, making model selection more accessible."}}
{"id": "2601.18642", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18642", "abs": "https://arxiv.org/abs/2601.18642", "authors": ["Lei Wei", "Xu Dong", "Xiao Peng", "Niantao Xie", "Bin Wang"], "title": "FadeMem: Biologically-Inspired Forgetting for Efficient Agent Memory", "comment": null, "summary": "Large language models deployed as autonomous agents face critical memory limitations, lacking selective forgetting mechanisms that lead to either catastrophic forgetting at context boundaries or information overload within them. While human memory naturally balances retention and forgetting through adaptive decay processes, current AI systems employ binary retention strategies that preserve everything or lose it entirely. We propose FadeMem, a biologically-inspired agent memory architecture that incorporates active forgetting mechanisms mirroring human cognitive efficiency. FadeMem implements differential decay rates across a dual-layer memory hierarchy, where retention is governed by adaptive exponential decay functions modulated by semantic relevance, access frequency, and temporal patterns. Through LLM-guided conflict resolution and intelligent memory fusion, our system consolidates related information while allowing irrelevant details to fade. Experiments on Multi-Session Chat, LoCoMo, and LTI-Bench demonstrate superior multi-hop reasoning and retrieval with 45\\% storage reduction, validating the effectiveness of biologically-inspired forgetting in agent memory systems.", "AI": {"tldr": "FadeMem introduces biologically-inspired active forgetting mechanisms for LLM agents, using adaptive decay rates to balance retention and forgetting, reducing storage by 45% while improving reasoning.", "motivation": "Current LLM agents lack selective forgetting mechanisms, leading to either catastrophic forgetting at context boundaries or information overload. Human memory naturally balances retention and forgetting through adaptive decay, but AI systems use binary retention strategies that preserve everything or lose it entirely.", "method": "FadeMem implements a dual-layer memory hierarchy with differential decay rates governed by adaptive exponential decay functions. These functions are modulated by semantic relevance, access frequency, and temporal patterns. The system uses LLM-guided conflict resolution and intelligent memory fusion to consolidate related information while allowing irrelevant details to fade.", "result": "Experiments on Multi-Session Chat, LoCoMo, and LTI-Bench demonstrate superior multi-hop reasoning and retrieval with 45% storage reduction, validating the effectiveness of biologically-inspired forgetting in agent memory systems.", "conclusion": "FadeMem successfully addresses memory limitations in autonomous LLM agents by incorporating biologically-inspired active forgetting mechanisms, achieving better memory efficiency and reasoning performance through adaptive decay processes."}}
{"id": "2601.17536", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17536", "abs": "https://arxiv.org/abs/2601.17536", "authors": ["Jiaming Liang", "Haowei Liu", "Chi-Man Pun"], "title": "OTI: A Model-free and Visually Interpretable Measure of Image Attackability", "comment": null, "summary": "Despite the tremendous success of neural networks, benign images can be corrupted by adversarial perturbations to deceive these models. Intriguingly, images differ in their attackability. Specifically, given an attack configuration, some images are easily corrupted, whereas others are more resistant. Evaluating image attackability has important applications in active learning, adversarial training, and attack enhancement. This prompts a growing interest in developing attackability measures. However, existing methods are scarce and suffer from two major limitations: (1) They rely on a model proxy to provide prior knowledge (e.g., gradients or minimal perturbation) to extract model-dependent image features. Unfortunately, in practice, many task-specific models are not readily accessible. (2) Extracted features characterizing image attackability lack visual interpretability, obscuring their direct relationship with the images. To address these, we propose a novel Object Texture Intensity (OTI), a model-free and visually interpretable measure of image attackability, which measures image attackability as the texture intensity of the image's semantic object. Theoretically, we describe the principles of OTI from the perspectives of decision boundaries as well as the mid- and high-frequency characteristics of adversarial perturbations. Comprehensive experiments demonstrate that OTI is effective and computationally efficient. In addition, our OTI provides the adversarial machine learning community with a visual understanding of attackability.", "AI": {"tldr": "Proposes Object Texture Intensity (OTI) - a model-free, visually interpretable measure of image attackability based on semantic object texture intensity.", "motivation": "Existing attackability measures rely on model proxies (gradients, minimal perturbations) which require access to task-specific models, and lack visual interpretability. Need for model-free, interpretable attackability assessment.", "method": "OTI measures image attackability as the texture intensity of the image's semantic object. Theoretically grounded in decision boundaries and frequency characteristics of adversarial perturbations.", "result": "OTI is effective and computationally efficient. Provides visual understanding of attackability without requiring model access, addressing limitations of existing methods.", "conclusion": "OTI offers a novel model-free, interpretable approach to measure image attackability, with applications in active learning, adversarial training, and attack enhancement."}}
{"id": "2601.18700", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18700", "abs": "https://arxiv.org/abs/2601.18700", "authors": ["Xingyu Sui", "Yanyan Zhao", "Yulin Hu", "Jiahe Guo", "Weixiang Zhao", "Bing Qin"], "title": "TEA-Bench: A Systematic Benchmarking of Tool-enhanced Emotional Support Dialogue Agent", "comment": null, "summary": "Emotional Support Conversation requires not only affective expression but also grounded instrumental support to provide trustworthy guidance. However, existing ESC systems and benchmarks largely focus on affective support in text-only settings, overlooking how external tools can enable factual grounding and reduce hallucination in multi-turn emotional support. We introduce TEA-Bench, the first interactive benchmark for evaluating tool-augmented agents in ESC, featuring realistic emotional scenarios, an MCP-style tool environment, and process-level metrics that jointly assess the quality and factual grounding of emotional support. Experiments on nine LLMs show that tool augmentation generally improves emotional support quality and reduces hallucination, but the gains are strongly capacity-dependent: stronger models use tools more selectively and effectively, while weaker models benefit only marginally. We further release TEA-Dialog, a dataset of tool-enhanced ESC dialogues, and find that supervised fine-tuning improves in-distribution support but generalizes poorly. Our results underscore the importance of tool use in building reliable emotional support agents.", "AI": {"tldr": "TEA-Bench is the first interactive benchmark for evaluating tool-augmented emotional support agents, showing that tool use improves support quality and reduces hallucination, but benefits depend on model capacity.", "motivation": "Existing emotional support systems focus only on affective support in text-only settings, overlooking how external tools can provide factual grounding and reduce hallucination in multi-turn emotional support conversations.", "method": "Introduces TEA-Bench with realistic emotional scenarios, MCP-style tool environment, and process-level metrics for quality and factual grounding. Evaluates nine LLMs and releases TEA-Dialog dataset for supervised fine-tuning experiments.", "result": "Tool augmentation generally improves emotional support quality and reduces hallucination, but gains are capacity-dependent: stronger models use tools more selectively and effectively, while weaker models benefit only marginally. Supervised fine-tuning improves in-distribution support but generalizes poorly.", "conclusion": "Tool use is crucial for building reliable emotional support agents, as it enables factual grounding and reduces hallucination, though effectiveness depends on model capacity and generalization remains challenging."}}
{"id": "2601.17555", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17555", "abs": "https://arxiv.org/abs/2601.17555", "authors": ["Justin Downes", "Sam Saltwick", "Anthony Chen"], "title": "Saliency Driven Imagery Preprocessing for Efficient Compression -- Industrial Paper", "comment": "Proceedings of the 31st ACM International Conference on Advances in Geographic Information Systems (2023)", "summary": "The compression of satellite imagery remains an important research area as hundreds of terabytes of images are collected every day, which drives up storage and bandwidth costs. Although progress has been made in increasing the resolution of these satellite images, many downstream tasks are only interested in small regions of any given image. These areas of interest vary by task but, once known, can be used to optimize how information within the image is encoded. Whereas standard image encoding methods, even those optimized for remote sensing, work on the whole image equally, there are emerging methods that can be guided by saliency maps to focus on important areas. In this work we show how imagery preprocessing techniques driven by saliency maps can be used with traditional lossy compression coding standards to create variable rate image compression within a single large satellite image. Specifically, we use variable sized smoothing kernels that map to different quantized saliency levels to process imagery pixels in order to optimize downstream compression and encoding schemes.", "AI": {"tldr": "Variable rate satellite image compression using saliency maps and smoothing kernels to optimize storage for task-specific regions of interest", "motivation": "Satellite imagery generates massive data volumes (hundreds of TB daily) with high storage/bandwidth costs, but many downstream tasks only need small regions of interest, creating opportunity for optimized compression", "method": "Use saliency maps to identify important regions, apply variable-sized smoothing kernels mapped to quantized saliency levels to preprocess imagery, then apply traditional lossy compression standards", "result": "Enables variable rate compression within single large satellite images by focusing compression quality on important areas while reducing quality in less important regions", "conclusion": "Saliency-driven preprocessing combined with traditional compression standards provides effective variable rate compression for satellite imagery, optimizing storage for task-specific needs"}}
{"id": "2601.18706", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18706", "abs": "https://arxiv.org/abs/2601.18706", "authors": ["Zhichao Yang", "Sepehr Janghorbani", "Dongxu Zhang", "Jun Han", "Qian Qian", "Andrew Ressler", "Gregory D. Lyng", "Sanjit Singh Batra", "Robert E. Tillman"], "title": "Health-SCORE: Towards Scalable Rubrics for Improving Health-LLMs", "comment": null, "summary": "Rubrics are essential for evaluating open-ended LLM responses, especially in safety-critical domains such as healthcare. However, creating high-quality and domain-specific rubrics typically requires significant human expertise time and development cost, making rubric-based evaluation and training difficult to scale. In this work, we introduce Health-SCORE, a generalizable and scalable rubric-based training and evaluation framework that substantially reduces rubric development costs without sacrificing performance. We show that Health-SCORE provides two practical benefits beyond standalone evaluation: it can be used as a structured reward signal to guide reinforcement learning with safety-aware supervision, and it can be incorporated directly into prompts to improve response quality through in-context learning. Across open-ended healthcare tasks, Health-SCORE achieves evaluation quality comparable to human-created rubrics while significantly lowering development effort, making rubric-based evaluation and training more scalable.", "AI": {"tldr": "Health-SCORE is a scalable framework that reduces rubric development costs for LLM evaluation in healthcare while maintaining quality comparable to human-created rubrics.", "motivation": "Creating high-quality, domain-specific rubrics for evaluating LLM responses in healthcare requires significant human expertise and development costs, making rubric-based evaluation and training difficult to scale.", "method": "Health-SCORE is a generalizable and scalable rubric-based training and evaluation framework that reduces rubric development costs while maintaining performance. It provides structured reward signals for reinforcement learning with safety-aware supervision and can be incorporated into prompts for in-context learning.", "result": "Across open-ended healthcare tasks, Health-SCORE achieves evaluation quality comparable to human-created rubrics while significantly lowering development effort.", "conclusion": "Health-SCORE makes rubric-based evaluation and training more scalable for healthcare applications by reducing development costs without sacrificing performance, enabling practical benefits like safety-aware reinforcement learning and improved response quality through in-context learning."}}
{"id": "2601.17566", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17566", "abs": "https://arxiv.org/abs/2601.17566", "authors": ["Qi Li", "Xinchao Wang"], "title": "Sponge Tool Attack: Stealthy Denial-of-Efficiency against Tool-Augmented Agentic Reasoning", "comment": null, "summary": "Enabling large language models (LLMs) to solve complex reasoning tasks is a key step toward artificial general intelligence. Recent work augments LLMs with external tools to enable agentic reasoning, achieving high utility and efficiency in a plug-and-play manner. However, the inherent vulnerabilities of such methods to malicious manipulation of the tool-calling process remain largely unexplored. In this work, we identify a tool-specific attack surface and propose Sponge Tool Attack (STA), which disrupts agentic reasoning solely by rewriting the input prompt under a strict query-only access assumption. Without any modification on the underlying model or the external tools, STA converts originally concise and efficient reasoning trajectories into unnecessarily verbose and convoluted ones before arriving at the final answer. This results in substantial computational overhead while remaining stealthy by preserving the original task semantics and user intent. To achieve this, we design STA as an iterative, multi-agent collaborative framework with explicit rewritten policy control, and generates benign-looking prompt rewrites from the original one with high semantic fidelity. Extensive experiments across 6 models (including both open-source models and closed-source APIs), 12 tools, 4 agentic frameworks, and 13 datasets spanning 5 domains validate the effectiveness of STA.", "AI": {"tldr": "Sponge Tool Attack (STA) is a novel attack method that disrupts LLM agentic reasoning by rewriting input prompts to create verbose, inefficient reasoning paths while preserving task semantics, causing computational overhead without modifying models or tools.", "motivation": "While LLM agents with external tools achieve high utility and efficiency, their vulnerabilities to malicious manipulation of the tool-calling process remain unexplored. The paper aims to identify and exploit these vulnerabilities.", "method": "STA is an iterative, multi-agent collaborative framework that rewrites input prompts under query-only access. It uses explicit rewritten policy control to generate benign-looking prompt rewrites with high semantic fidelity, converting concise reasoning into verbose trajectories.", "result": "Extensive experiments across 6 models (open-source and closed-source), 12 tools, 4 agentic frameworks, and 13 datasets spanning 5 domains validate STA's effectiveness in disrupting agentic reasoning with substantial computational overhead while remaining stealthy.", "conclusion": "The work reveals critical security vulnerabilities in LLM agentic reasoning systems and demonstrates that even query-only attacks can significantly degrade performance, highlighting the need for robust defenses against such prompt manipulation attacks."}}
{"id": "2601.18716", "categories": ["cs.AI", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2601.18716", "abs": "https://arxiv.org/abs/2601.18716", "authors": ["Naeyma N. Islam", "Thomas R. Caulfield"], "title": "Conditioned Generative Modeling of Molecular Glues: A Realistic AI Approach for Synthesizable Drug-like Molecules", "comment": "30 pages, 8 figures", "summary": "Alzheimer's disease (AD) is marked by the pathological accumulation of amyloid beta-42 (Abeta-42), contributing to synaptic dysfunction and neurodegeneration. While extracellular amyloid plaques are well-studied, increasing evidence highlights intracellular Abeta-42 as an early and toxic driver of disease progression. In this study, we present a novel, AI-assisted drug design approach to promote targeted degradation of Abeta-42 via the ubiquitin-proteasome system (UPS), using E3 ligase-directed molecular glues. We systematically evaluated the ternary complex formation potential of Abeta-42 with three E3 ligases: CRBN, VHL, and MDM2, through structure-based modeling, ADMET screening, and docking. We then developed a Ligase-Conditioned Junction Tree Variational Autoencoder (LC-JT-VAE) to generate ligase-specific small molecules, incorporating protein sequence embeddings and torsional angle-aware molecular graphs. Our results demonstrate that this generative model can produce chemically valid, novel, and target-specific molecular glues capable of facilitating Abeta-42 degradation. This integrated approach offers a promising framework for designing UPS-targeted therapies for neurodegenerative diseases.", "AI": {"tldr": "AI-assisted drug design generates E3 ligase-targeted molecular glues for intracellular A\u03b2-42 degradation in Alzheimer's disease.", "motivation": "Intracellular A\u03b2-42 is an early toxic driver in Alzheimer's disease, but current therapies don't effectively target it for degradation via the ubiquitin-proteasome system.", "method": "Systematically evaluated A\u03b2-42 ternary complex formation with CRBN, VHL, and MDM2 E3 ligases using structure-based modeling, ADMET screening, and docking. Developed LC-JT-VAE generative model incorporating protein sequence embeddings and torsional angle-aware molecular graphs to design ligase-specific molecular glues.", "result": "The generative model successfully produced chemically valid, novel, and target-specific molecular glues capable of facilitating A\u03b2-42 degradation through the UPS.", "conclusion": "This integrated AI-assisted approach provides a promising framework for designing UPS-targeted therapies for neurodegenerative diseases by enabling targeted degradation of intracellular A\u03b2-42."}}
{"id": "2601.17586", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2601.17586", "abs": "https://arxiv.org/abs/2601.17586", "authors": ["Sebastian Doerrich", "Francesco Di Salvo", "Jonas Alle", "Christian Ledig"], "title": "Stylizing ViT: Anatomy-Preserving Instance Style Transfer for Domain Generalization", "comment": "Accepted at 23rd IEEE International Symposium on Biomedical Imaging (IEEE ISBI 2026)", "summary": "Deep learning models in medical image analysis often struggle with generalizability across domains and demographic groups due to data heterogeneity and scarcity. Traditional augmentation improves robustness, but fails under substantial domain shifts. Recent advances in stylistic augmentation enhance domain generalization by varying image styles but fall short in terms of style diversity or by introducing artifacts into the generated images. To address these limitations, we propose Stylizing ViT, a novel Vision Transformer encoder that utilizes weight-shared attention blocks for both self- and cross-attention. This design allows the same attention block to maintain anatomical consistency through self-attention while performing style transfer via cross-attention. We assess the effectiveness of our method for domain generalization by employing it for data augmentation on three distinct image classification tasks in the context of histopathology and dermatology. Results demonstrate an improved robustness (up to +13% accuracy) over the state of the art while generating perceptually convincing images without artifacts. Additionally, we show that Stylizing ViT is effective beyond training, achieving a 17% performance improvement during inference when used for test-time augmentation. The source code is available at https://github.com/sdoerrich97/stylizing-vit .", "AI": {"tldr": "Stylizing ViT uses weight-shared attention blocks for both self- and cross-attention to enable artifact-free style transfer for medical image domain generalization, improving accuracy by up to 13%.", "motivation": "Deep learning models in medical imaging struggle with domain generalization due to data heterogeneity and scarcity. Traditional augmentation fails under substantial domain shifts, while existing stylistic augmentation methods lack style diversity or introduce artifacts.", "method": "Proposes Stylizing ViT, a Vision Transformer encoder with weight-shared attention blocks that perform both self-attention (for anatomical consistency) and cross-attention (for style transfer), enabling artifact-free style transfer for data augmentation.", "result": "Achieves up to 13% accuracy improvement over state-of-the-art methods on histopathology and dermatology classification tasks, generates perceptually convincing images without artifacts, and provides 17% performance improvement when used for test-time augmentation.", "conclusion": "Stylizing ViT effectively addresses domain generalization challenges in medical imaging through artifact-free style transfer, demonstrating superior performance for both training augmentation and test-time augmentation applications."}}
{"id": "2601.18735", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18735", "abs": "https://arxiv.org/abs/2601.18735", "authors": ["Jusheng Zhang", "Yijia Fan", "Kaitong Cai", "Jing Yang", "Jiawei Yao", "Jian Wang", "Guanlong Qu", "Ziliang Chen", "Keze Wang"], "title": "Why Keep Your Doubts to Yourself? Trading Visual Uncertainties in Multi-Agent Bandit Systems", "comment": "Accepted to ICLR 2026", "summary": "Vision-Language Models (VLMs) enable powerful multi-agent systems, but scaling them is economically unsustainable: coordinating heterogeneous agents under information asymmetry often spirals costs. Existing paradigms, such as Mixture-of-Agents and knowledge-based routers, rely on heuristic proxies that ignore costs and collapse uncertainty structure, leading to provably suboptimal coordination. We introduce Agora, a framework that reframes coordination as a decentralized market for uncertainty. Agora formalizes epistemic uncertainty into a structured, tradable asset (perceptual, semantic, inferential), and enforces profitability-driven trading among agents based on rational economic rules. A market-aware broker, extending Thompson Sampling, initiates collaboration and guides the system toward cost-efficient equilibria. Experiments on five multimodal benchmarks (MMMU, MMBench, MathVision, InfoVQA, CC-OCR) show that Agora outperforms strong VLMs and heuristic multi-agent strategies, e.g., achieving +8.5% accuracy over the best baseline on MMMU while reducing cost by over 3x. These results establish market-based coordination as a principled and scalable paradigm for building economically viable multi-agent visual intelligence systems.", "AI": {"tldr": "Agora: A market-based framework for cost-efficient coordination of vision-language agents by formalizing uncertainty as tradable assets and using economic principles to optimize collaboration.", "motivation": "Current multi-agent VLM systems are economically unsustainable due to high coordination costs under information asymmetry. Existing approaches use heuristic proxies that ignore costs and collapse uncertainty structure, leading to suboptimal coordination.", "method": "Agora reframes coordination as a decentralized market for uncertainty, formalizing epistemic uncertainty into structured, tradable assets (perceptual, semantic, inferential). It enforces profitability-driven trading among agents based on rational economic rules, with a market-aware broker extending Thompson Sampling to initiate collaboration and guide toward cost-efficient equilibria.", "result": "Experiments on five multimodal benchmarks (MMMU, MMBench, MathVision, InfoVQA, CC-OCR) show Agora outperforms strong VLMs and heuristic multi-agent strategies, achieving +8.5% accuracy over the best baseline on MMMU while reducing cost by over 3x.", "conclusion": "Market-based coordination establishes a principled and scalable paradigm for building economically viable multi-agent visual intelligence systems, addressing the economic sustainability challenge in scaling VLMs."}}
{"id": "2601.17657", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17657", "abs": "https://arxiv.org/abs/2601.17657", "authors": ["Taewan Cho", "Taeryang Kim", "Andrew Jaeyong Choi"], "title": "SPACE-CLIP: Spatial Perception via Adaptive CLIP Embeddings for Monocular Depth Estimation", "comment": null, "summary": "Contrastive Language-Image Pre-training (CLIP) has accomplished extraordinary success for semantic understanding but inherently struggles to perceive geometric structure. Existing methods attempt to bridge this gap by querying CLIP with textual prompts, a process that is often indirect and inefficient. This paper introduces a fundamentally different approach using a dual-pathway decoder. We present SPACE-CLIP, an architecture that unlocks and interprets latent geometric knowledge directly from a frozen CLIP vision encoder, completely bypassing the text encoder and its associated textual prompts. A semantic pathway interprets high-level features, dynamically conditioned on global context using feature-wise linear modulation (FiLM). In addition, a structural pathway extracts fine-grained spatial details from early layers. These complementary streams are hierarchically fused, enabling a robust synthesis of semantic context and precise geometry. Extensive experiments on the KITTI benchmark show that SPACE-CLIP dramatically outperforms previous CLIP-based methods. Our ablation studies validate that the synergistic fusion of our dual pathways is critical to this success. SPACE-CLIP offers a new, efficient, and architecturally elegant blueprint for repurposing large-scale vision models. The proposed method is not just a standalone depth estimator, but a readily integrable spatial perception module for the next generation of embodied AI systems, such as vision-language-action (VLA) models. Our model is available at https://github.com/taewan2002/space-clip", "AI": {"tldr": "SPACE-CLIP unlocks latent geometric knowledge from frozen CLIP vision encoder using dual-pathway decoder, bypassing text prompts for efficient spatial perception.", "motivation": "CLIP excels at semantic understanding but struggles with geometric structure perception. Existing methods use indirect textual prompts, which are inefficient for extracting spatial information.", "method": "Dual-pathway decoder architecture with: 1) Semantic pathway interpreting high-level features using FiLM conditioned on global context, and 2) Structural pathway extracting fine-grained spatial details from early layers. These streams are hierarchically fused.", "result": "Dramatically outperforms previous CLIP-based methods on KITTI benchmark. Ablation studies validate synergistic fusion of dual pathways is critical to success.", "conclusion": "SPACE-CLIP provides efficient, architecturally elegant blueprint for repurposing large-scale vision models as spatial perception modules for next-generation embodied AI systems like VLA models."}}
{"id": "2601.18744", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18744", "abs": "https://arxiv.org/abs/2601.18744", "authors": ["Fangxu Yu", "Xingang Guo", "Lingzhi Yuan", "Haoqiang Kang", "Hongyu Zhao", "Lianhui Qin", "Furong Huang", "Bin Hu", "Tianyi Zhou"], "title": "TSRBench: A Comprehensive Multi-task Multi-modal Time Series Reasoning Benchmark for Generalist Models", "comment": null, "summary": "Time series data is ubiquitous in real-world scenarios and crucial for critical applications ranging from energy management to traffic control. Consequently, the ability to reason over time series is a fundamental skill for generalist models to solve practical problems. However, this dimension is notably absent from existing benchmarks of generalist models. To bridge this gap, we introduce TSRBench, a comprehensive multi-modal benchmark designed to stress-test the full spectrum of time series reasoning capabilities. TSRBench features: i) a diverse set of 4125 problems from 14 domains, and is categorized into 4 major dimensions: Perception, Reasoning, Prediction, and Decision-Making. ii) 15 tasks from the 4 dimensions evaluating essential reasoning capabilities (e.g., numerical reasoning). Through extensive experiments, we evaluated over 30 leading proprietary and open-source LLMs, VLMs, and TSLLMs within TSRBench. Our findings reveal that: i) scaling laws hold for perception and reasoning but break down for prediction; ii) strong reasoning does not guarantee accurate context-aware forecasting, indicating a decoupling between semantic understanding and numerical prediction; and iii) despite the complementary nature of textual and visual represenations of time series as inputs, current multimodal models fail to effectively fuse them for reciprocal performance gains. TSRBench provides a standardized evaluation platform that not only highlights existing challenges but also offers valuable insights to advance generalist models. Our code and dataset are available at https://tsrbench.github.io/.", "AI": {"tldr": "TSRBench is a comprehensive multimodal benchmark with 4125 time series problems across 14 domains, evaluating perception, reasoning, prediction, and decision-making capabilities of generalist models.", "motivation": "Time series reasoning is crucial for real-world applications but absent from existing generalist model benchmarks, creating a gap in evaluating practical problem-solving capabilities.", "method": "Created TSRBench with 4125 problems from 14 domains across 4 dimensions (Perception, Reasoning, Prediction, Decision-Making) and 15 tasks, then evaluated over 30 leading LLMs, VLMs, and TSLLMs.", "result": "Scaling laws hold for perception/reasoning but break for prediction; strong reasoning doesn't guarantee accurate forecasting; multimodal models fail to effectively fuse textual/visual time series representations.", "conclusion": "TSRBench provides standardized evaluation revealing key challenges and insights to advance generalist models' time series reasoning capabilities."}}
{"id": "2601.17666", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17666", "abs": "https://arxiv.org/abs/2601.17666", "authors": ["Xinyue Pan", "Yuhao Chen", "Fengqing Zhu"], "title": "Training-Free Text-to-Image Compositional Food Generation via Prompt Grafting", "comment": "Accepted by CAI2026", "summary": "Real-world meal images often contain multiple food items, making reliable compositional food image generation important for applications such as image-based dietary assessment, where multi-food data augmentation is needed, and recipe visualization. However, modern text-to-image diffusion models struggle to generate accurate multi-food images due to object entanglement, where adjacent foods (e.g., rice and soup) fuse together because many foods do not have clear boundaries. To address this challenge, we introduce Prompt Grafting (PG), a training-free framework that combines explicit spatial cues in text with implicit layout guidance during sampling. PG runs a two-stage process where a layout prompt first establishes distinct regions and the target prompt is grafted once layout formation stabilizes. The framework enables food entanglement control: users can specify which food items should remain separated or be intentionally mixed by editing the arrangement of layouts. Across two food datasets, our method significantly improves the presence of target objects and provides qualitative evidence of controllable separation.", "AI": {"tldr": "Prompt Grafting (PG) is a training-free framework that improves multi-food image generation by combining spatial text cues with layout guidance to prevent food entanglement.", "motivation": "Real-world meal images contain multiple food items, but current text-to-image diffusion models struggle with accurate multi-food generation due to object entanglement where adjacent foods fuse together without clear boundaries.", "method": "Prompt Grafting (PG) uses a two-stage process: first, a layout prompt establishes distinct regions, then the target prompt is grafted once layout formation stabilizes. This combines explicit spatial cues in text with implicit layout guidance during sampling.", "result": "The method significantly improves target object presence across two food datasets and provides qualitative evidence of controllable separation, enabling users to specify which foods should remain separated or be intentionally mixed.", "conclusion": "PG addresses the challenge of food entanglement in multi-food image generation, offering a training-free solution with controllable separation for applications like dietary assessment and recipe visualization."}}
{"id": "2601.17673", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17673", "abs": "https://arxiv.org/abs/2601.17673", "authors": ["Weiyu Zhang", "Yuan Hu", "Yong Li", "Yu Liu"], "title": "Uni-RS: A Spatially Faithful Unified Understanding and Generation Model for Remote Sensing", "comment": null, "summary": "Unified remote sensing multimodal models exhibit a pronounced spatial reversal curse: Although they can accurately recognize and describe object locations in images, they often fail to faithfully execute the same spatial relations during text-to-image generation, where such relations constitute core semantic information in remote sensing. Motivated by this observation, we propose Uni-RS, the first unified multimodal model tailored for remote sensing, to explicitly address the spatial asymmetry between understanding and generation. Specifically, we first introduce explicit Spatial-Layout Planning to transform textual instructions into spatial layout plans, decoupling geometric planning from visual synthesis. We then impose Spatial-Aware Query Supervision to bias learnable queries toward spatial relations explicitly specified in the instruction. Finally, we develop Image-Caption Spatial Layout Variation to expose the model to systematic geometry-consistent spatial transformations. Extensive experiments across multiple benchmarks show that our approach substantially improves spatial faithfulness in text-to-image generation, while maintaining strong performance on multimodal understanding tasks like image captioning, visual grounding, and VQA tasks.", "AI": {"tldr": "Uni-RS addresses spatial reversal curse in remote sensing multimodal models by improving spatial faithfulness in text-to-image generation while maintaining strong multimodal understanding capabilities.", "motivation": "Remote sensing multimodal models suffer from spatial reversal curse - they can recognize object locations in images but fail to faithfully execute spatial relations during text-to-image generation, where spatial relations are core semantic information in remote sensing.", "method": "Three key components: 1) Explicit Spatial-Layout Planning to transform text instructions into spatial layout plans, 2) Spatial-Aware Query Supervision to bias queries toward specified spatial relations, 3) Image-Caption Spatial Layout Variation for geometry-consistent spatial transformations.", "result": "Extensive experiments across multiple benchmarks show substantial improvement in spatial faithfulness for text-to-image generation while maintaining strong performance on multimodal understanding tasks (image captioning, visual grounding, VQA).", "conclusion": "Uni-RS successfully addresses the spatial asymmetry between understanding and generation in remote sensing multimodal models, making it the first unified multimodal model tailored for remote sensing with improved spatial faithfulness."}}
{"id": "2601.17697", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17697", "abs": "https://arxiv.org/abs/2601.17697", "authors": ["Zexi Jia", "Jinchao Zhang", "Jie Zhou"], "title": "StyleDecoupler: Generalizable Artistic Style Disentanglement", "comment": null, "summary": "Representing artistic style is challenging due to its deep entanglement with semantic content. We propose StyleDecoupler, an information-theoretic framework that leverages a key insight: multi-modal vision models encode both style and content, while uni-modal models suppress style to focus on content-invariant features. By using uni-modal representations as content-only references, we isolate pure style features from multi-modal embeddings through mutual information minimization. StyleDecoupler operates as a plug-and-play module on frozen Vision-Language Models without fine-tuning. We also introduce WeART, a large-scale benchmark of 280K artworks across 152 styles and 1,556 artists. Experiments show state-of-the-art performance on style retrieval across WeART and WikiART, while enabling applications like style relationship mapping and generative model evaluation. We release our method and dataset at this url.", "AI": {"tldr": "StyleDecoupler is an information-theoretic framework that separates artistic style from semantic content in vision models by using uni-modal representations as content references and minimizing mutual information, enabling style retrieval and analysis without model fine-tuning.", "motivation": "Artistic style is deeply entangled with semantic content in visual representations, making it challenging to isolate and analyze style independently. Existing approaches struggle to separate these intertwined aspects effectively.", "method": "Uses uni-modal vision models (which suppress style for content-invariant features) as content references to isolate pure style features from multi-modal embeddings through mutual information minimization. Operates as plug-and-play module on frozen Vision-Language Models without fine-tuning.", "result": "Achieves state-of-the-art performance on style retrieval across WeART (new 280K artwork benchmark) and WikiART datasets. Enables applications like style relationship mapping and generative model evaluation.", "conclusion": "StyleDecoupler provides an effective framework for disentangling artistic style from content using information-theoretic principles, with practical applications in art analysis and evaluation while requiring no model fine-tuning."}}
{"id": "2601.17703", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17703", "abs": "https://arxiv.org/abs/2601.17703", "authors": ["Nikhil Kadivar", "Guansheng Li", "Jianlu Zheng", "John M. Higgins", "Ming Dao", "George Em Karniadakis", "Mengjia Xu"], "title": "An AI-enabled tool for quantifying overlapping red blood cell sickling dynamics in microfluidic assays", "comment": null, "summary": "Understanding sickle cell dynamics requires accurate identification of morphological transitions under diverse biophysical conditions, particularly in densely packed and overlapping cell populations. Here, we present an automated deep learning framework that integrates AI-assisted annotation, segmentation, classification, and instance counting to quantify red blood cell (RBC) populations across varying density regimes in time-lapse microscopy data. Experimental images were annotated using the Roboflow platform to generate labeled dataset for training an nnU-Net segmentation model. The trained network enables prediction of the temporal evolution of the sickle cell fraction, while a watershed algorithm resolves overlapping cells to enhance quantification accuracy. Despite requiring only a limited amount of labeled data for training, the framework achieves high segmentation performance, effectively addressing challenges associated with scarce manual annotations and cell overlap. By quantitatively tracking dynamic changes in RBC morphology, this approach can more than double the experimental throughput via densely packed cell suspensions, capture drug-dependent sickling behavior, and reveal distinct mechanobiological signatures of cellular morphological evolution. Overall, this AI-driven framework establishes a scalable and reproducible computational platform for investigating cellular biomechanics and assessing therapeutic efficacy in microphysiological systems.", "AI": {"tldr": "AI-driven framework for automated segmentation, classification, and tracking of sickle cell morphological transitions in dense populations using limited labeled data.", "motivation": "Need for accurate identification of sickle cell morphological transitions in densely packed and overlapping cell populations under diverse biophysical conditions, addressing challenges of scarce manual annotations.", "method": "Deep learning framework integrating AI-assisted annotation (Roboflow), nnU-Net segmentation, watershed algorithm for resolving overlapping cells, and temporal tracking of sickle cell fraction evolution.", "result": "High segmentation performance with limited training data, more than doubled experimental throughput via dense cell suspensions, captured drug-dependent sickling behavior, and revealed distinct mechanobiological signatures of cellular evolution.", "conclusion": "Establishes scalable, reproducible computational platform for investigating cellular biomechanics and assessing therapeutic efficacy in microphysiological systems."}}
{"id": "2601.17720", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17720", "abs": "https://arxiv.org/abs/2601.17720", "authors": ["Ting-Hsun Chi", "Chu-Rong Chen", "Chi-Tun Hsu", "Hsuan-Ting Lin", "Sheng-Yu Huang", "Cheng Sun", "Yu-Chiang Frank Wang"], "title": "Advancing Structured Priors for Sparse-Voxel Surface Reconstruction", "comment": null, "summary": "Reconstructing accurate surfaces with radiance fields has progressed rapidly, yet two promising explicit representations, 3D Gaussian Splatting and sparse-voxel rasterization, exhibit complementary strengths and weaknesses. 3D Gaussian Splatting converges quickly and carries useful geometric priors, but surface fidelity is limited by its point-like parameterization. Sparse-voxel rasterization provides continuous opacity fields and crisp geometry, but its typical uniform dense-grid initialization slows convergence and underutilizes scene structure. We combine the advantages of both by introducing a voxel initialization method that places voxels at plausible locations and with appropriate levels of detail, yielding a strong starting point for per-scene optimization. To further enhance depth consistency without blurring edges, we propose refined depth geometry supervision that converts multi-view cues into direct per-ray depth regularization. Experiments on standard benchmarks demonstrate improvements over prior methods in geometric accuracy, better fine-structure recovery, and more complete surfaces, while maintaining fast convergence.", "AI": {"tldr": "Combines 3D Gaussian Splatting and sparse-voxel rasterization advantages through smart voxel initialization and refined depth supervision for better surface reconstruction.", "motivation": "3D Gaussian Splatting converges quickly but has limited surface fidelity due to point-like parameterization, while sparse-voxel rasterization provides crisp geometry but suffers from slow convergence due to uniform dense-grid initialization. The paper aims to combine their complementary strengths.", "method": "1) Voxel initialization method that places voxels at plausible locations with appropriate detail levels for better starting point. 2) Refined depth geometry supervision that converts multi-view cues into direct per-ray depth regularization without blurring edges.", "result": "Experiments show improvements over prior methods in geometric accuracy, better fine-structure recovery, more complete surfaces, while maintaining fast convergence.", "conclusion": "The proposed approach successfully combines advantages of both 3D Gaussian Splatting and sparse-voxel rasterization, achieving superior surface reconstruction with better geometry and faster convergence."}}
{"id": "2601.17723", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17723", "abs": "https://arxiv.org/abs/2601.17723", "authors": ["Tayyab Nasir", "Daochang Liu", "Ajmal Mian"], "title": "Implicit Neural Representation-Based Continuous Single Image Super Resolution: An Empirical Study", "comment": null, "summary": "Implicit neural representation (INR) has become the standard approach for arbitrary-scale image super-resolution (ASSR). To date, no empirical study has systematically examined the effectiveness of existing methods, nor investigated the effects of different training recipes, such as scaling laws, objective design, and optimization strategies. A rigorous empirical analysis is essential not only for benchmarking performance and revealing true gains but also for establishing the current state of ASSR, identifying saturation limits, and highlighting promising directions. We fill this gap by comparing existing techniques across diverse settings and presenting aggregated performance results on multiple image quality metrics. We contribute a unified framework and code repository to facilitate reproducible comparisons. Furthermore, we investigate the impact of carefully controlled training configurations on perceptual image quality and examine a new loss function that penalizes intensity variations while preserving edges, textures, and finer details during training. We conclude the following key insights that have been previously overlooked: (1) Recent, more complex INR methods provide only marginal improvements over earlier methods. (2) Model performance is strongly correlated to training configurations, a factor overlooked in prior works. (3) The proposed loss enhances texture fidelity across architectures, emphasizing the role of objective design for targeted perceptual gains. (4) Scaling laws apply to INR-based ASSR, confirming predictable gains with increased model complexity and data diversity.", "AI": {"tldr": "Systematic empirical study of implicit neural representation (INR) methods for arbitrary-scale image super-resolution reveals that recent complex methods offer only marginal gains over simpler ones, training configurations significantly impact performance, and proper loss design improves texture fidelity.", "motivation": "No comprehensive empirical study exists for INR-based arbitrary-scale super-resolution (ASSR) methods, making it difficult to benchmark performance, understand true improvements, identify saturation limits, and guide future research directions.", "method": "Conducted systematic comparison of existing INR techniques across diverse settings, aggregated performance on multiple image quality metrics, created unified framework for reproducible comparisons, investigated impact of controlled training configurations, and proposed new loss function that penalizes intensity variations while preserving edges and textures.", "result": "Recent complex INR methods provide only marginal improvements over earlier methods; model performance strongly correlates with training configurations; proposed loss enhances texture fidelity across architectures; scaling laws apply to INR-based ASSR with predictable gains from increased model complexity and data diversity.", "conclusion": "Training configurations and objective design are crucial factors often overlooked in ASSR research, with simpler methods performing competitively against complex ones when properly configured, highlighting the importance of rigorous empirical analysis and reproducible frameworks for advancing the field."}}
{"id": "2601.17733", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2601.17733", "abs": "https://arxiv.org/abs/2601.17733", "authors": ["Junran Lu", "Yuanqi Li", "Hengji Li", "Jie Guo", "Yanwen Guo"], "title": "Flatten The Complex: Joint B-Rep Generation via Compositional $k$-Cell Particles", "comment": null, "summary": "Boundary Representation (B-Rep) is the widely adopted standard\n  in Computer-Aided Design (CAD) and manufacturing. However, generative modeling of B-Reps remains a formidable challenge due to their inherent heterogeneity as geometric cell complexes, which entangles topology with geometry across cells of varying orders (i.e., $k$-cells such as vertices, edges, faces). Previous methods typically rely on cascaded sequences to handle this hierarchy, which fails to fully exploit the geometric relationships between cells, such as adjacency and sharing, limiting context awareness and error recovery. To fill this gap, we introduce a novel paradigm that reformulates B-Reps into sets of compositional $k$-cell particles. Our approach encodes each topological entity as a composition of particles, where adjacent cells share identical latents at their interfaces, thereby promoting geometric coupling along shared boundaries. By decoupling the rigid hierarchy, our representation unifies vertices, edges, and faces, enabling the joint generation of topology and geometry with global context awareness.\n  We synthesize these particle sets using a multi-modal flow matching framework to handle unconditional generation as well as precise conditional tasks, such as 3D reconstruction from single-view or point cloud. Furthermore, the explicit and localized nature of our representation naturally extends to downstream tasks like local in-painting and enables the direct synthesis of non-manifold structures (e.g., wireframes). Extensive experiments demonstrate that our method produces high-fidelity CAD models with superior validity and editability compared to state-of-the-art methods.", "AI": {"tldr": "A novel particle-based representation for B-Reps that decouples rigid hierarchy and enables joint generation of topology and geometry with global context awareness.", "motivation": "Existing methods for generative modeling of B-Reps rely on cascaded sequences that fail to fully exploit geometric relationships between cells (adjacency, sharing), limiting context awareness and error recovery.", "method": "Reformulates B-Reps into sets of compositional k-cell particles where adjacent cells share identical latents at interfaces, enabling geometric coupling along shared boundaries. Uses multi-modal flow matching for unconditional generation and conditional tasks like 3D reconstruction.", "result": "Produces high-fidelity CAD models with superior validity and editability compared to state-of-the-art methods, and naturally extends to downstream tasks like local in-painting and non-manifold structure synthesis.", "conclusion": "The particle-based representation successfully addresses B-Rep heterogeneity by decoupling hierarchy, unifying vertices/edges/faces, and enabling joint topology-geometry generation with global context awareness."}}
{"id": "2601.17737", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17737", "abs": "https://arxiv.org/abs/2601.17737", "authors": ["Chenyu Mu", "Xin He", "Qu Yang", "Wanshun Chen", "Jiadi Yao", "Huang Liu", "Zihao Yi", "Bo Zhao", "Xingyu Chen", "Ruotian Ma", "Fanghua Ye", "Erkun Yang", "Cheng Deng", "Zhaopeng Tu", "Xiaolong Li", "Linus"], "title": "The Script is All You Need: An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation", "comment": null, "summary": "Recent advances in video generation have produced models capable of synthesizing stunning visual content from simple text prompts. However, these models struggle to generate long-form, coherent narratives from high-level concepts like dialogue, revealing a ``semantic gap'' between a creative idea and its cinematic execution. To bridge this gap, we introduce a novel, end-to-end agentic framework for dialogue-to-cinematic-video generation. Central to our framework is ScripterAgent, a model trained to translate coarse dialogue into a fine-grained, executable cinematic script. To enable this, we construct ScriptBench, a new large-scale benchmark with rich multimodal context, annotated via an expert-guided pipeline. The generated script then guides DirectorAgent, which orchestrates state-of-the-art video models using a cross-scene continuous generation strategy to ensure long-horizon coherence. Our comprehensive evaluation, featuring an AI-powered CriticAgent and a new Visual-Script Alignment (VSA) metric, shows our framework significantly improves script faithfulness and temporal fidelity across all tested video models. Furthermore, our analysis uncovers a crucial trade-off in current SOTA models between visual spectacle and strict script adherence, providing valuable insights for the future of automated filmmaking.", "AI": {"tldr": "A new agentic framework bridges the semantic gap between dialogue and cinematic video generation by translating dialogue into executable scripts and orchestrating video models for coherent long-form narratives.", "motivation": "Current video generation models struggle with long-form coherent narratives from high-level concepts like dialogue, revealing a semantic gap between creative ideas and cinematic execution.", "method": "Introduces an end-to-end agentic framework with ScripterAgent (translates dialogue to cinematic scripts), DirectorAgent (orchestrates video models with cross-scene continuous generation), and uses ScriptBench benchmark with expert-guided annotations.", "result": "Framework significantly improves script faithfulness and temporal fidelity across all tested video models, but reveals a trade-off between visual spectacle and strict script adherence in current SOTA models.", "conclusion": "The framework successfully bridges the dialogue-to-video semantic gap and provides valuable insights for automated filmmaking, highlighting the need to balance visual quality with narrative coherence."}}
{"id": "2601.17740", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2601.17740", "abs": "https://arxiv.org/abs/2601.17740", "authors": ["Cong Cao", "Ren Li", "Corentin Dumery", "Hao Li"], "title": "Learning Sewing Patterns via Latent Flow Matching of Implicit Fields", "comment": null, "summary": "Sewing patterns define the structural foundation of garments and are essential for applications such as fashion design, fabrication, and physical simulation. Despite progress in automated pattern generation, accurately modeling sewing patterns remains difficult due to the broad variability in panel geometry and seam arrangements. In this work, we introduce a sewing pattern modeling method based on an implicit representation. We represent each panel using a signed distance field that defines its boundary and an unsigned distance field that identifies seam endpoints, and encode these fields into a continuous latent space that enables differentiable meshing. A latent flow matching model learns distributions over panel combinations in this representation, and a stitching prediction module recovers seam relations from extracted edge segments. This formulation allows accurate modeling and generation of sewing patterns with complex structures. We further show that it can be used to estimate sewing patterns from images with improved accuracy relative to existing approaches, and supports applications such as pattern completion and refitting, providing a practical tool for digital fashion design.", "AI": {"tldr": "Implicit neural representation method for sewing pattern modeling that uses signed/unsigned distance fields and latent flow matching to generate and reconstruct garment patterns with complex structures.", "motivation": "Sewing patterns are fundamental for fashion design and simulation, but accurately modeling them is challenging due to the high variability in panel geometry and seam arrangements. Existing automated pattern generation methods struggle with this complexity.", "method": "Uses implicit representation: signed distance fields for panel boundaries and unsigned distance fields for seam endpoints, encoded into continuous latent space for differentiable meshing. Combines latent flow matching for panel combination distributions and stitching prediction module for seam relations from edge segments.", "result": "Enables accurate modeling and generation of sewing patterns with complex structures. Shows improved accuracy for pattern estimation from images compared to existing approaches, and supports applications like pattern completion and refitting.", "conclusion": "Provides a practical tool for digital fashion design through an implicit representation approach that effectively handles the variability and complexity of sewing patterns, enabling better pattern generation, reconstruction, and manipulation."}}
{"id": "2601.17741", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17741", "abs": "https://arxiv.org/abs/2601.17741", "authors": ["Jun Zhu", "Xinfeng Zhang", "Lv Tang", "Junhao Jiang", "Gai Zhang", "Jia Wang"], "title": "Frequency-aware Neural Representation for Videos", "comment": null, "summary": "Implicit Neural Representations (INRs) have emerged as a promising paradigm for video compression. However, existing INR-based frameworks typically suffer from inherent spectral bias, which favors low-frequency components and leads to over-smoothed reconstructions and suboptimal rate-distortion performance. In this paper, we propose FaNeRV, a Frequency-aware Neural Representation for videos, which explicitly decouples low- and high-frequency components to enable efficient and faithful video reconstruction. FaNeRV introduces a multi-resolution supervision strategy that guides the network to progressively capture global structures and fine-grained textures through staged supervision . To further enhance high-frequency reconstruction, we propose a dynamic high-frequency injection mechanism that adaptively emphasizes challenging regions. In addition, we design a frequency-decomposed network module to improve feature modeling across different spectral bands. Extensive experiments on standard benchmarks demonstrate that FaNeRV significantly outperforms state-of-the-art INR methods and achieves competitive rate-distortion performance against traditional codecs.", "AI": {"tldr": "FaNeRV is a frequency-aware neural representation for video compression that explicitly decouples low- and high-frequency components to overcome spectral bias and achieve better reconstruction quality.", "motivation": "Existing INR-based video compression methods suffer from spectral bias that favors low-frequency components, leading to over-smoothed reconstructions and suboptimal rate-distortion performance.", "method": "Proposes FaNeRV with: 1) multi-resolution supervision for progressive capture of global structures and fine textures, 2) dynamic high-frequency injection to emphasize challenging regions, and 3) frequency-decomposed network module for improved feature modeling across spectral bands.", "result": "Extensive experiments show FaNeRV significantly outperforms state-of-the-art INR methods and achieves competitive rate-distortion performance against traditional codecs.", "conclusion": "FaNeRV effectively addresses spectral bias in INR-based video compression through frequency-aware design, enabling efficient and faithful video reconstruction with competitive performance."}}
{"id": "2601.17743", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17743", "abs": "https://arxiv.org/abs/2601.17743", "authors": ["Jun Zhu", "Xinfeng Zhang", "Lv Tang", "Junhao Jiang", "Gai Zhang", "Jia Wang"], "title": "Video Compression with Hierarchical Temporal Neural Representation", "comment": null, "summary": "Video compression has recently benefited from implicit neural representations (INRs), which model videos as continuous functions. INRs offer compact storage and flexible reconstruction, providing a promising alternative to traditional codecs. However, most existing INR-based methods treat the temporal dimension as an independent input, limiting their ability to capture complex temporal dependencies. To address this, we propose a Hierarchical Temporal Neural Representation for Videos, TeNeRV. TeNeRV integrates short- and long-term dependencies through two key components. First, an Inter-Frame Feature Fusion (IFF) module aggregates features from adjacent frames, enforcing local temporal coherence and capturing fine-grained motion. Second, a GoP-Adaptive Modulation (GAM) mechanism partitions videos into Groups-of-Pictures and learns group-specific priors. The mechanism modulates network parameters, enabling adaptive representations across different GoPs. Extensive experiments demonstrate that TeNeRV consistently outperforms existing INR-based methods in rate-distortion performance, validating the effectiveness of our proposed approach.", "AI": {"tldr": "TeNeRV proposes a hierarchical temporal neural representation for video compression using INRs, addressing temporal dependency limitations through inter-frame feature fusion and GoP-adaptive modulation.", "motivation": "Current INR-based video compression methods treat temporal dimension as independent input, failing to capture complex temporal dependencies needed for efficient video representation.", "method": "TeNeRV uses two key components: 1) Inter-Frame Feature Fusion (IFF) module for local temporal coherence and fine-grained motion capture, and 2) GoP-Adaptive Modulation (GAM) mechanism that partitions videos into Groups-of-Pictures and learns group-specific priors to modulate network parameters.", "result": "Extensive experiments show TeNeRV consistently outperforms existing INR-based methods in rate-distortion performance.", "conclusion": "The hierarchical temporal approach effectively captures both short- and long-term dependencies, validating the effectiveness of TeNeRV for video compression using implicit neural representations."}}
{"id": "2601.17747", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17747", "abs": "https://arxiv.org/abs/2601.17747", "authors": ["Kaixuan Jiang", "Chen Wu", "Zhenghui Zhao", "Chengxi Han"], "title": "Bridging Supervision Gaps: A Unified Framework for Remote Sensing Change Detection", "comment": null, "summary": "Change detection (CD) aims to identify surface changes from multi-temporal remote sensing imagery. In real-world scenarios, Pixel-level change labels are expensive to acquire, and existing models struggle to adapt to scenarios with diverse annotation availability. To tackle this challenge, we propose a unified change detection framework (UniCD), which collaboratively handles supervised, weakly-supervised, and unsupervised tasks through a coupled architecture. UniCD eliminates architectural barriers through a shared encoder and multi-branch collaborative learning mechanism, achieving deep coupling of heterogeneous supervision signals. Specifically, UniCD consists of three supervision-specific branches. In the supervision branch, UniCD introduces the spatial-temporal awareness module (STAM), achieving efficient synergistic fusion of bi-temporal features. In the weakly-supervised branch, we construct change representation regularization (CRR), which steers model convergence from coarse-grained activations toward coherent and separable change modeling. In the unsupervised branch, we propose semantic prior-driven change inference (SPCI), which transforms unsupervised tasks into controlled weakly-supervised path optimization. Experiments on mainstream datasets demonstrate that UniCD achieves optimal performance across three tasks. It exhibits significant accuracy improvements in weakly and unsupervised scenarios, surpassing current state-of-the-art by 12.72% and 12.37% on LEVIR-CD, respectively.", "AI": {"tldr": "UniCD is a unified change detection framework that handles supervised, weakly-supervised, and unsupervised tasks through a shared encoder and multi-branch collaborative learning, achieving state-of-the-art performance across all three scenarios.", "motivation": "Pixel-level change labels are expensive to acquire in remote sensing, and existing models struggle to adapt to scenarios with diverse annotation availability, creating a need for a unified framework that can handle different supervision levels.", "method": "UniCD uses a shared encoder with three supervision-specific branches: 1) supervised branch with spatial-temporal awareness module for bi-temporal feature fusion, 2) weakly-supervised branch with change representation regularization for coherent change modeling, and 3) unsupervised branch with semantic prior-driven change inference that transforms unsupervised tasks into controlled weakly-supervised optimization.", "result": "UniCD achieves optimal performance across all three tasks, with significant accuracy improvements in weakly and unsupervised scenarios - surpassing current state-of-the-art by 12.72% and 12.37% on LEVIR-CD dataset respectively.", "conclusion": "The proposed UniCD framework successfully eliminates architectural barriers between different supervision levels through collaborative learning, providing a unified solution for change detection that adapts to varying annotation availability in real-world scenarios."}}
{"id": "2601.17756", "categories": ["cs.CV", "cs.AI", "cs.GR"], "pdf": "https://arxiv.org/pdf/2601.17756", "abs": "https://arxiv.org/abs/2601.17756", "authors": ["Ziyang Song", "Xinyu Gong", "Bangya Liu", "Zelin Zhao"], "title": "MV-S2V: Multi-View Subject-Consistent Video Generation", "comment": "13 pages, 9 figures", "summary": "Existing Subject-to-Video Generation (S2V) methods have achieved high-fidelity and subject-consistent video generation, yet remain constrained to single-view subject references. This limitation renders the S2V task reducible to an S2I + I2V pipeline, failing to exploit the full potential of video subject control. In this work, we propose and address the challenging Multi-View S2V (MV-S2V) task, which synthesizes videos from multiple reference views to enforce 3D-level subject consistency. Regarding the scarcity of training data, we first develop a synthetic data curation pipeline to generate highly customized synthetic data, complemented by a small-scale real-world captured dataset to boost the training of MV-S2V. Another key issue lies in the potential confusion between cross-subject and cross-view references in conditional generation. To overcome this, we further introduce Temporally Shifted RoPE (TS-RoPE) to distinguish between different subjects and distinct views of the same subject in reference conditioning. Our framework achieves superior 3D subject consistency w.r.t. multi-view reference images and high-quality visual outputs, establishing a new meaningful direction for subject-driven video generation. Our project page is available at <a href=\"https://szy-young.github.io/mv-s2v\">this URL</a>", "AI": {"tldr": "Proposes MV-S2V for multi-view subject-to-video generation using synthetic data and TS-RoPE to achieve 3D subject consistency.", "motivation": "Existing S2V methods are limited to single-view references, reducing the task to S2I+I2V pipeline and failing to exploit full video subject control potential. Multi-view references are needed for true 3D-level subject consistency.", "method": "1) Develop synthetic data curation pipeline for customized training data + small real-world dataset; 2) Introduce Temporally Shifted RoPE (TS-RoPE) to distinguish between cross-subject vs cross-view references in conditional generation.", "result": "Achieves superior 3D subject consistency with multi-view reference images and high-quality visual outputs, establishing new direction for subject-driven video generation.", "conclusion": "MV-S2V addresses limitations of single-view S2V by enabling multi-view subject references for true 3D consistency, using synthetic data and novel TS-RoPE mechanism to handle reference conditioning challenges."}}
{"id": "2601.17791", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17791", "abs": "https://arxiv.org/abs/2601.17791", "authors": ["Rabin Dulal", "Wenfeng Jia", "Lihong Zheng", "Jane Quinn"], "title": "Agreement-Driven Multi-View 3D Reconstruction for Live Cattle Weight Estimation", "comment": null, "summary": "Accurate cattle live weight estimation is vital for livestock management, welfare, and productivity. Traditional methods, such as manual weighing using a walk-over weighing system or proximate measurements using body condition scoring, involve manual handling of stock and can impact productivity from both a stock and economic perspective. To address these issues, this study investigated a cost-effective, non-contact method for live weight calculation in cattle using 3D reconstruction. The proposed pipeline utilized multi-view RGB images with SAM 3D-based agreement-guided fusion, followed by ensemble regression. Our approach generates a single 3D point cloud per animal and compares classical ensemble models with deep learning models under low-data conditions. Results show that SAM 3D with multi-view agreement fusion outperforms other 3D generation methods, while classical ensemble models provide the most consistent performance for practical farm scenarios (R$^2$ = 0.69 $\\pm$ 0.10, MAPE = 2.22 $\\pm$ 0.56 \\%), making this practical for on-farm implementation. These findings demonstrate that improving reconstruction quality is more critical than increasing model complexity for scalable deployment on farms where producing a large volume of 3D data is challenging.", "AI": {"tldr": "A 3D reconstruction pipeline using multi-view RGB images and SAM 3D with agreement-guided fusion achieves accurate cattle weight estimation (R\u00b2=0.69\u00b10.10, MAPE=2.22\u00b10.56%), outperforming traditional manual methods.", "motivation": "Traditional cattle weight estimation methods (manual weighing, body condition scoring) require physical handling that impacts animal welfare and farm productivity. There's a need for cost-effective, non-contact alternatives that minimize stress and operational disruption.", "method": "Multi-view RGB images processed through SAM 3D-based agreement-guided fusion to generate 3D point clouds per animal, followed by ensemble regression models (comparing classical ensembles vs. deep learning) under low-data conditions.", "result": "SAM 3D with multi-view agreement fusion outperforms other 3D generation methods. Classical ensemble models provide most consistent performance for practical farm scenarios (R\u00b2=0.69\u00b10.10, MAPE=2.22\u00b10.56%), making the approach suitable for on-farm implementation.", "conclusion": "For scalable farm deployment where large 3D datasets are challenging, improving reconstruction quality is more critical than increasing model complexity. The proposed non-contact method offers practical, cost-effective cattle weight estimation."}}
{"id": "2601.17818", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17818", "abs": "https://arxiv.org/abs/2601.17818", "authors": ["Wen Luo", "Peng Chen", "Xiaotao Huang", "LiQun Huang"], "title": "ViTCoP: Accelerating Large Vision-Language Models via Visual and Textual Semantic Collaborative Pruning", "comment": null, "summary": "Large Vision-Language Models (LVLMs) incur high computational costs due to significant redundancy in their visual tokens. To effectively reduce this cost, researchers have proposed various visual token pruning methods. However, existing methods are generally limited, either losing critical visual information prematurely due to pruning in the vision encoder, or leading to information redundancy among the selected tokens due to pruning in the Large Language Models (LLMs). To address these challenges, we propose a Visual and Textual Semantic Collaborative Pruning framework (ViTCoP) that combines redundancy filtering in the vision encoder with step-wise co-pruning within the LLM based on its hierarchical characteristics, to efficiently preserve critical and informationally diverse visual tokens. Meanwhile, to ensure compatibility with acceleration techniques like FlashAttention, we introduce the L2 norm of K-vectors as the token saliency metric in the LLM. Extensive experiments on various Large Vision-Language Models demonstrate that ViTCoP not only achieves state-of-the-art performance surpassing existing methods on both image and video understanding tasks, but also significantly reduces model inference latency and GPU memory consumption. Notably, its performance advantage over other methods becomes even more pronounced under extreme pruning rates.", "AI": {"tldr": "ViTCoP is a visual token pruning framework that combines redundancy filtering in vision encoder with step-wise co-pruning in LLM to efficiently preserve critical visual information while reducing computational costs.", "motivation": "Large Vision-Language Models have high computational costs due to significant redundancy in visual tokens. Existing pruning methods either lose critical visual information prematurely (pruning in vision encoder) or create information redundancy among selected tokens (pruning in LLMs).", "method": "Proposes Visual and Textual Semantic Collaborative Pruning (ViTCoP) that combines redundancy filtering in vision encoder with step-wise co-pruning within LLM based on hierarchical characteristics. Uses L2 norm of K-vectors as token saliency metric for compatibility with acceleration techniques like FlashAttention.", "result": "Achieves state-of-the-art performance on both image and video understanding tasks, significantly reduces model inference latency and GPU memory consumption. Performance advantage becomes more pronounced under extreme pruning rates.", "conclusion": "ViTCoP effectively addresses limitations of existing pruning methods by collaboratively pruning visual tokens across vision encoder and LLM, achieving better performance while reducing computational costs."}}
{"id": "2601.17830", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17830", "abs": "https://arxiv.org/abs/2601.17830", "authors": ["Mengmeng Wang", "Dengyang Jiang", "Liuzhuozheng Li", "Yucheng Lin", "Guojiang Shen", "Xiangjie Kong", "Yong Liu", "Guang Dai", "Jingdong Wang"], "title": "VAE-REPA: Variational Autoencoder Representation Alignment for Efficient Diffusion Training", "comment": null, "summary": "Denoising-based diffusion transformers, despite their strong generation performance, suffer from inefficient training convergence. Existing methods addressing this issue, such as REPA (relying on external representation encoders) or SRA (requiring dual-model setups), inevitably incur heavy computational overhead during training due to external dependencies. To tackle these challenges, this paper proposes \\textbf{\\namex}, a lightweight intrinsic guidance framework for efficient diffusion training. \\name leverages off-the-shelf pre-trained Variational Autoencoder (VAE) features: their reconstruction property ensures inherent encoding of visual priors like rich texture details, structural patterns, and basic semantic information. Specifically, \\name aligns the intermediate latent features of diffusion transformers with VAE features via a lightweight projection layer, supervised by a feature alignment loss. This design accelerates training without extra representation encoders or dual-model maintenance, resulting in a simple yet effective pipeline. Extensive experiments demonstrate that \\name improves both generation quality and training convergence speed compared to vanilla diffusion transformers, matches or outperforms state-of-the-art acceleration methods, and incurs merely 4\\% extra GFLOPs with zero additional cost for external guidance models.", "AI": {"tldr": "Lightweight intrinsic guidance framework using pre-trained VAE features to accelerate diffusion transformer training without external dependencies", "motivation": "Existing diffusion transformer acceleration methods (REPA, SRA) suffer from computational overhead due to external dependencies. Need efficient training convergence without heavy computational costs.", "method": "Aligns intermediate latent features of diffusion transformers with pre-trained VAE features via lightweight projection layer supervised by feature alignment loss. Uses VAE's reconstruction property for visual priors encoding.", "result": "Improves generation quality and training convergence speed compared to vanilla diffusion transformers, matches or outperforms state-of-the-art acceleration methods with only 4% extra GFLOPs and zero external guidance model cost.", "conclusion": "Proposed framework provides simple yet effective pipeline for efficient diffusion training by leveraging intrinsic VAE features, eliminating need for external representation encoders or dual-model setups."}}
{"id": "2601.17835", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17835", "abs": "https://arxiv.org/abs/2601.17835", "authors": ["Baowen Zhang", "Chenxing Jiang", "Heng Li", "Shaojie Shen", "Ping Tan"], "title": "Geometry-Grounded Gaussian Splatting", "comment": "16 pages, 15 figures", "summary": "Gaussian Splatting (GS) has demonstrated impressive quality and efficiency in novel view synthesis. However, shape extraction from Gaussian primitives remains an open problem. Due to inadequate geometry parameterization and approximation, existing shape reconstruction methods suffer from poor multi-view consistency and are sensitive to floaters. In this paper, we present a rigorous theoretical derivation that establishes Gaussian primitives as a specific type of stochastic solids. This theoretical framework provides a principled foundation for Geometry-Grounded Gaussian Splatting by enabling the direct treatment of Gaussian primitives as explicit geometric representations. Using the volumetric nature of stochastic solids, our method efficiently renders high-quality depth maps for fine-grained geometry extraction. Experiments show that our method achieves the best shape reconstruction results among all Gaussian Splatting-based methods on public datasets.", "AI": {"tldr": "Geometry-Grounded Gaussian Splatting enables direct shape extraction from Gaussian primitives by treating them as stochastic solids, achieving state-of-the-art reconstruction quality.", "motivation": "Gaussian Splatting shows great quality in novel view synthesis but struggles with shape extraction due to poor geometry parameterization and multi-view consistency issues, making shape reconstruction challenging.", "method": "Establishes Gaussian primitives as stochastic solids through rigorous theoretical derivation, enabling direct treatment as explicit geometric representations. Uses volumetric nature of stochastic solids to efficiently render high-quality depth maps for fine-grained geometry extraction.", "result": "Achieves best shape reconstruction results among all Gaussian Splatting-based methods on public datasets, with improved multi-view consistency and reduced sensitivity to floaters.", "conclusion": "The theoretical framework of treating Gaussian primitives as stochastic solids provides a principled foundation for Geometry-Grounded Gaussian Splatting, enabling high-quality shape extraction while maintaining the efficiency and quality advantages of Gaussian Splatting."}}
{"id": "2601.17857", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17857", "abs": "https://arxiv.org/abs/2601.17857", "authors": ["Lan Yang", "Minghan Yang", "Ke Li", "Honggang Zhang", "Kaiyue Pang", "Yi-Zhe Song"], "title": "SynMind: Reducing Semantic Hallucination in fMRI-Based Image Reconstruction", "comment": null, "summary": "Recent advances in fMRI-based image reconstruction have achieved remarkable photo-realistic fidelity. Yet, a persistent limitation remains: while reconstructed images often appear naturalistic and holistically similar to the target stimuli, they frequently suffer from severe semantic misalignment -- salient objects are often replaced or hallucinated despite high visual quality. In this work, we address this limitation by rethinking the role of explicit semantic interpretation in fMRI decoding. We argue that existing methods rely too heavily on entangled visual embeddings which prioritize low-level appearance cues -- such as texture and global gist -- over explicit semantic identity. To overcome this, we parse fMRI signals into rich, sentence-level semantic descriptions that mirror the hierarchical and compositional nature of human visual understanding. We achieve this by leveraging grounded VLMs to generate synthetic, human-like, multi-granularity textual representations that capture object identities and spatial organization. Built upon this foundation, we propose SynMind, a framework that integrates these explicit semantic encodings with visual priors to condition a pretrained diffusion model. Extensive experiments demonstrate that SynMind outperforms state-of-the-art methods across most quantitative metrics. Notably, by offloading semantic reasoning to our text-alignment module, SynMind surpasses competing methods based on SDXL while using the much smaller Stable Diffusion 1.4 and a single consumer GPU. Large-scale human evaluations further confirm that SynMind produces reconstructions more consistent with human visual perception. Neurovisualization analyses reveal that SynMind engages broader and more semantically relevant brain regions, mitigating the over-reliance on high-level visual areas.", "AI": {"tldr": "SynMind improves fMRI image reconstruction by generating explicit semantic descriptions from brain signals and using them to condition a diffusion model, achieving better semantic accuracy than visual-embedding methods.", "motivation": "Current fMRI-based image reconstruction methods produce natural-looking images but suffer from semantic misalignment - they hallucinate or replace salient objects despite good visual quality. This happens because existing methods rely too much on entangled visual embeddings that prioritize low-level appearance cues over explicit semantic identity.", "method": "Parse fMRI signals into rich, sentence-level semantic descriptions using grounded VLMs to generate synthetic, human-like textual representations capturing object identities and spatial organization. Then integrate these explicit semantic encodings with visual priors to condition a pretrained diffusion model (SynMind framework).", "result": "SynMind outperforms state-of-the-art methods across most quantitative metrics. It surpasses SDXL-based methods while using the smaller Stable Diffusion 1.4 and a single consumer GPU. Human evaluations confirm reconstructions are more consistent with human visual perception. Neurovisualization shows SynMind engages broader, more semantically relevant brain regions.", "conclusion": "By explicitly parsing fMRI signals into semantic descriptions and using them to condition image generation, SynMind addresses the semantic misalignment problem in fMRI reconstruction, producing images that better match human visual understanding while being more computationally efficient."}}
{"id": "2601.17862", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17862", "abs": "https://arxiv.org/abs/2601.17862", "authors": ["Jingsong Xia", "Siqi Wang"], "title": "Domain Generalization with Quantum Enhancement for Medical Image Classification: A Lightweight Approach for Cross-Center Deployment", "comment": null, "summary": "Medical image artificial intelligence models often achieve strong performance in single-center or single-device settings, yet their effectiveness frequently deteriorates in real-world cross-center deployment due to domain shift, limiting clinical generalizability. To address this challenge, we propose a lightweight domain generalization framework with quantum-enhanced collaborative learning, enabling robust generalization to unseen target domains without relying on real multi-center labeled data. Specifically, a MobileNetV2-based domain-invariant encoder is constructed and optimized through three key components: (1) multi-domain imaging shift simulation using brightness, contrast, sharpening, and noise perturbations to emulate heterogeneous acquisition conditions; (2) domain-adversarial training with gradient reversal to suppress domain-discriminative features; and (3) a lightweight quantum feature enhancement layer that applies parameterized quantum circuits for nonlinear feature mapping and entanglement modeling. In addition, a test-time adaptation strategy is employed during inference to further alleviate distribution shifts. Experiments on simulated multi-center medical imaging datasets demonstrate that the proposed method significantly outperforms baseline models without domain generalization or quantum enhancement on unseen domains, achieving reduced domain-specific performance variance and improved AUC and sensitivity. These results highlight the clinical potential of quantum-enhanced domain generalization under constrained computational resources and provide a feasible paradigm for hybrid quantum--classical medical imaging systems.", "AI": {"tldr": "Lightweight quantum-enhanced domain generalization framework for medical AI improves cross-center deployment without needing real multi-center labeled data.", "motivation": "Medical AI models degrade in real-world cross-center deployment due to domain shift, limiting clinical generalizability. Need robust generalization to unseen domains without requiring real multi-center labeled data.", "method": "MobileNetV2-based domain-invariant encoder with three components: (1) multi-domain imaging shift simulation using brightness/contrast/sharpening/noise perturbations, (2) domain-adversarial training with gradient reversal, (3) lightweight quantum feature enhancement layer using parameterized quantum circuits. Plus test-time adaptation during inference.", "result": "Significantly outperforms baselines without domain generalization or quantum enhancement on unseen domains, achieving reduced domain-specific performance variance and improved AUC and sensitivity on simulated multi-center medical imaging datasets.", "conclusion": "Demonstrates clinical potential of quantum-enhanced domain generalization under constrained computational resources, providing feasible paradigm for hybrid quantum-classical medical imaging systems."}}
{"id": "2601.17866", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17866", "abs": "https://arxiv.org/abs/2601.17866", "authors": ["Yoonwoo Jeong", "Cheng Sun", "Yu-Chiang Frank Wang", "Minsu Cho", "Jaesung Choe"], "title": "MV-SAM: Multi-view Promptable Segmentation using Pointmap Guidance", "comment": "Project page, https://jaesung-choe.github.io/mv_sam/index.html", "summary": "Promptable segmentation has emerged as a powerful paradigm in computer vision, enabling users to guide models in parsing complex scenes with prompts such as clicks, boxes, or textual cues. Recent advances, exemplified by the Segment Anything Model (SAM), have extended this paradigm to videos and multi-view images. However, the lack of 3D awareness often leads to inconsistent results, necessitating costly per-scene optimization to enforce 3D consistency. In this work, we introduce MV-SAM, a framework for multi-view segmentation that achieves 3D consistency using pointmaps -- 3D points reconstructed from unposed images by recent visual geometry models. Leveraging the pixel-point one-to-one correspondence of pointmaps, MV-SAM lifts images and prompts into 3D space, eliminating the need for explicit 3D networks or annotated 3D data. Specifically, MV-SAM extends SAM by lifting image embeddings from its pretrained encoder into 3D point embeddings, which are decoded by a transformer using cross-attention with 3D prompt embeddings. This design aligns 2D interactions with 3D geometry, enabling the model to implicitly learn consistent masks across views through 3D positional embeddings. Trained on the SA-1B dataset, our method generalizes well across domains, outperforming SAM2-Video and achieving comparable performance with per-scene optimization baselines on NVOS, SPIn-NeRF, ScanNet++, uCo3D, and DL3DV benchmarks. Code will be released.", "AI": {"tldr": "MV-SAM is a multi-view segmentation framework that achieves 3D consistency using pointmaps from unposed images, eliminating the need for explicit 3D networks or annotated 3D data.", "motivation": "Existing promptable segmentation models like SAM lack 3D awareness, leading to inconsistent results across views and requiring costly per-scene optimization for 3D consistency.", "method": "MV-SAM extends SAM by lifting image embeddings into 3D point embeddings using pointmaps (3D points from unposed images), and uses a transformer with cross-attention between 3D point embeddings and 3D prompt embeddings to learn consistent masks across views.", "result": "Outperforms SAM2-Video and achieves comparable performance with per-scene optimization baselines on multiple benchmarks including NVOS, SPIn-NeRF, ScanNet++, uCo3D, and DL3DV.", "conclusion": "MV-SAM provides an effective framework for 3D-consistent multi-view segmentation without requiring explicit 3D networks or annotated 3D data, demonstrating strong generalization across domains."}}
{"id": "2601.17868", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17868", "abs": "https://arxiv.org/abs/2601.17868", "authors": ["Zhihao He", "Tieyuan Chen", "Kangyu Wang", "Ziran Qin", "Yang Shao", "Chaofan Gan", "Shijie Li", "Zuxuan Wu", "Weiyao Lin"], "title": "VidLaDA: Bidirectional Diffusion Large Language Models for Efficient Video Understanding", "comment": null, "summary": "Standard Autoregressive Video LLMs inevitably suffer from causal masking biases that hinder global spatiotemporal modeling, leading to suboptimal understanding efficiency. We propose VidLaDA, a Video LLM based on Diffusion Language Model utilizing bidirectional attention to capture bidirectional dependencies. To further tackle the inference bottleneck of diffusion decoding on massive video tokens, we introduce MARS-Cache. This framework accelerates inference by combining asynchronous visual cache refreshing with frame-wise chunk attention, effectively pruning redundancy while preserving global connectivity via anchor tokens. Extensive experiments show VidLaDA outperforms diffusion baselines and rivals state-of-the-art autoregressive models (e.g., Qwen2.5-VL and LLaVA-Video), with MARS-Cache delivering over 12x speedup without compromising reasoning accuracy. Code and checkpoints are open-sourced at https://github.com/ziHoHe/VidLaDA.", "AI": {"tldr": "VidLaDA is a video language model using diffusion with bidirectional attention to overcome autoregressive biases, plus MARS-Cache for 12x faster inference via visual caching and chunk attention.", "motivation": "Standard autoregressive video LLMs suffer from causal masking biases that hinder global spatiotemporal modeling, leading to suboptimal understanding efficiency.", "method": "VidLaDA uses diffusion language model with bidirectional attention to capture bidirectional dependencies. MARS-Cache accelerates inference with asynchronous visual cache refreshing and frame-wise chunk attention, pruning redundancy while preserving global connectivity via anchor tokens.", "result": "VidLaDA outperforms diffusion baselines and rivals state-of-the-art autoregressive models (Qwen2.5-VL and LLaVA-Video), with MARS-Cache delivering over 12x speedup without compromising reasoning accuracy.", "conclusion": "The proposed VidLaDA with MARS-Cache effectively addresses autoregressive biases in video understanding while achieving significant inference speedup, making it competitive with top video LLMs."}}
{"id": "2601.17880", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17880", "abs": "https://arxiv.org/abs/2601.17880", "authors": ["Muhammad Umar Salman", "Mohammad Areeb Qazi", "Mohammed Talha Alam"], "title": "Quran-MD: A Fine-Grained Multilingual Multimodal Dataset of the Quran", "comment": "6 pages, 2 tables and 2 figures", "summary": "We present Quran MD, a comprehensive multimodal dataset of the Quran that integrates textual, linguistic, and audio dimensions at the verse and word levels. For each verse (ayah), the dataset provides its original Arabic text, English translation, and phonetic transliteration. To capture the rich oral tradition of Quranic recitation, we include verse-level audio from 32 distinct reciters, reflecting diverse recitation styles and dialectical nuances. At the word level, each token is paired with its corresponding Arabic script, English translation, transliteration, and an aligned audio recording, allowing fine-grained analysis of pronunciation, phonology, and semantic context. This dataset supports various applications, including natural language processing, speech recognition, text-to-speech synthesis, linguistic analysis, and digital Islamic studies. Bridging text and audio modalities across multiple reciters, this dataset provides a unique resource to advance computational approaches to Quranic recitation and study. Beyond enabling tasks such as ASR, tajweed detection, and Quranic TTS, it lays the foundation for multimodal embeddings, semantic retrieval, style transfer, and personalized tutoring systems that can support both research and community applications. The dataset is available at https://huggingface.co/datasets/Buraaq/quran-audio-text-dataset", "AI": {"tldr": "Quran MD is a multimodal dataset combining Quranic text, translations, transliterations, and audio recordings from 32 reciters at both verse and word levels for computational analysis.", "motivation": "To create a comprehensive resource that bridges the gap between textual and audio modalities of the Quran, capturing its rich oral tradition and enabling computational approaches to Quranic study.", "method": "Compiled a multimodal dataset with verse-level data (Arabic text, English translation, phonetic transliteration, audio from 32 reciters) and word-level data (Arabic script, English translation, transliteration, aligned audio recordings).", "result": "Created Quran MD dataset available on Hugging Face, providing fine-grained multimodal data supporting diverse applications from NLP to speech synthesis and linguistic analysis.", "conclusion": "This dataset serves as a unique foundational resource for advancing computational Quranic studies, enabling applications like ASR, tajweed detection, TTS, multimodal embeddings, and personalized tutoring systems."}}
{"id": "2601.17900", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17900", "abs": "https://arxiv.org/abs/2601.17900", "authors": ["Shengjun Zhang", "Min Chen", "Yibo Wei", "Mingyu Dong", "Yueqi Duan"], "title": "Revisiting 3D Reconstruction Kernels as Low-Pass Filters", "comment": "14 pages, 5 figures", "summary": "3D reconstruction is to recover 3D signals from the sampled discrete 2D pixels, with the goal to converge continuous 3D spaces. In this paper, we revisit 3D reconstruction from the perspective of signal processing, identifying the periodic spectral extension induced by discrete sampling as the fundamental challenge. Previous 3D reconstruction kernels, such as Gaussians, Exponential functions, and Student's t distributions, serve as the low pass filters to isolate the baseband spectrum. However, their unideal low-pass property results in the overlap of high-frequency components with low-frequency components in the discrete-time signal's spectrum. To this end, we introduce Jinc kernel with an instantaneous drop to zero magnitude exactly at the cutoff frequency, which is corresponding to the ideal low pass filters. As Jinc kernel suffers from low decay speed in the spatial domain, we further propose modulated kernels to strick an effective balance, and achieves superior rendering performance by reconciling spatial efficiency and frequency-domain fidelity. Experimental results have demonstrated the effectiveness of our Jinc and modulated kernels.", "AI": {"tldr": "The paper introduces Jinc kernels as ideal low-pass filters for 3D reconstruction, addressing spectral overlap issues in previous methods, and proposes modulated kernels to balance spatial efficiency with frequency fidelity.", "motivation": "The fundamental challenge in 3D reconstruction is periodic spectral extension caused by discrete sampling, where previous kernels (Gaussians, Exponential functions, Student's t distributions) have unideal low-pass properties leading to high-frequency components overlapping with low-frequency components in the spectrum.", "method": "Introduce Jinc kernel with instantaneous drop to zero magnitude at cutoff frequency (ideal low-pass filter), then propose modulated kernels to address Jinc's slow decay in spatial domain, achieving balance between spatial efficiency and frequency-domain fidelity.", "result": "Experimental results demonstrate the effectiveness of both Jinc and modulated kernels for 3D reconstruction.", "conclusion": "Jinc kernels provide ideal low-pass filtering for 3D reconstruction, and modulated kernels offer a practical solution that reconciles spatial efficiency with frequency-domain fidelity, leading to superior rendering performance."}}
{"id": "2601.17905", "categories": ["cs.CV", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.17905", "abs": "https://arxiv.org/abs/2601.17905", "authors": ["Jack Foster", "Kirill Paramonov", "Mete Ozay", "Umberto Michieli"], "title": "Feature-Space Generative Models for One-Shot Class-Incremental Learning", "comment": null, "summary": "Few-shot class-incremental learning (FSCIL) is a paradigm where a model, initially trained on a dataset of base classes, must adapt to an expanding problem space by recognizing novel classes with limited data. We focus on the challenging FSCIL setup where a model receives only a single sample (1-shot) for each novel class and no further training or model alterations are allowed after the base training phase. This makes generalization to novel classes particularly difficult. We propose a novel approach predicated on the hypothesis that base and novel class embeddings have structural similarity. We map the original embedding space into a residual space by subtracting the class prototype (i.e., the average class embedding) of input samples. Then, we leverage generative modeling with VAE or diffusion models to learn the multi-modal distribution of residuals over the base classes, and we use this as a valuable structural prior to improve recognition of novel classes. Our approach, Gen1S, consistently improves novel class recognition over the state of the art across multiple benchmarks and backbone architectures.", "AI": {"tldr": "Gen1S improves few-shot class-incremental learning with single samples by mapping embeddings to residual space and using generative models to capture structural priors from base classes.", "motivation": "FSCIL is challenging when models only get one sample per novel class and no further training after base training, making generalization difficult. The paper hypothesizes that base and novel class embeddings share structural similarity.", "method": "Map embedding space to residual space by subtracting class prototypes, then use VAE or diffusion models to learn multi-modal distribution of residuals from base classes as structural prior for novel class recognition.", "result": "Gen1S consistently improves novel class recognition over state-of-the-art across multiple benchmarks and backbone architectures.", "conclusion": "Using generative modeling on residual space provides valuable structural priors that significantly enhance few-shot class-incremental learning with single samples."}}
{"id": "2601.17918", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17918", "abs": "https://arxiv.org/abs/2601.17918", "authors": ["Dain Kim", "Jiwoo Lee", "Jaehoon Yun", "Yong Hoe Koo", "Qingyu Chen", "Hyunjae Kim", "Jaewoo Kang"], "title": "Benchmarking Direct Preference Optimization for Medical Large Vision-Language Models", "comment": "EACL 2026 (Findings)", "summary": "Large Vision-Language Models (LVLMs) hold significant promise for medical applications, yet their deployment is often constrained by insufficient alignment and reliability. While Direct Preference Optimization (DPO) has emerged as a potent framework for refining model responses, its efficacy in high-stakes medical contexts remains underexplored, lacking the rigorous empirical groundwork necessary to guide future methodological advances. To bridge this gap, we present the first comprehensive examination of diverse DPO variants within the medical domain, evaluating nine distinct formulations across two medical LVLMs: LLaVA-Med and HuatuoGPT-Vision. Our results reveal several critical limitations: current DPO approaches often yield inconsistent gains over supervised fine-tuning, with their efficacy varying significantly across different tasks and backbones. Furthermore, they frequently fail to resolve fundamental visual misinterpretation errors. Building on these insights, we present a targeted preference construction strategy as a proof-of-concept that explicitly addresses visual misinterpretation errors frequently observed in existing DPO models. This design yields a 3.6% improvement over the strongest existing DPO baseline on visual question-answering tasks. To support future research, we release our complete framework, including all training data, model checkpoints, and our codebase at https://github.com/dmis-lab/med-vlm-dpo.", "AI": {"tldr": "First comprehensive evaluation of DPO variants for medical LVLMs reveals inconsistent gains, failure to fix visual errors, and proposes targeted preference construction for 3.6% improvement.", "motivation": "LVLMs show promise for medical use but face alignment/reliability issues; DPO's effectiveness in high-stakes medical contexts is underexplored with insufficient empirical foundation.", "method": "Evaluated 9 DPO variants across two medical LVLMs (LLaVA-Med & HuatuoGPT-Vision), then developed targeted preference construction strategy addressing visual misinterpretation errors.", "result": "DPO approaches show inconsistent gains over supervised fine-tuning, vary by task/backbone, and fail to resolve visual errors; targeted strategy achieves 3.6% improvement over best DPO baseline.", "conclusion": "Current DPO methods have critical limitations for medical LVLMs; targeted preference construction addresses visual errors and improves performance, with framework released to support future research."}}
{"id": "2601.17927", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2601.17927", "abs": "https://arxiv.org/abs/2601.17927", "authors": ["Eashan Adhikarla", "Brian D. Davison"], "title": "RemEdit: Efficient Diffusion Editing with Riemannian Geometry", "comment": null, "summary": "Controllable image generation is fundamental to the success of modern generative AI, yet it faces a critical trade-off between semantic fidelity and inference speed. The RemEdit diffusion-based framework addresses this trade-off with two synergistic innovations. First, for editing fidelity, we navigate the latent space as a Riemannian manifold. A mamba-based module efficiently learns the manifold's structure, enabling direct and accurate geodesic path computation for smooth semantic edits. This control is further refined by a dual-SLERP blending technique and a goal-aware prompt enrichment pass from a Vision-Language Model. Second, for additional acceleration, we introduce a novel task-specific attention pruning mechanism. A lightweight pruning head learns to retain tokens essential to the edit, enabling effective optimization without the semantic degradation common in content-agnostic approaches. RemEdit surpasses prior state-of-the-art editing frameworks while maintaining real-time performance under 50% pruning. Consequently, RemEdit establishes a new benchmark for practical and powerful image editing. Source code: https://www.github.com/eashanadhikarla/RemEdit.", "AI": {"tldr": "RemEdit is a diffusion-based image editing framework that achieves superior semantic fidelity and real-time speed through Riemannian manifold navigation with Mamba modules and task-specific attention pruning.", "motivation": "Address the critical trade-off between semantic fidelity and inference speed in controllable image generation, where existing methods struggle to maintain both accuracy and efficiency during editing operations.", "method": "1) Riemannian manifold navigation using Mamba-based modules to learn manifold structure for direct geodesic path computation, 2) Dual-SLERP blending technique, 3) Goal-aware prompt enrichment from Vision-Language Model, 4) Task-specific attention pruning with lightweight pruning head to retain essential edit tokens.", "result": "RemEdit surpasses prior state-of-the-art editing frameworks while maintaining real-time performance under 50% pruning, establishing a new benchmark for practical and powerful image editing.", "conclusion": "RemEdit successfully addresses the fidelity-speed trade-off in controllable image generation through synergistic innovations in manifold navigation and attention optimization, enabling both accurate semantic edits and efficient real-time performance."}}
{"id": "2601.17934", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17934", "abs": "https://arxiv.org/abs/2601.17934", "authors": ["Vi Vu", "Thanh-Huy Nguyen", "Tien-Thinh Nguyen", "Ba-Thinh Lam", "Hoang-Thien Nguyen", "Tianyang Wang", "Xingjian Li", "Min Xu"], "title": "From Specialist to Generalist: Unlocking SAM's Learning Potential on Unlabeled Medical Images", "comment": "Accepted to ISBI 2026", "summary": "Foundation models like the Segment Anything Model (SAM) show strong generalization, yet adapting them to medical images remains difficult due to domain shift, scarce labels, and the inability of Parameter-Efficient Fine-Tuning (PEFT) to exploit unlabeled data. While conventional models like U-Net excel in semi-supervised medical learning, their potential to assist a PEFT SAM has been largely overlooked. We introduce SC-SAM, a specialist-generalist framework where U-Net provides point-based prompts and pseudo-labels to guide SAM's adaptation, while SAM serves as a powerful generalist supervisor to regularize U-Net. This reciprocal guidance forms a bidirectional co-training loop that allows both models to effectively exploit the unlabeled data. Across prostate MRI and polyp segmentation benchmarks, our method achieves state-of-the-art results, outperforming other existing semi-supervised SAM variants and even medical foundation models like MedSAM, highlighting the value of specialist-generalist cooperation for label-efficient medical image segmentation. Our code is available at https://github.com/vnlvi2k3/SC-SAM.", "AI": {"tldr": "SC-SAM is a specialist-generalist framework where U-Net provides prompts and pseudo-labels to guide SAM's adaptation to medical images, while SAM supervises U-Net, creating a bidirectional co-training loop that effectively exploits unlabeled data for label-efficient medical segmentation.", "motivation": "Foundation models like SAM have strong generalization but struggle with medical images due to domain shift, scarce labels, and PEFT's inability to use unlabeled data. Conventional models like U-Net excel in semi-supervised medical learning but their potential to assist PEFT SAM has been overlooked.", "method": "SC-SAM creates a specialist-generalist framework: U-Net (specialist) provides point-based prompts and pseudo-labels to guide SAM's adaptation, while SAM (generalist) serves as a powerful supervisor to regularize U-Net. This reciprocal guidance forms a bidirectional co-training loop that allows both models to effectively exploit unlabeled data.", "result": "Across prostate MRI and polyp segmentation benchmarks, SC-SAM achieves state-of-the-art results, outperforming other existing semi-supervised SAM variants and even medical foundation models like MedSAM.", "conclusion": "The framework demonstrates the value of specialist-generalist cooperation for label-efficient medical image segmentation, showing that combining the strengths of both specialized and generalist models can overcome domain adaptation challenges in medical imaging."}}
{"id": "2601.17939", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17939", "abs": "https://arxiv.org/abs/2601.17939", "authors": ["Chengkun Sun", "Jinqian Pan", "Renjie Liang", "Zhengkang Fan", "Xin Miao", "Jiang Bian", "Jie Xu"], "title": "DTC: A Deformable Transposed Convolution Module for Medical Image Segmentation", "comment": null, "summary": "In medical image segmentation, particularly in UNet-like architectures, upsampling is primarily used to transform smaller feature maps into larger ones, enabling feature fusion between encoder and decoder features and supporting multi-scale prediction. Conventional upsampling methods, such as transposed convolution and linear interpolation, operate on fixed positions: transposed convolution applies kernel elements to predetermined pixel or voxel locations, while linear interpolation assigns values based on fixed coordinates in the original feature map. These fixed-position approaches may fail to capture structural information beyond predefined sampling positions and can lead to artifacts or loss of detail. Inspired by deformable convolutions, we propose a novel upsampling method, Deformable Transposed Convolution (DTC), which learns dynamic coordinates (i.e., sampling positions) to generate high-resolution feature maps for both 2D and 3D medical image segmentation tasks. Experiments on 3D (e.g., BTCV15) and 2D datasets (e.g., ISIC18, BUSI) demonstrate that DTC can be effectively integrated into existing medical image segmentation models, consistently improving the decoder's feature reconstruction and detail recovery capability.", "AI": {"tldr": "Proposes Deformable Transposed Convolution (DTC), a novel upsampling method that learns dynamic sampling positions to improve feature reconstruction and detail recovery in medical image segmentation models.", "motivation": "Conventional upsampling methods (transposed convolution, linear interpolation) use fixed sampling positions, which may fail to capture structural information beyond predefined positions and can lead to artifacts or loss of detail in medical image segmentation.", "method": "Introduces Deformable Transposed Convolution (DTC) that learns dynamic coordinates (sampling positions) to generate high-resolution feature maps, inspired by deformable convolutions. Works for both 2D and 3D medical image segmentation tasks.", "result": "Experiments on 3D (BTCV15) and 2D datasets (ISIC18, BUSI) show DTC can be effectively integrated into existing medical image segmentation models, consistently improving decoder's feature reconstruction and detail recovery capability.", "conclusion": "DTC provides a more flexible upsampling approach that learns adaptive sampling positions, overcoming limitations of fixed-position methods and enhancing medical image segmentation performance."}}
{"id": "2601.18045", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18045", "abs": "https://arxiv.org/abs/2601.18045", "authors": ["Zhuangzhi Gao", "Feixiang Zhou", "He Zhao", "Xiuju Chen", "Xiaoxin Li", "Qinkai Yu", "Yitian Zhao", "Alena Shantsila", "Gregory Y. H. Lip", "Eduard Shantsila", "Yalin Zheng"], "title": "Leveraging Persistence Image to Enhance Robustness and Performance in Curvilinear Structure Segmentation", "comment": "Accepted by IEEE International Symposium on Biomedical Imaging (ISBI) 2026. 5 pages, 3 figures", "summary": "Segmenting curvilinear structures in medical images is essential for analyzing morphological patterns in clinical applications. Integrating topological properties, such as connectivity, improves segmentation accuracy and consistency. However, extracting and embedding such properties - especially from Persistence Diagrams (PD) - is challenging due to their non-differentiability and computational cost. Existing approaches mostly encode topology through handcrafted loss functions, which generalize poorly across tasks. In this paper, we propose PIs-Regressor, a simple yet effective module that learns persistence image (PI) - finite, differentiable representations of topological features - directly from data. Together with Topology SegNet, which fuses these features in both downsampling and upsampling stages, our framework integrates topology into the network architecture itself rather than auxiliary losses. Unlike existing methods that depend heavily on handcrafted loss functions, our approach directly incorporates topological information into the network structure, leading to more robust segmentation. Our design is flexible and can be seamlessly combined with other topology-based methods to further enhance segmentation performance. Experimental results show that integrating topological features enhances model robustness, effectively handling challenges like overexposure and blurring in medical imaging. Our approach on three curvilinear benchmarks demonstrate state-of-the-art performance in both pixel-level accuracy and topological fidelity.", "AI": {"tldr": "PIs-Regressor learns persistence images from data, and Topology SegNet fuses topological features in network architecture for robust curvilinear structure segmentation in medical images.", "motivation": "Segmenting curvilinear structures in medical images requires topological properties like connectivity for accuracy, but extracting these from Persistence Diagrams is challenging due to non-differentiability and computational cost. Existing methods use handcrafted loss functions that generalize poorly.", "method": "Proposes PIs-Regressor module that learns persistence images (differentiable topological representations) directly from data, combined with Topology SegNet that fuses these features in both downsampling and upsampling stages, integrating topology into network architecture rather than auxiliary losses.", "result": "Experimental results show enhanced model robustness against challenges like overexposure and blurring in medical imaging. Achieves state-of-the-art performance on three curvilinear benchmarks in both pixel-level accuracy and topological fidelity.", "conclusion": "The approach effectively integrates topological information directly into network architecture, offering flexible design that can be combined with other topology-based methods to further enhance segmentation performance."}}
{"id": "2601.17947", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17947", "abs": "https://arxiv.org/abs/2601.17947", "authors": ["Bora Yimenicioglu", "Vishal Manikanden"], "title": "FlowMorph: Physics-Consistent Self-Supervision for Label-Free Single-Cell Mechanics in Microfluidic Videos", "comment": null, "summary": "Mechanical properties of red blood cells (RBCs) are promising biomarkers for hematologic and systemic disease, motivating microfluidic assays that probe deformability at throughputs of $10^3$--$10^6$ cells per experiment. However, existing pipelines rely on supervised segmentation or hand-crafted kymographs and rarely encode the laminar Stokes-flow physics that governs RBC shape evolution. We introduce FlowMorph, a physics-consistent self-supervised framework that learns a label-free scalar mechanics proxy $k$ for each tracked RBC from short brightfield microfluidic videos. FlowMorph models each cell by a low-dimensional parametric contour, advances boundary points through a differentiable ''capsule-in-flow'' combining laminar advection and curvature-regularized elastic relaxation, and optimizes a loss coupling silhouette overlap, intra-cellular flow agreement, area conservation, wall constraints, and temporal smoothness, using only automatically derived silhouettes and optical flow.\n  Across four public RBC microfluidic datasets, FlowMorph achieves a mean silhouette IoU of $0.905$ on physics-rich videos with provided velocity fields and markedly improves area conservation and wall violations over purely data-driven baselines. On $\\sim 1.5\\times 10^5$ centered sequences, the scalar $k$ alone separates tank-treading from flipping dynamics with an AUC of $0.863$. Using only $200$ real-time deformability cytometry (RT-DC) events for calibration, a monotone map $E=g(k)$ predicts apparent Young's modulus with a mean absolute error of $0.118$\\,MPa on $600$ held-out cells and degrades gracefully under shifts in channel geometry, optics, and frame rate.", "AI": {"tldr": "FlowMorph: A physics-consistent self-supervised framework that learns a label-free scalar mechanics proxy for red blood cell deformability from microfluidic videos without manual annotations.", "motivation": "Existing RBC deformability assays rely on supervised segmentation or hand-crafted methods that don't encode the underlying Stokes-flow physics, limiting their accuracy and generalizability.", "method": "Models RBCs with low-dimensional parametric contours, advances boundaries through differentiable \"capsule-in-flow\" combining laminar advection and curvature-regularized elastic relaxation, optimizing a multi-term loss using only automatically derived silhouettes and optical flow.", "result": "Achieves mean silhouette IoU of 0.905, separates tank-treading from flipping dynamics with AUC 0.863, predicts apparent Young's modulus with MAE 0.118 MPa, and generalizes well across different experimental conditions.", "conclusion": "FlowMorph provides a physics-aware, self-supervised approach for RBC mechanics analysis that outperforms data-driven baselines and enables accurate deformability measurements without manual annotations."}}
{"id": "2601.18118", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18118", "abs": "https://arxiv.org/abs/2601.18118", "authors": ["Daeyoung Kim"], "title": "LungCRCT: Causal Representation based Lung CT Processing for Lung Cancer Treatment", "comment": null, "summary": "Due to silence in early stages, lung cancer has been one of the most leading causes of mortality in cancer patients world-wide. Moreover, major symptoms of lung cancer are hard to differentiate with other respiratory disease symptoms such as COPD, further leading patients to overlook cancer progression in early stages. Thus, to enhance survival rates in lung cancer, early detection from consistent proactive respiratory system monitoring becomes crucial. One of the most prevalent and effective methods for lung cancer monitoring would be low-dose computed tomography(LDCT) chest scans, which led to remarkable enhancements in lung cancer detection or tumor classification tasks under rapid advancements and applications of computer vision based AI models such as EfficientNet or ResNet in image processing. However, though advanced CNN models under transfer learning or ViT based models led to high performing lung cancer detections, due to its intrinsic limitations in terms of correlation dependence and low interpretability due to complexity, expansions of deep learning models to lung cancer treatment analysis or causal intervention analysis simulations are still limited. Therefore, this research introduced LungCRCT: a latent causal representation learning based lung cancer analysis framework that retrieves causal representations of factors within the physical causal mechanism of lung cancer progression. With the use of advanced graph autoencoder based causal discovery algorithms with distance Correlation disentanglement and entropy-based image reconstruction refinement, LungCRCT not only enables causal intervention analysis for lung cancer treatments, but also leads to robust, yet extremely light downstream models in malignant tumor classification tasks with an AUC score of 93.91%.", "AI": {"tldr": "LungCRCT: A causal representation learning framework for lung cancer analysis that enables causal intervention simulations and achieves 93.91% AUC in tumor classification.", "motivation": "Lung cancer has high mortality due to late detection and symptom overlap with other respiratory diseases. While AI models like CNNs and ViTs have improved detection, they lack interpretability and can't support causal analysis for treatment evaluation.", "method": "Uses latent causal representation learning with graph autoencoder-based causal discovery, distance correlation disentanglement, and entropy-based image reconstruction refinement to extract causal factors from lung cancer progression mechanisms.", "result": "Achieves 93.91% AUC in malignant tumor classification while enabling causal intervention analysis for lung cancer treatments. The framework produces robust yet lightweight downstream models.", "conclusion": "LungCRCT provides an interpretable causal analysis framework that goes beyond traditional deep learning approaches, enabling both accurate classification and treatment simulation capabilities for lung cancer."}}
{"id": "2601.17950", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17950", "abs": "https://arxiv.org/abs/2601.17950", "authors": ["Matthew Walmer", "Saksham Suri", "Anirud Aggarwal", "Abhinav Shrivastava"], "title": "UPLiFT: Efficient Pixel-Dense Feature Upsampling with Local Attenders", "comment": null, "summary": "The space of task-agnostic feature upsampling has emerged as a promising area of research to efficiently create denser features from pre-trained visual backbones. These methods act as a shortcut to achieve dense features for a fraction of the cost by learning to map low-resolution features to high-resolution versions. While early works in this space used iterative upsampling approaches, more recent works have switched to cross-attention-based methods, which risk falling into the same efficiency scaling problems of the backbones they are upsampling. In this work, we demonstrate that iterative upsampling methods can still compete with cross-attention-based methods; moreover, they can achieve state-of-the-art performance with lower inference costs. We propose UPLiFT, an architecture for Universal Pixel-dense Lightweight Feature Transforms. We also propose an efficient Local Attender operator to overcome the limitations of prior iterative feature upsampling methods. This operator uses an alternative attentional pooling formulation defined fully locally. We show that our Local Attender allows UPLiFT to maintain stable features throughout upsampling, enabling state-of-the-art performance with lower inference costs than existing pixel-dense feature upsamplers. In addition, we apply UPLiFT to generative downstream tasks and show that it achieves competitive performance with state-of-the-art Coupled Flow Matching models for VAE feature upsampling. Altogether, UPLiFT offers a versatile and efficient approach to creating denser features.", "AI": {"tldr": "UPLiFT is a lightweight iterative upsampling architecture that achieves state-of-the-art performance with lower inference costs than cross-attention methods, using a novel Local Attender operator for stable feature upsampling.", "motivation": "While cross-attention-based feature upsampling methods have become popular, they risk inheriting the efficiency scaling problems of the backbones they upsampling from. The authors aim to demonstrate that iterative upsampling methods can still compete with and even outperform cross-attention approaches while being more efficient.", "method": "Proposes UPLiFT (Universal Pixel-dense Lightweight Feature Transforms), an iterative upsampling architecture that uses a novel Local Attender operator. This operator employs an alternative attentional pooling formulation defined fully locally to overcome limitations of prior iterative methods and maintain stable features throughout upsampling.", "result": "UPLiFT achieves state-of-the-art performance with lower inference costs than existing pixel-dense feature upsamplers. It also shows competitive performance with state-of-the-art Coupled Flow Matching models for VAE feature upsampling in generative downstream tasks.", "conclusion": "UPLiFT offers a versatile and efficient approach to creating denser features, demonstrating that iterative upsampling methods can outperform cross-attention-based approaches while being more computationally efficient, making them suitable for practical applications."}}
{"id": "2601.18188", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18188", "abs": "https://arxiv.org/abs/2601.18188", "authors": ["Weiye Zhu", "Zekai Zhang", "Xiangchen Wang", "Hewei Pan", "Teng Wang", "Tiantian Geng", "Rongtao Xu", "Feng Zheng"], "title": "\\textsc{NaVIDA}: Vision-Language Navigation with Inverse Dynamics Augmentation", "comment": "18 pages, 14 figures", "summary": "Vision-and-Language Navigation (VLN) requires agents to interpret natural language instructions and act coherently in visually rich environments. However, most existing methods rely on reactive state-action mappings without explicitly modeling how actions causally transform subsequent visual observations. Lacking such vision-action causality, agents cannot anticipate the visual changes induced by its own actions, leading to unstable behaviors, weak generalization, and cumulative error along trajectory. To address these issues, we introduce \\textsc{NaVIDA} (\\textbf{Nav}igation with \\textbf{I}nverse \\textbf{D}ynamics \\textbf{A}ugmentation), a unified VLN framework that couples policy learning with action-grounded visual dynamics and adaptive execution. \\textsc{NaVIDA} augments training with chunk-based inverse-dynamics supervision to learn causal relationship between visual changes and corresponding actions. To structure this supervision and extend the effective planning range, \\textsc{NaVIDA} employs hierarchical probabilistic action chunking (HPAC), which organizes trajectories into multi-step chunks and provides discriminative, longer-range visual-change cues. To further curb error accumulation and stabilize behavior at inference, an entropy-guided mechanism adaptively sets the execution horizon of action chunks. Extensive experiments show that \\textsc{NaVIDA} achieves superior navigation performance compared to state-of-the-art methods with fewer parameters (3B vs. 8B). Real-world robot evaluations further validate the practical feasibility and effectiveness of our approach. Code and data will be available upon acceptance.", "AI": {"tldr": "NaVIDA introduces a VLN framework that learns vision-action causality through inverse dynamics supervision and hierarchical action chunking to improve navigation stability and generalization.", "motivation": "Current VLN methods lack explicit modeling of how actions causally transform visual observations, leading to unstable behaviors, weak generalization, and cumulative trajectory errors.", "method": "NaVIDA couples policy learning with action-grounded visual dynamics using chunk-based inverse-dynamics supervision and hierarchical probabilistic action chunking (HPAC) for longer-range planning, plus entropy-guided adaptive execution.", "result": "Achieves superior navigation performance compared to state-of-the-art methods with fewer parameters (3B vs 8B), validated through extensive experiments and real-world robot evaluations.", "conclusion": "NaVIDA demonstrates that explicitly modeling vision-action causality through inverse dynamics and adaptive chunk execution significantly improves VLN performance and practical feasibility."}}
{"id": "2601.17977", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17977", "abs": "https://arxiv.org/abs/2601.17977", "authors": ["Jinchen Gu", "Nan Zhao", "Lei Qiu", "Lu Zhang"], "title": "Domain-Expert-Guided Hybrid Mixture-of-Experts for Medical AI: Integrating Data-Driven Learning with Clinical Priors", "comment": "4 pages; 3 figures; accepted by International Symposium on Biomedical Imaging (ISBI) 2026", "summary": "Mixture-of-Experts (MoE) models increase representational capacity with modest computational cost, but their effectiveness in specialized domains such as medicine is limited by small datasets. In contrast, clinical practice offers rich expert knowledge, such as physician gaze patterns and diagnostic heuristics, that models cannot reliably learn from limited data. Combining data-driven experts, which capture novel patterns, with domain-expert-guided experts, which encode accumulated clinical insights, provides complementary strengths for robust and clinically meaningful learning. To this end, we propose Domain-Knowledge-Guided Hybrid MoE (DKGH-MoE), a plug-and-play and interpretable module that unifies data-driven learning with domain expertise. DKGH-MoE integrates a data-driven MoE to extract novel features from raw imaging data, and a domain-expert-guided MoE incorporates clinical priors, specifically clinician eye-gaze cues, to emphasize regions of high diagnostic relevance. By integrating domain expert insights with data-driven features, DKGH-MoE improves both performance and interpretability.", "AI": {"tldr": "DKGH-MoE combines data-driven MoE with domain-expert-guided MoE using clinical gaze patterns to improve medical image analysis with limited data.", "motivation": "MoE models struggle in specialized domains like medicine due to small datasets, while clinical practice offers rich expert knowledge (physician gaze patterns, diagnostic heuristics) that models can't reliably learn from limited data alone.", "method": "Proposes Domain-Knowledge-Guided Hybrid MoE (DKGH-MoE) with two components: 1) data-driven MoE to extract novel features from raw imaging data, and 2) domain-expert-guided MoE that incorporates clinical priors (clinician eye-gaze cues) to emphasize diagnostically relevant regions.", "result": "By integrating domain expert insights with data-driven features, DKGH-MoE improves both performance and interpretability in medical image analysis tasks.", "conclusion": "DKGH-MoE provides a plug-and-play, interpretable module that unifies data-driven learning with domain expertise, offering complementary strengths for robust and clinically meaningful learning in medical applications."}}
{"id": "2601.18250", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18250", "abs": "https://arxiv.org/abs/2601.18250", "authors": ["Kang Yu", "Dingyu Wang", "Zimu Yuan", "Nan Zhou", "Jiajun Liu", "Jiaxin Liu", "Shanggui Liu", "Yaoyan Zheng", "Huishu Yuan", "Di Huang", "Dong Jiang"], "title": "A multimodal vision foundation model for generalizable knee pathology", "comment": null, "summary": "Musculoskeletal disorders represent a leading cause of global disability, creating an urgent demand for precise interpretation of medical imaging. Current artificial intelligence (AI) approaches in orthopedics predominantly rely on task-specific, supervised learning paradigms. These methods are inherently fragmented, require extensive annotated datasets, and often lack generalizability across different modalities and clinical scenarios. The development of foundation models in this field has been constrained by the scarcity of large-scale, curated, and open-source musculoskeletal datasets. To address these challenges, we introduce OrthoFoundation, a multimodal vision foundation model optimized for musculoskeletal pathology. We constructed a pre-training dataset of 1.2 million unlabeled knee X-ray and MRI images from internal and public databases. Utilizing a Dinov3 backbone, the model was trained via self-supervised contrastive learning to capture robust radiological representations. OrthoFoundation achieves state-of-the-art (SOTA) performance across 14 downstream tasks. It attained superior accuracy in X-ray osteoarthritis diagnosis and ranked first in MRI structural injury detection. The model demonstrated remarkable label efficiency, matching supervised baselines using only 50% of labeled data. Furthermore, despite being pre-trained on knee images, OrthoFoundation exhibited exceptional cross-anatomy generalization to the hip, shoulder, and ankle. OrthoFoundation represents a significant advancement toward general-purpose AI for musculoskeletal imaging. By learning fundamental, joint-agnostic radiological semantics from large-scale multimodal data, it overcomes the limitations of conventional models, which provides a robust framework for reducing annotation burdens and enhancing diagnostic accuracy in clinical practice.", "AI": {"tldr": "OrthoFoundation is a multimodal vision foundation model for musculoskeletal pathology that achieves SOTA performance across 14 downstream tasks with exceptional cross-anatomy generalization.", "motivation": "Current AI approaches in orthopedics are fragmented, require extensive annotations, lack generalizability, and foundation models are constrained by scarce large-scale musculoskeletal datasets.", "method": "Constructed 1.2M unlabeled knee X-ray/MRI dataset, used Dinov3 backbone with self-supervised contrastive learning to capture robust radiological representations.", "result": "Achieved SOTA across 14 tasks, superior X-ray osteoarthritis diagnosis, top-ranked MRI injury detection, matched supervised baselines with only 50% labeled data, and demonstrated cross-anatomy generalization to hip/shoulder/ankle.", "conclusion": "OrthoFoundation represents a significant advance toward general-purpose AI for musculoskeletal imaging by learning joint-agnostic radiological semantics, reducing annotation burdens, and enhancing diagnostic accuracy."}}
{"id": "2601.18001", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18001", "abs": "https://arxiv.org/abs/2601.18001", "authors": ["Aqsa Yousaf", "Sint Sint Win", "Megan Coffee", "Habeeb Olufowobi"], "title": "MorphXAI: An Explainable Framework for Morphological Analysis of Parasites in Blood Smear Images", "comment": "Accepted at WACV 2026", "summary": "Parasitic infections remain a pressing global health challenge, particularly in low-resource settings where diagnosis still depends on labor-intensive manual inspection of blood smears and the availability of expert domain knowledge. While deep learning models have shown strong performance in automating parasite detection, their clinical usefulness is constrained by limited interpretability. Existing explainability methods are largely restricted to visual heatmaps or attention maps, which highlight regions of interest but fail to capture the morphological traits that clinicians rely on for diagnosis. In this work, we present MorphXAI, an explainable framework that unifies parasite detection with fine-grained morphological analysis. MorphXAI integrates morphological supervision directly into the prediction pipeline, enabling the model to localize parasites while simultaneously characterizing clinically relevant attributes such as shape, curvature, visible dot count, flagellum presence, and developmental stage. To support this task, we curate a clinician-annotated dataset of three parasite species (Leishmania, Trypanosoma brucei, and Trypanosoma cruzi) with detailed morphological labels, establishing a new benchmark for interpretable parasite analysis. Experimental results show that MorphXAI not only improves detection performance over the baseline but also provides structured, biologically meaningful explanations.", "AI": {"tldr": "MorphXAI is an explainable AI framework that combines parasite detection with morphological analysis to provide clinically meaningful explanations beyond just visual heatmaps.", "motivation": "Current deep learning models for parasite detection lack interpretability, relying only on visual heatmaps that don't capture the morphological traits clinicians use for diagnosis, limiting their clinical usefulness.", "method": "MorphXAI integrates morphological supervision directly into the prediction pipeline, enabling simultaneous parasite localization and characterization of clinically relevant attributes like shape, curvature, visible dot count, flagellum presence, and developmental stage.", "result": "The framework improves detection performance over baseline models and provides structured, biologically meaningful explanations, supported by a new clinician-annotated dataset of three parasite species with detailed morphological labels.", "conclusion": "MorphXAI establishes a new benchmark for interpretable parasite analysis by unifying detection with fine-grained morphological analysis, addressing the critical need for clinically useful AI explanations in parasitology."}}
{"id": "2601.18252", "categories": ["cs.CV", "cs.AI", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.18252", "abs": "https://arxiv.org/abs/2601.18252", "authors": ["Chao Wang", "Xuanying Li", "Cheng Dai", "Jinglei Feng", "Yuxiang Luo", "Yuqi Ouyang", "Hao Qin"], "title": "Co-PLNet: A Collaborative Point-Line Network for Prompt-Guided Wireframe Parsing", "comment": null, "summary": "Wireframe parsing aims to recover line segments and their junctions to form a structured geometric representation useful for downstream tasks such as Simultaneous Localization and Mapping (SLAM). Existing methods predict lines and junctions separately and reconcile them post-hoc, causing mismatches and reduced robustness. We present Co-PLNet, a point-line collaborative framework that exchanges spatial cues between the two tasks, where early detections are converted into spatial prompts via a Point-Line Prompt Encoder (PLP-Encoder), which encodes geometric attributes into compact and spatially aligned maps. A Cross-Guidance Line Decoder (CGL-Decoder) then refines predictions with sparse attention conditioned on complementary prompts, enforcing point-line consistency and efficiency. Experiments on Wireframe and YorkUrban show consistent improvements in accuracy and robustness, together with favorable real-time efficiency, demonstrating our effectiveness for structured geometry perception.", "AI": {"tldr": "Co-PLNet is a point-line collaborative framework for wireframe parsing that exchanges spatial cues between line and junction detection tasks to improve accuracy and robustness.", "motivation": "Existing wireframe parsing methods predict lines and junctions separately and reconcile them post-hoc, causing mismatches and reduced robustness, which limits their usefulness for downstream tasks like SLAM.", "method": "Co-PLNet uses a point-line collaborative framework with a Point-Line Prompt Encoder (PLP-Encoder) that converts early detections into spatial prompts, and a Cross-Guidance Line Decoder (CGL-Decoder) that refines predictions with sparse attention conditioned on complementary prompts.", "result": "Experiments on Wireframe and YorkUrban datasets show consistent improvements in accuracy and robustness, together with favorable real-time efficiency.", "conclusion": "The proposed Co-PLNet framework demonstrates effectiveness for structured geometry perception by enforcing point-line consistency and achieving better performance than separate prediction methods."}}
{"id": "2601.18008", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18008", "abs": "https://arxiv.org/abs/2601.18008", "authors": ["Asiegbu Miracle Kanu-Asiegbu", "Nitin Jotwani", "Xiaoxiao Du"], "title": "Strip-Fusion: Spatiotemporal Fusion for Multispectral Pedestrian Detection", "comment": "This work has been accepted for publication in IEEE Robotics and Automation Letters (RA-L). Code available at: https://github.com/akanuasiegbu/stripfusion", "summary": "Pedestrian detection is a critical task in robot perception. Multispectral modalities (visible light and thermal) can boost pedestrian detection performance by providing complementary visual information. Several gaps remain with multispectral pedestrian detection methods. First, existing approaches primarily focus on spatial fusion and often neglect temporal information. Second, RGB and thermal image pairs in multispectral benchmarks may not always be perfectly aligned. Pedestrians are also challenging to detect due to varying lighting conditions, occlusion, etc. This work proposes Strip-Fusion, a spatial-temporal fusion network that is robust to misalignment in input images, as well as varying lighting conditions and heavy occlusions. The Strip-Fusion pipeline integrates temporally adaptive convolutions to dynamically weigh spatial-temporal features, enabling our model to better capture pedestrian motion and context over time. A novel Kullback-Leibler divergence loss was designed to mitigate modality imbalance between visible and thermal inputs, guiding feature alignment toward the more informative modality during training. Furthermore, a novel post-processing algorithm was developed to reduce false positives. Extensive experimental results show that our method performs competitively for both the KAIST and the CVC-14 benchmarks. We also observed significant improvements compared to previous state-of-the-art on challenging conditions such as heavy occlusion and misalignment.", "AI": {"tldr": "Strip-Fusion: A spatial-temporal fusion network for multispectral pedestrian detection that handles misalignment, lighting variations, and occlusion using temporally adaptive convolutions and KL divergence loss.", "motivation": "Existing multispectral pedestrian detection methods focus on spatial fusion but neglect temporal information, and RGB-thermal image pairs may not be perfectly aligned. Pedestrians are challenging to detect due to varying lighting conditions, occlusion, and other factors.", "method": "Proposes Strip-Fusion with temporally adaptive convolutions to dynamically weigh spatial-temporal features, KL divergence loss to mitigate modality imbalance between visible and thermal inputs, and a novel post-processing algorithm to reduce false positives.", "result": "Competitive performance on KAIST and CVC-14 benchmarks, with significant improvements over previous state-of-the-art on challenging conditions like heavy occlusion and misalignment.", "conclusion": "Strip-Fusion effectively addresses limitations in multispectral pedestrian detection by incorporating temporal information, handling modality misalignment, and improving robustness to challenging conditions."}}
{"id": "2601.18451", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.18451", "abs": "https://arxiv.org/abs/2601.18451", "authors": ["Xuanmeng Sha", "Liyun Zhang", "Tomohiro Mashita", "Naoya Chiba", "Yuki Uranishi"], "title": "3DGesPolicy: Phoneme-Aware Holistic Co-Speech Gesture Generation Based on Action Control", "comment": "13 pages, 5 figures", "summary": "Generating holistic co-speech gestures that integrate full-body motion with facial expressions suffers from semantically incoherent coordination on body motion and spatially unstable meaningless movements due to existing part-decomposed or frame-level regression methods, We introduce 3DGesPolicy, a novel action-based framework that reformulates holistic gesture generation as a continuous trajectory control problem through diffusion policy from robotics. By modeling frame-to-frame variations as unified holistic actions, our method effectively learns inter-frame holistic gesture motion patterns and ensures both spatially and semantically coherent movement trajectories that adhere to realistic motion manifolds. To further bridge the gap in expressive alignment, we propose a Gesture-Audio-Phoneme (GAP) fusion module that can deeply integrate and refine multi-modal signals, ensuring structured and fine-grained alignment between speech semantics, body motion, and facial expressions. Extensive quantitative and qualitative experiments on the BEAT2 dataset demonstrate the effectiveness of our 3DGesPolicy across other state-of-the-art methods in generating natural, expressive, and highly speech-aligned holistic gestures.", "AI": {"tldr": "3DGesPolicy: A diffusion policy framework for generating holistic co-speech gestures with coordinated body motion and facial expressions by treating gesture generation as continuous trajectory control.", "motivation": "Existing methods for holistic co-speech gesture generation produce semantically incoherent coordination between body motion and facial expressions, and create spatially unstable movements due to part-decomposed or frame-level regression approaches.", "method": "Reformulates holistic gesture generation as continuous trajectory control using diffusion policy from robotics, modeling frame-to-frame variations as unified holistic actions. Includes Gesture-Audio-Phoneme (GAP) fusion module for deep multi-modal signal integration.", "result": "Extensive experiments on BEAT2 dataset show 3DGesPolicy outperforms state-of-the-art methods in generating natural, expressive, and highly speech-aligned holistic gestures.", "conclusion": "The action-based framework with diffusion policy effectively learns inter-frame holistic gesture patterns, ensuring spatially and semantically coherent movement trajectories that adhere to realistic motion manifolds."}}
{"id": "2601.18464", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18464", "abs": "https://arxiv.org/abs/2601.18464", "authors": ["Wenbin Wei", "Suyuan Yao", "Cheng Huang", "Xiangyu Gao"], "title": "Fair-Eye Net: A Fair, Trustworthy, Multimodal Integrated Glaucoma Full Chain AI System", "comment": null, "summary": "Glaucoma is a top cause of irreversible blindness globally, making early detection and longitudinal follow-up pivotal to preventing permanent vision loss. Current screening and progression assessment, however, rely on single tests or loosely linked examinations, introducing subjectivity and fragmented care. Limited access to high-quality imaging tools and specialist expertise further compromises consistency and equity in real-world use. To address these gaps, we developed Fair-Eye Net, a fair, reliable multimodal AI system closing the clinical loop from glaucoma screening to follow-up and risk alerting. It integrates fundus photos, OCT structural metrics, VF functional indices, and demographic factors via a dual-stream heterogeneous fusion architecture, with an uncertainty-aware hierarchical gating strategy for selective prediction and safe referral. A fairness constraint reduces missed diagnoses in disadvantaged subgroups. Experimental results show it achieved an AUC of 0.912 (96.7% specificity), cut racial false-negativity disparity by 73.4% (12.31% to 3.28%), maintained stable cross-domain performance, and enabled 3-12 months of early risk alerts (92% sensitivity, 88% specificity). Unlike post hoc fairness adjustments, Fair-Eye Net optimizes fairness as a primary goal with clinical reliability via multitask learning, offering a reproducible path for clinical translation and large-scale deployment to advance global eye health equity.", "AI": {"tldr": "Fair-Eye Net is a multimodal AI system for glaucoma screening to follow-up that integrates fundus photos, OCT, VF, and demographics with fairness constraints to reduce diagnostic disparities while maintaining clinical reliability.", "motivation": "Current glaucoma screening and progression assessment rely on single tests or loosely linked examinations, introducing subjectivity and fragmented care. Limited access to imaging tools and specialist expertise compromises consistency and equity in real-world use.", "method": "Developed Fair-Eye Net with dual-stream heterogeneous fusion architecture integrating fundus photos, OCT structural metrics, VF functional indices, and demographic factors. Uses uncertainty-aware hierarchical gating for selective prediction and safe referral, with fairness constraint to reduce missed diagnoses in disadvantaged subgroups.", "result": "Achieved AUC of 0.912 (96.7% specificity), cut racial false-negativity disparity by 73.4% (12.31% to 3.28%), maintained stable cross-domain performance, and enabled 3-12 months early risk alerts (92% sensitivity, 88% specificity).", "conclusion": "Fair-Eye Net optimizes fairness as a primary goal with clinical reliability via multitask learning, offering a reproducible path for clinical translation and large-scale deployment to advance global eye health equity."}}
{"id": "2601.18049", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18049", "abs": "https://arxiv.org/abs/2601.18049", "authors": ["Yunfei Qiu", "Qiqiong Ma", "Tianhua Lv", "Li Fang", "Shudong Zhou", "Wei Yao"], "title": "Semi-Supervised Hyperspectral Image Classification with Edge-Aware Superpixel Label Propagation and Adaptive Pseudo-Labeling", "comment": null, "summary": "Significant progress has been made in semi-supervised hyperspectral image (HSI) classification regarding feature extraction and classification performance. However, due to high annotation costs and limited sample availability, semi-supervised learning still faces challenges such as boundary label diffusion and pseudo-label instability. To address these issues, this paper proposes a novel semi-supervised hyperspectral classification framework integrating spatial prior information with a dynamic learning mechanism. First, we design an Edge-Aware Superpixel Label Propagation (EASLP) module. By integrating edge intensity penalty with neighborhood correction strategy, it mitigates label diffusion from superpixel segmentation while enhancing classification robustness in boundary regions. Second, we introduce a Dynamic History-Fused Prediction (DHP) method. By maintaining historical predictions and dynamically weighting them with current results, DHP smoothens pseudo-label fluctuations and improves temporal consistency and noise resistance. Concurrently, incorporating condifence and consistency measures, the Adaptive Tripartite Sample Categorization (ATSC) strategy implements hierarchical utilization of easy, ambiguous, and hard samples, leading to enhanced pseudo-label quality and learning efficiency. The Dynamic Reliability-Enhanced Pseudo-Label Framework (DREPL), composed of DHP and ATSC, strengthens pseudo-label stability across temporal and sample domains. Through synergizes operation with EASLP, it achieves spatio-temporal consistency optimization. Evaluations on four benchmark datasets demonstrate its capability to maintain superior classification performance.", "AI": {"tldr": "Proposes DREPL framework with EASLP for spatial prior integration and dynamic learning to address boundary label diffusion and pseudo-label instability in semi-supervised HSI classification.", "motivation": "Address challenges in semi-supervised HSI classification: high annotation costs, limited samples, boundary label diffusion, and pseudo-label instability that hinder performance.", "method": "Three components: 1) EASLP module with edge intensity penalty and neighborhood correction for spatial prior; 2) DHP method with historical predictions for temporal consistency; 3) ATSC strategy with hierarchical sample categorization. Combined into DREPL framework.", "result": "Demonstrates superior classification performance on four benchmark datasets, showing improved robustness in boundary regions and enhanced pseudo-label stability.", "conclusion": "The proposed framework effectively addresses spatial and temporal consistency issues in semi-supervised HSI classification, achieving better performance through integrated spatial priors and dynamic learning mechanisms."}}
{"id": "2601.18088", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18088", "abs": "https://arxiv.org/abs/2601.18088", "authors": ["Jianshu Chao", "Tianhua Lv", "Qiqiong Ma", "Yunfei Qiu", "Li Fang", "Huifang Shen", "Wei Yao"], "title": "Cross-Domain Transfer with Self-Supervised Spectral-Spatial Modeling for Hyperspectral Image Classification", "comment": null, "summary": "Self-supervised learning has demonstrated considerable potential in hyperspectral representation, yet its application in cross-domain transfer scenarios remains under-explored. Existing methods, however, still rely on source domain annotations and are susceptible to distribution shifts, leading to degraded generalization performance in the target domain. To address this, this paper proposes a self-supervised cross-domain transfer framework that learns transferable spectral-spatial joint representations without source labels and achieves efficient adaptation under few samples in the target domain. During the self-supervised pre-training phase, a Spatial-Spectral Transformer (S2Former) module is designed. It adopts a dual-branch spatial-spectral transformer and introduces a bidirectional cross-attention mechanism to achieve spectral-spatial collaborative modeling: the spatial branch enhances structural awareness through random masking, while the spectral branch captures fine-grained differences. Both branches mutually guide each other to improve semantic consistency. We further propose a Frequency Domain Constraint (FDC) to maintain frequency-domain consistency through real Fast Fourier Transform (rFFT) and high-frequency magnitude loss, thereby enhancing the model's capability to discern fine details and boundaries. During the fine-tuning phase, we introduce a Diffusion-Aligned Fine-tuning (DAFT) distillation mechanism. This aligns semantic evolution trajectories through a teacher-student structure, enabling robust transfer learning under low-label conditions. Experimental results demonstrate stable classification performance and strong cross-domain adaptability across four hyperspectral datasets, validating the method's effectiveness under resource-constrained conditions.", "AI": {"tldr": "Self-supervised cross-domain transfer framework for hyperspectral data using spectral-spatial transformers and frequency constraints, achieving robust adaptation with few target samples.", "motivation": "Existing self-supervised methods for hyperspectral representation rely on source domain annotations and suffer from distribution shifts, leading to poor generalization in cross-domain transfer scenarios.", "method": "Two-phase approach: 1) Self-supervised pre-training with Spatial-Spectral Transformer (S2Former) using dual-branch architecture with bidirectional cross-attention and Frequency Domain Constraint (FDC) for fine detail preservation; 2) Fine-tuning with Diffusion-Aligned Fine-tuning (DAFT) distillation mechanism for teacher-student semantic alignment under low-label conditions.", "result": "Demonstrates stable classification performance and strong cross-domain adaptability across four hyperspectral datasets, validating effectiveness under resource-constrained conditions.", "conclusion": "Proposed framework successfully learns transferable spectral-spatial representations without source labels and achieves efficient adaptation with few target samples, addressing limitations of existing cross-domain transfer methods for hyperspectral data."}}
{"id": "2601.18098", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18098", "abs": "https://arxiv.org/abs/2601.18098", "authors": ["Chuang Yang", "Haozhao Ma", "Xu Han", "Yuan Yuan", "Qi Wang"], "title": "Text-Pass Filter: An Efficient Scene Text Detector", "comment": null, "summary": "To pursue an efficient text assembling process, existing methods detect texts via the shrink-mask expansion strategy. However, the shrinking operation loses the visual features of text margins and confuses the foreground and background difference, which brings intrinsic limitations to recognize text features. We follow this issue and design Text-Pass Filter (TPF) for arbitrary-shaped text detection. It segments the whole text directly, which avoids the intrinsic limitations. It is noteworthy that different from previous whole text region-based methods, TPF can separate adhesive texts naturally without complex decoding or post-processing processes, which makes it possible for real-time text detection. Concretely, we find that the band-pass filter allows through components in a specified band of frequencies, called its passband but blocks components with frequencies above or below this band. It provides a natural idea for extracting whole texts separately. By simulating the band-pass filter, TPF constructs a unique feature-filter pair for each text. In the inference stage, every filter extracts the corresponding matched text by passing its pass-feature and blocking other features. Meanwhile, considering the large aspect ratio problem of ribbon-like texts makes it hard to recognize texts wholly, a Reinforcement Ensemble Unit (REU) is designed to enhance the feature consistency of the same text and to enlarge the filter's recognition field to help recognize whole texts. Furthermore, a Foreground Prior Unit (FPU) is introduced to encourage TPF to discriminate the difference between the foreground and background, which improves the feature-filter pair quality. Experiments demonstrate the effectiveness of REU and FPU while showing the TPF's superiority.", "AI": {"tldr": "TPF is a novel arbitrary-shaped text detection method that uses feature-filter pairs inspired by band-pass filters to directly segment whole texts, avoiding limitations of shrink-mask approaches while naturally separating adhesive texts without complex post-processing.", "motivation": "Existing shrink-mask expansion methods lose visual features of text margins and confuse foreground/background differences, creating intrinsic limitations for text feature recognition. There's a need for more efficient text detection that can handle arbitrary shapes and adhesive texts without complex post-processing.", "method": "Text-Pass Filter (TPF) simulates band-pass filters by constructing unique feature-filter pairs for each text. Each filter extracts its matched text by passing pass-features and blocking others. Includes Reinforcement Ensemble Unit (REU) to enhance feature consistency for ribbon-like texts and enlarge recognition field, and Foreground Prior Unit (FPU) to improve foreground/background discrimination.", "result": "Experiments demonstrate effectiveness of REU and FPU components while showing TPF's superiority over existing methods. The method enables real-time text detection by naturally separating adhesive texts without complex decoding or post-processing.", "conclusion": "TPF provides an efficient solution for arbitrary-shaped text detection that overcomes limitations of shrink-mask methods, naturally handles adhesive texts, and enables real-time performance through its unique feature-filter pair approach inspired by band-pass filtering principles."}}
{"id": "2601.18099", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18099", "abs": "https://arxiv.org/abs/2601.18099", "authors": ["Akbar Saadat"], "title": "Computational Framework for Estimating Relative Gaussian Blur Kernels between Image Pairs", "comment": "9 pages, 14 input images, 3 TikZ images. arXiv admin note: substantial text overlap with arXiv:2601.04779. substantial text overlap with arXiv:2601.04779. substantial text overlap with arXiv:2601.04779. substantial text overlap with arXiv:2601.04779", "summary": "Following the earlier verification for Gaussian model in \\cite{ASaa2026}, this paper introduces a zero training forward computational framework for the model to realize it in real time applications. The framework is based on discrete calculation of the analytic expression of the defocused image from the sharper one for the application range of the standard deviation of the Gaussian kernels and selecting the best matches. The analytic expression yields multiple solutions at certain image points, but is filtered down to a single solution using similarity measures over neighboring points.The framework is structured to handle cases where two given images are partial blurred versions of each other. Experimental evaluations on real images demonstrate that the proposed framework achieves a mean absolute error (MAE) below $1.7\\%$ in estimating synthetic blur values. Furthermore, the discrepancy between actual blurred image intensities and their corresponding estimates remains under $2\\%$, obtained by applying the extracted defocus filters to less blurred images.", "AI": {"tldr": "Zero-training forward computational framework for real-time defocus blur estimation using analytic expressions and similarity filtering.", "motivation": "To enable real-time applications of Gaussian blur estimation without requiring training data, building on previous verification work.", "method": "Discrete calculation of analytic expression for defocused images from sharper ones, handling multiple solutions through similarity filtering over neighboring points, and supporting partial blur cases.", "result": "Achieves MAE below 1.7% in estimating synthetic blur values, with intensity discrepancy under 2% when applying extracted filters to less blurred images.", "conclusion": "The framework successfully enables real-time defocus blur estimation with high accuracy without requiring training, validated on real images."}}
{"id": "2601.18100", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18100", "abs": "https://arxiv.org/abs/2601.18100", "authors": ["James Tribble", "Hao Wang", "Si-En Hong", "Chaoyi Zhou", "Ashish Bastola", "Siyu Huang", "Abolfazl Razi"], "title": "Spatial-Conditioned Reasoning in Long-Egocentric Videos", "comment": null, "summary": "Long-horizon egocentric video presents significant challenges for visual navigation due to viewpoint drift and the absence of persistent geometric context. Although recent vision-language models perform well on image and short-video reasoning, their spatial reasoning capability in long egocentric sequences remains limited. In this work, we study how explicit spatial signals influence VLM-based video understanding without modifying model architectures or inference procedures. We introduce Sanpo-D, a fine-grained re-annotation of the Google Sanpo dataset, and benchmark multiple VLMs on navigation-oriented spatial queries. To examine input-level inductive bias, we further fuse depth maps with RGB frames and evaluate their impact on spatial reasoning. Our results reveal a trade-off between general-purpose accuracy and spatial specialization, showing that depth-aware and spatially grounded representations can improve performance on safety-critical tasks such as pedestrian and obstruction detection.", "AI": {"tldr": "VLMs struggle with spatial reasoning in long egocentric videos; adding depth maps improves spatial reasoning without model changes, showing trade-off between general accuracy and spatial specialization.", "motivation": "Vision-language models (VLMs) perform well on image and short-video reasoning but have limited spatial reasoning capability in long egocentric sequences due to viewpoint drift and lack of persistent geometric context.", "method": "Introduce Sanpo-D (fine-grained re-annotation of Google Sanpo dataset), benchmark VLMs on navigation-oriented spatial queries, and fuse depth maps with RGB frames to examine input-level inductive bias without modifying model architectures.", "result": "Depth-aware and spatially grounded representations improve performance on safety-critical tasks like pedestrian and obstruction detection, revealing a trade-off between general-purpose accuracy and spatial specialization.", "conclusion": "Explicit spatial signals (like depth maps) can enhance VLM-based video understanding for spatial reasoning tasks without architectural changes, particularly benefiting safety-critical navigation applications."}}
{"id": "2601.18739", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18739", "abs": "https://arxiv.org/abs/2601.18739", "authors": ["Ignacio Antequera-S\u00e1nchez", "Juan Luis Su\u00e1rez-D\u00edaz", "Rosana Montes", "Francisco Herrera"], "title": "SeNeDiF-OOD: Semantic Nested Dichotomy Fusion for Out-of-Distribution Detection Methodology in Open-World Classification. A Case Study on Monument Style Classification", "comment": "28 pages", "summary": "Out-of-distribution (OOD) detection is a fundamental requirement for the reliable deployment of artificial intelligence applications in open-world environments. However, addressing the heterogeneous nature of OOD data, ranging from low-level corruption to semantic shifts, remains a complex challenge that single-stage detectors often fail to resolve. To address this issue, we propose SeNeDiF-OOD, a novel methodology based on Semantic Nested Dichotomy Fusion. This framework decomposes the detection task into a hierarchical structure of binary fusion nodes, where each layer is designed to integrate decision boundaries aligned with specific levels of semantic abstraction. To validate the proposed framework, we present a comprehensive case study using MonuMAI, a real-world architectural style recognition system exposed to an open environment. This application faces a diverse range of inputs, including non-monument images, unknown architectural styles, and adversarial attacks, making it an ideal testbed for our proposal. Through extensive experimental evaluation in this domain, results demonstrate that our hierarchical fusion methodology significantly outperforms traditional baselines, effectively filtering these diverse OOD categories while preserving in-distribution performance.", "AI": {"tldr": "SeNeDiF-OOD is a hierarchical OOD detection framework using Semantic Nested Dichotomy Fusion that outperforms traditional methods by decomposing detection into binary fusion nodes aligned with semantic abstraction levels.", "motivation": "Current OOD detection methods struggle with heterogeneous OOD data (from low-level corruption to semantic shifts) in open-world environments, and single-stage detectors often fail to handle this complexity.", "method": "Proposes SeNeDiF-OOD based on Semantic Nested Dichotomy Fusion, which decomposes detection into hierarchical binary fusion nodes where each layer integrates decision boundaries aligned with specific semantic abstraction levels.", "result": "Extensive evaluation using MonuMAI (architectural style recognition system) shows the framework significantly outperforms traditional baselines, effectively filtering diverse OOD categories while preserving in-distribution performance.", "conclusion": "The hierarchical fusion methodology provides an effective solution for handling heterogeneous OOD data in open-world environments, demonstrated through successful application in a real-world architectural recognition system."}}
{"id": "2601.18135", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18135", "abs": "https://arxiv.org/abs/2601.18135", "authors": ["Jiahao Lyu", "Minghua Zhao", "Xuewen Huang", "Yifei Chen", "Shuangli Du", "Jing Hu", "Cheng Shi", "Zhiyong Lv"], "title": "Forward Consistency Learning with Gated Context Aggregation for Video Anomaly Detection", "comment": "It has been submitted to the KBS journal", "summary": "As a crucial element of public security, video anomaly detection (VAD) aims to measure deviations from normal patterns for various events in real-time surveillance systems. However, most existing VAD methods rely on large-scale models to pursue extreme accuracy, limiting their feasibility on resource-limited edge devices. Moreover, mainstream prediction-based VAD detects anomalies using only single-frame future prediction errors, overlooking the richer constraints from longer-term temporal forward information. In this paper, we introduce FoGA, a lightweight VAD model that performs Forward consistency learning with Gated context Aggregation, containing about 2M parameters and tailored for potential edge devices. Specifically, we propose a Unet-based method that performs feature extraction on consecutive frames to generate both immediate and forward predictions. Then, we introduce a gated context aggregation module into the skip connections to dynamically fuse encoder and decoder features at the same spatial scale. Finally, the model is jointly optimized with a novel forward consistency loss, and a hybrid anomaly measurement strategy is adopted to integrate errors from both immediate and forward frames for more accurate detection. Extensive experiments demonstrate the effectiveness of the proposed method, which substantially outperforms state-of-the-art competing methods, running up to 155 FPS. Hence, our FoGA achieves an excellent trade-off between performance and the efficiency metric.", "AI": {"tldr": "FoGA is a lightweight video anomaly detection model with only 2M parameters that uses forward consistency learning and gated context aggregation for efficient edge deployment, achieving state-of-the-art performance at 155 FPS.", "motivation": "Existing VAD methods rely on large models unsuitable for edge devices, and prediction-based approaches use only single-frame future prediction, missing richer temporal constraints from longer-term forward information.", "method": "U-Net-based feature extraction on consecutive frames for immediate and forward predictions, gated context aggregation in skip connections for dynamic feature fusion, joint optimization with forward consistency loss, and hybrid anomaly measurement integrating errors from both immediate and forward frames.", "result": "Substantially outperforms state-of-the-art methods while running at 155 FPS, achieving excellent performance-efficiency trade-off with only 2M parameters.", "conclusion": "FoGA provides an effective lightweight solution for video anomaly detection on resource-limited edge devices through forward consistency learning and gated context aggregation."}}
{"id": "2601.18157", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18157", "abs": "https://arxiv.org/abs/2601.18157", "authors": ["Aniket Rege", "Arka Sadhu", "Yuliang Li", "Kejie Li", "Ramya Korlakai Vinayak", "Yuning Chai", "Yong Jae Lee", "Hyo Jin Kim"], "title": "Agentic Very Long Video Understanding", "comment": "26 pages, 7 figures, 8 tables", "summary": "The advent of always-on personal AI assistants, enabled by all-day wearable devices such as smart glasses, demands a new level of contextual understanding, one that goes beyond short, isolated events to encompass the continuous, longitudinal stream of egocentric video. Achieving this vision requires advances in long-horizon video understanding, where systems must interpret and recall visual and audio information spanning days or even weeks. Existing methods, including large language models and retrieval-augmented generation, are constrained by limited context windows and lack the ability to perform compositional, multi-hop reasoning over very long video streams. In this work, we address these challenges through EGAgent, an enhanced agentic framework centered on entity scene graphs, which represent people, places, objects, and their relationships over time. Our system equips a planning agent with tools for structured search and reasoning over these graphs, as well as hybrid visual and audio search capabilities, enabling detailed, cross-modal, and temporally coherent reasoning. Experiments on the EgoLifeQA and Video-MME (Long) datasets show that our method achieves state-of-the-art performance on EgoLifeQA (57.5%) and competitive performance on Video-MME (Long) (74.1%) for complex longitudinal video understanding tasks.", "AI": {"tldr": "EGAgent: An agentic framework using entity scene graphs for long-horizon video understanding in always-on AI assistants, achieving SOTA on EgoLifeQA.", "motivation": "Always-on personal AI assistants (smart glasses) need to understand continuous, longitudinal egocentric video spanning days/weeks, but existing methods have limited context windows and lack compositional reasoning over long video streams.", "method": "EGAgent framework uses entity scene graphs (people, places, objects, relationships over time) with planning agents equipped with structured search/reasoning tools and hybrid visual/audio search for cross-modal, temporally coherent reasoning.", "result": "Achieves state-of-the-art 57.5% on EgoLifeQA and competitive 74.1% on Video-MME (Long) for complex longitudinal video understanding tasks.", "conclusion": "EGAgent addresses long-horizon video understanding challenges through entity scene graphs and agentic reasoning, enabling advanced contextual understanding for always-on AI assistants."}}
{"id": "2601.18168", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18168", "abs": "https://arxiv.org/abs/2601.18168", "authors": ["Zehua Liu", "Shihao Zou", "Jincai Huang", "Yanfang Zhang", "Chao Tong", "Weixin Si"], "title": "TempDiffReg: Temporal Diffusion Model for Non-Rigid 2D-3D Vascular Registration", "comment": "Accepted by IEEE BIBM 2025", "summary": "Transarterial chemoembolization (TACE) is a preferred treatment option for hepatocellular carcinoma and other liver malignancies, yet it remains a highly challenging procedure due to complex intra-operative vascular navigation and anatomical variability. Accurate and robust 2D-3D vessel registration is essential to guide microcatheter and instruments during TACE, enabling precise localization of vascular structures and optimal therapeutic targeting. To tackle this issue, we develop a coarse-to-fine registration strategy. First, we introduce a global alignment module, structure-aware perspective n-point (SA-PnP), to establish correspondence between 2D and 3D vessel structures. Second, we propose TempDiffReg, a temporal diffusion model that performs vessel deformation iteratively by leveraging temporal context to capture complex anatomical variations and local structural changes. We collected data from 23 patients and constructed 626 paired multi-frame samples for comprehensive evaluation. Experimental results demonstrate that the proposed method consistently outperforms state-of-the-art (SOTA) methods in both accuracy and anatomical plausibility. Specifically, our method achieves a mean squared error (MSE) of 0.63 mm and a mean absolute error (MAE) of 0.51 mm in registration accuracy, representing 66.7\\% lower MSE and 17.7\\% lower MAE compared to the most competitive existing approaches. It has the potential to assist less-experienced clinicians in safely and efficiently performing complex TACE procedures, ultimately enhancing both surgical outcomes and patient care. Code and data are available at: \\textcolor{blue}{https://github.com/LZH970328/TempDiffReg.git}", "AI": {"tldr": "Coarse-to-fine 2D-3D vessel registration method for TACE procedures using SA-PnP for global alignment and TempDiffReg temporal diffusion model for vessel deformation, achieving state-of-the-art accuracy.", "motivation": "TACE is challenging due to complex vascular navigation and anatomical variability. Accurate 2D-3D vessel registration is essential for guiding microcatheters and instruments during TACE procedures to enable precise localization and optimal therapeutic targeting.", "method": "Two-stage coarse-to-fine approach: 1) SA-PnP (structure-aware perspective n-point) for global alignment establishing 2D-3D vessel correspondence, 2) TempDiffReg temporal diffusion model that iteratively performs vessel deformation using temporal context to capture anatomical variations and local structural changes.", "result": "Achieves MSE of 0.63 mm and MAE of 0.51 mm in registration accuracy, representing 66.7% lower MSE and 17.7% lower MAE compared to most competitive existing approaches. Outperforms SOTA methods in both accuracy and anatomical plausibility on dataset of 23 patients with 626 paired multi-frame samples.", "conclusion": "The method has potential to assist less-experienced clinicians in safely and efficiently performing complex TACE procedures, enhancing surgical outcomes and patient care. Code and data are publicly available."}}
{"id": "2601.18172", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18172", "abs": "https://arxiv.org/abs/2601.18172", "authors": ["Lin Huang", "Yujuan Tan", "Weisheng Li", "Shitai Shan", "Liu Liu", "Bo Liu", "Linlin Shen", "Jing Yu", "Yue Niu"], "title": "YOLO-DS: Fine-Grained Feature Decoupling via Dual-Statistic Synergy Operator for Object Detection", "comment": null, "summary": "One-stage object detection, particularly the YOLO series, strikes a favorable balance between accuracy and efficiency. However, existing YOLO detectors lack explicit modeling of heterogeneous object responses within shared feature channels, which limits further performance gains. To address this, we propose YOLO-DS, a framework built around a novel Dual-Statistic Synergy Operator (DSO). The DSO decouples object features by jointly modeling the channel-wise mean and the peak-to-mean difference. Building upon the DSO, we design two lightweight gating modules: the Dual-Statistic Synergy Gating (DSG) module for adaptive channel-wise feature selection, and the Multi-Path Segmented Gating (MSG) module for depth-wise feature weighting. On the MS-COCO benchmark, YOLO-DS consistently outperforms YOLOv8 across five model scales (N, S, M, L, X), achieving AP gains of 1.1% to 1.7% with only a minimal increase in inference latency. Extensive visualization, ablation, and comparative studies validate the effectiveness of our approach, demonstrating its superior capability in discriminating heterogeneous objects with high efficiency.", "AI": {"tldr": "YOLO-DS improves YOLO detectors by modeling heterogeneous object responses using a Dual-Statistic Synergy Operator and gating modules, achieving 1.1-1.7% AP gains on COCO with minimal latency increase.", "motivation": "Existing YOLO detectors lack explicit modeling of heterogeneous object responses within shared feature channels, limiting further performance improvements despite their good accuracy-efficiency balance.", "method": "Proposes YOLO-DS framework with Dual-Statistic Synergy Operator (DSO) that decouples object features by jointly modeling channel-wise mean and peak-to-mean difference. Uses two lightweight gating modules: DSG for adaptive channel-wise feature selection and MSG for depth-wise feature weighting.", "result": "Outperforms YOLOv8 across all five model scales (N, S, M, L, X) on MS-COCO benchmark with AP gains of 1.1% to 1.7%, with only minimal increase in inference latency.", "conclusion": "YOLO-DS effectively addresses the limitation of modeling heterogeneous object responses in YOLO detectors, achieving superior discrimination of heterogeneous objects with high efficiency, validated by extensive experiments."}}
{"id": "2601.18190", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18190", "abs": "https://arxiv.org/abs/2601.18190", "authors": ["Yifan Li", "Shiying Wang", "Jianqiang Huang"], "title": "Multi-Perspective Subimage CLIP with Keyword Guidance for Remote Sensing Image-Text Retrieval", "comment": "7 pages, 3 figures. Code: https://github.com/Lcrucial1f/MPS-CLIP", "summary": "Vision-Language Pre-training (VLP) models like CLIP have significantly advanced Remote Sensing Image-Text Retrieval (RSITR). However, existing methods predominantly rely on coarse-grained global alignment, which often overlooks the dense, multi-scale semantics inherent in overhead imagery. Moreover, adapting these heavy models via full fine-tuning incurs prohibitive computational costs and risks catastrophic forgetting. To address these challenges, we propose MPS-CLIP, a parameter-efficient framework designed to shift the retrieval paradigm from global matching to keyword-guided fine-grained alignment. Specifically, we leverage a Large Language Model (LLM) to extract core semantic keywords, guiding the Segment Anything Model (SamGeo) to generate semantically relevant sub-perspectives. To efficiently adapt the frozen backbone, we introduce a Gated Global Attention (G^2A) adapter, which captures global context and long-range dependencies with minimal overhead. Furthermore, a Multi-Perspective Representation (MPR) module aggregates these local cues into robust multi-perspective embeddings. The framework is optimized via a hybrid objective combining multi-perspective contrastive and weighted triplet losses, which dynamically selects maximum-response perspectives to suppress noise and enforce precise semantic matching. Extensive experiments on the RSICD and RSITMD benchmarks demonstrate that MPS-CLIP achieves state-of-the-art performance with 35.18% and 48.40% mean Recall (mR), respectively, significantly outperforming full fine-tuning baselines and recent competitive methods. Code is available at https://github.com/Lcrucial1f/MPS-CLIP.", "AI": {"tldr": "MPS-CLIP is a parameter-efficient VLP framework for remote sensing image-text retrieval that shifts from global alignment to keyword-guided fine-grained matching using LLM-extracted keywords and SAM-generated sub-perspectives.", "motivation": "Existing VLP models for RSITR rely on coarse-grained global alignment that overlooks dense, multi-scale semantics in overhead imagery, and full fine-tuning of heavy models is computationally expensive and risks catastrophic forgetting.", "method": "Uses LLM to extract semantic keywords, guides Segment Anything Model (SamGeo) to generate semantically relevant sub-perspectives, introduces Gated Global Attention (G^2A) adapter for efficient backbone adaptation, and aggregates local cues via Multi-Perspective Representation (MPR) module with hybrid contrastive and weighted triplet losses.", "result": "Achieves state-of-the-art performance with 35.18% and 48.40% mean Recall (mR) on RSICD and RSITMD benchmarks, significantly outperforming full fine-tuning baselines and recent competitive methods.", "conclusion": "MPS-CLIP successfully shifts RSITR from global matching to keyword-guided fine-grained alignment while maintaining parameter efficiency, demonstrating superior performance through multi-perspective semantic understanding."}}
{"id": "2601.18192", "categories": ["cs.CV", "cs.HC", "cs.MM"], "pdf": "https://arxiv.org/pdf/2601.18192", "abs": "https://arxiv.org/abs/2601.18192", "authors": ["Tian-Yi Zhou", "Xuan-Hao Liu", "Bao-Liang Lu", "Wei-Long Zheng"], "title": "MindCine: Multimodal EEG-to-Video Reconstruction with Large-Scale Pretrained Models", "comment": null, "summary": "Reconstructing human dynamic visual perception from electroencephalography (EEG) signals is of great research significance since EEG's non-invasiveness and high temporal resolution. However, EEG-to-video reconstruction remains challenging due to: 1) Single Modality: existing studies solely align EEG signals with the text modality, which ignores other modalities and are prone to suffer from overfitting problems; 2) Data Scarcity: current methods often have difficulty training to converge with limited EEG-video data. To solve the above problems, we propose a novel framework MindCine to achieve high-fidelity video reconstructions on limited data. We employ a multimodal joint learning strategy to incorporate beyond-text modalities in the training stage and leverage a pre-trained large EEG model to relieve the data scarcity issue for decoding semantic information, while a Seq2Seq model with causal attention is specifically designed for decoding perceptual information. Extensive experiments demonstrate that our model outperforms state-of-the-art methods both qualitatively and quantitatively. Additionally, the results underscore the effectiveness of the complementary strengths of different modalities and demonstrate that leveraging a large-scale EEG model can further enhance reconstruction performance by alleviating the challenges associated with limited data.", "AI": {"tldr": "MindCine is a novel EEG-to-video reconstruction framework that uses multimodal joint learning and pre-trained large EEG models to overcome data scarcity and achieve high-fidelity video reconstruction from limited EEG data.", "motivation": "EEG-to-video reconstruction is challenging due to two main problems: 1) Existing methods only align EEG with text modality, ignoring other modalities and causing overfitting, and 2) Data scarcity makes training difficult to converge with limited EEG-video data.", "method": "Proposes MindCine framework with: 1) Multimodal joint learning strategy incorporating beyond-text modalities, 2) Leveraging pre-trained large EEG model to address data scarcity for semantic decoding, and 3) Seq2Seq model with causal attention specifically designed for perceptual information decoding.", "result": "Extensive experiments show the model outperforms state-of-the-art methods both qualitatively and quantitatively. Results demonstrate effectiveness of complementary strengths of different modalities and that large-scale EEG models enhance reconstruction by alleviating limited data challenges.", "conclusion": "MindCine successfully addresses key challenges in EEG-to-video reconstruction by combining multimodal learning with pre-trained EEG models, achieving superior performance on limited data and highlighting the importance of leveraging multiple modalities and large-scale pre-training."}}
{"id": "2601.18195", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18195", "abs": "https://arxiv.org/abs/2601.18195", "authors": ["Linhan Cao", "Wei Sun", "Weixia Zhang", "Xiangyang Zhu", "Kaiwei Zhang", "Jun Jia", "Dandan Zhu", "Guangtao Zhai", "Xiongkuo Min"], "title": "QualiRAG: Retrieval-Augmented Generation for Visual Quality Understanding", "comment": null, "summary": "Visual quality assessment (VQA) is increasingly shifting from scalar score prediction toward interpretable quality understanding -- a paradigm that demands \\textit{fine-grained spatiotemporal perception} and \\textit{auxiliary contextual information}. Current approaches rely on supervised fine-tuning or reinforcement learning on curated instruction datasets, which involve labor-intensive annotation and are prone to dataset-specific biases. To address these challenges, we propose \\textbf{QualiRAG}, a \\textit{training-free} \\textbf{R}etrieval-\\textbf{A}ugmented \\textbf{G}eneration \\textbf{(RAG)} framework that systematically leverages the latent perceptual knowledge of large multimodal models (LMMs) for visual quality perception. Unlike conventional RAG that retrieves from static corpora, QualiRAG dynamically generates auxiliary knowledge by decomposing questions into structured requests and constructing four complementary knowledge sources: \\textit{visual metadata}, \\textit{subject localization}, \\textit{global quality summaries}, and \\textit{local quality descriptions}, followed by relevance-aware retrieval for evidence-grounded reasoning. Extensive experiments show that QualiRAG achieves substantial improvements over open-source general-purpose LMMs and VQA-finetuned LMMs on visual quality understanding tasks, and delivers competitive performance on visual quality comparison tasks, demonstrating robust quality assessment capabilities without any task-specific training. The code will be publicly available at https://github.com/clh124/QualiRAG.", "AI": {"tldr": "QualiRAG is a training-free RAG framework that leverages large multimodal models' latent knowledge for visual quality assessment without fine-tuning, using dynamic knowledge generation from four complementary sources.", "motivation": "Current VQA approaches require supervised fine-tuning or RL on curated datasets, which are labor-intensive and prone to biases. There's a need for training-free methods that can perform fine-grained spatiotemporal perception and leverage auxiliary contextual information.", "method": "QualiRAG is a retrieval-augmented generation framework that dynamically generates auxiliary knowledge by decomposing questions into structured requests and constructing four knowledge sources: visual metadata, subject localization, global quality summaries, and local quality descriptions, followed by relevance-aware retrieval for evidence-grounded reasoning.", "result": "QualiRAG achieves substantial improvements over open-source general-purpose LMMs and VQA-finetuned LMMs on visual quality understanding tasks, and delivers competitive performance on visual quality comparison tasks without any task-specific training.", "conclusion": "The framework demonstrates robust quality assessment capabilities through training-free RAG, effectively leveraging LMMs' latent perceptual knowledge and providing an alternative to labor-intensive supervised approaches."}}
{"id": "2601.18222", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18222", "abs": "https://arxiv.org/abs/2601.18222", "authors": ["Mengfan He", "Liangzheng Sun", "Chunyu Li", "Ziyang Meng"], "title": "HomoFM: Deep Homography Estimation with Flow Matching", "comment": null, "summary": "Deep homography estimation has broad applications in computer vision and robotics. Remarkable progresses have been achieved while the existing methods typically treat it as a direct regression or iterative refinement problem and often struggling to capture complex geometric transformations or generalize across different domains. In this work, we propose HomoFM, a new framework that introduces the flow matching technique from generative modeling into the homography estimation task for the first time. Unlike the existing methods, we formulate homography estimation problem as a velocity field learning problem. By modeling a continuous and point-wise velocity field that transforms noisy distributions into registered coordinates, the proposed network recovers high-precision transformations through a conditional flow trajectory. Furthermore, to address the challenge of domain shifts issue, e.g., the cases of multimodal matching or varying illumination scenarios, we integrate a gradient reversal layer (GRL) into the feature extraction backbone. This domain adaptation strategy explicitly constrains the encoder to learn domain-invariant representations, significantly enhancing the network's robustness. Extensive experiments demonstrate the effectiveness of the proposed method, showing that HomoFM outperforms state-of-the-art methods in both estimation accuracy and robustness on standard benchmarks. Code and data resource are available at https://github.com/hmf21/HomoFM.", "AI": {"tldr": "HomoFM introduces flow matching for homography estimation, treating it as velocity field learning with domain adaptation via gradient reversal layers for robustness across domains.", "motivation": "Existing deep homography estimation methods struggle with complex geometric transformations and domain generalization (e.g., multimodal matching, varying illumination).", "method": "Formulates homography estimation as velocity field learning using flow matching, with domain adaptation via gradient reversal layers (GRL) in feature extraction backbone.", "result": "Outperforms state-of-the-art methods in both estimation accuracy and robustness on standard benchmarks.", "conclusion": "HomoFM effectively addresses domain shift challenges in homography estimation through flow matching and domain adaptation, achieving superior performance."}}
{"id": "2601.18228", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18228", "abs": "https://arxiv.org/abs/2601.18228", "authors": ["Sahil Naik", "Soham Bagayatkar", "Pavankumar Singh"], "title": "Facial Emotion Recognition on FER-2013 using an EfficientNetB2-Based Approach", "comment": "6 pages, 4 figures", "summary": "Detection of human emotions based on facial images in real-world scenarios is a difficult task due to low image quality, variations in lighting, pose changes, background distractions, small inter-class variations, noisy crowd-sourced labels, and severe class imbalance, as observed in the FER-2013 dataset of 48x48 grayscale images. Although recent approaches using large CNNs such as VGG and ResNet achieve reasonable accuracy, they are computationally expensive and memory-intensive, limiting their practicality for real-time applications. We address these challenges using a lightweight and efficient facial emotion recognition pipeline based on EfficientNetB2, trained using a two-stage warm-up and fine-tuning strategy. The model is enhanced with AdamW optimization, decoupled weight decay, label smoothing (epsilon = 0.06) to reduce annotation noise, and clipped class weights to mitigate class imbalance, along with dropout, mixed-precision training, and extensive real-time data augmentation. The model is trained using a stratified 87.5%/12.5% train-validation split while keeping the official test set intact, achieving a test accuracy of 68.78% with nearly ten times fewer parameters than VGG16-based baselines. Experimental results, including per-class metrics and learning dynamics, demonstrate stable training and strong generalization, making the proposed approach suitable for real-time and edge-based applications.", "AI": {"tldr": "Lightweight EfficientNetB2-based facial emotion recognition pipeline achieves 68.78% accuracy on FER-2013 with 10x fewer parameters than VGG16, using advanced training techniques for real-time applications.", "motivation": "Real-world facial emotion recognition faces challenges like low image quality, lighting variations, pose changes, background distractions, small inter-class variations, noisy labels, and class imbalance in datasets like FER-2013. Existing large CNN models (VGG, ResNet) are computationally expensive and memory-intensive, limiting real-time practicality.", "method": "Uses EfficientNetB2 architecture with two-stage warm-up and fine-tuning strategy. Enhanced with AdamW optimization, decoupled weight decay, label smoothing (\u03b5=0.06) to reduce annotation noise, clipped class weights to mitigate class imbalance, dropout, mixed-precision training, and extensive real-time data augmentation. Trained with stratified 87.5%/12.5% train-validation split.", "result": "Achieves 68.78% test accuracy on FER-2013 dataset with nearly ten times fewer parameters than VGG16-based baselines. Demonstrates stable training, strong generalization, and suitability for real-time applications through experimental results including per-class metrics and learning dynamics.", "conclusion": "The proposed lightweight EfficientNetB2-based pipeline effectively addresses computational efficiency and real-world challenges in facial emotion recognition, making it suitable for real-time and edge-based applications while maintaining competitive accuracy."}}
{"id": "2601.18240", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18240", "abs": "https://arxiv.org/abs/2601.18240", "authors": ["Mengyuan Jin", "Zehui Liao", "Yong Xia"], "title": "V-Loop: Visual Logical Loop Verification for Hallucination Detection in Medical Visual Question Answering", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have shown remarkable capability in assisting disease diagnosis in medical visual question answering (VQA). However, their outputs remain vulnerable to hallucinations (i.e., responses that contradict visual facts), posing significant risks in high-stakes medical scenarios. Recent introspective detection methods, particularly uncertainty-based approaches, offer computational efficiency but are fundamentally indirect, as they estimate predictive uncertainty for an image-question pair rather than verifying the factual correctness of a specific answer. To address this limitation, we propose Visual Logical Loop Verification (V-Loop), a training-free and plug-and-play framework for hallucination detection in medical VQA. V-Loop introduces a bidirectional reasoning process that forms a visually grounded logical loop to verify factual correctness. Given an input, the MLLM produces an answer for the primary input pair. V-Loop extracts semantic units from the primary QA pair, generates a verification question by conditioning on the answer unit to re-query the question unit, and enforces visual attention consistency to ensure answering both primary question and verification question rely on the same image evidence. If the verification answer matches the expected semantic content, the logical loop closes, indicating factual grounding; otherwise, the primary answer is flagged as hallucinated. Extensive experiments on multiple medical VQA benchmarks and MLLMs show that V-Loop consistently outperforms existing introspective methods, remains highly efficient, and further boosts uncertainty-based approaches when used in combination.", "AI": {"tldr": "V-Loop is a training-free framework for detecting hallucinations in medical VQA by creating a bidirectional logical loop that verifies answer consistency through visual attention matching.", "motivation": "Current MLLMs for medical VQA suffer from hallucinations that pose risks in high-stakes medical scenarios. Existing uncertainty-based detection methods are indirect and don't verify factual correctness of specific answers.", "method": "V-Loop extracts semantic units from QA pairs, generates verification questions by conditioning on answers to re-query questions, and enforces visual attention consistency to ensure both questions rely on same image evidence. A closed logical loop indicates factual grounding.", "result": "V-Loop consistently outperforms existing introspective methods across multiple medical VQA benchmarks and MLLMs, remains highly efficient, and further boosts uncertainty-based approaches when combined.", "conclusion": "V-Loop provides an effective, training-free solution for hallucination detection in medical VQA by establishing visually grounded logical verification loops, addressing critical safety concerns in medical applications."}}
{"id": "2601.18242", "categories": ["cs.CV", "cs.NI"], "pdf": "https://arxiv.org/pdf/2601.18242", "abs": "https://arxiv.org/abs/2601.18242", "authors": ["Zerui Kang", "Yishen Lim", "Zhouyou Gu", "Seung-Woo Ko", "Tony Q. S. Quek", "Jihong Park"], "title": "Vision-Language-Model-Guided Differentiable Ray Tracing for Fast and Accurate Multi-Material RF Parameter Estimation", "comment": null, "summary": "Accurate radio-frequency (RF) material parameters are essential for electromagnetic digital twins in 6G systems, yet gradient-based inverse ray tracing (RT) remains sensitive to initialization and costly under limited measurements. This paper proposes a vision-language-model (VLM) guided framework that accelerates and stabilizes multi-material parameter estimation in a differentiable RT (DRT) engine. A VLM parses scene images to infer material categories and maps them to quantitative priors via an ITU-R material table, yielding informed conductivity initializations. The VLM further selects informative transmitter/receiver placements that promote diverse, material-discriminative paths. Starting from these priors, the DRT performs gradient-based refinement using measured received signal strengths. Experiments in NVIDIA Sionna on indoor scenes show 2-4$\\times$ faster convergence and 10-100$\\times$ lower final parameter error compared with uniform or random initialization and random placement baselines, achieving sub-0.1\\% mean relative error with only a few receivers. Complexity analyses indicate per-iteration time scales near-linearly with the number of materials and measurement setups, while VLM-guided placement reduces the measurements required for accurate recovery. Ablations over RT depth and ray counts confirm further accuracy gains without significant per-iteration overhead. Results demonstrate that semantic priors from VLMs effectively guide physics-based optimization for fast and reliable RF material estimation.", "AI": {"tldr": "VLM-guided framework accelerates RF material parameter estimation by using vision-language models to provide semantic priors for initialization and measurement placement in differentiable ray tracing.", "motivation": "Accurate RF material parameters are crucial for 6G electromagnetic digital twins, but current gradient-based inverse ray tracing methods are sensitive to initialization and computationally expensive with limited measurements.", "method": "Uses vision-language model to parse scene images, infer material categories, map to quantitative priors via ITU-R material table for initialization, and select informative transmitter/receiver placements. Then performs gradient-based refinement using differentiable ray tracing with measured signal strengths.", "result": "2-4\u00d7 faster convergence and 10-100\u00d7 lower final parameter error compared to baselines, achieving sub-0.1% mean relative error with few receivers. VLM-guided placement reduces required measurements while maintaining accuracy.", "conclusion": "Semantic priors from vision-language models effectively guide physics-based optimization for fast and reliable RF material estimation, enabling efficient electromagnetic digital twin construction for 6G systems."}}
{"id": "2601.18260", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18260", "abs": "https://arxiv.org/abs/2601.18260", "authors": ["Eytan Kats", "Kai Geissler", "Daniel Mensing", "Jochen G. Hirsch", "Stefan Heldman", "Mattias P. Heinrich"], "title": "Depth to Anatomy: Learning Internal Organ Locations from Surface Depth Images", "comment": "preprint", "summary": "Automated patient positioning plays an important role in optimizing scanning procedure and improving patient throughput. Leveraging depth information captured by RGB-D cameras presents a promising approach for estimating internal organ positions, thereby enabling more accurate and efficient positioning. In this work, we propose a learning-based framework that directly predicts the 3D locations and shapes of multiple internal organs from single 2D depth images of the body surface. Utilizing a large-scale dataset of full-body MRI scans, we synthesize depth images paired with corresponding anatomical segmentations to train a unified convolutional neural network architecture. Our method accurately localizes a diverse set of anatomical structures, including bones and soft tissues, without requiring explicit surface reconstruction. Experimental results demonstrate the potential of integrating depth sensors into radiology workflows to streamline scanning procedures and enhance patient experience through automated patient positioning.", "AI": {"tldr": "A learning-based framework predicts 3D organ locations and shapes from single 2D depth images for automated patient positioning in radiology.", "motivation": "Automated patient positioning is important for optimizing scanning procedures and improving patient throughput. Depth information from RGB-D cameras offers a promising approach for estimating internal organ positions to enable more accurate and efficient positioning.", "method": "A learning-based framework that directly predicts 3D locations and shapes of multiple internal organs from single 2D depth images of the body surface. Uses a large-scale dataset of full-body MRI scans to synthesize depth images paired with anatomical segmentations to train a unified convolutional neural network architecture.", "result": "The method accurately localizes a diverse set of anatomical structures including bones and soft tissues without requiring explicit surface reconstruction. Experimental results demonstrate the potential of integrating depth sensors into radiology workflows.", "conclusion": "The framework shows promise for streamlining scanning procedures and enhancing patient experience through automated patient positioning by leveraging depth sensors in radiology workflows."}}
{"id": "2601.18263", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18263", "abs": "https://arxiv.org/abs/2601.18263", "authors": ["Subhajeet Das", "Susmita Ghosh", "Abhiroop Chatterjee"], "title": "Revisiting Aerial Scene Classification on the AID Benchmark", "comment": "Presented at the IEEE India Geoscience and Remote Sensing Symposium 2025 and accepted for publication in IEEE Xplore", "summary": "Aerial images play a vital role in urban planning and environmental preservation, as they consist of various structures, representing different types of buildings, forests, mountains, and unoccupied lands. Due to its heterogeneous nature, developing robust models for scene classification remains a challenge. In this study, we conduct a literature review of various machine learning methods for aerial image classification. Our survey covers a range of approaches from handcrafted features (e.g., SIFT, LBP) to traditional CNNs (e.g., VGG, GoogLeNet), and advanced deep hybrid networks. In this connection, we have also designed Aerial-Y-Net, a spatial attention-enhanced CNN with multi-scale feature fusion mechanism, which acts as an attention-based model and helps us to better understand the complexities of aerial images. Evaluated on the AID dataset, our model achieves 91.72% accuracy, outperforming several baseline architectures.", "AI": {"tldr": "Survey of ML methods for aerial image classification plus new Aerial-Y-Net model with spatial attention and multi-scale fusion achieving 91.72% accuracy on AID dataset.", "motivation": "Aerial images are crucial for urban planning and environmental preservation but are heterogeneous and challenging to classify. Robust models are needed to handle the complex structures and diverse elements (buildings, forests, mountains, unoccupied lands) in aerial imagery.", "method": "1) Literature review covering handcrafted features (SIFT, LBP), traditional CNNs (VGG, GoogLeNet), and advanced deep hybrid networks. 2) Proposed Aerial-Y-Net: spatial attention-enhanced CNN with multi-scale feature fusion mechanism that acts as an attention-based model to better understand aerial image complexities.", "result": "Aerial-Y-Net achieves 91.72% accuracy on the AID dataset, outperforming several baseline architectures.", "conclusion": "The paper provides a comprehensive survey of aerial image classification methods and demonstrates that attention-based models with multi-scale feature fusion (like Aerial-Y-Net) can effectively handle the heterogeneous nature of aerial images and achieve state-of-the-art performance."}}
{"id": "2601.18301", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18301", "abs": "https://arxiv.org/abs/2601.18301", "authors": ["Seyedali Mousavi", "Seyedhamidreza Mousavi", "Masoud Daneshtalab"], "title": "Contextual Range-View Projection for 3D LiDAR Point Clouds", "comment": null, "summary": "Range-view projection provides an efficient method for transforming 3D LiDAR point clouds into 2D range image representations, enabling effective processing with 2D deep learning models. However, a major challenge in this projection is the many-to-one conflict, where multiple 3D points are mapped onto the same pixel in the range image, requiring a selection strategy. Existing approaches typically retain the point with the smallest depth (closest to the LiDAR), disregarding semantic relevance and object structure, which leads to the loss of important contextual information. In this paper, we extend the depth-based selection rule by incorporating contextual information from both instance centers and class labels, introducing two mechanisms: \\textit{Centerness-Aware Projection (CAP)} and \\textit{Class-Weighted-Aware Projection (CWAP)}. In CAP, point depths are adjusted according to their distance from the instance center, thereby prioritizing central instance points over noisy boundary and background points. In CWAP, object classes are prioritized through user-defined weights, offering flexibility in the projection strategy. Our evaluations on the SemanticKITTI dataset show that CAP preserves more instance points during projection, achieving up to a 3.1\\% mIoU improvement compared to the baseline. Furthermore, CWAP enhances the performance of targeted classes while having a negligible impact on the performance of other classes", "AI": {"tldr": "Range-view projection for LiDAR point clouds suffers from many-to-one conflicts where multiple 3D points map to same 2D pixel. Existing depth-based selection loses semantic info. New methods CAP and CWAP incorporate instance centers and class weights to preserve important contextual information.", "motivation": "Current range-view projection methods for LiDAR point clouds use depth-based selection (closest point) which discards semantically important information about object structure and context, leading to loss of valuable instance and class information during the many-to-one projection conflict.", "method": "Two novel projection mechanisms: 1) Centerness-Aware Projection (CAP) adjusts point depths based on distance from instance center, prioritizing central points over boundary/background noise. 2) Class-Weighted-Aware Projection (CWAP) uses user-defined class weights to prioritize specific object classes during projection.", "result": "On SemanticKITTI dataset, CAP preserves more instance points and achieves up to 3.1% mIoU improvement over baseline. CWAP enhances performance of targeted classes while having negligible impact on other classes.", "conclusion": "Incorporating contextual information (instance centers and class weights) into range-view projection selection strategies significantly improves semantic segmentation performance by preserving important structural and semantic information that depth-only selection discards."}}
{"id": "2601.18305", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18305", "abs": "https://arxiv.org/abs/2601.18305", "authors": ["Xuan Wang", "Siyuan Su", "Quantong Fu", "Yongxiang Hu", "Yangfan Zhou"], "title": "SwipeGen: Bridging the Execution Gap in GUI Agents via Human-like Swipe Synthesis", "comment": "15 pages, 3 figures. Under review. Code and dataset will be released upon acceptance", "summary": "With the widespread adoption of Graphical User Interface (GUI) agents for automating GUI interaction tasks, substantial research focused on improving GUI perception to ground task instructions into concrete action steps. However, the step execution capability of these agents has gradually emerged as a new bottleneck for task completion. In particular, existing GUI agents often adopt overly simplified strategies for handling swipe interactions, preventing them from accurately replicating human-like behavior. To address this limitation, we decompose human swipe gestures into multiple quantifiable dimensions and propose an automated pipeline SwipeGen to synthesize human-like swipe interactions through GUI exploration. Based on this pipeline, we construct and release the first benchmark for evaluating the swipe execution capability of GUI agents. Furthermore, leveraging the synthesized data, we propose GUISwiper, a GUI agent with enhanced interaction execution capabilities. Experimental results demonstrate that GUISwiper achieves a swipe execution accuracy of 69.07%, representing a 214% improvement over existing VLM baselines.", "AI": {"tldr": "Proposes SwipeGen pipeline to synthesize human-like swipe interactions for GUI agents, creates benchmark for swipe execution evaluation, and develops GUISwiper agent with 214% improvement over baselines.", "motivation": "Existing GUI agents have poor swipe interaction execution capabilities, using overly simplified strategies that fail to replicate human-like behavior, making swipe execution a bottleneck for task completion.", "method": "Decomposes human swipe gestures into quantifiable dimensions, creates SwipeGen pipeline for automated synthesis of human-like swipe interactions through GUI exploration, and builds GUISwiper agent using synthesized data.", "result": "GUISwiper achieves 69.07% swipe execution accuracy, representing a 214% improvement over existing VLM baselines, and the first benchmark for evaluating GUI agent swipe execution capability is released.", "conclusion": "The proposed approach successfully addresses the swipe execution bottleneck in GUI agents by synthesizing human-like interactions and demonstrates significant performance improvements through the GUISwiper agent."}}
{"id": "2601.18330", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18330", "abs": "https://arxiv.org/abs/2601.18330", "authors": ["Muhammad Ali Shah", "Muhammad Mansoor Alam", "Saddam Hussain Khan"], "title": "A Tumor Aware DenseNet Swin Hybrid Learning with Boosted and Hierarchical Feature Spaces for Large-Scale Brain MRI Classification", "comment": "33 Pages, 8 Tables, Figures 16", "summary": "This study proposes an efficient Densely Swin Hybrid (EDSH) framework for brain tumor MRI analysis, designed to jointly capture fine grained texture patterns and long range contextual dependencies. Two tumor aware experimental setups are introduced to address class-specific diagnostic challenges. The first setup employs a Boosted Feature Space (BFS), where independently customized DenseNet and Swint branches learn complementary local and global representations that are dimension aligned, fused, and boosted, enabling highly sensitive detection of diffuse glioma patterns by successfully learning the features of irregular shape, poorly defined mass, and heterogeneous texture. The second setup adopts a hierarchical DenseNet Swint architecture with Deep Feature Extraction have Dual Residual connections (DFE and DR), in which DenseNet serves as a stem CNN for structured local feature learning, while Swin_t models global tumor morphology, effectively suppressing false negatives in meningioma and pituitary tumor classification by learning the features of well defined mass, location (outside brain) and enlargments in tumors (dural tail or upward extension). DenseNet is customized at the input level to match MRI spatial characteristics, leveraging dense residual connectivity to preserve texture information and mitigate vanishing-gradient effects. In parallel, Swint is tailored through task aligned patch embedding and shifted-window self attention to efficiently capture hierarchical global dependencies. Extensive evaluation on a large-scale MRI dataset (stringent 40,260 images across four tumor classes) demonstrates consistent superiority over standalone CNNs, Vision Transformers, and hybrids, achieving 98.50 accuracy and recall on the test unseen dataset.", "AI": {"tldr": "Proposes EDSH framework for brain tumor MRI analysis using hybrid DenseNet-Swin architecture to capture both local texture patterns and global contextual dependencies, achieving 98.50% accuracy on large-scale dataset.", "motivation": "To address the challenge of analyzing brain tumor MRI images that require capturing both fine-grained texture patterns (for irregular glioma patterns) and long-range contextual dependencies (for well-defined meningioma/pituitary tumors), overcoming limitations of standalone CNNs or Vision Transformers.", "method": "Two tumor-aware experimental setups: 1) Boosted Feature Space (BFS) with independently customized DenseNet and Swin branches for complementary local/global representations, dimension alignment, fusion, and boosting; 2) Hierarchical DenseNet-Swin architecture with Deep Feature Extraction and Dual Residual connections, where DenseNet serves as stem CNN for local features and Swin models global tumor morphology. Customizations include dense residual connectivity for texture preservation and task-aligned patch embedding with shifted-window self-attention.", "result": "Achieved 98.50% accuracy and recall on test unseen dataset with 40,260 MRI images across four tumor classes, demonstrating consistent superiority over standalone CNNs, Vision Transformers, and other hybrid approaches.", "conclusion": "The EDSH framework effectively addresses class-specific diagnostic challenges in brain tumor MRI analysis by combining complementary strengths of DenseNet (local texture) and Swin Transformer (global context), providing a robust solution for accurate tumor classification across diverse tumor types."}}
{"id": "2601.18336", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2601.18336", "abs": "https://arxiv.org/abs/2601.18336", "authors": ["Isaac Deutsch", "Nicolas Mo\u00ebnne-Loccoz", "Gavriel State", "Zan Gojcic"], "title": "PPISP: Physically-Plausible Compensation and Control of Photometric Variations in Radiance Field Reconstruction", "comment": "For more details and updates, please visit our project website: https://research.nvidia.com/labs/sil/projects/ppisp/", "summary": "Multi-view 3D reconstruction methods remain highly sensitive to photometric inconsistencies arising from camera optical characteristics and variations in image signal processing (ISP). Existing mitigation strategies such as per-frame latent variables or affine color corrections lack physical grounding and generalize poorly to novel views. We propose the Physically-Plausible ISP (PPISP) correction module, which disentangles camera-intrinsic and capture-dependent effects through physically based and interpretable transformations. A dedicated PPISP controller, trained on the input views, predicts ISP parameters for novel viewpoints, analogous to auto exposure and auto white balance in real cameras. This design enables realistic and fair evaluation on novel views without access to ground-truth images. PPISP achieves SoTA performance on standard benchmarks, while providing intuitive control and supporting the integration of metadata when available. The source code is available at: https://github.com/nv-tlabs/ppisp", "AI": {"tldr": "PPISP introduces a physically-based ISP correction module that disentangles camera-intrinsic and capture-dependent effects for more robust multi-view 3D reconstruction, achieving state-of-the-art performance with better generalization to novel views.", "motivation": "Multi-view 3D reconstruction is sensitive to photometric inconsistencies from camera optics and ISP variations. Existing methods lack physical grounding and generalize poorly to novel views.", "method": "Proposes Physically-Plausible ISP (PPISP) correction module with physically-based transformations to disentangle camera-intrinsic and capture-dependent effects. Includes a PPISP controller trained on input views to predict ISP parameters for novel viewpoints, similar to real camera auto exposure/white balance.", "result": "Achieves state-of-the-art performance on standard benchmarks, enables realistic evaluation on novel views without ground-truth images, provides intuitive control, and supports metadata integration.", "conclusion": "PPISP offers a physically-grounded solution to photometric inconsistencies in multi-view 3D reconstruction, improving generalization to novel views while maintaining interpretability and control."}}
{"id": "2601.18340", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18340", "abs": "https://arxiv.org/abs/2601.18340", "authors": ["Bingzheng Qu", "Kehai Chen", "Xuefeng Bai", "Jun Yu", "Min Zhang"], "title": "Beyond Rigid: Benchmarking Non-Rigid Video Editing", "comment": null, "summary": "Despite the remarkable progress in text-driven video editing, generating coherent non-rigid deformations remains a critical challenge, often plagued by physical distortion and temporal flicker. To bridge this gap, we propose NRVBench, the first dedicated and comprehensive benchmark designed to evaluate non-rigid video editing. First, we curate a high-quality dataset consisting of 180 non-rigid motion videos from six physics-based categories, equipped with 2,340 fine-grained task instructions and 360 multiple-choice questions. Second, we propose NRVE-Acc, a novel evaluation metric based on Vision-Language Models that can rigorously assess physical compliance, temporal consistency, and instruction alignment, overcoming the limitations of general metrics in capturing complex dynamics. Third, we introduce a training-free baseline, VM-Edit, which utilizes a dual-region denoising mechanism to achieve structure-aware control, balancing structural preservation and dynamic deformation. Extensive experiments demonstrate that while current methods have shortcomings in maintaining physical plausibility, our method achieves excellent performance across both standard and proposed metrics. We believe the benchmark could serve as a standard testing platform for advancing physics-aware video editing.", "AI": {"tldr": "NRVBench is the first benchmark for evaluating non-rigid video editing, featuring a curated dataset, a new evaluation metric (NRVE-Acc), and a training-free baseline method (VM-Edit) that addresses physical distortion and temporal flicker issues.", "motivation": "Current text-driven video editing methods struggle with generating coherent non-rigid deformations, often suffering from physical distortion and temporal flicker. There's a lack of dedicated benchmarks to properly evaluate these complex dynamics in physics-based video editing.", "method": "Three main contributions: 1) Curated dataset of 180 non-rigid motion videos across six physics-based categories with 2,340 task instructions and 360 multiple-choice questions; 2) NRVE-Acc metric using Vision-Language Models to assess physical compliance, temporal consistency, and instruction alignment; 3) VM-Edit baseline method with dual-region denoising mechanism for structure-aware control.", "result": "Extensive experiments show current methods have shortcomings in maintaining physical plausibility, while the proposed VM-Edit method achieves excellent performance across both standard and proposed metrics. The benchmark effectively identifies limitations in existing approaches.", "conclusion": "NRVBench serves as a comprehensive testing platform for advancing physics-aware video editing, addressing the critical challenge of non-rigid deformation generation and providing tools for rigorous evaluation of physical compliance and temporal consistency."}}
{"id": "2601.18346", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18346", "abs": "https://arxiv.org/abs/2601.18346", "authors": ["Sijing Wu", "Yunhao Li", "Zicheng Zhang", "Qi Jia", "Xinyue Li", "Huiyu Duan", "Xiongkuo Min", "Guangtao Zhai"], "title": "Q-Bench-Portrait: Benchmarking Multimodal Large Language Models on Portrait Image Quality Perception", "comment": null, "summary": "Recent advances in multimodal large language models (MLLMs) have demonstrated impressive performance on existing low-level vision benchmarks, which primarily focus on generic images. However, their capabilities to perceive and assess portrait images, a domain characterized by distinct structural and perceptual properties, remain largely underexplored. To this end, we introduce Q-Bench-Portrait, the first holistic benchmark specifically designed for portrait image quality perception, comprising 2,765 image-question-answer triplets and featuring (1) diverse portrait image sources, including natural, synthetic distortion, AI-generated, artistic, and computer graphics images; (2) comprehensive quality dimensions, covering technical distortions, AIGC-specific distortions, and aesthetics; and (3) a range of question formats, including single-choice, multiple-choice, true/false, and open-ended questions, at both global and local levels. Based on Q-Bench-Portrait, we evaluate 20 open-source and 5 closed-source MLLMs, revealing that although current models demonstrate some competence in portrait image perception, their performance remains limited and imprecise, with a clear gap relative to human judgments. We hope that the proposed benchmark will foster further research into enhancing the portrait image perception capabilities of both general-purpose and domain-specific MLLMs.", "AI": {"tldr": "Q-Bench-Portrait is the first comprehensive benchmark for evaluating multimodal LLMs on portrait image quality perception, covering diverse image sources, quality dimensions, and question formats, revealing significant gaps between current models and human judgment.", "motivation": "Current MLLMs show impressive performance on generic low-level vision benchmarks, but their capabilities for portrait image perception remain largely unexplored. Portraits have distinct structural and perceptual properties that require specialized evaluation.", "method": "Created Q-Bench-Portrait with 2,765 image-question-answer triplets featuring: (1) diverse portrait sources (natural, synthetic distortion, AI-generated, artistic, computer graphics), (2) comprehensive quality dimensions (technical distortions, AIGC-specific distortions, aesthetics), and (3) various question formats (single/multiple-choice, true/false, open-ended) at global and local levels.", "result": "Evaluation of 20 open-source and 5 closed-source MLLMs shows current models have limited and imprecise portrait perception capabilities, with a clear performance gap relative to human judgments.", "conclusion": "The benchmark reveals significant limitations in current MLLMs' portrait perception and aims to foster research into enhancing both general-purpose and domain-specific MLLMs' capabilities in this important visual domain."}}
{"id": "2601.18368", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18368", "abs": "https://arxiv.org/abs/2601.18368", "authors": ["Caterina Fuster-Barcel\u00f3", "Claudia Castrill\u00f3n", "Laura Rodrigo-Mu\u00f1oz", "Victor Manuel Vega-Su\u00e1rez", "Nicol\u00e1s P\u00e9rez-Fern\u00e1ndez", "Gorka Bastarrika", "Arrate Mu\u00f1oz-Barrutia"], "title": "OREHAS: A fully automated deep-learning pipeline for volumetric endolymphatic hydrops quantification in MRI", "comment": null, "summary": "We present OREHAS (Optimized Recognition & Evaluation of volumetric Hydrops in the Auditory System), the first fully automatic pipeline for volumetric quantification of endolymphatic hydrops (EH) from routine 3D-SPACE-MRC and 3D-REAL-IR MRI. The system integrates three components -- slice classification, inner ear localization, and sequence-specific segmentation -- into a single workflow that computes per-ear endolymphatic-to-vestibular volume ratios (ELR) directly from whole MRI volumes, eliminating the need for manual intervention.\n  Trained with only 3 to 6 annotated slices per patient, OREHAS generalized effectively to full 3D volumes, achieving Dice scores of 0.90 for SPACE-MRC and 0.75 for REAL-IR. In an external validation cohort with complete manual annotations, OREHAS closely matched expert ground truth (VSI = 74.3%) and substantially outperformed the clinical syngo.via software (VSI = 42.5%), which tended to overestimate endolymphatic volumes. Across 19 test patients, vestibular measurements from OREHAS were consistent with syngo.via, while endolymphatic volumes were systematically smaller and more physiologically realistic.\n  These results show that reliable and reproducible EH quantification can be achieved from standard MRI using limited supervision. By combining efficient deep-learning-based segmentation with a clinically aligned volumetric workflow, OREHAS reduces operator dependence, ensures methodological consistency. Besides, the results are compatible with established imaging protocols. The approach provides a robust foundation for large-scale studies and for recalibrating clinical diagnostic thresholds based on accurate volumetric measurements of the inner ear.", "AI": {"tldr": "OREHAS is the first fully automatic pipeline for volumetric quantification of endolymphatic hydrops from routine MRI, achieving high accuracy with minimal training data and outperforming existing clinical software.", "motivation": "Current methods for quantifying endolymphatic hydrops (EH) from MRI require manual intervention, are operator-dependent, and lack consistency. There's a need for automated, reliable quantification to support large-scale studies and improve clinical diagnosis.", "method": "OREHAS integrates three components into a single workflow: slice classification, inner ear localization, and sequence-specific segmentation. It computes endolymphatic-to-vestibular volume ratios (ELR) directly from whole MRI volumes using deep learning, trained with only 3-6 annotated slices per patient.", "result": "Achieved Dice scores of 0.90 for SPACE-MRC and 0.75 for REAL-IR. In external validation, closely matched expert ground truth (VSI = 74.3%) and substantially outperformed clinical syngo.via software (VSI = 42.5%). Produced more physiologically realistic endolymphatic volumes across 19 test patients.", "conclusion": "Reliable EH quantification can be achieved from standard MRI with limited supervision. OREHAS reduces operator dependence, ensures consistency, and provides a robust foundation for large-scale studies and recalibrating clinical diagnostic thresholds based on accurate volumetric measurements."}}
{"id": "2601.18372", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18372", "abs": "https://arxiv.org/abs/2601.18372", "authors": ["Christos Petrou", "Harris Partaourides", "Athanasios Balomenos", "Yannis Kopsinis", "Sotirios Chatzis"], "title": "Gaze Prediction in Virtual Reality Without Eye Tracking Using Visual and Head Motion Cues", "comment": null, "summary": "Gaze prediction plays a critical role in Virtual Reality (VR) applications by reducing sensor-induced latency and enabling computationally demanding techniques such as foveated rendering, which rely on anticipating user attention. However, direct eye tracking is often unavailable due to hardware limitations or privacy concerns. To address this, we present a novel gaze prediction framework that combines Head-Mounted Display (HMD) motion signals with visual saliency cues derived from video frames. Our method employs UniSal, a lightweight saliency encoder, to extract visual features, which are then fused with HMD motion data and processed through a time-series prediction module. We evaluate two lightweight architectures, TSMixer and LSTM, for forecasting future gaze directions. Experiments on the EHTask dataset, along with deployment on commercial VR hardware, show that our approach consistently outperforms baselines such as Center-of-HMD and Mean Gaze. These results demonstrate the effectiveness of predictive gaze modeling in reducing perceptual lag and enhancing natural interaction in VR environments where direct eye tracking is constrained.", "AI": {"tldr": "Novel gaze prediction framework for VR using HMD motion and visual saliency when direct eye tracking is unavailable, outperforming baseline methods.", "motivation": "Direct eye tracking is often unavailable in VR due to hardware limitations or privacy concerns, but gaze prediction is critical for reducing latency and enabling techniques like foveated rendering.", "method": "Combines HMD motion signals with visual saliency cues from video frames using UniSal encoder, fuses features with motion data, and processes through time-series prediction modules (TSMixer or LSTM).", "result": "Outperforms baselines (Center-of-HMD and Mean Gaze) on EHTask dataset and commercial VR hardware deployment, demonstrating effective gaze prediction without direct eye tracking.", "conclusion": "Predictive gaze modeling reduces perceptual lag and enhances natural interaction in VR environments where direct eye tracking is constrained, showing practical viability."}}
{"id": "2601.18385", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18385", "abs": "https://arxiv.org/abs/2601.18385", "authors": ["Rinka Kawano", "Masaki Kawamura"], "title": "Estimation of geometric transformation matrices using grid-shaped pilot signals", "comment": null, "summary": "Digital watermarking techniques are essential to prevent unauthorized use of images. Since pirated images are often geometrically distorted by operations such as scaling and cropping, accurate synchronization - detecting the embedding position of the watermark - is critical for proper extraction. In particular, cropping changes the origin of the image, making synchronization difficult. However, few existing methods are robust against cropping. To address this issue, we propose a watermarking method that estimates geometric transformations applied to a stego image using a pilot signal, allowing synchronization even after cropping. A grid-shaped pilot signal with distinct horizontal and vertical values is embedded in the image. When the image is transformed, the grid is also distorted. By analyzing this distortion, the transformation matrix can be estimated. Applying the Radon transform to the distorted image allows estimation of the grid angles and intervals. In addition, since the horizontal and vertical grid lines are encoded differently, the grid orientation can be determined, which reduces ambiguity. To validate our method, we performed simulations with anisotropic scaling, rotation, shearing, and cropping. The results show that the proposed method accurately estimates transformation matrices with low error under both single and composite attacks.", "AI": {"tldr": "Proposed watermarking method uses grid-shaped pilot signal to estimate geometric transformations (scaling, rotation, shearing, cropping) for accurate synchronization in watermark extraction.", "motivation": "Existing watermarking methods lack robustness against cropping attacks, which change image origin and make synchronization difficult. Accurate synchronization is critical for watermark extraction from geometrically distorted images.", "method": "Embed grid-shaped pilot signal with distinct horizontal/vertical values. Use Radon transform to analyze grid distortion after attacks, estimate transformation matrix from grid angles/intervals. Different encoding of horizontal/vertical lines resolves orientation ambiguity.", "result": "Method accurately estimates transformation matrices with low error under single and composite attacks including anisotropic scaling, rotation, shearing, and cropping.", "conclusion": "The proposed pilot signal-based approach enables robust synchronization against geometric transformations including cropping, addressing a key limitation of existing watermarking methods."}}
{"id": "2601.18386", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18386", "abs": "https://arxiv.org/abs/2601.18386", "authors": ["Gabriel Lee Jun Rong", "Christos Korgialas", "Dion Jia Xu Ho", "Pai Chet Ng", "Xiaoxiao Miao", "Konstantinos N. Plataniotis"], "title": "ARMOR: Agentic Reasoning for Methods Orchestration and Reparameterization for Robust Adversarial Attacks", "comment": null, "summary": "Existing automated attack suites operate as static ensembles with fixed sequences, lacking strategic adaptation and semantic awareness. This paper introduces the Agentic Reasoning for Methods Orchestration and Reparameterization (ARMOR) framework to address these limitations. ARMOR orchestrates three canonical adversarial primitives, Carlini-Wagner (CW), Jacobian-based Saliency Map Attack (JSMA), and Spatially Transformed Attacks (STA) via Vision Language Models (VLM)-guided agents that collaboratively generate and synthesize perturbations through a shared ``Mixing Desk\". Large Language Models (LLMs) adaptively tune and reparameterize parallel attack agents in a real-time, closed-loop system that exploits image-specific semantic vulnerabilities. On standard benchmarks, ARMOR achieves improved cross-architecture transfer and reliably fools both settings, delivering a blended output for blind targets and selecting the best attack or blended attacks for white-box targets using a confidence-and-SSIM score.", "AI": {"tldr": "ARMOR framework uses VLM-guided agents to orchestrate CW, JSMA, and STA attacks via a shared \"Mixing Desk\" with LLM-driven adaptive tuning for semantic-aware adversarial attacks.", "motivation": "Existing automated attack suites are static ensembles with fixed sequences, lacking strategic adaptation and semantic awareness, limiting their effectiveness against modern defenses.", "method": "ARMOR orchestrates three adversarial primitives (CW, JSMA, STA) via VLM-guided agents that collaboratively generate perturbations through a shared \"Mixing Desk.\" LLMs adaptively tune and reparameterize parallel attack agents in real-time to exploit image-specific semantic vulnerabilities.", "result": "On standard benchmarks, ARMOR achieves improved cross-architecture transfer and reliably fools both settings, delivering blended output for blind targets and selecting best attack or blended attacks for white-box targets using confidence-and-SSIM score.", "conclusion": "ARMOR represents a significant advancement in adversarial attack frameworks by introducing semantic-aware, adaptive orchestration of multiple attack strategies through VLM/LLM-guided agents, overcoming limitations of static attack ensembles."}}
{"id": "2601.18392", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18392", "abs": "https://arxiv.org/abs/2601.18392", "authors": ["Moritz Rempe", "Lukas T. Rotkopf", "Marco Schlimbach", "Helmut Becker", "Fabian H\u00f6rst", "Johannes Haubold", "Philipp Dammann", "Kevin Kr\u00f6ninger", "Jens Kleesiek"], "title": "Efficient Complex-Valued Vision Transformers for MRI Classification Directly from k-Space", "comment": null, "summary": "Deep learning applications in Magnetic Resonance Imaging (MRI) predominantly operate on reconstructed magnitude images, a process that discards phase information and requires computationally expensive transforms. Standard neural network architectures rely on local operations (convolutions or grid-patches) that are ill-suited for the global, non-local nature of raw frequency-domain (k-Space) data. In this work, we propose a novel complex-valued Vision Transformer (kViT) designed to perform classification directly on k-Space data. To bridge the geometric disconnect between current architectures and MRI physics, we introduce a radial k-Space patching strategy that respects the spectral energy distribution of the frequency-domain. Extensive experiments on the fastMRI and in-house datasets demonstrate that our approach achieves classification performance competitive with state-of-the-art image-domain baselines (ResNet, EfficientNet, ViT). Crucially, kViT exhibits superior robustness to high acceleration factors and offers a paradigm shift in computational efficiency, reducing VRAM consumption during training by up to 68$\\times$ compared to standard methods. This establishes a pathway for resource-efficient, direct-from-scanner AI analysis.", "AI": {"tldr": "kViT: A complex-valued Vision Transformer that performs MRI classification directly on raw k-space data, achieving competitive performance with image-domain methods while being 68\u00d7 more memory-efficient.", "motivation": "Current deep learning MRI methods discard phase information by using reconstructed magnitude images, requiring expensive transforms. Standard architectures (convolutions, grid patches) are ill-suited for the global, non-local nature of k-space data.", "method": "Proposes kViT, a complex-valued Vision Transformer with radial k-space patching strategy that respects the spectral energy distribution of frequency-domain data, enabling direct classification on raw k-space.", "result": "Achieves classification performance competitive with state-of-the-art image-domain baselines (ResNet, EfficientNet, ViT), shows superior robustness to high acceleration factors, and reduces VRAM consumption by up to 68\u00d7 during training.", "conclusion": "Establishes a pathway for resource-efficient, direct-from-scanner AI analysis by eliminating the need for image reconstruction and enabling efficient processing of raw k-space data."}}
{"id": "2601.18407", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18407", "abs": "https://arxiv.org/abs/2601.18407", "authors": ["Jon Sporring", "David Stansby"], "title": "Larger than memory image processing", "comment": "10 pages", "summary": "This report addresses larger-than-memory image analysis for petascale datasets such as 1.4 PB electron-microscopy volumes and 150 TB human-organ atlases. We argue that performance is fundamentally I/O-bound. We show that structuring analysis as streaming passes over data is crucial. For 3D volumes, two representations are popular: stacks of 2D slices (e.g., directories or multi-page TIFF) and 3D chunked layouts (e.g., Zarr/HDF5). While for a few algorithms, chunked layout on disk is crucial to keep disk I/O at a minimum, we show how the slice-based streaming architecture can be built on top of either image representation in a manner that minimizes disk I/O. This is in particular advantageous for algorithms relying on neighbouring values, since the slicing streaming architecture is 1D, which implies that there are only 2 possible sweeping orders, both of which are aligned with the order in which images are read from the disk. This is in contrast to 3D chunks, in which any sweep cannot be done without accessing each chunk at least 9 times. We formalize this with sweep-based execution (natural 2D/3D orders), windowed operations, and overlap-aware tiling to minimize redundant access. Building on these principles, we introduce a domain-specific language (DSL) that encodes algorithms with intrinsic knowledge of their optimal streaming and memory use; the DSL performs compile-time and run-time pipeline analyses to automatically select window sizes, fuse stages, tee and zip streams, and schedule passes for limited-RAM machines, yielding near-linear I/O scans and predictable memory footprints. The approach integrates with existing tooling for segmentation and morphology but reframes pre/post-processing as pipelines that privilege sequential read/write patterns, delivering substantial throughput gains for extremely large images without requiring full-volume residency in memory.", "AI": {"tldr": "A streaming-based approach for large-scale image analysis that minimizes I/O for petascale datasets by structuring algorithms as sequential passes over data, using a DSL to optimize memory usage and access patterns.", "motivation": "Address performance bottlenecks in analyzing petascale image datasets (1.4PB EM volumes, 150TB organ atlases) where performance is fundamentally I/O-bound, requiring efficient data access patterns to handle larger-than-memory datasets.", "method": "Propose slice-based streaming architecture that works with both 2D slice stacks and 3D chunked layouts, using sweep-based execution, windowed operations, and overlap-aware tiling. Introduce a DSL that encodes algorithms with knowledge of optimal streaming patterns and performs compile-time/run-time pipeline analysis to automatically optimize window sizes, fuse stages, and schedule passes for limited-RAM machines.", "result": "Achieves near-linear I/O scans and predictable memory footprints, delivering substantial throughput gains for extremely large images without requiring full-volume residency in memory, while integrating with existing segmentation and morphology tooling.", "conclusion": "Structuring analysis as streaming passes over data with intelligent pipeline optimization enables efficient processing of petascale image datasets by minimizing redundant disk access and managing memory constraints, making large-scale image analysis practical on limited-RAM systems."}}
{"id": "2601.18414", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18414", "abs": "https://arxiv.org/abs/2601.18414", "authors": ["Aura Loredana Dan"], "title": "Comparative Evaluation of Machine Learning Algorithms for Affective State Recognition from Children's Drawings", "comment": "9 pages, 8 figures", "summary": "Autism spectrum disorder (ASD) represents a neurodevelopmental condition characterized by difficulties in expressing emotions and communication, particularly during early childhood. Understanding the affective state of children at an early age remains challenging, as conventional assessment methods are often intrusive, subjective, or difficult to apply consistently. This paper builds upon previous work on affective state recognition from children's drawings by presenting a comparative evaluation of machine learning models for emotion classification. Three deep learning architectures -- MobileNet, EfficientNet, and VGG16 -- are evaluated within a unified experimental framework to analyze classification performance, robustness, and computational efficiency. The models are trained using transfer learning on a dataset of children's drawings annotated with emotional labels provided by psychological experts. The results highlight important trade-offs between lightweight and deeper architectures when applied to drawing-based affective computing tasks, particularly in mobile and real-time application contexts.", "AI": {"tldr": "Comparative evaluation of MobileNet, EfficientNet, and VGG16 for emotion classification from children's drawings to aid early ASD assessment.", "motivation": "Early detection of emotional states in children with ASD is challenging due to intrusive/subjective conventional methods; drawings offer a non-intrusive alternative for affective state recognition.", "method": "Three deep learning architectures (MobileNet, EfficientNet, VGG16) evaluated using transfer learning on expert-annotated children's drawings dataset within unified experimental framework.", "result": "Analysis reveals trade-offs between lightweight vs. deeper architectures for drawing-based affective computing, particularly important for mobile/real-time applications.", "conclusion": "Comparative evaluation provides insights into model selection for emotion classification from children's drawings, supporting development of non-intrusive ASD assessment tools."}}
{"id": "2601.18448", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18448", "abs": "https://arxiv.org/abs/2601.18448", "authors": ["Lloyd Austin Courtenay"], "title": "On Procrustes Contamination in Machine Learning Applications of Geometric Morphometrics", "comment": "17 pages, 5 figures, Preprint pending review", "summary": "Geometric morphometrics (GMM) is widely used to quantify shape variation, more recently serving as input for machine learning (ML) analyses. Standard practice aligns all specimens via Generalized Procrustes Analysis (GPA) prior to splitting data into training and test sets, potentially introducing statistical dependence and contaminating downstream predictive models. Here, the effects of GPA-induced contamination are formally characterised using controlled 2D and 3D simulations across varying sample sizes, landmark densities, and allometric patterns. A novel realignment procedure is proposed, whereby test specimens are aligned to the training set prior to model fitting, eliminating cross-sample dependency. Simulations reveal a robust \"diagonal\" in sample-size vs. landmark-space, reflecting the scaling of RMSE under isotropic variation, with slopes analytically derived from the degrees of freedom in Procrustes tangent space. The importance of spatial autocorrelation among landmarks is further demonstrated using linear and convolutional regression models, highlighting performance degradation when landmark relationships are ignored. This work establishes the need for careful preprocessing in ML applications of GMM, provides practical guidelines for realignment, and clarifies fundamental statistical constraints inherent to Procrustes shape space.", "AI": {"tldr": "Standard GPA alignment before train-test split contaminates ML models; proposed realignment of test to training set eliminates dependency; simulations show contamination effects scale with sample size and landmark density.", "motivation": "Standard practice of aligning all specimens via GPA before splitting data introduces statistical dependence between training and test sets, potentially contaminating downstream predictive models in machine learning applications of geometric morphometrics.", "method": "Used controlled 2D and 3D simulations across varying sample sizes, landmark densities, and allometric patterns. Proposed novel realignment procedure where test specimens are aligned to the training set prior to model fitting. Analyzed effects using linear and convolutional regression models.", "result": "Simulations revealed a robust \"diagonal\" in sample-size vs. landmark-space reflecting RMSE scaling under isotropic variation. Demonstrated performance degradation when landmark spatial autocorrelation is ignored. Established analytical relationships between contamination effects and degrees of freedom in Procrustes tangent space.", "conclusion": "Careful preprocessing is essential for ML applications of GMM. The proposed realignment procedure eliminates cross-sample dependency. The work provides practical guidelines and clarifies fundamental statistical constraints inherent to Procrustes shape space."}}
{"id": "2601.18493", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18493", "abs": "https://arxiv.org/abs/2601.18493", "authors": ["Sara Tehrani", "Yonghao Xu", "Leif Haglund", "Amanda Berg", "Michael Felsberg"], "title": "DisasterInsight: A Multimodal Benchmark for Function-Aware and Grounded Disaster Assessment", "comment": "Under review at ICPR 2026", "summary": "Timely interpretation of satellite imagery is critical for disaster response, yet existing vision-language benchmarks for remote sensing largely focus on coarse labels and image-level recognition, overlooking the functional understanding and instruction robustness required in real humanitarian workflows. We introduce DisasterInsight, a multimodal benchmark designed to evaluate vision-language models (VLMs) on realistic disaster analysis tasks. DisasterInsight restructures the xBD dataset into approximately 112K building-centered instances and supports instruction-diverse evaluation across multiple tasks, including building-function classification, damage-level and disaster-type classification, counting, and structured report generation aligned with humanitarian assessment guidelines.\n  To establish domain-adapted baselines, we propose DI-Chat, obtained by fine-tuning existing VLM backbones on disaster-specific instruction data using parameter-efficient Low-Rank Adaptation (LoRA). Extensive experiments on state-of-the-art generic and remote-sensing VLMs reveal substantial performance gaps across tasks, particularly in damage understanding and structured report generation. DI-Chat achieves significant improvements on damage-level and disaster-type classification as well as report generation quality, while building-function classification remains challenging for all evaluated models. DisasterInsight provides a unified benchmark for studying grounded multimodal reasoning in disaster imagery.", "AI": {"tldr": "DisasterInsight is a new multimodal benchmark for evaluating vision-language models on realistic disaster analysis tasks using building-centered satellite imagery, with DI-Chat as a domain-adapted baseline model.", "motivation": "Existing remote sensing benchmarks focus on coarse labels and image-level recognition, lacking the functional understanding and instruction robustness needed for real humanitarian disaster response workflows.", "method": "Restructured xBD dataset into 112K building-centered instances, created instruction-diverse evaluation across multiple tasks, and developed DI-Chat by fine-tuning VLM backbones with LoRA on disaster-specific instruction data.", "result": "Significant performance gaps found in existing VLMs, especially for damage understanding and structured report generation. DI-Chat showed substantial improvements on damage-level/disaster-type classification and report generation, though building-function classification remained challenging for all models.", "conclusion": "DisasterInsight provides a unified benchmark for studying grounded multimodal reasoning in disaster imagery, addressing critical gaps in current remote sensing evaluation frameworks for humanitarian applications."}}
{"id": "2601.18532", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18532", "abs": "https://arxiv.org/abs/2601.18532", "authors": ["Devon Levy", "Bar Assayag", "Laura Gaspar", "Ilan Shimshoni", "Bella Specktor-Fadida"], "title": "From Cold Start to Active Learning: Embedding-Based Scan Selection for Medical Image Segmentation", "comment": "19 pages without references", "summary": "Accurate segmentation annotations are critical for disease monitoring, yet manual labeling remains a major bottleneck due to the time and expertise required. Active learning (AL) alleviates this burden by prioritizing informative samples for annotation, typically through a diversity-based cold-start phase followed by uncertainty-driven selection. We propose a novel cold-start sampling strategy that combines foundation-model embeddings with clustering, including automatic selection of the number of clusters and proportional sampling across clusters, to construct a diverse and representative initial training. This is followed by an uncertainty-based AL framework that integrates spatial diversity to guide sample selection. The proposed method is intuitive and interpretable, enabling visualization of the feature-space distribution of candidate samples. We evaluate our approach on three datasets spanning X-ray and MRI modalities. On the CheXmask dataset, the cold-start strategy outperforms random selection, improving Dice from 0.918 to 0.929 and reducing the Hausdorff distance from 32.41 to 27.66 mm. In the AL setting, combined entropy and diversity selection improves Dice from 0.919 to 0.939 and reduces the Hausdorff distance from 30.10 to 19.16 mm. On the Montgomery dataset, cold-start gains are substantial, with Dice improving from 0.928 to 0.950 and Hausdorff distance decreasing from 14.22 to 9.38 mm. On the SynthStrip dataset, cold-start selection slightly affects Dice but reduces the Hausdorff distance from 9.43 to 8.69 mm, while active learning improves Dice from 0.816 to 0.826 and reduces the Hausdorff distance from 7.76 to 6.38 mm. Overall, the proposed framework consistently outperforms baseline methods in low-data regimes, improving segmentation accuracy.", "AI": {"tldr": "Novel active learning framework combining foundation-model embeddings with clustering for cold-start sampling, followed by uncertainty-based selection with spatial diversity, consistently improves medical image segmentation accuracy across multiple datasets.", "motivation": "Manual segmentation annotation is time-consuming and requires expertise, creating a bottleneck for disease monitoring. Active learning can reduce annotation burden by prioritizing informative samples, but existing cold-start strategies need improvement for better diversity and representativeness.", "method": "Proposes a two-phase active learning framework: 1) Cold-start phase uses foundation-model embeddings with clustering (including automatic cluster number selection and proportional sampling) to create diverse initial training set; 2) Uncertainty-based phase integrates spatial diversity for sample selection. The method is interpretable with feature-space visualization capabilities.", "result": "Consistent improvements across three medical imaging datasets: CheXmask (Dice: 0.918\u21920.929 cold-start, 0.919\u21920.939 AL; Hausdorff: 32.41\u219227.66mm cold-start, 30.10\u219219.16mm AL), Montgomery (Dice: 0.928\u21920.950; Hausdorff: 14.22\u21929.38mm), and SynthStrip (Hausdorff: 9.43\u21928.69mm cold-start; Dice: 0.816\u21920.826 AL; Hausdorff: 7.76\u21926.38mm AL).", "conclusion": "The proposed active learning framework with foundation-model-based cold-start sampling and uncertainty-driven selection with spatial diversity consistently outperforms baselines in low-data regimes, offering an intuitive and interpretable approach to reduce annotation burden while improving segmentation accuracy."}}
{"id": "2601.18543", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18543", "abs": "https://arxiv.org/abs/2601.18543", "authors": ["Kaixun Jiang", "Yuzheng Wang", "Junjie Zhou", "Pandeng Li", "Zhihang Liu", "Chen-Wei Xie", "Zhaoyu Chen", "Yun Zheng", "Wenqiang Zhang"], "title": "GenAgent: Scaling Text-to-Image Generation via Agentic Multimodal Reasoning", "comment": null, "summary": "We introduce GenAgent, unifying visual understanding and generation through an agentic multimodal model. Unlike unified models that face expensive training costs and understanding-generation trade-offs, GenAgent decouples these capabilities through an agentic framework: understanding is handled by the multimodal model itself, while generation is achieved by treating image generation models as invokable tools. Crucially, unlike existing modular systems constrained by static pipelines, this design enables autonomous multi-turn interactions where the agent generates multimodal chains-of-thought encompassing reasoning, tool invocation, judgment, and reflection to iteratively refine outputs. We employ a two-stage training strategy: first, cold-start with supervised fine-tuning on high-quality tool invocation and reflection data to bootstrap agent behaviors; second, end-to-end agentic reinforcement learning combining pointwise rewards (final image quality) and pairwise rewards (reflection accuracy), with trajectory resampling for enhanced multi-turn exploration. GenAgent significantly boosts base generator(FLUX.1-dev) performance on GenEval++ (+23.6\\%) and WISE (+14\\%). Beyond performance gains, our framework demonstrates three key properties: 1) cross-tool generalization to generators with varying capabilities, 2) test-time scaling with consistent improvements across interaction rounds, and 3) task-adaptive reasoning that automatically adjusts to different tasks. Our code will be available at \\href{https://github.com/deep-kaixun/GenAgent}{this url}.", "AI": {"tldr": "GenAgent is an agentic multimodal model that decouples visual understanding and generation capabilities through a tool-invocation framework, enabling autonomous multi-turn interactions with iterative refinement via chains-of-thought.", "motivation": "Existing unified models face expensive training costs and understanding-generation trade-offs, while modular systems are constrained by static pipelines. There's a need for a flexible framework that can autonomously interact with and control image generation tools.", "method": "Agentic framework where multimodal model handles understanding and invokes image generation models as tools. Two-stage training: 1) supervised fine-tuning on tool invocation and reflection data, 2) end-to-end agentic reinforcement learning with pointwise (image quality) and pairwise (reflection accuracy) rewards, plus trajectory resampling for multi-turn exploration.", "result": "Significant performance boosts: +23.6% on GenEval++ and +14% on WISE benchmarks over base generator (FLUX.1-dev). Demonstrates cross-tool generalization, test-time scaling with consistent improvements across interaction rounds, and task-adaptive reasoning.", "conclusion": "GenAgent provides an effective agentic framework that unifies visual understanding and generation through tool invocation, enabling autonomous multi-turn interactions with iterative refinement, while demonstrating strong performance gains and desirable properties like generalization and scalability."}}
{"id": "2601.18547", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2601.18547", "abs": "https://arxiv.org/abs/2601.18547", "authors": ["Qing Ding", "Mai Xu", "Shengxi Li", "Xin Deng", "Xin Zou"], "title": "REMAC: Reference-Based Martian Asymmetrical Image Compression", "comment": "Accepted for publication in IEEE Transactions on Geoscience and Remote Sensing (TGRS). 2025 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. 18 pages, 20 figures", "summary": "To expedite space exploration on Mars, it is indispensable to develop an efficient Martian image compression method for transmitting images through the constrained Mars-to-Earth communication channel. Although the existing learned compression methods have achieved promising results for natural images from earth, there remain two critical issues that hinder their effectiveness for Martian image compression: 1) They overlook the highly-limited computational resources on Mars; 2) They do not utilize the strong \\textit{inter-image} similarities across Martian images to advance image compression performance. Motivated by our empirical analysis of the strong \\textit{intra-} and \\textit{inter-image} similarities from the perspective of texture, color, and semantics, we propose a reference-based Martian asymmetrical image compression (REMAC) approach, which shifts computational complexity from the encoder to the resource-rich decoder and simultaneously improves compression performance. To leverage \\textit{inter-image} similarities, we propose a reference-guided entropy module and a ref-decoder that utilize useful information from reference images, reducing redundant operations at the encoder and achieving superior compression performance. To exploit \\textit{intra-image} similarities, the ref-decoder adopts a deep, multi-scale architecture with enlarged receptive field size to model long-range spatial dependencies. Additionally, we develop a latent feature recycling mechanism to further alleviate the extreme computational constraints on Mars. Experimental results show that REMAC reduces encoder complexity by 43.51\\% compared to the state-of-the-art method, while achieving a BD-PSNR gain of 0.2664 dB.", "AI": {"tldr": "REMAC is a reference-based Martian image compression method that reduces encoder complexity by 43.51% while improving compression performance by leveraging inter-image similarities from reference images.", "motivation": "Current learned compression methods are ineffective for Martian images because they ignore Mars' limited computational resources and fail to utilize strong inter-image similarities across Martian images, which could improve compression performance.", "method": "Proposes REMAC with reference-guided entropy module and ref-decoder to leverage inter-image similarities, deep multi-scale architecture for intra-image similarities, and latent feature recycling to reduce computational load on Mars.", "result": "REMAC reduces encoder complexity by 43.51% compared to state-of-the-art methods while achieving a BD-PSNR gain of 0.2664 dB, demonstrating both efficiency and performance improvements.", "conclusion": "REMAC successfully addresses Martian image compression challenges by shifting computational complexity to Earth-based decoders and leveraging inter-image similarities, making it suitable for Mars exploration with constrained communication channels."}}
{"id": "2601.18555", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18555", "abs": "https://arxiv.org/abs/2601.18555", "authors": ["Roberto Di Via", "Vito Paolo Pastore", "Francesca Odone", "Si\u00f4n Glyn-Jones", "Irina Voiculescu"], "title": "Automated Landmark Detection for assessing hip conditions: A Cross-Modality Validation of MRI versus X-ray", "comment": "Accepted at International Symposium on Biomedical Imaging (ISBI 2026)", "summary": "Many clinical screening decisions are based on angle measurements. In particular, FemoroAcetabular Impingement (FAI) screening relies on angles traditionally measured on X-rays. However, assessing the height and span of the impingement area requires also a 3D view through an MRI scan. The two modalities inform the surgeon on different aspects of the condition. In this work, we conduct a matched-cohort validation study (89 patients, paired MRI/X-ray) using standard heatmap regression architectures to assess cross-modality clinical equivalence. Seen that landmark detection has been proven effective on X-rays, we show that MRI also achieves equivalent localisation and diagnostic accuracy for cam-type impingement. Our method demonstrates clinical feasibility for FAI assessment in coronal views of 3D MRI volumes, opening the possibility for volumetric analysis through placing further landmarks. These results support integrating automated FAI assessment into routine MRI workflows. Code is released at https://github.com/Malga-Vision/Landmarks-Hip-Conditions", "AI": {"tldr": "Automated landmark detection on MRI achieves equivalent accuracy to X-rays for cam-type FAI assessment, enabling volumetric analysis and integration into routine MRI workflows.", "motivation": "FAI screening traditionally uses X-ray angle measurements, but 3D MRI is needed to assess impingement area height and span. Current methods require both modalities, creating workflow inefficiencies.", "method": "Matched-cohort validation study (89 patients, paired MRI/X-ray) using standard heatmap regression architectures to assess cross-modality clinical equivalence for landmark detection.", "result": "MRI achieves equivalent localization and diagnostic accuracy to X-rays for cam-type impingement assessment, demonstrating clinical feasibility in coronal MRI views.", "conclusion": "Results support integrating automated FAI assessment into routine MRI workflows, opening possibilities for volumetric analysis through additional landmark placement."}}
{"id": "2601.18556", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18556", "abs": "https://arxiv.org/abs/2601.18556", "authors": ["Jingsong Xia", "Siqi Wang"], "title": "Generative Diffusion Augmentation with Quantum-Enhanced Discrimination for Medical Image Diagnosis", "comment": null, "summary": "In biomedical engineering, artificial intelligence has become a pivotal tool for enhancing medical diagnostics, particularly in medical image classification tasks such as detecting pneumonia from chest X-rays and breast cancer screening. However, real-world medical datasets frequently exhibit severe class imbalance, where positive samples substantially outnumber negative samples, leading to biased models with low recall rates for minority classes. This imbalance not only compromises diagnostic accuracy but also poses clinical misdiagnosis risks. To address this challenge, we propose SDA-QEC (Simplified Diffusion Augmentation with Quantum-Enhanced Classification), an innovative framework that integrates simplified diffusion-based data augmentation with quantum-enhanced feature discrimination. Our approach employs a lightweight diffusion augmentor to generate high-quality synthetic samples for minority classes, rebalancing the training distribution. Subsequently, a quantum feature layer embedded within MobileNetV2 architecture enhances the model's discriminative capability through high-dimensional feature mapping in Hilbert space. Comprehensive experiments on coronary angiography image classification demonstrate that SDA-QEC achieves 98.33% accuracy, 98.78% AUC, and 98.33% F1-score, significantly outperforming classical baselines including ResNet18, MobileNetV2, DenseNet121, and VGG16. Notably, our framework simultaneously attains 98.33% sensitivity and 98.33% specificity, achieving a balanced performance critical for clinical deployment. The proposed method validates the feasibility of integrating generative augmentation with quantum-enhanced modeling in real-world medical imaging tasks, offering a novel research pathway for developing highly reliable medical AI systems in small-sample, highly imbalanced, and high-risk diagnostic scenarios.", "AI": {"tldr": "SDA-QEC framework combines simplified diffusion augmentation with quantum-enhanced classification to address class imbalance in medical image analysis, achieving state-of-the-art performance on coronary angiography classification.", "motivation": "Real-world medical datasets suffer from severe class imbalance where positive samples outnumber negative samples, leading to biased models with low recall for minority classes and clinical misdiagnosis risks.", "method": "SDA-QEC integrates lightweight diffusion-based data augmentation to generate synthetic minority class samples, combined with a quantum feature layer embedded in MobileNetV2 architecture for enhanced discriminative capability through high-dimensional Hilbert space mapping.", "result": "Achieves 98.33% accuracy, 98.78% AUC, 98.33% F1-score, and balanced 98.33% sensitivity and specificity on coronary angiography classification, significantly outperforming classical baselines like ResNet18, MobileNetV2, DenseNet121, and VGG16.", "conclusion": "The framework validates the feasibility of integrating generative augmentation with quantum-enhanced modeling for medical imaging tasks, offering a novel pathway for reliable AI systems in small-sample, imbalanced, high-risk diagnostic scenarios."}}
{"id": "2601.18560", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18560", "abs": "https://arxiv.org/abs/2601.18560", "authors": ["Li Fang", "Tianyu Li", "Yanghong Lin", "Shudong Zhou", "Wei Yao"], "title": "AI-enabled Satellite Edge Computing: A Single-Pixel Feature based Shallow Classification Model for Hyperspectral Imaging", "comment": null, "summary": "As the important component of the Earth observation system, hyperspectral imaging satellites provide high-fidelity and enriched information for the formulation of related policies due to the powerful spectral measurement capabilities. However, the transmission speed of the satellite downlink has become a major bottleneck in certain applications, such as disaster monitoring and emergency mapping, which demand a fast response ability. We propose an efficient AI-enabled Satellite Edge Computing paradigm for hyperspectral image classification, facilitating the satellites to attain autonomous decision-making. To accommodate the resource constraints of satellite platforms, the proposed method adopts a lightweight, non-deep learning framework integrated with a few-shot learning strategy. Moreover, onboard processing on satellites could be faced with sensor failure and scan pattern errors, which result in degraded image quality with bad/misaligned pixels and mixed noise. To address these challenges, we develop a novel two-stage pixel-wise label propagation scheme that utilizes only intrinsic spectral features at the single pixel level without the necessity to consider spatial structural information as requested by deep neural networks. In the first stage, initial pixel labels are obtained by propagating selected anchor labels through the constructed anchor-pixel affinity matrix. Subsequently, a top-k pruned sparse graph is generated by directly computing pixel-level similarities. In the second stage, a closed-form solution derived from the sparse graph is employed to replace iterative computations. Furthermore, we developed a rank constraint-based graph clustering algorithm to determine the anchor labels.", "AI": {"tldr": "Proposes AI-enabled satellite edge computing for hyperspectral image classification using lightweight non-deep learning with few-shot learning to overcome downlink bottlenecks and enable autonomous decision-making on resource-constrained satellites.", "motivation": "Hyperspectral satellites provide valuable data but face downlink transmission bottlenecks, especially for time-sensitive applications like disaster monitoring. Onboard processing is needed for autonomous decision-making, but must overcome satellite resource constraints and image quality issues from sensor failures.", "method": "Lightweight non-deep learning framework with few-shot learning strategy. Two-stage pixel-wise label propagation using intrinsic spectral features only (no spatial information needed). First stage: propagate anchor labels via anchor-pixel affinity matrix. Second stage: use closed-form solution from top-k pruned sparse graph. Rank constraint-based graph clustering determines anchor labels.", "result": "Enables efficient onboard hyperspectral image classification on resource-constrained satellites, addressing downlink bottlenecks while handling degraded image quality from sensor failures and scan errors.", "conclusion": "The proposed AI-enabled satellite edge computing paradigm with lightweight non-deep learning and two-stage pixel-wise label propagation enables autonomous decision-making on satellites, overcoming transmission bottlenecks and resource constraints while handling real-world image degradation issues."}}
{"id": "2601.18577", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18577", "abs": "https://arxiv.org/abs/2601.18577", "authors": ["Sangwon Jang", "Taekyung Ki", "Jaehyeong Jo", "Saining Xie", "Jaehong Yoon", "Sung Ju Hwang"], "title": "Self-Refining Video Sampling", "comment": "Project page: https://agwmon.github.io/self-refine-video/", "summary": "Modern video generators still struggle with complex physical dynamics, often falling short of physical realism. Existing approaches address this using external verifiers or additional training on augmented data, which is computationally expensive and still limited in capturing fine-grained motion. In this work, we present self-refining video sampling, a simple method that uses a pre-trained video generator trained on large-scale datasets as its own self-refiner. By interpreting the generator as a denoising autoencoder, we enable iterative inner-loop refinement at inference time without any external verifier or additional training. We further introduce an uncertainty-aware refinement strategy that selectively refines regions based on self-consistency, which prevents artifacts caused by over-refinement. Experiments on state-of-the-art video generators demonstrate significant improvements in motion coherence and physics alignment, achieving over 70\\% human preference compared to the default sampler and guidance-based sampler.", "AI": {"tldr": "Self-refining video sampling improves physical realism in video generation through iterative inner-loop refinement using the generator as its own refiner, with uncertainty-aware selective refinement to prevent artifacts.", "motivation": "Modern video generators struggle with complex physical dynamics and lack physical realism. Existing approaches using external verifiers or augmented data training are computationally expensive and limited in capturing fine-grained motion.", "method": "Self-refining video sampling interprets a pre-trained video generator as a denoising autoencoder to enable iterative inner-loop refinement at inference time without external verifiers or additional training. Includes uncertainty-aware refinement strategy that selectively refines regions based on self-consistency to prevent over-refinement artifacts.", "result": "Experiments on state-of-the-art video generators show significant improvements in motion coherence and physics alignment, achieving over 70% human preference compared to default sampler and guidance-based sampler.", "conclusion": "The proposed self-refining video sampling method effectively enhances physical realism in video generation through self-contained refinement, outperforming existing approaches without requiring additional training or external verification."}}
{"id": "2601.18585", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2601.18585", "abs": "https://arxiv.org/abs/2601.18585", "authors": ["Chenxi Liu", "Selena Ling", "Alec Jacobson"], "title": "GimmBO: Interactive Generative Image Model Merging via Bayesian Optimization", "comment": null, "summary": "Fine-tuning-based adaptation is widely used to customize diffusion-based image generation, leading to large collections of community-created adapters that capture diverse subjects and styles. Adapters derived from the same base model can be merged with weights, enabling the synthesis of new visual results within a vast and continuous design space. To explore this space, current workflows rely on manual slider-based tuning, an approach that scales poorly and makes weight selection difficult, even when the candidate set is limited to 20-30 adapters. We propose GimmBO to support interactive exploration of adapter merging for image generation through Preferential Bayesian Optimization (PBO). Motivated by observations from real-world usage, including sparsity and constrained weight ranges, we introduce a two-stage BO backend that improves sampling efficiency and convergence in high-dimensional spaces. We evaluate our approach with simulated users and a user study, demonstrating improved convergence, high success rates, and consistent gains over BO and line-search baselines, and further show the flexibility of the framework through several extensions.", "AI": {"tldr": "GimmBO uses Preferential Bayesian Optimization to help users explore adapter merging for diffusion models, improving over manual slider tuning.", "motivation": "Current workflows for exploring merged adapters rely on manual slider tuning, which scales poorly and makes weight selection difficult even with limited adapter sets.", "method": "Proposes GimmBO with a two-stage Bayesian Optimization backend that improves sampling efficiency and convergence in high-dimensional spaces, motivated by real-world usage patterns like sparsity and constrained weight ranges.", "result": "Evaluation shows improved convergence, high success rates, and consistent gains over BO and line-search baselines, with framework flexibility demonstrated through extensions.", "conclusion": "GimmBO enables interactive exploration of adapter merging for image generation, addressing scalability issues of manual approaches."}}
{"id": "2601.18589", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2601.18589", "abs": "https://arxiv.org/abs/2601.18589", "authors": ["KV Karthikeya", "Ashok Kumar Das", "Shantanu Pal", "Vivekananda Bhat K", "Arun Sekar Rajasekaran"], "title": "AGSP-DSA: An Adaptive Graph Signal Processing Framework for Robust Multimodal Fusion with Dynamic Semantic Alignment", "comment": null, "summary": "In this paper, we introduce an Adaptive Graph Signal Processing with Dynamic Semantic Alignment (AGSP DSA) framework to perform robust multimodal data fusion over heterogeneous sources, including text, audio, and images. The requested approach uses a dual-graph construction to learn both intra-modal and inter-modal relations, spectral graph filtering to boost the informative signals, and effective node embedding with Multi-scale Graph Convolutional Networks (GCNs). Semantic aware attention mechanism: each modality may dynamically contribute to the context with respect to contextual relevance. The experimental outcomes on three benchmark datasets, including CMU-MOSEI, AVE, and MM-IMDB, show that AGSP-DSA performs as the state of the art. More precisely, it achieves 95.3% accuracy, 0.936 F1-score, and 0.924 mAP on CMU-MOSEI, improving MM-GNN by 2.6 percent in accuracy. It gets 93.4% accuracy and 0.911 F1-score on AVE and 91.8% accuracy and 0.886 F1-score on MM-IMDB, which demonstrate good generalization and robustness in the missing modality setting. These findings verify the efficiency of AGSP-DSA in promoting multimodal learning in sentiment analysis, event recognition and multimedia classification.", "AI": {"tldr": "AGSP-DSA framework uses dual-graph construction, spectral filtering, and semantic attention for robust multimodal fusion across text, audio, and images, achieving state-of-the-art results on sentiment analysis, event recognition, and multimedia classification tasks.", "motivation": "To address the challenge of robust multimodal data fusion over heterogeneous sources (text, audio, images) by learning both intra-modal and inter-modal relations while handling missing modalities and dynamic semantic relevance.", "method": "Adaptive Graph Signal Processing with Dynamic Semantic Alignment framework featuring: 1) dual-graph construction for intra-modal and inter-modal relations, 2) spectral graph filtering to enhance informative signals, 3) multi-scale Graph Convolutional Networks for node embedding, and 4) semantic-aware attention mechanism for dynamic modality contribution based on contextual relevance.", "result": "State-of-the-art performance on three benchmarks: CMU-MOSEI (95.3% accuracy, 0.936 F1, 0.924 mAP, improving MM-GNN by 2.6%), AVE (93.4% accuracy, 0.911 F1), and MM-IMDB (91.8% accuracy, 0.886 F1). Demonstrates strong generalization and robustness in missing modality settings.", "conclusion": "AGSP-DSA effectively promotes multimodal learning for sentiment analysis, event recognition, and multimedia classification by adaptively aligning semantics across modalities and handling heterogeneous data fusion with robustness to missing information."}}
{"id": "2601.18597", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18597", "abs": "https://arxiv.org/abs/2601.18597", "authors": ["Yu Xia", "Chang Liu", "Tianqi Xiang", "Zhigang Tu"], "title": "EFSI-DETR: Efficient Frequency-Semantic Integration for Real-Time Small Object Detection in UAV Imagery", "comment": null, "summary": "Real-time small object detection in Unmanned Aerial Vehicle (UAV) imagery remains challenging due to limited feature representation and ineffective multi-scale fusion. Existing methods underutilize frequency information and rely on static convolutional operations, which constrain the capacity to obtain rich feature representations and hinder the effective exploitation of deep semantic features. To address these issues, we propose EFSI-DETR, a novel detection framework that integrates efficient semantic feature enhancement with dynamic frequency-spatial guidance. EFSI-DETR comprises two main components: (1) a Dynamic Frequency-Spatial Unified Synergy Network (DyFusNet) that jointly exploits frequency and spatial cues for robust multi-scale feature fusion, (2) an Efficient Semantic Feature Concentrator (ESFC) that enables deep semantic extraction with minimal computational cost. Furthermore, a Fine-grained Feature Retention (FFR) strategy is adopted to incorporate spatially rich shallow features during fusion to preserve fine-grained details, crucial for small object detection in UAV imagery. Extensive experiments on VisDrone and CODrone benchmarks demonstrate that our EFSI-DETR achieves the state-of-the-art performance with real-time efficiency, yielding improvement of \\textbf{1.6}\\% and \\textbf{5.8}\\% in AP and AP$_{s}$ on VisDrone, while obtaining \\textbf{188} FPS inference speed on a single RTX 4090 GPU.", "AI": {"tldr": "EFSI-DETR: A real-time small object detection framework for UAV imagery that integrates dynamic frequency-spatial guidance with efficient semantic feature enhancement, achieving state-of-the-art performance on VisDrone and CODrone benchmarks.", "motivation": "Real-time small object detection in UAV imagery is challenging due to limited feature representation and ineffective multi-scale fusion. Existing methods underutilize frequency information and rely on static convolutional operations, constraining rich feature representation and hindering effective exploitation of deep semantic features.", "method": "Proposes EFSI-DETR with three key components: 1) Dynamic Frequency-Spatial Unified Synergy Network (DyFusNet) for joint frequency-spatial multi-scale feature fusion, 2) Efficient Semantic Feature Concentrator (ESFC) for deep semantic extraction with minimal computational cost, and 3) Fine-grained Feature Retention (FFR) strategy to preserve spatially rich shallow features crucial for small object detection.", "result": "Achieves state-of-the-art performance on VisDrone and CODrone benchmarks with 1.6% AP and 5.8% AP\u209b improvements on VisDrone, while obtaining 188 FPS inference speed on a single RTX 4090 GPU, demonstrating real-time efficiency.", "conclusion": "EFSI-DETR effectively addresses small object detection challenges in UAV imagery by integrating dynamic frequency-spatial guidance with efficient semantic enhancement, achieving superior performance with real-time inference capabilities."}}
{"id": "2601.18619", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18619", "abs": "https://arxiv.org/abs/2601.18619", "authors": ["Jorge Quesada", "Ghassan AlRegib"], "title": "Scale-Aware Self-Supervised Learning for Segmentation of Small and Sparse Structures", "comment": null, "summary": "Self-supervised learning (SSL) has emerged as a powerful strategy for representation learning under limited annotation regimes, yet its effectiveness remains highly sensitive to many factors, especially the nature of the target task. In segmentation, existing pipelines are typically tuned to large, homogeneous regions, but their performance drops when objects are small, sparse, or locally irregular. In this work, we propose a scale-aware SSL adaptation that integrates small-window cropping into the augmentation pipeline, zooming in on fine-scale structures during pretraining. We evaluate this approach across two domains with markedly different data modalities: seismic imaging, where the goal is to segment sparse faults, and neuroimaging, where the task is to delineate small cellular structures. In both settings, our method yields consistent improvements over standard and state-of-the-art baselines under label constraints, improving accuracy by up to 13% for fault segmentation and 5% for cell delineation. In contrast, large-scale features such as seismic facies or tissue regions see little benefit, underscoring that the value of SSL depends critically on the scale of the target objects. Our findings highlight the need to align SSL design with object size and sparsity, offering a general principle for buil ding more effective representation learning pipelines across scientific imaging domains.", "AI": {"tldr": "Scale-aware SSL adaptation using small-window cropping improves segmentation of small/sparse objects but not large-scale features.", "motivation": "Standard SSL methods work well for large, homogeneous regions but perform poorly on small, sparse, or irregular objects in segmentation tasks.", "method": "Proposed scale-aware SSL adaptation that integrates small-window cropping into augmentation pipeline to focus on fine-scale structures during pretraining.", "result": "Consistent improvements over standard and SOTA baselines: up to 13% for seismic fault segmentation and 5% for neuroimaging cell delineation. Large-scale features saw little benefit.", "conclusion": "SSL effectiveness depends critically on target object scale; SSL design must align with object size and sparsity for effective representation learning across scientific imaging domains."}}
{"id": "2601.18623", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18623", "abs": "https://arxiv.org/abs/2601.18623", "authors": ["Zihao Wang", "Yuzhou Chen", "Shaogang Ren"], "title": "Adaptive Domain Shift in Diffusion Models for Cross-Modality Image Translation", "comment": "Paper accepted as a conference paper at ICLR 2026", "summary": "Cross-modal image translation remains brittle and inefficient. Standard diffusion approaches often rely on a single, global linear transfer between domains. We find that this shortcut forces the sampler to traverse off-manifold, high-cost regions, inflating the correction burden and inviting semantic drift. We refer to this shared failure mode as fixed-schedule domain transfer. In this paper, we embed domain-shift dynamics directly into the generative process. Our model predicts a spatially varying mixing field at every reverse step and injects an explicit, target-consistent restoration term into the drift. This in-step guidance keeps large updates on-manifold and shifts the model's role from global alignment to local residual correction. We provide a continuous-time formulation with an exact solution form and derive a practical first-order sampler that preserves marginal consistency. Empirically, across translation tasks in medical imaging, remote sensing, and electroluminescence semantic mapping, our framework improves structural fidelity and semantic consistency while converging in fewer denoising steps.", "AI": {"tldr": "Proposes a diffusion-based image translation method with spatially varying mixing fields and target-consistent restoration to avoid off-manifold sampling and semantic drift.", "motivation": "Standard diffusion approaches for cross-modal image translation use global linear transfer between domains, forcing samplers to traverse off-manifold regions, increasing correction burden and causing semantic drift (fixed-schedule domain transfer problem).", "method": "Embeds domain-shift dynamics directly into generative process by predicting spatially varying mixing fields at every reverse step and injecting explicit target-consistent restoration term into drift. Provides continuous-time formulation with exact solution and derives practical first-order sampler preserving marginal consistency.", "result": "Improves structural fidelity and semantic consistency across translation tasks in medical imaging, remote sensing, and electroluminescence semantic mapping while converging in fewer denoising steps.", "conclusion": "The proposed approach shifts model's role from global alignment to local residual correction, keeping updates on-manifold and addressing limitations of fixed-schedule domain transfer in diffusion-based image translation."}}
{"id": "2601.18625", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18625", "abs": "https://arxiv.org/abs/2601.18625", "authors": ["Zequn Xie"], "title": "CONQUER: Context-Aware Representation with Query Enhancement for Text-Based Person Search", "comment": "Accepted by ICASSP 2026", "summary": "Text-Based Person Search (TBPS) aims to retrieve pedestrian images from large galleries using natural language descriptions. This task, essential for public safety applications, is hindered by cross-modal discrepancies and ambiguous user queries. We introduce CONQUER, a two-stage framework designed to address these challenges by enhancing cross-modal alignment during training and adaptively refining queries at inference. During training, CONQUER employs multi-granularity encoding, complementary pair mining, and context-guided optimal matching based on Optimal Transport to learn robust embeddings. At inference, a plug-and-play query enhancement module refines vague or incomplete queries via anchor selection and attribute-driven enrichment, without requiring retraining of the backbone. Extensive experiments on CUHK-PEDES, ICFG-PEDES, and RSTPReid demonstrate that CONQUER consistently outperforms strong baselines in both Rank-1 accuracy and mAP, yielding notable improvements in cross-domain and incomplete-query scenarios. These results highlight CONQUER as a practical and effective solution for real-world TBPS deployment. Source code is available at https://github.com/zqxie77/CONQUER.", "AI": {"tldr": "CONQUER is a two-stage framework for text-based person search that improves cross-modal alignment during training and adaptively refines queries at inference, achieving state-of-the-art performance across multiple datasets.", "motivation": "Text-Based Person Search (TBPS) faces challenges from cross-modal discrepancies between text and images, and ambiguous/incomplete user queries, limiting its effectiveness for real-world public safety applications.", "method": "Two-stage framework: 1) Training stage uses multi-granularity encoding, complementary pair mining, and context-guided optimal transport matching to learn robust embeddings; 2) Inference stage employs plug-and-play query enhancement module with anchor selection and attribute-driven enrichment to refine vague queries.", "result": "CONQUER consistently outperforms strong baselines on CUHK-PEDES, ICFG-PEDES, and RSTPReid datasets in both Rank-1 accuracy and mAP, with notable improvements in cross-domain and incomplete-query scenarios.", "conclusion": "CONQUER provides a practical and effective solution for real-world TBPS deployment by addressing both training-time cross-modal alignment and inference-time query refinement challenges."}}
{"id": "2601.18633", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18633", "abs": "https://arxiv.org/abs/2601.18633", "authors": ["Tong Shi", "Melonie de Almeida", "Daniela Ivanova", "Nicolas Pugeault", "Paul Henderson"], "title": "Splat-Portrait: Generalizing Talking Heads with Gaussian Splatting", "comment": null, "summary": "Talking Head Generation aims at synthesizing natural-looking talking videos from speech and a single portrait image. Previous 3D talking head generation methods have relied on domain-specific heuristics such as warping-based facial motion representation priors to animate talking motions, yet still produce inaccurate 3D avatar reconstructions, thus undermining the realism of generated animations. We introduce Splat-Portrait, a Gaussian-splatting-based method that addresses the challenges of 3D head reconstruction and lip motion synthesis. Our approach automatically learns to disentangle a single portrait image into a static 3D reconstruction represented as static Gaussian Splatting, and a predicted whole-image 2D background. It then generates natural lip motion conditioned on input audio, without any motion driven priors. Training is driven purely by 2D reconstruction and score-distillation losses, without 3D supervision nor landmarks. Experimental results demonstrate that Splat-Portrait exhibits superior performance on talking head generation and novel view synthesis, achieving better visual quality compared to previous works. Our project code and supplementary documents are public available at https://github.com/stonewalking/Splat-portrait.", "AI": {"tldr": "Splat-Portrait is a Gaussian-splatting-based method for 3D talking head generation that learns to disentangle portrait images into static 3D reconstructions and generates lip motion from audio without motion priors or 3D supervision.", "motivation": "Previous 3D talking head generation methods rely on domain-specific heuristics and warping-based facial motion priors, which produce inaccurate 3D avatar reconstructions and undermine animation realism.", "method": "Uses Gaussian splatting to automatically disentangle single portrait images into static 3D reconstructions (static Gaussian Splatting) and 2D backgrounds, then generates lip motion conditioned on audio without motion priors. Training uses only 2D reconstruction and score-distillation losses without 3D supervision or landmarks.", "result": "Experimental results show superior performance on talking head generation and novel view synthesis, achieving better visual quality compared to previous works.", "conclusion": "Splat-Portrait effectively addresses 3D head reconstruction and lip motion synthesis challenges, producing more realistic animations without relying on motion priors or 3D supervision."}}
{"id": "2601.18698", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18698", "abs": "https://arxiv.org/abs/2601.18698", "authors": ["Xiao Liu", "Jiawei Zhang"], "title": "Are Video Generation Models Geographically Fair? An Attraction-Centric Evaluation of Global Visual Knowledge", "comment": "Work in progress", "summary": "Recent advances in text-to-video generation have produced visually compelling results, yet it remains unclear whether these models encode geographically equitable visual knowledge. In this work, we investigate the geo-equity and geographically grounded visual knowledge of text-to-video models through an attraction-centric evaluation. We introduce Geo-Attraction Landmark Probing (GAP), a systematic framework for assessing how faithfully models synthesize tourist attractions from diverse regions, and construct GEOATTRACTION-500, a benchmark of 500 globally distributed attractions spanning varied regions and popularity levels. GAP integrates complementary metrics that disentangle overall video quality from attraction-specific knowledge, including global structural alignment, fine-grained keypoint-based alignment, and vision-language model judgments, all validated against human evaluation. Applying GAP to the state-of-the-art text-to-video model Sora 2, we find that, contrary to common assumptions of strong geographic bias, the model exhibits a relatively uniform level of geographically grounded visual knowledge across regions, development levels, and cultural groupings, with only weak dependence on attraction popularity. These results suggest that current text-to-video models express global visual knowledge more evenly than expected, highlighting both their promise for globally deployed applications and the need for continued evaluation as such systems evolve.", "AI": {"tldr": "Text-to-video models show surprisingly uniform geographic visual knowledge across regions, challenging assumptions of strong geographic bias.", "motivation": "To investigate whether text-to-video generation models encode geographically equitable visual knowledge, as it's unclear if current models have geographic biases in their visual representations.", "method": "Developed Geo-Attraction Landmark Probing (GAP) framework with complementary metrics (structural alignment, keypoint alignment, VLM judgments) and created GEOATTRACTION-500 benchmark of 500 globally distributed attractions to evaluate text-to-video models.", "result": "Sora 2 exhibits relatively uniform geographically grounded visual knowledge across regions, development levels, and cultural groups, with only weak dependence on attraction popularity, contrary to expectations of strong geographic bias.", "conclusion": "Current text-to-video models express global visual knowledge more evenly than expected, showing promise for globally deployed applications but requiring continued evaluation as systems evolve."}}
