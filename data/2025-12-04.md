<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 115]
- [cs.RO](#cs.RO) [Total: 32]
- [cs.AI](#cs.AI) [Total: 17]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Hierarchical Process Reward Models are Symbolic Vision Learners](https://arxiv.org/abs/2512.03126)
*Shan Zhang,Aotian Chen,Kai Zou,Jindong Gu,Yuan Xue,Anton van den Hengel*

Main category: cs.CV

TL;DR: A self-supervised symbolic auto-encoder for diagrams that uses geometric primitives and hierarchical reward modeling to achieve interpretable understanding, outperforming pixel-based and large language models on reconstruction, perception, and reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Symbolic computer vision offers interpretable diagram understanding through logical rules and structured representations, which differs fundamentally from pixel-based models that focus on textures and colors. There's a need for learning paradigms that can parse diagrams into geometric primitives and their relationships while maintaining interpretability.

Method: Proposes a self-supervised symbolic auto-encoder that encodes diagrams into structured primitives (points, lines, shapes) and their relationships, then decodes them via an executable engine. Uses Symbolic Hierarchical Process Reward Modeling with hierarchical step-level parsing rewards to enforce geometric consistency. Introduces stabilization mechanisms to address poor exploration in vanilla reinforcement learning during diagram reconstruction.

Result: Achieves 98.2% reduction in MSE for geometric diagram reconstruction, surpasses GPT-4o by 0.6% with a 7B model on chart reconstruction, improves by +13% on MathGlance perception benchmark, and by +3% on MathVerse and GeoQA reasoning benchmarks.

Conclusion: The approach effectively integrates neural network reasoning with symbolic interpretability through reasoning-grounded visual rewards, demonstrating superior performance across multiple diagram understanding tasks compared to both pixel-based models and large language models.

Abstract: Symbolic computer vision represents diagrams through explicit logical rules and structured representations, enabling interpretable understanding in machine vision. This requires fundamentally different learning paradigms from pixel-based visual models. Symbolic visual learners parse diagrams into geometric primitives-points, lines, and shapes-whereas pixel-based learners operate on textures and colors. We propose a novel self-supervised symbolic auto-encoder that encodes diagrams into structured primitives and their interrelationships within the latent space, and decodes them through our executable engine to reconstruct the input diagrams. Central to this architecture is Symbolic Hierarchical Process Reward Modeling, which applies hierarchical step-level parsing rewards to enforce point-on-line, line-on-shape, and shape-on-relation consistency. Since vanilla reinforcement learning exhibits poor exploration in the policy space during diagram reconstruction; we thus introduce stabilization mechanisms to balance exploration and exploitation. We fine-tune our symbolic encoder on downstream tasks, developing a neuro-symbolic system that integrates the reasoning capabilities of neural networks with the interpretability of symbolic models through reasoning-grounded visual rewards. Evaluations across reconstruction, perception, and reasoning tasks demonstrate the effectiveness of our approach: achieving a 98.2% reduction in MSE for geometric diagram reconstruction, surpassing GPT-4o by 0.6% with a 7B model on chart reconstruction, and improving by +13% on the MathGlance perception benchmark, and by +3% on MathVerse and GeoQA reasoning benchmarks.

</details>


### [2] [Drainage: A Unifying Framework for Addressing Class Uncertainty](https://arxiv.org/abs/2512.03182)
*Yasser Taha,Grégoire Montavon,Nils Körber*

Main category: cs.CV

TL;DR: Proposes a "drainage node" framework for handling noisy labels, class ambiguity, and OOD samples by reallocating probability mass to uncertainty while maintaining end-to-end training.


<details>
  <summary>Details</summary>
Motivation: Address challenges in deep learning with noisy labels, class ambiguity, and need to robustly reject out-of-distribution or corrupted samples in a unified manner.

Method: Add a "drainage node" at network output to reallocate probability mass toward uncertainty, preserving end-to-end training and differentiability, providing escape route for ambiguous/anomalous/noisy samples.

Result: Achieves up to 9% accuracy improvement over existing approaches in high-noise regimes on CIFAR-10/100 with instance-dependent/asymmetric noise; matches/surpasses SOTA on real-world datasets like mini-WebVision, mini-ImageNet, Clothing-1M; shows denoising effect where drainage neuron absorbs corrupt data.

Conclusion: Drainage node framework provides unified solution for noisy labels and uncertainty handling with applications beyond classification including web-scale dataset cleaning and open-set applications.

Abstract: Modern deep learning faces significant challenges with noisy labels, class ambiguity, as well as the need to robustly reject out-of-distribution or corrupted samples. In this work, we propose a unified framework based on the concept of a "drainage node'' which we add at the output of the network. The node serves to reallocate probability mass toward uncertainty, while preserving desirable properties such as end-to-end training and differentiability. This mechanism provides a natural escape route for highly ambiguous, anomalous, or noisy samples, particularly relevant for instance-dependent and asymmetric label noise. In systematic experiments involving the addition of varying proportions of instance-dependent noise or asymmetric noise to CIFAR-10/100 labels, our drainage formulation achieves an accuracy increase of up to 9\% over existing approaches in the high-noise regime. Our results on real-world datasets, such as mini-WebVision, mini-ImageNet and Clothing-1M, match or surpass existing state-of-the-art methods. Qualitative analysis reveals a denoising effect, where the drainage neuron consistently absorbs corrupt, mislabeled, or outlier data, leading to more stable decision boundaries. Furthermore, our drainage formulation enables applications well beyond classification, with immediate benefits for web-scale, semi-supervised dataset cleaning, and open-set applications.

</details>


### [3] [Does Head Pose Correction Improve Biometric Facial Recognition?](https://arxiv.org/abs/2512.03199)
*Justin Norman,Hany Farid*

Main category: cs.CV

TL;DR: AI-driven head-pose correction and image restoration can improve facial recognition accuracy when selectively applied, but naive use degrades performance.


<details>
  <summary>Details</summary>
Motivation: Biometric facial recognition models suffer accuracy drops with real-world images due to poor quality, non-frontal poses, and occlusions.

Method: Used model-agnostic forensic-evaluation pipeline to assess three restoration approaches: 3D reconstruction (NextFace), 2D frontalization (CFR-GAN), and feature enhancement (CodeFormer).

Result: Naive application of restoration techniques substantially degrades accuracy, but selective application of CFR-GAN combined with CodeFormer yields meaningful improvements.

Conclusion: Targeted, selective restoration (specifically CFR-GAN + CodeFormer) can enhance facial recognition accuracy for challenging real-world images.

Abstract: Biometric facial recognition models often demonstrate significant decreases in accuracy when processing real-world images, often characterized by poor quality, non-frontal subject poses, and subject occlusions. We investigate whether targeted, AI-driven, head-pose correction and image restoration can improve recognition accuracy. Using a model-agnostic, large-scale, forensic-evaluation pipeline, we assess the impact of three restoration approaches: 3D reconstruction (NextFace), 2D frontalization (CFR-GAN), and feature enhancement (CodeFormer). We find that naive application of these techniques substantially degrades facial recognition accuracy. However, we also find that selective application of CFR-GAN combined with CodeFormer yields meaningful improvements.

</details>


### [4] [Flux4D: Flow-based Unsupervised 4D Reconstruction](https://arxiv.org/abs/2512.03210)
*Jingkang Wang,Henry Che,Yun Chen,Ze Yang,Lily Goli,Sivabalan Manivasagam,Raquel Urtasun*

Main category: cs.CV

TL;DR: Flux4D: A simple, scalable, unsupervised framework for 4D reconstruction of large-scale dynamic scenes using 3D Gaussians and motion dynamics prediction.


<details>
  <summary>Details</summary>
Motivation: Existing differentiable rendering methods like NeRF and 3DGS have scalability limitations and require annotations for actor motion decoupling. Self-supervised methods still face per-scene optimization constraints and hyperparameter sensitivity.

Method: Directly predicts 3D Gaussians and their motion dynamics using only photometric losses and "as static as possible" regularization. Learns decomposition of dynamic elements from raw data without pre-trained models or foundational priors by training across many scenes.

Result: Enables efficient reconstruction within seconds, scales effectively to large datasets, and generalizes well to unseen environments including rare/unknown objects. Outperforms existing methods on outdoor driving datasets in scalability, generalization, and reconstruction quality.

Conclusion: Flux4D provides a simple, scalable, and fully unsupervised solution for 4D reconstruction of large-scale dynamic scenes, addressing key limitations of current methods while achieving superior performance.

Abstract: Reconstructing large-scale dynamic scenes from visual observations is a fundamental challenge in computer vision, with critical implications for robotics and autonomous systems. While recent differentiable rendering methods such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have achieved impressive photorealistic reconstruction, they suffer from scalability limitations and require annotations to decouple actor motion. Existing self-supervised methods attempt to eliminate explicit annotations by leveraging motion cues and geometric priors, yet they remain constrained by per-scene optimization and sensitivity to hyperparameter tuning. In this paper, we introduce Flux4D, a simple and scalable framework for 4D reconstruction of large-scale dynamic scenes. Flux4D directly predicts 3D Gaussians and their motion dynamics to reconstruct sensor observations in a fully unsupervised manner. By adopting only photometric losses and enforcing an "as static as possible" regularization, Flux4D learns to decompose dynamic elements directly from raw data without requiring pre-trained supervised models or foundational priors simply by training across many scenes. Our approach enables efficient reconstruction of dynamic scenes within seconds, scales effectively to large datasets, and generalizes well to unseen environments, including rare and unknown objects. Experiments on outdoor driving datasets show Flux4D significantly outperforms existing methods in scalability, generalization, and reconstruction quality.

</details>


### [5] [Object Counting with GPT-4o and GPT-5: A Comparative Study](https://arxiv.org/abs/2512.03233)
*Richard Füzesséry,Kaziwa Saleh,Sándor Szénási,Zoltán Vámossy*

Main category: cs.CV

TL;DR: Zero-shot object counting using GPT-4o and GPT-5 with only textual prompts achieves performance comparable to state-of-the-art methods on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Existing zero-shot object counting methods require annotated data and visual exemplars. Large language models (LLMs) have strong reasoning and data understanding capabilities, suggesting they could perform counting tasks without supervision using only textual prompts.

Method: Leverage visual capabilities of multi-modal LLMs (GPT-4o and GPT-5) to perform object counting in zero-shot manner using only textual prompts, without any training or visual exemplars.

Result: Both models achieve performance comparable to state-of-the-art zero-shot approaches on FSC-147 dataset, and in some cases even surpass them. Results also evaluated on CARPK dataset.

Conclusion: Multi-modal LLMs can effectively perform zero-shot object counting using only textual prompts, demonstrating their potential for unsupervised counting tasks without requiring annotated data or visual exemplars.

Abstract: Zero-shot object counting attempts to estimate the number of object instances belonging to novel categories that the vision model performing the counting has never encountered during training. Existing methods typically require large amount of annotated data and often require visual exemplars to guide the counting process. However, large language models (LLMs) are powerful tools with remarkable reasoning and data understanding abilities, which suggest the possibility of utilizing them for counting tasks without any supervision. In this work we aim to leverage the visual capabilities of two multi-modal LLMs, GPT-4o and GPT-5, to perform object counting in a zero-shot manner using only textual prompts. We evaluate both models on the FSC-147 and CARPK datasets and provide a comparative analysis. Our findings show that the models achieve performance comparable to the state-of-the-art zero-shot approaches on FSC-147, in some cases, even surpass them.

</details>


### [6] [LLM-Guided Material Inference for 3D Point Clouds](https://arxiv.org/abs/2512.03237)
*Nafiseh Izadyar,Teseo Schneider*

Main category: cs.CV

TL;DR: LLM-based zero-shot method infers material composition from 3D point clouds by decoupling object semantics from material assignment.


<details>
  <summary>Details</summary>
Motivation: Existing 3D shape datasets and models focus only on geometry, ignoring material properties that determine object appearance, creating a gap in comprehensive 3D understanding.

Method: Two-stage LLM approach: first stage predicts object semantics from point clouds, second stage assigns plausible materials to each geometric segment conditioned on inferred semantics, both operating zero-shot without task-specific training.

Result: Achieves high semantic and material plausibility across 1,000 shapes from Fusion/ABS and ShapeNet, evaluated using LLM-as-a-Judge implemented in DeepEval.

Conclusion: Language models can serve as general-purpose priors for bridging geometric reasoning and material understanding in 3D data.

Abstract: Most existing 3D shape datasets and models focus solely on geometry, overlooking the material properties that determine how objects appear. We introduce a two-stage large language model (LLM) based method for inferring material composition directly from 3D point clouds with coarse segmentations. Our key insight is to decouple reasoning about what an object is from what it is made of. In the first stage, an LLM predicts the object's semantic; in the second stage, it assigns plausible materials to each geometric segment, conditioned on the inferred semantics. Both stages operate in a zero-shot manner, without task-specific training. Because existing datasets lack reliable material annotations, we evaluate our method using an LLM-as-a-Judge implemented in DeepEval. Across 1,000 shapes from Fusion/ABS and ShapeNet, our method achieves high semantic and material plausibility. These results demonstrate that language models can serve as general-purpose priors for bridging geometric reasoning and material understanding in 3D data.

</details>


### [7] [2-Shots in the Dark: Low-Light Denoising with Minimal Data Acquisition](https://arxiv.org/abs/2512.03245)
*Liying Lu,Raphaël Achddou,Sabine Süsstrunk*

Main category: cs.CV

TL;DR: Single-image noise synthesis method using Fourier-domain spectral sampling for low-light denoising, requiring only one noisy image and one dark frame per ISO setting.


<details>
  <summary>Details</summary>
Motivation: Learning-based denoisers need large paired datasets of clean-noisy images which are difficult to collect, making noise synthesis an attractive alternative. Existing methods rely on simplified parametric models or large datasets.

Method: Proposes a practical noise synthesis method using only one noisy image and one dark frame per ISO. Uses Poisson distribution for signal-dependent noise and Fourier-domain spectral sampling algorithm to model signal-independent noise while maintaining spatial and statistical properties.

Result: The method generates diverse noise realizations that accurately model real sensor noise and leads to state-of-the-art performances on multiple low-light denoising benchmarks.

Conclusion: The proposed noise synthesis approach is accurate, practical, and outperforms competing methods without requiring large datasets or simplified parametric models.

Abstract: Raw images taken in low-light conditions are very noisy due to low photon count and sensor noise. Learning-based denoisers have the potential to reconstruct high-quality images. For training, however, these denoisers require large paired datasets of clean and noisy images, which are difficult to collect. Noise synthesis is an alternative to large-scale data acquisition: given a clean image, we can synthesize a realistic noisy counterpart. In this work, we propose a general and practical noise synthesis method that requires only one single noisy image and one single dark frame per ISO setting. We represent signal-dependent noise with a Poisson distribution and introduce a Fourier-domain spectral sampling algorithm to accurately model signal-independent noise. The latter generates diverse noise realizations that maintain the spatial and statistical properties of real sensor noise. As opposed to competing approaches, our method neither relies on simplified parametric models nor on large sets of clean-noisy image pairs. Our synthesis method is not only accurate and practical, it also leads to state-of-the-art performances on multiple low-light denoising benchmarks.

</details>


### [8] [PixPerfect: Seamless Latent Diffusion Local Editing with Discriminative Pixel-Space Refinement](https://arxiv.org/abs/2512.03247)
*Haitian Zheng,Yuan Yao,Yongsheng Yu,Yuqian Zhou,Jiebo Luo,Zhe Lin*

Main category: cs.CV

TL;DR: PixPerfect is a pixel-level refinement framework that fixes color/texture artifacts in Latent Diffusion Model edits using discriminative pixel space, artifact simulation, and direct refinement.


<details>
  <summary>Details</summary>
Motivation: Latent Diffusion Models produce high-quality image inpainting/editing but suffer from pixel-level inconsistencies like chromatic shifts, texture mismatches, and visible seams. Existing solutions fail to fully eliminate these artifacts and don't generalize well across different LDM architectures.

Method: PixPerfect uses three key components: 1) Differentiable discriminative pixel space that amplifies/suppresses subtle color/texture discrepancies, 2) Comprehensive artifact simulation pipeline exposing the refiner to realistic local editing artifacts during training, and 3) Direct pixel-space refinement scheme ensuring broad applicability across diverse latent representations and tasks.

Result: Extensive experiments on inpainting, object removal, and insertion benchmarks show PixPerfect substantially enhances perceptual fidelity and downstream editing performance, establishing a new standard for robust and high-fidelity localized image editing.

Conclusion: PixPerfect delivers seamless, high-fidelity local edits across diverse LDM architectures and tasks, addressing the fundamental pixel-level inconsistency problem in latent diffusion models.

Abstract: Latent Diffusion Models (LDMs) have markedly advanced the quality of image inpainting and local editing. However, the inherent latent compression often introduces pixel-level inconsistencies, such as chromatic shifts, texture mismatches, and visible seams along editing boundaries. Existing remedies, including background-conditioned latent decoding and pixel-space harmonization, usually fail to fully eliminate these artifacts in practice and do not generalize well across different latent representations or tasks. We introduce PixPerfect, a pixel-level refinement framework that delivers seamless, high-fidelity local edits across diverse LDM architectures and tasks. PixPerfect leverages (i) a differentiable discriminative pixel space that amplifies and suppresses subtle color and texture discrepancies, (ii) a comprehensive artifact simulation pipeline that exposes the refiner to realistic local editing artifacts during training, and (iii) a direct pixel-space refinement scheme that ensures broad applicability across diverse latent representations and tasks. Extensive experiments on inpainting, object removal, and insertion benchmarks demonstrate that PixPerfect substantially enhances perceptual fidelity and downstream editing performance, establishing a new standard for robust and high-fidelity localized image editing.

</details>


### [9] [PyroFocus: A Deep Learning Approach to Real-Time Wildfire Detection in Multispectral Remote Sensing Imagery](https://arxiv.org/abs/2512.03257)
*Mark Moussa,Andre Williams,Seth Roffe,Douglas Morton*

Main category: cs.CV

TL;DR: Two-stage deep learning pipeline (PyroFocus) for real-time wildfire classification and intensity estimation from thermal imagery, optimized for onboard edge deployment.


<details>
  <summary>Details</summary>
Motivation: Increasing wildfire frequency/severity requires low-latency, computationally efficient onboard detection methods for airborne/spaceborne missions, but high-dimensional thermal data and limited resources make real-time processing challenging.

Method: Systematic evaluation of deep learning architectures (CNNs and Transformers) for multi-class fire classification, plus PyroFocus - a two-stage pipeline that first classifies fire (no fire/active fire/post-fire) then performs FRP regression/segmentation to reduce inference time.

Result: The two-stage pipeline achieves strong speed-accuracy trade-offs, demonstrating significant potential for real-time edge deployment in wildfire monitoring missions using MASTER sensor data.

Conclusion: PyroFocus offers an effective solution for real-time wildfire detection and intensity estimation that balances computational efficiency with accuracy, suitable for future onboard deployment in monitoring systems.

Abstract: Rapid and accurate wildfire detection is crucial for emergency response and environmental management. In airborne and spaceborne missions, real-time algorithms must distinguish between no fire, active fire, and post-fire conditions, and estimate fire intensity. Multispectral and hyperspectral thermal imagers provide rich spectral information, but high data dimensionality and limited onboard resources make real-time processing challenging. As wildfires increase in frequency and severity, the need for low-latency and computationally efficient onboard detection methods is critical.
  We present a systematic evaluation of multiple deep learning architectures, including custom Convolutional Neural Networks (CNNs) and Transformer-based models, for multi-class fire classification. We also introduce PyroFocus, a two-stage pipeline that performs fire classification followed by fire radiative power (FRP) regression or segmentation to reduce inference time and computational cost for onboard deployment. Using data from NASA's MODIS/ASTER Airborne Simulator (MASTER), which is similar to a next-generation fire detection sensor, we compare accuracy, inference latency, and resource efficiency.
  Experimental results show that the proposed two-stage pipeline achieves strong trade-offs between speed and accuracy, demonstrating significant potential for real-time edge deployment in future wildfire monitoring missions.

</details>


### [10] [SpatialReasoner: Active Perception for Large-Scale 3D Scene Understanding](https://arxiv.org/abs/2512.03284)
*Hongpei Zheng,Shijie Li,Yanran Li,Hujun Yin*

Main category: cs.CV

TL;DR: H²U3D is a new 3D VQA dataset for house-scale scenes with multi-floor environments, paired with SpatialReasoner - an active perception framework that uses reinforcement learning for efficient 3D scene exploration.


<details>
  <summary>Details</summary>
Motivation: Current vision-language models struggle with spatial reasoning in large-scale 3D environments beyond room-scale scenarios, limiting their practical applications in real-world house-scale settings.

Method: Created H²U3D dataset with automated annotation pipeline for hierarchical coarse-to-fine visual representations and chain-of-thought QA pairs. Developed SpatialReasoner framework with two-stage training: supervised cold start followed by reinforcement learning with adaptive exploration reward.

Result: SpatialReasoner achieves SOTA on H²U3D, outperforming GPT-4o and Gemini-2.5-Pro. It uses only 3-4 images on average vs. baselines requiring 16+ images, demonstrating efficient exploration.

Conclusion: The coarse-to-fine active exploration paradigm enables effective spatial reasoning in large-scale 3D environments with significantly reduced visual observations, advancing practical 3D scene understanding.

Abstract: Spatial reasoning in large-scale 3D environments remains challenging for current vision-language models, which are typically constrained to room-scale scenarios. We introduce H$^2$U3D (Holistic House Understanding in 3D), a 3D visual question answering dataset designed for house-scale scene understanding. H$^2$U3D features multi-floor environments spanning up to three floors and 10-20 rooms, covering more than 300 m$^2$. Through an automated annotation pipeline, it constructs hierarchical coarse-to-fine visual representations and generates diverse question-answer pairs with chain-of-thought annotations. We further propose SpatialReasoner, an active perception framework that autonomously invokes spatial tools to explore 3D scenes based on textual queries. SpatialReasoner is trained through a two-stage strategy: a supervised cold start followed by reinforcement learning with an adaptive exploration reward that promotes efficient exploration while discouraging redundant operations. Extensive experiments demonstrate that SpatialReasoner achieves state-of-the-art performance on H$^2$U3D, outperforming strong baselines including GPT-4o and Gemini-2.5-Pro. Notably, our method attains superior results while using only 3-4 images in total on average, compared to baselines requiring 16+ images, highlighting the effectiveness of our coarse-to-fine active exploration paradigm.

</details>


### [11] [NavMapFusion: Diffusion-based Fusion of Navigation Maps for Online Vectorized HD Map Construction](https://arxiv.org/abs/2512.03317)
*Thomas Monninger,Zihan Zhang,Steffen Staab,Sihao Ding*

Main category: cs.CV

TL;DR: NavMapFusion uses diffusion models to fuse low-fidelity navigation maps with high-fidelity sensor data for real-time online HD map construction in autonomous driving.


<details>
  <summary>Details</summary>
Motivation: Traditional HD maps are static and outdated, while real-world environments constantly change. Navigation-grade SD maps are widely available but lack sufficient resolution for autonomous driving. The paper aims to leverage coarse navigation maps as priors to guide online map construction from sensor data.

Method: Proposes NavMapFusion, a diffusion-based framework that performs iterative denoising conditioned on both high-fidelity sensor data and low-fidelity navigation maps. The method treats discrepancies between prior maps and online perception as noise in the diffusion process, where consistent regions reinforce map construction while outdated segments are suppressed.

Result: On nuScenes benchmark, NavMapFusion conditioned on coarse road lines from OpenStreetMap achieves 21.4% relative improvement at 100m range, with even stronger improvements at larger perception ranges, while maintaining real-time capabilities.

Conclusion: Diffusion-based map construction provides a robust framework for map fusion, enabling accurate and up-to-date environment representations by combining low-fidelity priors with high-fidelity sensor data, leading to safer and more reliable autonomous driving.

Abstract: Accurate environmental representations are essential for autonomous driving, providing the foundation for safe and efficient navigation. Traditionally, high-definition (HD) maps are providing this representation of the static road infrastructure to the autonomous system a priori. However, because the real world is constantly changing, such maps must be constructed online from on-board sensor data. Navigation-grade standard-definition (SD) maps are widely available, but their resolution is insufficient for direct deployment. Instead, they can be used as coarse prior to guide the online map construction process. We propose NavMapFusion, a diffusion-based framework that performs iterative denoising conditioned on high-fidelity sensor data and on low-fidelity navigation maps. This paper strives to answer: (1) How can coarse, potentially outdated navigation maps guide online map construction? (2) What advantages do diffusion models offer for map fusion? We demonstrate that diffusion-based map construction provides a robust framework for map fusion. Our key insight is that discrepancies between the prior map and online perception naturally correspond to noise within the diffusion process; consistent regions reinforce the map construction, whereas outdated segments are suppressed. On the nuScenes benchmark, NavMapFusion conditioned on coarse road lines from OpenStreetMap data reaches a 21.4% relative improvement on 100 m, and even stronger improvements on larger perception ranges, while maintaining real-time capabilities. By fusing low-fidelity priors with high-fidelity sensor data, the proposed method generates accurate and up-to-date environment representations, guiding towards safer and more reliable autonomous driving. The code is available at https://github.com/tmonnin/navmapfusion

</details>


### [12] [Step-by-step Layered Design Generation](https://arxiv.org/abs/2512.03335)
*Faizan Farooq Khan,K J Joseph,Koustava Goswami,Mohamed Elhoseiny,Balaji Vasan Srinivasan*

Main category: cs.CV

TL;DR: SLEDGE: A step-by-step layered design generation framework that models design updates as atomic changes based on sequential instructions, addressing the gap between single-step generation and the iterative nature of real design processes.


<details>
  <summary>Details</summary>
Motivation: Existing approaches treat design synthesis as single-step generation, underestimating the complexity of the iterative, step-by-step creative process where designers progressively refine their work through careful modifications.

Method: Proposes Step-by-Step Layered Design Generation problem setting and SLEDGE framework that models each design update as an atomic, layered change over previous state, grounded in sequential instructions, leveraging multi-modal LLMs.

Result: Exhaustive experimental analysis and comparison with state-of-the-art approaches demonstrates the efficacy of SLEDGE, supported by a new evaluation suite including dataset and benchmark.

Conclusion: The work introduces a pragmatic and under-explored research area of step-by-step design generation, hoping to attract attention to modeling the iterative nature of creative design processes.

Abstract: Design generation, in its essence, is a step-by-step process where designers progressively refine and enhance their work through careful modifications. Despite this fundamental characteristic, existing approaches mainly treat design synthesis as a single-step generation problem, significantly underestimating the inherent complexity of the creative process. To bridge this gap, we propose a novel problem setting called Step-by-Step Layered Design Generation, which tasks a machine learning model with generating a design that adheres to a sequence of instructions from a designer. Leveraging recent advancements in multi-modal LLMs, we propose SLEDGE: Step-by-step LayEred Design GEnerator to model each update to a design as an atomic, layered change over its previous state, while being grounded in the instruction. To complement our new problem setting, we introduce a new evaluation suite, including a dataset and a benchmark. Our exhaustive experimental analysis and comparison with state-of-the-art approaches tailored to our new setup demonstrate the efficacy of our approach. We hope our work will attract attention to this pragmatic and under-explored research area.

</details>


### [13] [ProtoEFNet: Dynamic Prototype Learning for Inherently Interpretable Ejection Fraction Estimation in Echocardiography](https://arxiv.org/abs/2512.03339)
*Yeganeh Ghamary,Victoria Wu,Hooman Vaseli,Christina Luong,Teresa Tsang,Siavash Bigdeli,Purang Abolmaesumi*

Main category: cs.CV

TL;DR: ProtoEFNet is a video-based prototype learning model for continuous ejection fraction regression that provides interpretable cardiac motion patterns while maintaining accuracy comparable to black-box models.


<details>
  <summary>Details</summary>
Motivation: Traditional EF estimation requires manual tracing and expertise, making it time-consuming and variable. Current deep learning methods lack transparency, reducing clinical trust, and post-hoc explanations don't guide internal reasoning.

Method: ProtoEFNet uses video-based prototype learning with dynamic spatiotemporal prototypes that capture clinically meaningful cardiac motion patterns. It employs Prototype Angular Separation (PAS) loss to enforce discriminative representations across the continuous EF spectrum.

Result: ProtoEFNet achieves accuracy comparable to non-interpretable models on the EchonetDynamic dataset. The PAS loss boosts performance with a 2% F1 score increase from 77.67±2.68 to 79.64±2.10.

Conclusion: ProtoEFNet provides clinically relevant insights while maintaining competitive accuracy, addressing the transparency limitations of black-box EF prediction models and enhancing clinical trust.

Abstract: Ejection fraction (EF) is a crucial metric for assessing cardiac function and diagnosing conditions such as heart failure. Traditionally, EF estimation requires manual tracing and domain expertise, making the process time-consuming and subject to interobserver variability. Most current deep learning methods for EF prediction are black-box models with limited transparency, which reduces clinical trust. Some post-hoc explainability methods have been proposed to interpret the decision-making process after the prediction is made. However, these explanations do not guide the model's internal reasoning and therefore offer limited reliability in clinical applications. To address this, we introduce ProtoEFNet, a novel video-based prototype learning model for continuous EF regression. The model learns dynamic spatiotemporal prototypes that capture clinically meaningful cardiac motion patterns. Additionally, the proposed Prototype Angular Separation (PAS) loss enforces discriminative representations across the continuous EF spectrum. Our experiments on the EchonetDynamic dataset show that ProtoEFNet can achieve accuracy on par with its non-interpretable counterpart while providing clinically relevant insight. The ablation study shows that the proposed loss boosts performance with a 2% increase in F1 score from 77.67$\pm$2.68 to 79.64$\pm$2.10. Our source code is available at: https://github.com/DeepRCL/ProtoEF

</details>


### [14] [HalluGen: Synthesizing Realistic and Controllable Hallucinations for Evaluating Image Restoration](https://arxiv.org/abs/2512.03345)
*Seunghoi Kim,Henry F. J. Tregidgo,Chen Jin,Matteo Figini,Daniel C. Alexander*

Main category: cs.CV

TL;DR: HalluGen is a diffusion-based framework that generates realistic hallucinations for safety-critical image restoration, creating the first large-scale hallucination dataset to enable systematic evaluation and detection of hallucination artifacts.


<details>
  <summary>Details</summary>
Motivation: Generative models in image restoration produce plausible but incorrect structures (hallucinations) that are particularly dangerous in safety-critical domains like medical imaging, industrial inspection, and remote sensing. Current progress is hindered by the circular dependency that evaluating hallucinations requires labeled data, but such labels are costly and subjective to obtain.

Method: HalluGen is a diffusion-based framework that synthesizes realistic hallucinations with controllable type, location, and severity. It produces perceptually realistic but semantically incorrect outputs (demonstrated by segmentation IoU dropping from 0.86 to 0.36). The framework was used to construct a large-scale hallucination dataset of 4,350 annotated images from 1,450 brain MR images for low-field MRI enhancement.

Result: Created the first large-scale hallucination dataset enabling systematic evaluation. Developed SHAFE (Semantic Hallucination Assessment via Feature Evaluation), a feature-based metric with soft-attention pooling that improves hallucination sensitivity over traditional metrics. Trained reference-free hallucination detectors that generalize to real restoration failures.

Conclusion: HalluGen and its open dataset establish the first scalable foundation for evaluating hallucinations in safety-critical image restoration, addressing the critical need for systematic assessment of hallucination artifacts in domains where reliability is paramount.

Abstract: Generative models are prone to hallucinations: plausible but incorrect structures absent in the ground truth. This issue is problematic in image restoration for safety-critical domains such as medical imaging, industrial inspection, and remote sensing, where such errors undermine reliability and trust. For example, in low-field MRI, widely used in resource-limited settings, restoration models are essential for enhancing low-quality scans, yet hallucinations can lead to serious diagnostic errors. Progress has been hindered by a circular dependency: evaluating hallucinations requires labeled data, yet such labels are costly and subjective. We introduce HalluGen, a diffusion-based framework that synthesizes realistic hallucinations with controllable type, location, and severity, producing perceptually realistic but semantically incorrect outputs (segmentation IoU drops from 0.86 to 0.36). Using HalluGen, we construct the first large-scale hallucination dataset comprising 4,350 annotated images derived from 1,450 brain MR images for low-field enhancement, enabling systematic evaluation of hallucination detection and mitigation. We demonstrate its utility in two applications: (1) benchmarking image quality metrics and developing Semantic Hallucination Assessment via Feature Evaluation (SHAFE), a feature-based metric with soft-attention pooling that improves hallucination sensitivity over traditional metrics; and (2) training reference-free hallucination detectors that generalize to real restoration failures. Together, HalluGen and its open dataset establish the first scalable foundation for evaluating hallucinations in safety-critical image restoration.

</details>


### [15] [Hierarchical Attention for Sparse Volumetric Anomaly Detection in Subclinical Keratoconus](https://arxiv.org/abs/2512.03346)
*Lynn Kandakji,William Woof,Nikolas Pontikos*

Main category: cs.CV

TL;DR: Hierarchical attention models outperform 2D/3D CNNs and ViTs for detecting subtle, spatially distributed anomalies in 3D medical imaging, achieving 21-23% higher sensitivity/specificity for subclinical keratoconus detection.


<details>
  <summary>Details</summary>
Motivation: Current deep learning architectures have suboptimal inductive biases for detecting weak, spatially distributed anomalies in volumetric medical imaging: 2D/3D CNNs impose excessive locality, while Vision Transformers use unconstrained global attention, leaving the optimal structure for sparse volumetric pattern recognition unresolved.

Method: Controlled comparison of 16 modern deep learning architectures spanning 2D/3D convolutional, hybrid, and volumetric transformer families for subclinical keratoconus detection from 3D anterior segment OCT volumes, with mechanistic analyses including attention-distance measurements and representational similarity studies.

Result: Hierarchical attention models achieved 21-23% higher sensitivity and specificity in the sparse anomaly regime compared to other architectures, with mechanistic analyses showing their advantage stems from precise spatial scale alignment that matches the intermediate, multi-slice extent of subclinical abnormalities.

Conclusion: Hierarchical attention provides a superior and more parameter-efficient inductive bias for volumetric anomaly detection, establishing it as a principled approach for early pathological change analysis in 3D medical imaging, with design guidance for future systems.

Abstract: The detection of weak, spatially distributed anomalies in volumetric medical imaging remains a major challenge. The subtle, non-adjacent nature of early disease signals is often lost due to suboptimal architectural inductive biases: 2D/3D CNNs impose strong locality, while ViTs diffuse unconstrained global attention. This conflict leaves the optimal inductive structure for robust, sparse volumetric pattern recognition unresolved. This study presents a controlled comparison of sixteen modern deep learning architectures spanning 2D/3D convolutional, hybrid, and volumetric transformer families for subclinical keratoconus (SKC) detection from 3D anterior segment OCT volumes. We demonstrate that hierarchical attention models offer a superior and more parameter-efficient inductive bias, surpassing the performance of both 2D and 3D CNNs and ViTs. Our results show 21-23% higher sensitivity and specificity in the sparse anomaly (subclinical) regime. Mechanistic analyses reveal that this advantage stems from precise spatial scale alignment: hierarchical windowing produces effective receptive fields matched to the intermediate, multi-slice extent of subclinical abnormalities. This avoids excessive CNN locality and diffuse global attention. Attention-distance measurements confirm a key insight into architectural adaptation: the required spatial integration length shifts significantly based on the signal strength, with subclinical cases necessitating longer integration compared to both healthy and manifest disease states. Representational similarity and auxiliary age/sex prediction tasks further support the generalizability of these inductive principles. The findings provide design guidance for future volumetric anomaly detection systems, establishing hierarchical attention as a principled and effective approach for early pathological change analysis in 3D medical imaging.

</details>


### [16] [SeeU: Seeing the Unseen World via 4D Dynamics-aware Generation](https://arxiv.org/abs/2512.03350)
*Yu Yuan,Tharindu Wickremasinghe,Zeeshan Nadir,Xijun Wang,Yiheng Chi,Stanley H. Chan*

Main category: cs.CV

TL;DR: SeeU is a 2D→4D→2D framework that learns continuous 4D world dynamics from 2D observations to generate unseen visual content with physical consistency.


<details>
  <summary>Details</summary>
Motivation: Current visual understanding, prediction, and generation methods operate directly on 2D observations, which are discrete projections of the 4D world (3D space + time), leading to suboptimal performance.

Method: Three-stage framework: 1) Reconstruct 4D world from sparse monocular 2D frames (2D→4D), 2) Learn continuous 4D dynamics using low-rank representation and physical constraints (discrete→continuous 4D), 3) Roll world forward in time and re-project to 2D at sampled times/viewpoints with spatial-temporal context awareness (4D→2D).

Result: Achieves continuous and physically-consistent novel visual generation, demonstrating strong potential in multiple tasks including unseen temporal generation, unseen spatial generation, and video editing.

Conclusion: By modeling dynamics in 4D rather than directly on 2D observations, SeeU enables superior visual content generation with physical consistency across temporal and spatial dimensions.

Abstract: Images and videos are discrete 2D projections of the 4D world (3D space + time). Most visual understanding, prediction, and generation operate directly on 2D observations, leading to suboptimal performance. We propose SeeU, a novel approach that learns the continuous 4D dynamics and generate the unseen visual contents. The principle behind SeeU is a new 2D$\to$4D$\to$2D learning framework. SeeU first reconstructs the 4D world from sparse and monocular 2D frames (2D$\to$4D). It then learns the continuous 4D dynamics on a low-rank representation and physical constraints (discrete 4D$\to$continuous 4D). Finally, SeeU rolls the world forward in time, re-projects it back to 2D at sampled times and viewpoints, and generates unseen regions based on spatial-temporal context awareness (4D$\to$2D). By modeling dynamics in 4D, SeeU achieves continuous and physically-consistent novel visual generation, demonstrating strong potentials in multiple tasks including unseen temporal generation, unseen spatial generation, and video editing.

</details>


### [17] [A Hybrid Deep Learning Framework with Explainable AI for Lung Cancer Classification with DenseNet169 and SVM](https://arxiv.org/abs/2512.03359)
*Md Rashidul Islam,Bakary Gibba,Altagi Abdallah Bakheit Abdelgadir*

Main category: cs.CV

TL;DR: Deep learning system achieves 98% accuracy for lung cancer classification using DenseNet169 with attention mechanisms and SVM with MobileNetV2 features, enhanced by Grad-CAM and SHAP for interpretability.


<details>
  <summary>Details</summary>
Motivation: Lung cancer is highly deadly worldwide, and early diagnosis is crucial for improving patient survival rates. Manual interpretation of CT scans is time-consuming and prone to human error, necessitating automated systems for more accurate and efficient diagnosis.

Method: The study proposes a deep learning-based system using the IQOTHNCCD lung cancer dataset with CT scans categorized as Normal, Benign, and Malignant. Two approaches were developed: 1) DenseNet169 with Squeeze-and-Excitation blocks for attention-based feature extraction, Focal Loss for handling class imbalance, and Feature Pyramid Network for multi-scale feature fusion; 2) SVM model using MobileNetV2 for feature extraction. For interpretability, Grad-CAM visualizes decision-making regions in CT scans, and SHAP explains feature contributions in the SVM model.

Result: Both DenseNet169 and SVM models achieved 98% accuracy on the lung cancer classification task, demonstrating robust performance suitable for real-world medical applications.

Conclusion: The proposed deep learning system shows potential to improve lung cancer diagnosis with high accuracy, transparency, and robustness, opening possibilities for clinical implementation to enhance early detection and patient outcomes.

Abstract: Lung cancer is a very deadly disease worldwide, and its early diagnosis is crucial for increasing patient survival rates. Computed tomography (CT) scans are widely used for lung cancer diagnosis as they can give detailed lung structures. However, manual interpretation is time-consuming and prone to human error. To surmount this challenge, the study proposes a deep learning-based automatic lung cancer classification system to enhance detection accuracy and interpretability. The IQOTHNCCD lung cancer dataset is utilized, which is a public CT scan dataset consisting of cases categorized into Normal, Benign, and Malignant and used DenseNet169, which includes Squeezeand-Excitation blocks for attention-based feature extraction, Focal Loss for handling class imbalance, and a Feature Pyramid Network (FPN) for multi-scale feature fusion. In addition, an SVM model was developed using MobileNetV2 for feature extraction, improving its classification performance. For model interpretability enhancement, the study integrated Grad-CAM for the visualization of decision-making regions in CT scans and SHAP (Shapley Additive Explanations) for explanation of feature contributions within the SVM model. Intensive evaluation was performed, and it was found that both DenseNet169 and SVM models achieved 98% accuracy, suggesting their robustness for real-world medical practice. These results open up the potential for deep learning to improve the diagnosis of lung cancer by a higher level of accuracy, transparency, and robustness.

</details>


### [18] [FireSentry: A Multi-Modal Spatio-temporal Benchmark Dataset for Fine-Grained Wildfire Spread Forecasting](https://arxiv.org/abs/2512.03369)
*Nan Zhou,Huandong Wang,Jiahao Li,Han Li,Yali Song,Qiuhua Wang,Yong Li,Xinlei Chen*

Main category: cs.CV

TL;DR: FireSentry introduces a high-resolution wildfire dataset with sub-meter spatial and sub-second temporal resolution, plus FiReDiff, a dual-modality model that predicts infrared video sequences first, then segments fire masks, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing wildfire prediction research focuses on coarse spatiotemporal scales using low-resolution satellite data, which limits high-precision localized fire dynamics modeling capabilities. There's a need for fine-grained wildfire spread prediction to enhance emergency response and decision-making.

Method: 1) Created FireSentry dataset using synchronized UAV platforms with visible/infrared video streams, environmental measurements, and manually validated fire masks. 2) Proposed FiReDiff, a novel dual-modality paradigm that first predicts future video sequences in infrared modality, then precisely segments fire masks based on the generated dynamics.

Result: FiReDiff achieves state-of-the-art performance with significant gains: video quality improvements of 39.2% PSNR, 36.1% SSIM, 50.0% LPIPS, 29.4% FVD; and mask accuracy gains of 3.3% AUPRC, 59.1% F1 score, 42.9% IoU, 62.5% MSE compared to existing generative models.

Conclusion: The FireSentry benchmark dataset and FiReDiff paradigm collectively advance fine-grained wildfire forecasting and dynamic disaster simulation. The dataset is publicly available to support further research in this critical area.

Abstract: Fine-grained wildfire spread prediction is crucial for enhancing emergency response efficacy and decision-making precision. However, existing research predominantly focuses on coarse spatiotemporal scales and relies on low-resolution satellite data, capturing only macroscopic fire states while fundamentally constraining high-precision localized fire dynamics modeling capabilities. To bridge this gap, we present FireSentry, a provincial-scale multi-modal wildfire dataset characterized by sub-meter spatial and sub-second temporal resolution. Collected using synchronized UAV platforms, FireSentry provides visible and infrared video streams, in-situ environmental measurements, and manually validated fire masks. Building on FireSentry, we establish a comprehensive benchmark encompassing physics-based, data-driven, and generative models, revealing the limitations of existing mask-only approaches. Our analysis proposes FiReDiff, a novel dual-modality paradigm that first predicts future video sequences in the infrared modality, and then precisely segments fire masks in the mask modality based on the generated dynamics. FiReDiff achieves state-of-the-art performance, with video quality gains of 39.2% in PSNR, 36.1% in SSIM, 50.0% in LPIPS, 29.4% in FVD, and mask accuracy gains of 3.3% in AUPRC, 59.1% in F1 score, 42.9% in IoU, and 62.5% in MSE when applied to generative models. The FireSentry benchmark dataset and FiReDiff paradigm collectively advance fine-grained wildfire forecasting and dynamic disaster simulation. The processed benchmark dataset is publicly available at: https://github.com/Munan222/FireSentry-Benchmark-Dataset.

</details>


### [19] [ShelfGaussian: Shelf-Supervised Open-Vocabulary Gaussian-based 3D Scene Understanding](https://arxiv.org/abs/2512.03370)
*Lingjun Zhao,Yandong Luo,James Hay,Lu Gan*

Main category: cs.CV

TL;DR: ShelfGaussian is an open-vocabulary multi-modal Gaussian-based 3D scene understanding framework that leverages vision foundation models for supervision, achieving state-of-the-art zero-shot semantic occupancy prediction.


<details>
  <summary>Details</summary>
Motivation: Existing Gaussian-based methods have limitations: closed-set semantic Gaussians require annotated 3D labels and neglect rendering ability, while open-set Gaussian representations use purely 2D self-supervision leading to degraded geometry and limited to camera-only settings. There's a need to fully exploit Gaussian potential for comprehensive 3D scene understanding.

Method: Proposes Multi-Modal Gaussian Transformer enabling Gaussians to query features from diverse sensor modalities, and Shelf-Supervised Learning Paradigm that efficiently optimizes Gaussians with vision foundation model features jointly at both 2D image and 3D scene levels.

Result: Achieves state-of-the-art zero-shot semantic occupancy prediction performance on Occ3D-nuScenes benchmark. Successfully evaluated on unmanned ground vehicle (UGV) across diverse urban scenarios, demonstrating strong in-the-wild performance.

Conclusion: ShelfGaussian effectively bridges the gap between closed-set and open-set Gaussian methods by leveraging vision foundation models for multi-modal 3D scene understanding, enabling superior performance in both perception and planning tasks across diverse real-world scenarios.

Abstract: We introduce ShelfGaussian, an open-vocabulary multi-modal Gaussian-based 3D scene understanding framework supervised by off-the-shelf vision foundation models (VFMs). Gaussian-based methods have demonstrated superior performance and computational efficiency across a wide range of scene understanding tasks. However, existing methods either model objects as closed-set semantic Gaussians supervised by annotated 3D labels, neglecting their rendering ability, or learn open-set Gaussian representations via purely 2D self-supervision, leading to degraded geometry and limited to camera-only settings. To fully exploit the potential of Gaussians, we propose a Multi-Modal Gaussian Transformer that enables Gaussians to query features from diverse sensor modalities, and a Shelf-Supervised Learning Paradigm that efficiently optimizes Gaussians with VFM features jointly at 2D image and 3D scene levels. We evaluate ShelfGaussian on various perception and planning tasks. Experiments on Occ3D-nuScenes demonstrate its state-of-the-art zero-shot semantic occupancy prediction performance. ShelfGaussian is further evaluated on an unmanned ground vehicle (UGV) to assess its in the-wild performance across diverse urban scenarios. Project website: https://lunarlab-gatech.github.io/ShelfGaussian/.

</details>


### [20] [MOS: Mitigating Optical-SAR Modality Gap for Cross-Modal Ship Re-Identification](https://arxiv.org/abs/2512.03404)
*Yujian Zhao,Hankun Liu,Guanglin Niu*

Main category: cs.CV

TL;DR: MOS framework reduces optical-SAR modality gap for ship re-identification using modality-consistent representation learning and cross-modal data generation with diffusion models.


<details>
  <summary>Details</summary>
Motivation: Cross-modal ship ReID between optical and SAR imagery is critical for maritime surveillance but challenging due to substantial modality gap between optical and SAR images.

Method: Two components: (1) Modality-Consistent Representation Learning with denoised SAR processing and class-wise modality alignment loss; (2) Cross-modal Data Generation and Feature fusion using brownian bridge diffusion model to synthesize cross-modal samples for feature fusion.

Result: Significantly surpasses state-of-the-art on HOSS ReID dataset with +3.0%, +6.2%, and +16.4% R1 accuracy improvements under ALL to ALL, Optical to SAR, and SAR to Optical settings respectively.

Conclusion: MOS effectively mitigates optical-SAR modality gap and achieves modality-consistent feature learning for robust cross-modal ship re-identification.

Abstract: Cross-modal ship re-identification (ReID) between optical and synthetic aperture radar (SAR) imagery has recently emerged as a critical yet underexplored task in maritime intelligence and surveillance. However, the substantial modality gap between optical and SAR images poses a major challenge for robust identification. To address this issue, we propose MOS, a novel framework designed to mitigate the optical-SAR modality gap and achieve modality-consistent feature learning for optical-SAR cross-modal ship ReID. MOS consists of two core components: (1) Modality-Consistent Representation Learning (MCRL) applies denoise SAR image procession and a class-wise modality alignment loss to align intra-identity feature distributions across modalities. (2) Cross-modal Data Generation and Feature fusion (CDGF) leverages a brownian bridge diffusion model to synthesize cross-modal samples, which are subsequently fused with original features during inference to enhance alignment and discriminability. Extensive experiments on the HOSS ReID dataset demonstrate that MOS significantly surpasses state-of-the-art methods across all evaluation protocols, achieving notable improvements of +3.0%, +6.2%, and +16.4% in R1 accuracy under the ALL to ALL, Optical to SAR, and SAR to Optical settings, respectively. The code and trained models will be released upon publication.

</details>


### [21] [ViDiC: Video Difference Captioning](https://arxiv.org/abs/2512.03405)
*Jiangtao Wu,Shihao Li,Zhaozhou Bian,Yuanxing Zhang,Jialu Chen,Runzhe Wen,An Ping,Yiwen He,Jiakai Wang,Jiaheng Liu*

Main category: cs.CV

TL;DR: ViDiC introduces Video Difference Captioning task and ViDiC-1K dataset to evaluate MLLMs' ability to describe similarities/differences between video pairs, revealing significant performance gaps in existing models.


<details>
  <summary>Details</summary>
Motivation: Existing vision-language systems lack capability for comparative perception of compositional, spatial, and temporal changes in dynamic scenes. While Image Difference Captioning exists for static images, it fails to capture motion continuity, event evolution, or editing consistency over time in videos.

Method: Introduces ViDiC task and ViDiC-1K dataset with 1,000 curated video pairs annotated with over 4,000 comparative checklist items across 7 categories. Proposes dual-checklist framework using LLM-as-a-Judge protocol to separately measure similarity and difference accuracy.

Result: Experiments on 19 representative multimodal models reveal significant performance gap in their comparative description and difference perception abilities, showing current limitations in video understanding and comparative reasoning.

Conclusion: ViDiC-1K serves as a challenging benchmark to advance video understanding, edit awareness, and comparative reasoning in multimodal intelligence, laying foundation for improved video difference captioning capabilities.

Abstract: Understanding visual differences between dynamic scenes requires the comparative perception of compositional, spatial, and temporal changes--a capability that remains underexplored in existing vision-language systems. While prior work on Image Difference Captioning (IDC) has enabled models to describe semantic changes between static images, these approaches fail to capture motion continuity, event evolution, or editing consistency over time. We introduce the ViDiC (Video Difference Captioning) task and its corresponding ViDiC-1K dataset, designed to evaluate the ability of Multimodal Large Language Models (MLLMs) to provide fine-grained descriptions of similarities and differences between video pairs. ViDiC-1K comprises 1,000 curated video pairs annotated with over 4,000 comparative checklist items, covering seven categories: subject, style, background, cinematography, motion, location, and playback techniques. To ensure reliable evaluation, we propose a dual-checklist framework that measures the accuracy of similarity and difference separately, based on the LLM-as-a-Judge protocol. Experiments on nineteen representative multimodal models reveal a significant performance gap in their comparative description and difference perception abilities. We hope ViDiC-1K can be a challenging benchmark that lays a solid foundation for advancing video understanding, edit awareness, and comparative reasoning in multimodal intelligence.

</details>


### [22] [YOLOA: Real-Time Affordance Detection via LLM Adapter](https://arxiv.org/abs/2512.03418)
*Yuqi Ji,Junjie Ke,Lihuo He,Jun Liu,Kaifan Zhang,Yu-Kun Lai,Guiguang Ding,Xinbo Gao*

Main category: cs.CV

TL;DR: YOLOA is a real-time affordance detection model that jointly handles object detection and affordance learning using an LLM adapter, achieving state-of-the-art accuracy with real-time performance.


<details>
  <summary>Details</summary>
Motivation: Current affordance detection methods either focus only on "how" objects can be used while neglecting "what" and "where" aspects, or treat object detection and affordance learning as independent tasks without effective interaction and real-time capability.

Method: YOLOA uses a lightweight detector with object detection and affordance learning branches refined through an LLM adapter. The LLM adapter interacts with preliminary predictions to generate more accurate class priors, box offsets, and affordance gates during training.

Result: Achieves state-of-the-art accuracy (52.8 mAP on ADG-Det, 73.1 mAP on IIT-Heat) while maintaining real-time performance (up to 89.77 FPS, up to 846.24 FPS for lightweight variant).

Conclusion: YOLOA successfully addresses the "what-where-how" challenge in affordance detection by jointly handling object detection and affordance learning with effective interaction through an LLM adapter, achieving an excellent trade-off between accuracy and efficiency.

Abstract: Affordance detection aims to jointly address the fundamental "what-where-how" challenge in embodied AI by understanding "what" an object is, "where" the object is located, and "how" it can be used. However, most affordance learning methods focus solely on "how" objects can be used while neglecting the "what" and "where" aspects. Other affordance detection methods treat object detection and affordance learning as two independent tasks, lacking effective interaction and real-time capability. To overcome these limitations, we introduce YOLO Affordance (YOLOA), a real-time affordance detection model that jointly handles these two tasks via a large language model (LLM) adapter. Specifically, YOLOA employs a lightweight detector consisting of object detection and affordance learning branches refined through the LLM Adapter. During training, the LLM Adapter interacts with object and affordance preliminary predictions to refine both branches by generating more accurate class priors, box offsets, and affordance gates. Experiments on our relabeled ADG-Det and IIT-Heat benchmarks demonstrate that YOLOA achieves state-of-the-art accuracy (52.8 / 73.1 mAP on ADG-Det / IIT-Heat) while maintaining real-time performance (up to 89.77 FPS, and up to 846.24 FPS for the lightweight variant). This indicates that YOLOA achieves an excellent trade-off between accuracy and efficiency.

</details>


### [23] [DM3D: Deformable Mamba via Offset-Guided Gaussian Sequencing for Point Cloud Understanding](https://arxiv.org/abs/2512.03424)
*Bin Liu,Chunyang Wang,Xuelian Liu*

Main category: cs.CV

TL;DR: DM3D is a deformable Mamba architecture for point cloud understanding that introduces adaptive serialization to overcome SSMs' reliance on input order, achieving SOTA performance across multiple tasks.


<details>
  <summary>Details</summary>
Motivation: State Space Models (SSMs) show promise for long-sequence modeling but depend on input order, which conflicts with the irregular nature of point clouds. Existing approaches use fixed serialization strategies that can't adapt to diverse geometric structures.

Method: Proposes DM3D with: 1) Offset-guided Gaussian sequencing unifying local resampling and global reordering; 2) Gaussian-based KNN Resampling (GKR) for adaptive neighborhood reorganization; 3) Gaussian-based Differentiable Reordering (GDR) for end-to-end serialization optimization; 4) Tri-Path Frequency Fusion for feature complementarity and aliasing reduction.

Result: Extensive experiments show DM3D achieves state-of-the-art performance on benchmark datasets for classification, few-shot learning, and part segmentation.

Conclusion: Adaptive serialization effectively unlocks SSMs' potential for point cloud understanding, with DM3D demonstrating that structure-aware sequencing is crucial for handling irregular point cloud data.

Abstract: State Space Models (SSMs) demonstrate significant potential for long-sequence modeling, but their reliance on input order conflicts with the irregular nature of point clouds. Existing approaches often rely on predefined serialization strategies, which cannot adjust based on diverse geometric structures. To overcome this limitation, we propose \textbf{DM3D}, a deformable Mamba architecture for point cloud understanding. Specifically, DM3D introduces an offset-guided Gaussian sequencing mechanism that unifies local resampling and global reordering within a deformable scan. The Gaussian-based KNN Resampling (GKR) enhances structural awareness by adaptively reorganizing neighboring points, while the Gaussian-based Differentiable Reordering (GDR) enables end-to-end optimization of serialization order. Furthermore, a Tri-Path Frequency Fusion module enhances feature complementarity and reduces aliasing. Together, these components enable structure-adaptive serialization of point clouds. Extensive experiments on benchmark datasets show that DM3D achieves state-of-the-art performance in classification, few-shot learning, and part segmentation, demonstrating that adaptive serialization effectively unlocks the potential of SSMs for point cloud understanding.

</details>


### [24] [Multi-Aspect Knowledge-Enhanced Medical Vision-Language Pretraining with Multi-Agent Data Generation](https://arxiv.org/abs/2512.03445)
*Xieji Li,Siyuan Yan,Yingsheng Liu,H. Peter Soyer,Monika Janda,Victoria Mar,Zongyuan Ge*

Main category: cs.CV

TL;DR: A novel vision-language pretraining framework for medical imaging that uses multi-agent data generation and ontology-based knowledge enhancement to handle noisy web data and unstructured medical texts, achieving SOTA zero-shot performance in dermatology tasks.


<details>
  <summary>Details</summary>
Motivation: Existing VLP methods struggle with noise in web-collected medical data and the complexity of unstructured long medical texts, limiting their effectiveness in medical image analysis.

Method: Two-stage approach: 1) MAGEN system for data quality enhancement via foundation model-assisted captioning and retrieval-based verification, 2) O-MAKE pretraining that decomposes long texts into knowledge aspects for fine-grained alignment and ontology-guided concept modeling.

Result: Achieves state-of-the-art zero-shot performance on disease classification and cross-modal retrieval across eight dermatology datasets, with release of Derm1M-AgentAug dataset containing over 400k skin-image-text pairs.

Conclusion: The proposed framework effectively addresses data quality and text complexity challenges in medical VLP, demonstrating strong performance in dermatology applications with potential for broader medical imaging domains.

Abstract: Vision-language pretraining (VLP) has emerged as a powerful paradigm in medical image analysis, enabling representation learning from large-scale image-text pairs without relying on expensive manual annotations. However, existing methods often struggle with the noise inherent in web-collected data and the complexity of unstructured long medical texts. To address these challenges, we propose a novel VLP framework integrating a Multi-Agent data GENeration (MAGEN) system and Ontology-based Multi-Aspect Knowledge-Enhanced (O-MAKE) pretraining. First, MAGEN enhances data quality by synthesizing knowledge-enriched descriptions via a foundation model-assisted captioning and retrieval-based verification pipeline. Second, O-MAKE addresses the difficulty of learning from long, unstructured texts by decomposing them into distinct knowledge aspects. This facilitates fine-grained alignment at both global and patch levels, while explicitly modeling medical concept relationships through ontology-guided mechanisms. We validate our framework in the field of dermatology, where comprehensive experiments demonstrate the effectiveness of each component. Our approach achieves state-of-the-art zero-shot performance on disease classification and cross-modal retrieval tasks across eight datasets. Our code and the augmented dataset Derm1M-AgentAug, comprising over 400k skin-image-text pairs, will be released at https://github.com/SiyuanYan1/Derm1M.

</details>


### [25] [Generalization Evaluation of Deep Stereo Matching Methods for UAV-Based Forestry Applications](https://arxiv.org/abs/2512.03427)
*Yida Lin,Bing Xue,Mengjie Zhang,Sam Schofield,Richard Green*

Main category: cs.CV

TL;DR: First systematic zero-shot evaluation of 8 stereo depth estimation methods for UAV forestry operations, revealing scene-dependent performance patterns and identifying DEFOM as optimal for vegetation-dense environments.


<details>
  <summary>Details</summary>
Motivation: Autonomous UAV forestry operations need robust depth estimation with strong cross-domain generalization, but existing evaluations focus on urban/indoor scenarios, leaving a critical gap for specialized vegetation-dense environments.

Method: Systematic zero-shot evaluation of 8 state-of-the-art stereo methods (RAFT-Stereo, IGEV, IGEV++, BridgeDepth, StereoAnywhere, DEFOM, ACVNet, PSMNet, TCstereo) trained exclusively on Scene Flow and evaluated without fine-tuning on 4 standard benchmarks plus novel 5,313-pair Canterbury forestry dataset captured with ZED Mini camera.

Result: Foundation models excel on structured scenes (BridgeDepth: 0.23 px on ETH3D; DEFOM: 0.35-4.65 px across benchmarks), iterative methods maintain cross-domain robustness (IGEV++: 0.36-6.77 px). Critical finding: RAFT-Stereo exhibits catastrophic ETH3D failure (26.23 px EPE, 98% error rate) due to negative disparity predictions. DEFOM identified as optimal for forestry with superior depth smoothness, occlusion handling, and cross-domain consistency.

Conclusion: DEFOM emerges as the gold-standard baseline for vegetation depth estimation in forestry applications, demonstrating the importance of specialized evaluation for domain-specific autonomous operations and revealing critical performance variations across different scene types.

Abstract: Autonomous UAV forestry operations require robust depth estimation methods with strong cross-domain generalization. However, existing evaluations focus on urban and indoor scenarios, leaving a critical gap for specialized vegetation-dense environments. We present the first systematic zero-shot evaluation of eight state-of-the-art stereo methods--RAFT-Stereo, IGEV, IGEV++, BridgeDepth, StereoAnywhere, DEFOM (plus baseline methods ACVNet, PSMNet, TCstereo)--spanning iterative refinement, foundation model, and zero-shot adaptation paradigms. All methods are trained exclusively on Scene Flow and evaluated without fine-tuning on four standard benchmarks (ETH3D, KITTI 2012/2015, Middlebury) plus a novel 5,313-pair Canterbury forestry dataset captured with ZED Mini camera (1920x1080). Performance reveals scene-dependent patterns: foundation models excel on structured scenes (BridgeDepth: 0.23 px on ETH3D, 0.83-1.07 px on KITTI; DEFOM: 0.35-4.65 px across benchmarks), while iterative methods maintain cross-domain robustness (IGEV++: 0.36-6.77 px; IGEV: 0.33-21.91 px). Critical finding: RAFT-Stereo exhibits catastrophic ETH3D failure (26.23 px EPE, 98 percent error rate) due to negative disparity predictions, while performing normally on KITTI (0.90-1.11 px). Qualitative evaluation on Canterbury forestry dataset identifies DEFOM as the optimal gold-standard baseline for vegetation depth estimation, exhibiting superior depth smoothness, occlusion handling, and cross-domain consistency compared to IGEV++, despite IGEV++'s finer detail preservation.

</details>


### [26] [GalaxyDiT: Efficient Video Generation with Guidance Alignment and Adaptive Proxy in Diffusion Transformers](https://arxiv.org/abs/2512.03451)
*Zhiye Song,Steve Dai,Ben Keller,Brucek Khailany*

Main category: cs.CV

TL;DR: GalaxyDiT is a training-free method that accelerates video diffusion models by 1.87-2.37× with minimal quality loss, using guidance alignment and systematic proxy selection for computational reuse.


<details>
  <summary>Details</summary>
Motivation: Current diffusion models for video generation (DiTs with CFG) require intensive computation - dozens of iterative steps per video, with CFG doubling compute requirements. This inefficiency hinders broader adoption in downstream applications.

Method: GalaxyDiT uses training-free acceleration through guidance alignment and systematic proxy selection for reuse metrics. It identifies optimal proxies for each video model via rank-order correlation analysis, ensuring optimal computational reuse across model families and parameter scales.

Result: Achieves 1.87× speedup on Wan2.1-1.3B and 2.37× on Wan2.1-14B with only 0.97% and 0.72% drops on VBench-2.0. Maintains superior fidelity at high speedup rates, exceeding prior SOTA by 5-10 dB in PSNR.

Conclusion: GalaxyDiT provides an effective training-free acceleration method for video diffusion models that significantly reduces computational requirements while maintaining high quality, enabling broader adoption in applications.

Abstract: Diffusion models have revolutionized video generation, becoming essential tools in creative content generation and physical simulation. Transformer-based architectures (DiTs) and classifier-free guidance (CFG) are two cornerstones of this success, enabling strong prompt adherence and realistic video quality. Despite their versatility and superior performance, these models require intensive computation. Each video generation requires dozens of iterative steps, and CFG doubles the required compute. This inefficiency hinders broader adoption in downstream applications.
  We introduce GalaxyDiT, a training-free method to accelerate video generation with guidance alignment and systematic proxy selection for reuse metrics. Through rank-order correlation analysis, our technique identifies the optimal proxy for each video model, across model families and parameter scales, thereby ensuring optimal computational reuse. We achieve $1.87\times$ and $2.37\times$ speedup on Wan2.1-1.3B and Wan2.1-14B with only 0.97% and 0.72% drops on the VBench-2.0 benchmark. At high speedup rates, our approach maintains superior fidelity to the base model, exceeding prior state-of-the-art approaches by 5 to 10 dB in peak signal-to-noise ratio (PSNR).

</details>


### [27] [Label-Efficient Hyperspectral Image Classification via Spectral FiLM Modulation of Low-Level Pretrained Diffusion Features](https://arxiv.org/abs/2512.03430)
*Yuzhen Hu,Biplab Banerjee,Saurabh Prasad*

Main category: cs.CV

TL;DR: A label-efficient hyperspectral image classification framework using frozen diffusion model features and spectral-spatial fusion with FiLM modules.


<details>
  <summary>Details</summary>
Motivation: Hyperspectral imaging faces challenges with low spatial resolution and sparse annotations, requiring label-efficient methods that can leverage pretrained models for better feature extraction.

Method: Uses frozen diffusion model pretrained on natural images to extract spatial features from early denoising timesteps, then integrates spectral information via lightweight FiLM-based fusion module for adaptive multimodal learning under sparse supervision.

Result: Outperforms state-of-the-art approaches on two recent hyperspectral datasets using only sparse training labels, with ablation studies confirming benefits of diffusion-derived features and spectral-aware fusion.

Conclusion: Pretrained diffusion models can enable domain-agnostic, label-efficient representation learning for remote sensing and broader scientific imaging applications.

Abstract: Hyperspectral imaging (HSI) enables detailed land cover classification, yet low spatial resolution and sparse annotations pose significant challenges. We present a label-efficient framework that leverages spatial features from a frozen diffusion model pretrained on natural images. Our approach extracts low-level representations from high-resolution decoder layers at early denoising timesteps, which transfer effectively to the low-texture structure of HSI. To integrate spectral and spatial information, we introduce a lightweight FiLM-based fusion module that adaptively modulates frozen spatial features using spectral cues, enabling robust multimodal learning under sparse supervision. Experiments on two recent hyperspectral datasets demonstrate that our method outperforms state-of-the-art approaches using only the provided sparse training labels. Ablation studies further highlight the benefits of diffusion-derived features and spectral-aware fusion. Overall, our results indicate that pretrained diffusion models can support domain-agnostic, label-efficient representation learning for remote sensing and broader scientific imaging tasks.

</details>


### [28] [Think Before You Drive: World Model-Inspired Multimodal Grounding for Autonomous Vehicles](https://arxiv.org/abs/2512.03454)
*Haicheng Liao,Huanming Shen,Bonan Wang,Yongkang Li,Yihong Tang,Chengyue Wang,Dingyi Zhuang,Kehua Chen,Hai Yang,Chengzhong Xu,Zhenning Li*

Main category: cs.CV

TL;DR: ThinkDeeper is a visual grounding framework for autonomous driving that uses a spatial-aware world model to reason about future scene states before localizing objects, achieving state-of-the-art performance across multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing visual grounding methods for autonomous vehicles struggle with ambiguous, context-dependent instructions because they lack reasoning over 3D spatial relations and anticipated scene evolution. Current approaches don't consider how scenes will change over time, which is critical for disambiguation in dynamic driving environments.

Method: Proposes ThinkDeeper with two key components: 1) Spatial-Aware World Model (SA-WM) that distills current scene into command-aware latent states and rolls out future latent states for forward-looking reasoning, and 2) Hypergraph-guided decoder that hierarchically fuses these states with multimodal input to capture higher-order spatial dependencies. Also introduces DrivePilot dataset created using RAG and CoT-prompted LLM pipeline.

Result: ThinkDeeper ranks #1 on Talk2Car leaderboard and surpasses state-of-the-art baselines on DrivePilot, MoCAD, and RefCOCO/+/g benchmarks. Shows strong robustness in challenging scenes (long-text, multi-agent, ambiguity) and retains superior performance even when trained on 50% of data.

Conclusion: The framework demonstrates that reasoning about future spatial states before making grounding decisions significantly improves visual grounding performance in autonomous driving, with strong generalization across diverse benchmarks and challenging scenarios.

Abstract: Interpreting natural-language commands to localize target objects is critical for autonomous driving (AD). Existing visual grounding (VG) methods for autonomous vehicles (AVs) typically struggle with ambiguous, context-dependent instructions, as they lack reasoning over 3D spatial relations and anticipated scene evolution. Grounded in the principles of world models, we propose ThinkDeeper, a framework that reasons about future spatial states before making grounding decisions. At its core is a Spatial-Aware World Model (SA-WM) that learns to reason ahead by distilling the current scene into a command-aware latent state and rolling out a sequence of future latent states, providing forward-looking cues for disambiguation. Complementing this, a hypergraph-guided decoder then hierarchically fuses these states with the multimodal input, capturing higher-order spatial dependencies for robust localization. In addition, we present DrivePilot, a multi-source VG dataset in AD, featuring semantic annotations generated by a Retrieval-Augmented Generation (RAG) and Chain-of-Thought (CoT)-prompted LLM pipeline. Extensive evaluations on six benchmarks, ThinkDeeper ranks #1 on the Talk2Car leaderboard and surpasses state-of-the-art baselines on DrivePilot, MoCAD, and RefCOCO/+/g benchmarks. Notably, it shows strong robustness and efficiency in challenging scenes (long-text, multi-agent, ambiguity) and retains superior performance even when trained on 50% of the data.

</details>


### [29] [Text-Printed Image: Bridging the Image-Text Modality Gap for Text-centric Training of Large Vision-Language Models](https://arxiv.org/abs/2512.03463)
*Shojiro Yamabe,Futa Waseda,Daiki Shiono,Tsubasa Takahashi*

Main category: cs.CV

TL;DR: TPI (Text-Printed Image) enables effective text-centric training for LVLMs by rendering text on white canvases, bridging the modality gap without real images.


<details>
  <summary>Details</summary>
Motivation: Task-specific fine-tuning for LVLMs requires costly image-text pairs. Text is more available and editable than images, but training on raw text alone yields limited gains due to the image-text modality gap.

Method: Propose Text-Printed Image (TPI) - generating synthetic images by directly rendering textual descriptions on plain white canvases. This simple rendering projects text into image modality and preserves text semantics better than text-to-image models.

Result: Across four models and seven benchmarks, TPI enables more effective text-centric training than synthetic images from diffusion models. Also works as low-cost data-augmentation strategy with practical utility.

Conclusion: Text-centric training with TPI shows significant potential and charts a path toward fully automated data generation for LVLMs, offering scalable, low-cost alternative to image collection.

Abstract: Recent large vision-language models (LVLMs) have been applied to diverse VQA tasks. However, achieving practical performance typically requires task-specific fine-tuning with large numbers of image-text pairs, which are costly to collect. In this work, we study text-centric training, a setting where only textual descriptions are available and no real images are provided, as a paradigm for low-cost data scaling. Unlike images, whose collection is often restricted by privacy constraints and scarcity in niche domains, text is widely available. Moreover, text is easily editable, enabling automatic diversification and expansion with LLMs at minimal human effort. While this offers clear advantages over image collection in terms of scalability and cost, training on raw text without images still yields limited gains on VQA tasks because of the image-text modality gap. To address this issue, we propose a Text-Printed Image (TPI), which generates synthetic images by directly rendering the given textual description on a plain white canvas. This simple rendering projects text into the image modality and can be integrated into arbitrary existing LVLM training pipelines at low cost. Moreover, TPI preserves the semantics of the text, whereas text-to-image models often fail to do. Across four models and seven benchmarks, our systematic experiments show that TPI enables more effective text-centric training than synthetic images generated by a diffusion model. We further explore TPI as a low-cost data-augmentation strategy and demonstrate its practical utility. Overall, our findings highlight the significant potential of text-centric training and, more broadly, chart a path toward fully automated data generation for LVLMs.

</details>


### [30] [LM-CartSeg: Automated Segmentation of Lateral and Medial Cartilage and Subchondral Bone for Radiomics Analysis](https://arxiv.org/abs/2512.03449)
*Tongxu Zhang*

Main category: cs.CV

TL;DR: LM-CartSeg is an automatic pipeline for knee MRI cartilage/bone segmentation, lateral/medial compartmentalization, and radiomics analysis, achieving high accuracy and providing quality-controlled ROIs for multi-center OA studies.


<details>
  <summary>Details</summary>
Motivation: Existing knee MRI radiomics work relies on manual ROIs with poor quality control reporting, requiring robust automatic segmentation that captures both cartilage and subchondral bone in anatomically meaningful compartments.

Method: Two 3D nnU-Net models trained on different datasets, with zero-shot predictions fused and refined using geometric rules: connected-component cleaning, 10mm subchondral bone band construction, and data-driven tibial lateral/medial split using PCA and k-means clustering.

Result: Post-processing improved segmentation accuracy dramatically (ASSD: 2.63→0.36mm, HD95: 25.2→3.35mm, DSC: 0.91). Geometric compartmentalization was stable across datasets, and only 6-12% of radiomic features were strongly correlated with size/thickness.

Conclusion: LM-CartSeg provides automatic, quality-controlled ROIs and radiomic features with discriminative information beyond simple morphometry, offering a practical foundation for multi-center knee osteoarthritis radiomics studies.

Abstract: Background and Objective: Radiomics of knee MRI requires robust, anatomically meaningful regions of interest (ROIs) that jointly capture cartilage and subchondral bone. Most existing work relies on manual ROIs and rarely reports quality control (QC). We present LM-CartSeg, a fully automatic pipeline for cartilage/bone segmentation, geometric lateral/medial (L/M) compartmentalisation and radiomics analysis. Methods: Two 3D nnU-Net models were trained on SKM-TEA (138 knees) and OAIZIB-CM (404 knees). At test time, zero-shot predictions were fused and refined by simple geometric rules: connected-component cleaning, construction of 10 mm subchondral bone bands in physical space, and a data-driven tibial L/M split based on PCA and k-means. Segmentation was evaluated on an OAIZIB-CM test set (103 knees) and on SKI-10 (100 knees). QC used volume and thickness signatures. From 10 ROIs we extracted 4 650 non-shape radiomic features to study inter-compartment similarity, dependence on ROI size, and OA vs. non-OA classification on OAIZIB-CM Results: Post-processing improved macro ASSD on OAIZIB-CM from 2.63 to 0.36 mm and HD95 from 25.2 to 3.35 mm, with DSC 0.91; zero-shot DSC on SKI-10 was 0.80. The geometric L/M rule produced stable compartments across datasets, whereas a direct L/M nnU-Net showed domain-dependent side swaps. Only 6 to 12 percent of features per ROI were strongly correlated with volume or thickness. Radiomics-based models models restricted to size-linked features. Conclusions: LM-CartSeg yields automatic, QCd ROIs and radiomic features that carry discriminative information beyond simple morphometry, providing a practical foundation for multi-centre knee OA radiomics studies.

</details>


### [31] [NAS-LoRA: Empowering Parameter-Efficient Fine-Tuning for Visual Foundation Models with Searchable Adaptation](https://arxiv.org/abs/2512.03499)
*Renqi Chen,Haoyang Su,Shixiang Tang*

Main category: cs.CV

TL;DR: NAS-LoRA: A parameter-efficient fine-tuning method that integrates neural architecture search blocks between SAM's encoder and decoder to bridge semantic gaps for specialized domains while reducing training costs.


<details>
  <summary>Details</summary>
Motivation: SAM lacks spatial priors in its Transformer encoder, hindering high-level semantic learning for specialized domains like medical/agricultural imaging. Existing LoRA methods don't incorporate inductive bias.

Method: Proposes NAS-LoRA with lightweight NAS blocks between encoder and decoder to dynamically optimize prior knowledge integration. Uses stage-wise optimization to balance weight updates and architectural adjustments.

Result: Improves existing PEFT methods while reducing training cost by 24.14% without increasing inference cost. Demonstrates NAS potential for enhancing PEFT in visual foundation models.

Conclusion: NAS-LoRA effectively bridges semantic gaps between pre-trained SAM and specialized domains through dynamic architecture optimization, offering efficient adaptation with reduced training overhead.

Abstract: The Segment Anything Model (SAM) has emerged as a powerful visual foundation model for image segmentation. However, adapting SAM to specific downstream tasks, such as medical and agricultural imaging, remains a significant challenge. To address this, Low-Rank Adaptation (LoRA) and its variants have been widely employed to enhancing SAM's adaptation performance on diverse domains. Despite advancements, a critical question arises: can we integrate inductive bias into the model? This is particularly relevant since the Transformer encoder in SAM inherently lacks spatial priors within image patches, potentially hindering the acquisition of high-level semantic information. In this paper, we propose NAS-LoRA, a new Parameter-Efficient Fine-Tuning (PEFT) method designed to bridge the semantic gap between pre-trained SAM and specialized domains. Specifically, NAS-LoRA incorporates a lightweight Neural Architecture Search (NAS) block between the encoder and decoder components of LoRA to dynamically optimize the prior knowledge integrated into weight updates. Furthermore, we propose a stage-wise optimization strategy to help the ViT encoder balance weight updates and architectural adjustments, facilitating the gradual learning of high-level semantic information. Various Experiments demonstrate our NAS-LoRA improves existing PEFT methods, while reducing training cost by 24.14% without increasing inference cost, highlighting the potential of NAS in enhancing PEFT for visual foundation models.

</details>


### [32] [KeyPointDiffuser: Unsupervised 3D Keypoint Learning via Latent Diffusion Models](https://arxiv.org/abs/2512.03450)
*Rhys Newbury,Juyan Zhang,Tin Tran,Hanna Kurniawati,Dana Kulić*

Main category: cs.CV

TL;DR: Unsupervised 3D keypoint learning framework that bridges generative modeling with structural representation, achieving improved consistency over prior methods.


<details>
  <summary>Details</summary>
Motivation: Existing unsupervised keypoint methods aren't designed for unconditional generative settings, creating a gap in modern 3D generative pipelines that needs bridging.

Method: Learn spatially structured 3D keypoints from point cloud data in unsupervised manner, then use these keypoints to condition an Elucidated Diffusion Model (EDM) for shape reconstruction.

Result: Keypoints exhibit repeatable spatial structure across instances, support smooth interpolation, and achieve 6 percentage-point improvement in keypoint consistency over prior approaches across diverse object categories.

Conclusion: The framework successfully bridges unsupervised keypoint learning with generative modeling, providing compact, interpretable representations that capture geometric variation for 3D objects.

Abstract: Understanding and representing the structure of 3D objects in an unsupervised manner remains a core challenge in computer vision and graphics. Most existing unsupervised keypoint methods are not designed for unconditional generative settings, restricting their use in modern 3D generative pipelines; our formulation explicitly bridges this gap. We present an unsupervised framework for learning spatially structured 3D keypoints from point cloud data. These keypoints serve as a compact and interpretable representation that conditions an Elucidated Diffusion Model (EDM) to reconstruct the full shape. The learned keypoints exhibit repeatable spatial structure across object instances and support smooth interpolation in keypoint space, indicating that they capture geometric variation. Our method achieves strong performance across diverse object categories, yielding a 6 percentage-point improvement in keypoint consistency compared to prior approaches.

</details>


### [33] [Rethinking Prompt Design for Inference-time Scaling in Text-to-Visual Generation](https://arxiv.org/abs/2512.03534)
*Subin Kim,Sangwoo Mo,Mamshad Nayeem Rizve,Yiran Xu,Difan Liu,Jinwoo Shin,Tobias Hinz*

Main category: cs.CV

TL;DR: PRIS is a framework that adaptively revises prompts during inference by reviewing generated visuals, identifying failure patterns, and redesigning prompts before regeneration, achieving better alignment than fixed-prompt scaling.


<details>
  <summary>Details</summary>
Motivation: Current text-to-visual generation approaches scale visual generation (more steps/seeds) but keep prompts fixed, leading to quality plateaus. The prompt is crucial for guiding generation but isn't adapted based on generated outputs.

Method: PRIS (Prompt Redesign for Inference-time Scaling) reviews generated visuals, identifies recurring failure patterns, and redesigns prompts accordingly before regenerating. Introduces element-level factual correction verifier for fine-grained alignment assessment between prompt attributes and visuals.

Result: Extensive experiments on text-to-image and text-to-video benchmarks show effectiveness, including 15% gain on VBench 2.0. Demonstrates that jointly scaling prompts and visuals is key to leveraging inference-time scaling laws.

Conclusion: Adaptive prompt revision during inference, combined with fine-grained alignment verification, significantly improves text-to-visual generation quality by addressing the limitations of fixed-prompt scaling approaches.

Abstract: Achieving precise alignment between user intent and generated visuals remains a central challenge in text-to-visual generation, as a single attempt often fails to produce the desired output. To handle this, prior approaches mainly scale the visual generation process (e.g., increasing sampling steps or seeds), but this quickly leads to a quality plateau. This limitation arises because the prompt, crucial for guiding generation, is kept fixed. To address this, we propose Prompt Redesign for Inference-time Scaling, coined PRIS, a framework that adaptively revises the prompt during inference in response to the scaled visual generations. The core idea of PRIS is to review the generated visuals, identify recurring failure patterns across visuals, and redesign the prompt accordingly before regenerating the visuals with the revised prompt. To provide precise alignment feedback for prompt revision, we introduce a new verifier, element-level factual correction, which evaluates the alignment between prompt attributes and generated visuals at a fine-grained level, achieving more accurate and interpretable assessments than holistic measures. Extensive experiments on both text-to-image and text-to-video benchmarks demonstrate the effectiveness of our approach, including a 15% gain on VBench 2.0. These results highlight that jointly scaling prompts and visuals is key to fully leveraging scaling laws at inference-time. Visualizations are available at the website: https://subin-kim-cv.github.io/PRIS.

</details>


### [34] [CookAnything: A Framework for Flexible and Consistent Multi-Step Recipe Image Generation](https://arxiv.org/abs/2512.03540)
*Ruoxuan Zhang,Bin Wen,Hongxia Xie,Yi Yao,Songhan Zuo,Jian-Yu Jiang-Lin,Hong-Han Shuai,Wen-Huang Cheng*

Main category: cs.CV

TL;DR: CookAnything is a diffusion-based framework that generates coherent image sequences from cooking instructions of any length, addressing limitations of current methods that can't handle variable recipe lengths or maintain consistency across steps.


<details>
  <summary>Details</summary>
Motivation: Current diffusion models struggle with structured multi-step scenarios like recipe illustration, and existing methods generate fixed numbers of images regardless of actual instruction structure, lacking flexibility for natural recipe variability.

Method: Three key components: (1) Step-wise Regional Control (SRC) aligns textual steps with image regions in single denoising process; (2) Flexible RoPE provides step-aware positional encoding for temporal coherence and spatial diversity; (3) Cross-Step Consistency Control (CSCC) maintains ingredient consistency across steps.

Result: Experimental results on recipe illustration benchmarks show CookAnything outperforms existing methods in both training-based and training-free settings, supporting scalable, high-quality visual synthesis of complex multi-step instructions.

Conclusion: The framework enables flexible generation of coherent, semantically distinct image sequences from arbitrary-length cooking instructions, with significant potential for instructional media and procedural content creation applications.

Abstract: Cooking is a sequential and visually grounded activity, where each step such as chopping, mixing, or frying carries both procedural logic and visual semantics. While recent diffusion models have shown strong capabilities in text-to-image generation, they struggle to handle structured multi-step scenarios like recipe illustration. Additionally, current recipe illustration methods are unable to adjust to the natural variability in recipe length, generating a fixed number of images regardless of the actual instructions structure. To address these limitations, we present CookAnything, a flexible and consistent diffusion-based framework that generates coherent, semantically distinct image sequences from textual cooking instructions of arbitrary length. The framework introduces three key components: (1) Step-wise Regional Control (SRC), which aligns textual steps with corresponding image regions within a single denoising process; (2) Flexible RoPE, a step-aware positional encoding mechanism that enhances both temporal coherence and spatial diversity; and (3) Cross-Step Consistency Control (CSCC), which maintains fine-grained ingredient consistency across steps. Experimental results on recipe illustration benchmarks show that CookAnything performs better than existing methods in training-based and training-free settings. The proposed framework supports scalable, high-quality visual synthesis of complex multi-step instructions and holds significant potential for broad applications in instructional media, and procedural content creation.

</details>


### [35] [GeoVideo: Introducing Geometric Regularization into Video Generation Model](https://arxiv.org/abs/2512.03453)
*Yunpeng Bai,Shaoheng Fang,Chaohui Yu,Fan Wang,Qixing Huang*

Main category: cs.CV

TL;DR: Adding geometric regularization via depth prediction to video diffusion models improves 3D consistency and reduces temporal artifacts.


<details>
  <summary>Details</summary>
Motivation: Current video generation methods lack explicit 3D modeling, leading to temporal inconsistencies, implausible motions, and structural artifacts in generated videos.

Method: Augment latent diffusion models with per-frame depth prediction and enforce structural consistency using a multi-view geometric loss that aligns depth maps across frames in a shared 3D coordinate system.

Result: Experiments across multiple datasets show significantly more stable and geometrically consistent results compared to existing baselines.

Conclusion: The proposed method bridges appearance generation with 3D structure modeling, improving spatio-temporal coherence, shape consistency, and physical plausibility in video generation.

Abstract: Recent advances in video generation have enabled the synthesis of high-quality and visually realistic clips using diffusion transformer models. However, most existing approaches operate purely in the 2D pixel space and lack explicit mechanisms for modeling 3D structures, often resulting in temporally inconsistent geometries, implausible motions, and structural artifacts. In this work, we introduce geometric regularization losses into video generation by augmenting latent diffusion models with per-frame depth prediction. We adopted depth as the geometric representation because of the great progress in depth prediction and its compatibility with image-based latent encoders. Specifically, to enforce structural consistency over time, we propose a multi-view geometric loss that aligns the predicted depth maps across frames within a shared 3D coordinate system. Our method bridges the gap between appearance generation and 3D structure modeling, leading to improved spatio-temporal coherence, shape consistency, and physical plausibility. Experiments across multiple datasets show that our approach produces significantly more stable and geometrically consistent results than existing baselines.

</details>


### [36] [V-ITI: Mitigating Hallucinations in Multimodal Large Language Models via Visual Inference-Time Intervention](https://arxiv.org/abs/2512.03542)
*Nan Sun,Zhenyu Zhang,Xixun Lin,Kun Wang,Yanmin Shang,Naibin Gu,Shuohuan Wang,Yu Sun,Hua Wu,Haifeng Wang,Yanan Cao*

Main category: cs.CV

TL;DR: V-ITI: A lightweight visual inference-time intervention framework that detects visual neglect via head-level activation patterns and intervenes only when needed to reduce hallucinations in MLLMs.


<details>
  <summary>Details</summary>
Motivation: MLLMs suffer from hallucinations (content inconsistent with input visuals) due to visual neglect, where models fail to prioritize input images. Existing methods cause "over-intervention" by intervening at all times, introducing new hallucinations and computational overhead.

Method: Proposes V-ITI with two components: 1) Visual Neglect Detector that identifies visual neglect via head-level discriminative probes analyzing activation patterns, and 2) Visual Recall Intervenor that modulates activations with prestored visual activation information only when visual neglect is detected.

Result: Extensive experiments across eight benchmarks and different MLLM families demonstrate that V-ITI consistently mitigates vision-related hallucinations while preserving general task performance.

Conclusion: V-ITI addresses the "when to intervene" problem overlooked by previous methods, providing a lightweight solution that reduces hallucinations without over-intervention, making MLLMs more reliable for precision-sensitive domains.

Abstract: Multimodal Large Language Models (MLLMs) excel in numerous vision-language tasks yet suffer from hallucinations, producing content inconsistent with input visuals, that undermine reliability in precision-sensitive domains. This issue stems from a fundamental problem of visual neglect, where models fail to adequately prioritize input images. Existing methods typically alleviate hallucinations by intervening in the attention score or output logits, focusing on "how to intervene" but overlooking the prerequisite "when to intervene", which leads to the "over-intervention" problem and subsequently introduces new hallucinations and unnecessary computational overhead. To address this gap, we first investigate the mechanism of visual neglect and reveal it can be accurately detected via head-level activation patterns in MLLMs. We thus propose V-ITI, a lightweight visual inference-time intervention framework integrating a Visual Neglect Detector that identifies visual neglect via head-level discriminative probes and a Visual Recall Intervenor that modulates activations with prestored visual activation information only when the visual neglect is detected. Extensive experiments across eight benchmarks and different MLLM families demonstrate that V-ITI consistently mitigates vision-related hallucinations while preserving general task performance.

</details>


### [37] [CSMapping: Scalable Crowdsourced Semantic Mapping and Topology Inference for Autonomous Driving](https://arxiv.org/abs/2512.03510)
*Zhijian Qiao,Zehuan Yu,Tong Li,Chih-Chung Chou,Wenchao Ding,Shaojie Shen*

Main category: cs.CV

TL;DR: CSMapping: A crowdsourced mapping system that uses latent diffusion models for semantic mapping and trajectory clustering for topological mapping, achieving SOTA performance with quality that scales with data volume.


<details>
  <summary>Details</summary>
Motivation: Crowdsourcing enables scalable autonomous driving map construction, but low-cost sensor noise limits quality improvement with increased data volume. Current methods struggle with noise robustness and plausible completion in unobserved areas.

Method: 1) Semantic mapping: Train latent diffusion model on HD maps to learn generative prior, incorporate via constrained MAP optimization in latent space with robust initialization and efficient optimization. 2) Topological mapping: Apply confidence-weighted k-medoids clustering and kinematic refinement to trajectories for smooth centerlines.

Result: Achieves state-of-the-art semantic and topological mapping performance on nuScenes, Argoverse 2, and proprietary datasets, with quality consistently increasing with more crowdsourced data.

Conclusion: CSMapping successfully addresses the noise robustness challenge in crowdsourced mapping, enabling scalable high-quality map construction through generative priors and robust optimization techniques.

Abstract: Crowdsourcing enables scalable autonomous driving map construction, but low-cost sensor noise hinders quality from improving with data volume. We propose CSMapping, a system that produces accurate semantic maps and topological road centerlines whose quality consistently increases with more crowdsourced data. For semantic mapping, we train a latent diffusion model on HD maps (optionally conditioned on SD maps) to learn a generative prior of real-world map structure, without requiring paired crowdsourced/HD-map supervision. This prior is incorporated via constrained MAP optimization in latent space, ensuring robustness to severe noise and plausible completion in unobserved areas. Initialization uses a robust vectorized mapping module followed by diffusion inversion; optimization employs efficient Gaussian-basis reparameterization, projected gradient descent zobracket multi-start, and latent-space factor-graph for global consistency. For topological mapping, we apply confidence-weighted k-medoids clustering and kinematic refinement to trajectories, yielding smooth, human-like centerlines robust to trajectory variation. Experiments on nuScenes, Argoverse 2, and a large proprietary dataset achieve state-of-the-art semantic and topological mapping performance, with thorough ablation and scalability studies.

</details>


### [38] [PosA-VLA: Enhancing Action Generation via Pose-Conditioned Anchor Attention](https://arxiv.org/abs/2512.03724)
*Ziwen Li,Xin Wang,Hanlue Zhang,Runnan Chen,Runqi Lin,Xiao He,Han Huang,Yandong Guo,Fakhri Karray,Tongliang Liu,Mingming Gong*

Main category: cs.CV

TL;DR: PosA-VLA improves VLA models by anchoring visual attention via pose-conditioned supervision to reduce redundant actions and improve precision in embodied tasks.


<details>
  <summary>Details</summary>
Motivation: Current Vision-Language-Action (VLA) models struggle with consistent and precise target-oriented actions, generating redundant or unstable motions due to spatially uniform perception fields that get distracted by irrelevant objects in complex environments.

Method: Proposes PosA-VLA framework with pose-conditioned supervision to anchor visual attention toward task-relevant regions, using a lightweight architecture without auxiliary perception modules like segmentation or grounding networks.

Result: Extensive experiments show the method executes embodied tasks with precise and time-efficient behavior across diverse robotic manipulation benchmarks and demonstrates robust generalization in challenging environments.

Conclusion: The pose-conditioned anchor attention mechanism enables better alignment of instruction semantics with actionable visual cues, improving action generation precision and efficiency for real-world VLA applications.

Abstract: The Vision-Language-Action (VLA) models have demonstrated remarkable performance on embodied tasks and shown promising potential for real-world applications. However, current VLAs still struggle to produce consistent and precise target-oriented actions, as they often generate redundant or unstable motions along trajectories, limiting their applicability in time-sensitive scenarios.In this work, we attribute these redundant actions to the spatially uniform perception field of existing VLAs, which causes them to be distracted by target-irrelevant objects, especially in complex environments.To address this issue, we propose an efficient PosA-VLA framework that anchors visual attention via pose-conditioned supervision, consistently guiding the model's perception toward task-relevant regions. The pose-conditioned anchor attention mechanism enables the model to better align instruction semantics with actionable visual cues, thereby improving action generation precision and efficiency. Moreover, our framework adopts a lightweight architecture and requires no auxiliary perception modules (e.g., segmentation or grounding networks), ensuring efficient inference. Extensive experiments verify that our method executes embodied tasks with precise and time-efficient behavior across diverse robotic manipulation benchmarks and shows robust generalization in a variety of challenging environments.

</details>


### [39] [Dynamic Content Moderation in Livestreams: Combining Supervised Classification with MLLM-Boosted Similarity Matching](https://arxiv.org/abs/2512.03553)
*Wei Chee Yew,Hailun Xu,Sanjay Saha,Xiaotian Fan,Hiok Hian Ong,David Yuchen Wang,Kanchan Sarkar,Zhenheng Yang,Danhui Guan*

Main category: cs.CV

TL;DR: Hybrid content moderation framework combines supervised classification for known violations with similarity matching for novel cases, using multimodal inputs and MLLM distillation to achieve scalable detection in livestreaming platforms.


<details>
  <summary>Details</summary>
Motivation: Content moderation is critical but challenging for large-scale video platforms, especially in livestreaming where it must be timely, multimodal, and robust to evolving unwanted content that traditional methods struggle to detect.

Method: Hybrid framework with two pipelines: supervised classification for known violations and reference-based similarity matching for novel/subtle cases. Multimodal inputs (text, audio, visual) processed through both pipelines, with MLLM distilling knowledge into each to boost accuracy while keeping inference lightweight.

Result: Classification pipeline achieves 67% recall at 80% precision; similarity pipeline achieves 76% recall at 80% precision. Large-scale A/B tests show 6-8% reduction in user views of unwanted livestreams.

Conclusion: The hybrid approach demonstrates scalable and adaptable multimodal content governance capable of addressing both explicit violations and emerging adversarial behaviors in production environments.

Abstract: Content moderation remains a critical yet challenging task for large-scale user-generated video platforms, especially in livestreaming environments where moderation must be timely, multimodal, and robust to evolving forms of unwanted content. We present a hybrid moderation framework deployed at production scale that combines supervised classification for known violations with reference-based similarity matching for novel or subtle cases. This hybrid design enables robust detection of both explicit violations and novel edge cases that evade traditional classifiers. Multimodal inputs (text, audio, visual) are processed through both pipelines, with a multimodal large language model (MLLM) distilling knowledge into each to boost accuracy while keeping inference lightweight. In production, the classification pipeline achieves 67% recall at 80% precision, and the similarity pipeline achieves 76% recall at 80% precision. Large-scale A/B tests show a 6-8% reduction in user views of unwanted livestreams}. These results demonstrate a scalable and adaptable approach to multimodal content governance, capable of addressing both explicit violations and emerging adversarial behaviors.

</details>


### [40] [Difference Decomposition Networks for Infrared Small Target Detection](https://arxiv.org/abs/2512.03470)
*Chen Hu,Mingyu Zhou,Shuai Yuan,Hongbo Hu,Xiangyu Qiu,Junhai Luo,Tian Pu,Xiyin Li*

Main category: cs.CV

TL;DR: Proposed Basis Decomposition Module (BDM) for infrared small target detection, extended to spatial and temporal difference decomposition modules, achieving SOTA performance on both single-frame and multi-frame datasets.


<details>
  <summary>Details</summary>
Motivation: Infrared small target detection faces challenges due to lack of target texture and severe background clutter, where background obscures targets, necessitating better target enhancement and background suppression.

Method: Developed Basis Decomposition Module (BDM) that decomposes complex features into basis features to enhance target information while eliminating redundancy. Extended BDM to create spatial difference decomposition modules (SD²M, SD³M) and temporal difference decomposition module (TD²M). Built SD²Net for single-frame detection using U-shaped architecture with SD²M/SD³M, and STD²Net for multi-frame detection by adding TD²M for motion information.

Result: Achieved state-of-the-art performance on both SISTD and MISTD datasets. SD²Net performed well compared to established networks on single-frame detection. STD²Net achieved 87.68% mIoU on MISTD datasets, significantly outperforming SD²Net's 64.97% mIoU.

Conclusion: The proposed basis decomposition approach effectively addresses infrared small target detection challenges by enhancing targets and suppressing backgrounds through feature decomposition, with spatial and temporal extensions providing strong performance improvements, especially for multi-frame scenarios.

Abstract: Infrared small target detection (ISTD) faces two major challenges: a lack of discernible target texture and severe background clutter, which results in the background obscuring the target. To enhance targets and suppress backgrounds, we propose the Basis Decomposition Module (BDM) as an extensible and lightweight module based on basis decomposition, which decomposes a complex feature into several basis features and enhances certain information while eliminating redundancy. Extending BDM leads to a series of modules, including the Spatial Difference Decomposition Module (SD$^\mathrm{2}$M), Spatial Difference Decomposition Downsampling Module (SD$^\mathrm{3}$M), and Temporal Difference Decomposition Module (TD$^\mathrm{2}$M). Based on these modules, we develop the Spatial Difference Decomposition Network (SD$^\mathrm{2}$Net) for single-frame ISTD (SISTD) and the Spatiotemporal Difference Decomposition Network (STD$^\mathrm{2}$Net) for multi-frame ISTD (MISTD). SD$^\mathrm{2}$Net integrates SD$^\mathrm{2}$M and SD$^\mathrm{3}$M within an adapted U-shaped architecture. We employ TD$^\mathrm{2}$M to introduce motion information, which transforms SD$^\mathrm{2}$Net into STD$^\mathrm{2}$Net. Extensive experiments on SISTD and MISTD datasets demonstrate state-of-the-art (SOTA) performance. On the SISTD task, SD$^\mathrm{2}$Net performs well compared to most established networks. On the MISTD datasets, STD$^\mathrm{2}$Net achieves a mIoU of 87.68\%, outperforming SD$^\mathrm{2}$Net, which achieves a mIoU of 64.97\%. Our codes are available: https://github.com/greekinRoma/IRSTD_HC_Platform.

</details>


### [41] [MUT3R: Motion-aware Updating Transformer for Dynamic 3D Reconstruction](https://arxiv.org/abs/2512.03939)
*Guole Shen,Tianchen Deng,Xingrui Qin,Nailin Wang,Jianyu Wang,Yanbo Wang,Yongtao Chen,Hesheng Wang,Jingchuan Wang*

Main category: cs.CV

TL;DR: MUT3R is a training-free framework that uses attention-derived motion cues from pretrained transformers to suppress dynamic content in streaming 3D reconstruction, improving temporal consistency without retraining.


<details>
  <summary>Details</summary>
Motivation: Stateful recurrent neural networks for 3D reconstruction suffer from motion-induced artifacts where non-rigid regions corrupt attention propagation. The authors discovered that pretrained transformers already encode implicit motion cues in their attention patterns but don't explicitly use them.

Method: Analyzed self-attention maps across transformer layers to find consistent patterns where dynamic regions are naturally down-weighted. Introduced MUT3R with an attention-level gating module that applies these attention-derived motion cues to suppress dynamic content in early transformer layers during inference, without any retraining or fine-tuning.

Result: The training-free approach stabilizes geometric reasoning in streaming scenarios, leading to improvements in temporal consistency and camera pose robustness across multiple dynamic benchmarks.

Conclusion: MUT3R offers a simple, training-free pathway toward motion-aware streaming reconstruction by letting pretrained transformers diagnose their own motion cues and self-correct, addressing motion artifacts without additional training.

Abstract: Recent stateful recurrent neural networks have achieved remarkable progress on static 3D reconstruction but remain vulnerable to motion-induced artifacts, where non-rigid regions corrupt attention propagation between the spatial memory and image feature. By analyzing the internal behaviors of the state and image token updating mechanism, we find that aggregating self-attention maps across layers reveals a consistent pattern: dynamic regions are naturally down-weighted, exposing an implicit motion cue that the pretrained transformer already encodes but never explicitly uses. Motivated by this observation, we introduce MUT3R, a training-free framework that applies the attention-derived motion cue to suppress dynamic content in the early layers of the transformer during inference. Our attention-level gating module suppresses the influence of dynamic regions before their artifacts propagate through the feature hierarchy. Notably, we do not retrain or fine-tune the model; we let the pretrained transformer diagnose its own motion cues and correct itself. This early regulation stabilizes geometric reasoning in streaming scenarios and leads to improvements in temporal consistency and camera pose robustness across multiple dynamic benchmarks, offering a simple and training-free pathway toward motion-aware streaming reconstruction.

</details>


### [42] [MKSNet: Advanced Small Object Detection in Remote Sensing Imagery with Multi-Kernel and Dual Attention Mechanisms](https://arxiv.org/abs/2512.03640)
*Jiahao Zhang,Xiao Zhao,Guangyu Gao*

Main category: cs.CV

TL;DR: MKSNet introduces Multi-Kernel Selection mechanism and dual attention for better small object detection in remote sensing images, outperforming SOTA on DOTA-v1.0 and HRSC2016 benchmarks.


<details>
  <summary>Details</summary>
Motivation: Small object detection in remote sensing imagery faces challenges due to information loss in deep CNN layers, spatial redundancy, and complex backgrounds that obscure small targets.

Method: Proposes Multi-Kernel Selection Network (MKSNet) with: 1) Multi-Kernel Selection mechanism using large convolutional kernels for extensive contextual information capture with adaptive kernel size selection; 2) Dual attention mechanism combining spatial attention (fine-tuning spatial weights) and channel attention (optimizing channel information selection).

Result: MKSNet substantially surpasses existing state-of-the-art models on DOTA-v1.0 and HRSC2016 benchmarks, demonstrating superior ability to handle multi-scale and high-resolution remote sensing data.

Conclusion: MKSNet effectively addresses small object detection challenges in remote sensing through its innovative Multi-Kernel Selection and dual attention mechanisms, confirming its effectiveness and innovation in the field.

Abstract: Deep convolutional neural networks (DCNNs) have substantially advanced object detection capabilities, particularly in remote sensing imagery. However, challenges persist, especially in detecting small objects where the high resolution of these images and the small size of target objects often result in a loss of critical information in the deeper layers of conventional CNNs. Additionally, the extensive spatial redundancy and intricate background details typical in remote-sensing images tend to obscure these small targets. To address these challenges, we introduce Multi-Kernel Selection Network (MKSNet), a novel network architecture featuring a novel Multi-Kernel Selection mechanism. The MKS mechanism utilizes large convolutional kernels to effectively capture an extensive range of contextual information. This innovative design allows for adaptive kernel size selection, significantly enhancing the network's ability to dynamically process and emphasize crucial spatial details for small object detection. Furthermore, MKSNet also incorporates a dual attention mechanism, merging spatial and channel attention modules. The spatial attention module adaptively fine-tunes the spatial weights of feature maps, focusing more intensively on relevant regions while mitigating background noise. Simultaneously, the channel attention module optimizes channel information selection, improving feature representation and detection accuracy. Empirical evaluations on the DOTA-v1.0 and HRSC2016 benchmark demonstrate that MKSNet substantially surpasses existing state-of-the-art models in detecting small objects in remote sensing images. These results highlight MKSNet's superior ability to manage the complexities associated with multi-scale and high-resolution image data, confirming its effectiveness and innovation in remote sensing object detection.

</details>


### [43] [Procedural Mistake Detection via Action Effect Modeling](https://arxiv.org/abs/2512.03474)
*Wenliang Guo,Yujiang Pu,Yu Kong*

Main category: cs.CV

TL;DR: AEM framework jointly models action execution and outcomes for mistake detection, using effect-aware representations and achieving SOTA on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing approaches focus on how actions are performed but overlook what they produce (action effects), yet many errors manifest in outcomes rather than execution itself.

Method: Action Effect Modeling (AEM) framework: identifies action outcomes via effect frame selection, extracts visual grounding and scene graph cues, aligns them in shared latent space, and uses prompt-based detector with task-specific prompts.

Result: Achieves state-of-the-art performance on EgoPER and CaptainCook4D benchmarks under challenging one-class classification (OCC) setting.

Conclusion: Modeling both execution and outcome yields more reliable mistake detection, and effect-aware representations have potential for broader downstream applications.

Abstract: Mistake detection in procedural tasks is essential for building intelligent systems that support learning and task execution. Existing approaches primarily analyze how an action is performed, while overlooking what it produces, i.e., the \textbf{action effect}. Yet many errors manifest not in the execution itself but in the resulting outcome, such as an unintended object state or incorrect spatial arrangement. To address this gap, we propose Action Effect Modeling (AEM), a unified framework that jointly captures action execution and its outcomes through a probabilistic formulation. AEM first identifies the outcome of an action by selecting the most informative effect frame based on semantic relevance and visual quality. It then extracts complementary cues from visual grounding and symbolic scene graphs, aligning them in a shared latent space to form robust effect-aware representations. To detect mistakes, we further design a prompt-based detector that incorporates task-specific prompts and aligns each action segment with its intended execution semantics. Our approach achieves state-of-the-art performance on the EgoPER and CaptainCook4D benchmarks under the challenging one-class classification (OCC) setting. These results demonstrate that modeling both execution and outcome yields more reliable mistake detection, and highlight the potential of effect-aware representations to benefit a broader range of downstream applications.

</details>


### [44] [SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL](https://arxiv.org/abs/2512.04069)
*Siyi Chen,Mikaela Angelina Uy,Chan Hee Song,Faisal Ladhak,Adithyavairavan Murali,Qing Qu,Stan Birchfield,Valts Blukis,Jonathan Tremblay*

Main category: cs.CV

TL;DR: DIRL framework trains VLMs to coordinate multiple visual tools for precise spatial reasoning, achieving SOTA on benchmarks and enabling real-world robot manipulation.


<details>
  <summary>Details</summary>
Motivation: VLMs lack metrically precise spatial reasoning needed for embodied applications, and existing approaches either rely on handcrafted prompting or fixed tool pipelines that limit optimal tool-use discovery.

Method: Double Interactive Reinforcement Learning (DIRL): two-phase training where VLMs learn multi-tool coordination through interactive exploration. Teaching phase combines single-tool specialist demonstrations with frontier model traces; exploration phase refines coordination via continued RL.

Result: SpaceTools achieves state-of-the-art on spatial understanding benchmarks (RoboSpatial-Home, BLINK, BOP-ASK) with +12% over SFT and +16% over RL baselines, and demonstrates reliable real-world manipulation with 7-DOF robot.

Conclusion: DIRL enables VLMs to effectively coordinate multiple visual tools for precise spatial reasoning, bridging the gap between qualitative understanding and metric precision needed for embodied applications.

Abstract: Vision Language Models (VLMs) demonstrate strong qualitative visual understanding, but struggle with metrically precise spatial reasoning required for embodied applications. The agentic paradigm promises that VLMs can use a wide variety of tools that could augment these capabilities, such as depth estimators, segmentation models, and pose estimators. Yet it remains an open challenge how to realize this vision without solely relying on handcrafted prompting strategies or enforcing fixed, predefined tool pipelines that limit VLMs' ability to discover optimal tool-use patterns. Reinforcement Learning could overcome this gap, but has so far been limited to reasoning with a single visual tool due to the large search space in multi-tool reasoning. We introduce Double Interactive Reinforcement Learning (DIRL), a two-phase training framework where VLMs learn to coordinate multiple tools through interactive exploration and feedback. In the teaching phase, we combine demonstrations from a single tool specialist trained via interactive RL with traces from a frontier model using all tools. In the exploration phase, the model further refines multi-tool coordination through continued RL. Our model, SpaceTools, with tool-augmented spatial reasoning ability, achieves state-of-the-art performance on spatial understanding benchmarks (RoboSpatial-Home, BLINK, BOP-ASK) and demonstrates reliable real-world manipulation using a 7-DOF robot as a tool. DIRL provides substantial improvements over the vanilla SFT (+12% on RoboSpatial) and RL (+16% on RoboSpatial) baselines. Project page: https://spacetools.github.io/.

</details>


### [45] [ToG-Bench: Task-Oriented Spatio-Temporal Grounding in Egocentric Videos](https://arxiv.org/abs/2512.03666)
*Qi'ao Xu,Tianwen Qian,Yuqian Fu,Kailing Li,Yang Jiao,Jiacheng Zhang,Xiaoling Wang,Liang He*

Main category: cs.CV

TL;DR: ToG-Bench: First task-oriented spatio-temporal video grounding benchmark for egocentric videos, requiring task-based object localization rather than descriptive instructions.


<details>
  <summary>Details</summary>
Motivation: Existing STVG studies focus on object-centric descriptive instructions but neglect task-oriented reasoning needed for embodied agents to accomplish goal-directed interactions.

Method: Built on ScanNet videos with 100 annotated clips and 2,704 task-oriented instructions using semi-automated pipeline combining foundation model annotation and human refinement. Introduces task-level evaluation metrics for multi-object and explicit-implicit grounding.

Result: Benchmarked 7 state-of-the-art MLLMs, revealing intrinsic challenges of task-oriented STVG and substantial performance gaps across explicit-implicit and multi-object grounding.

Conclusion: Highlights difficulty of bridging perception and interaction in embodied scenarios, establishing new benchmark for task-oriented video grounding research.

Abstract: A core capability towards general embodied intelligence lies in localizing task-relevant objects from an egocentric perspective, formulated as Spatio-Temporal Video Grounding (STVG). Despite recent progress, existing STVG studies remain largely confined to object-centric and descriptive instructions, neglecting the task-oriented reasoning that is crucial for embodied agents to accomplish goal-directed interactions. To bridge this gap, we introduce \textbf{ToG-Bench}, the first task-oriented spatio-temporal video grounding benchmark for egocentric videos. ToG-Bench is characterized by three key features: (1) \textbf{Task-oriented Grounding}, which requires identifying and localizing objects based on intended tasks rather than straightforward descriptions; (2) \textbf{Explicit-Implicit Dual Grounding}, where target objects can be either explicitly mentioned or implicitly inferred by contextual reasoning; (3) \textbf{One-to-Many Grounding}, where a single instruction may correspond to multiple objects involved in task execution. Built upon videos sourced from ScanNet, ToG-Bench comprises 100 annotated clips with 2,704 task-oriented grounding instructions, constructed via a semi-automated pipeline that combines foundation model annotation and human refinement. In addition, we introduce a set of task-level evaluation metrics tailored for multi-object and explicit-implicit object grounding, and systematically benchmark seven state-of-the-art MLLMs. Extensive experiments reveal the intrinsic challenges of task-oriented STVG and substantial performance gaps across explicit-implicit and multi-object grounding, highlighting the difficulty of bridging perception and interaction in embodied scenarios. Data and code will be released at: \href{https://github.com/qaxuDev/ToG-Bench}{https://github.com/qaxuDev/ToG-Bench}..

</details>


### [46] [Fairness-Aware Fine-Tuning of Vision-Language Models for Medical Glaucoma Diagnosis](https://arxiv.org/abs/2512.03477)
*Zijian Gu,Yuxi Liu,Zhenhao Zhang,Song Wang*

Main category: cs.CV

TL;DR: Fairness-aware Low-Rank Adaptation (LoRA) for medical vision-language models reduces diagnostic accuracy disparities across demographic groups by 69% while maintaining overall accuracy, using only 0.24% trainable parameters.


<details>
  <summary>Details</summary>
Motivation: Medical vision-language models show expert-level performance but exhibit significant diagnostic accuracy disparities across demographic groups, creating fairness concerns in healthcare AI applications.

Method: Introduces fairness-aware Low-Rank Adaptation with three approaches: FR-LoRA (MaxAccGap regularization), GR-LoRA (inverse frequency gradient weighting), and Hybrid-LoRA (combining both). Uses differentiable MaxAccGap loss for end-to-end optimization of accuracy parity.

Result: GR-LoRA reduces diagnostic accuracy disparities by 69% while maintaining 53.15% overall accuracy on 10,000 glaucoma fundus images. Race-specific optimization yields 60% disparity reduction with minimal accuracy trade-off.

Conclusion: The approach enables practical deployment of fair medical AI in resource-constrained settings by requiring only 0.24% trainable parameters while significantly improving fairness without substantial accuracy loss.

Abstract: Vision-language models achieve expert-level performance on medical imaging tasks but exhibit significant diagnostic accuracy disparities across demographic groups. We introduce fairness-aware Low-Rank Adaptation for medical VLMs, combining parameter efficiency with explicit fairness optimization. Our key algorithmic contribution is a differentiable MaxAccGap loss that enables end-to-end optimization of accuracy parity across demographic groups. We propose three methods: FR-LoRA integrates MaxAccGap regularization into the training objective, GR-LoRA applies inverse frequency weighting to balance gradient contributions, and Hybrid-LoRA combines both mechanisms.Evaluated on 10,000 glaucoma fundus images, GR-LoRA reduces diagnostic accuracy disparities by 69% while maintaining 53.15% overall accuracy. Ablation studies reveal that strong regularization strength achieves optimal fairness with minimal accuracy trade-off, and race-specific optimization yields 60% disparity reduction. Our approach requires only 0.24% trainable parameters, enabling practical deployment of fair medical AI in resource-constrained healthcare settings.

</details>


### [47] [Out-of-the-box: Black-box Causal Attacks on Object Detectors](https://arxiv.org/abs/2512.03730)
*Melane Navaratnarajah,David A. Kelly,Hana Chockler*

Main category: cs.CV

TL;DR: BlackCAtt is a black-box adversarial attack method that uses causal pixel sets to create explainable, imperceptible attacks on object detectors across different architectures.


<details>
  <summary>Details</summary>
Motivation: Existing adversarial perturbation methods are often white-box, architecture-specific, and lack explainability. There's a need for black-box, architecture-agnostic attacks that provide insights into why attacks work, enabling better understanding and prevention.

Method: BlackCAtt combines minimal, causally sufficient pixel sets with bounding boxes from object detectors to create adversarial attacks. It treats detectors as black boxes and works across different architectures by identifying causal pixels that lead to detection failures.

Result: BlackCAtt outperforms baseline methods significantly: 2.7× better at removing detections, 3.86× better at changing detections, and 5.75× better at triggering spurious detections on COCO dataset. Attacks are imperceptible due to minimal pixel modifications.

Conclusion: Causal pixel identification enables more precisely targeted, less perceptible, and explainable black-box attacks on object detectors, demonstrating the power of causal analysis for understanding and creating effective adversarial perturbations.

Abstract: Adversarial perturbations are a useful way to expose vulnerabilities in object detectors. Existing perturbation methods are frequently white-box and architecture specific. More importantly, while they are often successful, it is rarely clear why they work. Insights into the mechanism of this success would allow developers to understand and analyze these attacks, as well as fine-tune the model to prevent them. This paper presents BlackCAtt, a black-box algorithm and a tool, which uses minimal, causally sufficient pixel sets to construct explainable, imperceptible, reproducible, architecture-agnostic attacks on object detectors. BlackCAtt combines causal pixels with bounding boxes produced by object detectors to create adversarial attacks that lead to the loss, modification or addition of a bounding box. BlackCAtt works across different object detectors of different sizes and architectures, treating the detector as a black box. We compare the performance of BlackCAtt with other black-box attack methods and show that identification of causal pixels leads to more precisely targeted and less perceptible attacks. On the COCO test dataset, our approach is 2.7 times better than the baseline in removing a detection, 3.86 times better in changing a detection, and 5.75 times better in triggering new, spurious, detections. The attacks generated by BlackCAtt are very close to the original image, and hence imperceptible, demonstrating the power of causal pixels.

</details>


### [48] [Towards Object-centric Understanding for Instructional Videos](https://arxiv.org/abs/2512.03479)
*Wenliang Guo,Yu Kong*

Main category: cs.CV

TL;DR: Proposes Object-IVQA, a new object-centric video QA benchmark for procedural understanding, and an agent framework that outperforms existing VLMs.


<details>
  <summary>Details</summary>
Motivation: Existing action-centric methods fail to handle flexible real-world procedures where step order depends on object states. Need to shift to object-centric paradigm focusing on state transitions.

Method: Introduces Object-IVQA benchmark with 107 videos and 514 QA pairs for object-centric reasoning. Proposes agent framework with object-centric planning, perception, analysis and generation tools for explicit evidence retrieval and multi-hop reasoning.

Result: Existing large vision-language models struggle with object-level recognition and reasoning. The proposed framework achieves substantial improvements over baseline methods.

Conclusion: Object-centric paradigm is crucial for procedural understanding. The benchmark and framework advance reasoning about complex real-world tasks through state transition analysis.

Abstract: Understanding procedural activities is crucial for developing future assistive AI that can reason about complex real-world tasks. Existing action-centric methods struggle with the flexibility of real procedures, where step order varies depending on object states. In this work, we propose to shift the focus to an object-centric paradigm by regarding actions as mechanisms that drive state transitions. To advance this direction, we introduce Object-IVQA, a long-form instructional video benchmark with 107 videos and 514 open-ended question-answer pairs annotated with temporally grounded evidence. The benchmark evaluates four dimensions of object-centric reasoning, including state evolution, precondition verification, counterfactual reasoning and mistake recognition. We further propose an agent framework that orchestrates object-centric planning, perception, analysis and generation tools, enabling explicit evidence retrieval and multi-hop reasoning across disjoint segments. Experiments show that existing large vision-language models struggle in object-level recognition and reasoning, whereas our framework achieves substantially improvement.

</details>


### [49] [Research on Brain Tumor Classification Method Based on Improved ResNet34 Network](https://arxiv.org/abs/2512.03751)
*Yufeng Li,Wenchao Zhao,Bo Dang,Weimin Wang*

Main category: cs.CV

TL;DR: Improved ResNet34 model achieves 98.8% accuracy for brain tumor classification with 20% fewer parameters than original ResNet34.


<details>
  <summary>Details</summary>
Motivation: Manual brain tumor classification is time-consuming and labor-intensive, while existing shallow CNN models have suboptimal accuracy. Need for more efficient and accurate automated classification methods.

Method: Enhanced ResNet34 with multi-scale feature extraction: multi-scale input module as first layer, Inception v2 module as residual downsampling layer, and channel attention mechanism for feature weighting.

Result: 98.8% average classification accuracy (1% higher than ResNet34) with only 80% of original parameters, validated through five-fold cross-validation.

Conclusion: The improved network achieves higher accuracy with fewer parameters, providing an efficient and effective solution for brain tumor image classification.

Abstract: Previously, image interpretation in radiology relied heavily on manual methods. However, manual classification of brain tumor medical images is time-consuming and labor-intensive. Even with shallow convolutional neural network models, the accuracy is not ideal. To improve the efficiency and accuracy of brain tumor image classification, this paper proposes a brain tumor classification model based on an improved ResNet34 network. This model uses the ResNet34 residual network as the backbone network and incorporates multi-scale feature extraction. It uses a multi-scale input module as the first layer of the ResNet34 network and an Inception v2 module as the residual downsampling layer. Furthermore, a channel attention mechanism module assigns different weights to different channels of the image from a channel domain perspective, obtaining more important feature information. The results after a five-fold crossover experiment show that the average classification accuracy of the improved network model is approximately 98.8%, which is not only 1% higher than ResNet34, but also only 80% of the number of parameters of the original model. Therefore, the improved network model not only improves accuracy but also reduces clutter, achieving a classification effect with fewer parameters and higher accuracy.

</details>


### [50] [EEA: Exploration-Exploitation Agent for Long Video Understanding](https://arxiv.org/abs/2512.03500)
*Te Yang,Xiangyu Zhu,Bo Wang,Quan Chen,Peng Jiang,Zhen Lei*

Main category: cs.CV

TL;DR: EEA is a video agent framework that balances exploration-exploitation using semantic guidance and hierarchical tree search for efficient long-form video understanding.


<details>
  <summary>Details</summary>
Motivation: Current approaches to long-form video understanding suffer from either severe computational overhead due to dense preprocessing, or fail to effectively balance exploration and exploitation, resulting in incomplete information coverage and inefficiency.

Method: EEA uses semantic guidance with hierarchical tree search, autonomously discovers and updates task-relevant semantic queries, collects matching frames as semantic anchors, preferentially explores semantically relevant frames while ensuring coverage, and adaptively combines intrinsic VLM rewards with semantic priors by modeling uncertainty.

Result: Experiments across various long-video benchmarks validate the superior performance and computational efficiency of the proposed method.

Conclusion: EEA provides an effective framework for long-form video understanding that achieves exploration-exploitation balance through semantic guidance and hierarchical tree search, offering both superior performance and computational efficiency.

Abstract: Long-form video understanding requires efficient navigation of extensive visual data to pinpoint sparse yet critical information. Current approaches to longform video understanding either suffer from severe computational overhead due to dense preprocessing, or fail to effectively balance exploration and exploitation, resulting in incomplete information coverage and inefficiency. In this work, we introduce EEA, a novel video agent framework that archives exploration-exploitation balance through semantic guidance with hierarchical tree search process. EEA autonomously discovers and dynamically updates task-relevant semantic queries, and collects video frames closely matched to these queries as semantic anchors. During the tree search process, instead of uniform expansion, EEA preferentially explores semantically relevant frames while ensuring sufficient coverage within unknown segments. Moreover, EEA adaptively combines intrinsic rewards from visionlanguage models (VLMs) with semantic priors by explicitly modeling uncertainty to achieve stable and precise evaluation of video segments. Experiments across various long-video benchmarks validate the superior performance and computational efficiency of our proposed method.

</details>


### [51] [AdaptVision: Efficient Vision-Language Models via Adaptive Visual Acquisition](https://arxiv.org/abs/2512.03794)
*Zichuan Lin,Yicheng Liu,Yang Yang,Lvfang Tao,Deheng Ye*

Main category: cs.CV

TL;DR: AdaptVision enables VLMs to autonomously determine the minimum visual tokens needed per sample using a coarse-to-fine approach with selective region cropping, trained via reinforcement learning with decoupled policy optimization.


<details>
  <summary>Details</summary>
Motivation: Current efficient VLM approaches use fixed-ratio visual token compression, which is passive and cannot adapt to varying task requirements, leading to unnecessary computational overhead.

Method: AdaptVision uses a coarse-to-fine approach: processes compressed low-resolution images first, then selectively invokes a bounding box tool to crop key regions when needed. Trained with reinforcement learning using Decoupled Turn Policy Optimization (DTPO) that separates tool learning and accuracy improvement objectives.

Result: AdaptVision achieves superior performance on multiple VQA benchmarks while consuming substantially fewer visual tokens than state-of-the-art efficient VLM methods.

Conclusion: The proposed adaptive visual token acquisition paradigm enables VLMs to autonomously determine optimal visual information needs, balancing accuracy and efficiency through a human-inspired active vision mechanism.

Abstract: Vision-Language Models (VLMs) have achieved remarkable success in visual question answering tasks, but their reliance on large numbers of visual tokens introduces significant computational overhead. While existing efficient VLM approaches reduce visual tokens through fixed-ratio compression, they operate passively and lack the ability to adapt to varying task requirements. This motivates a fundamental question: Can VLMs autonomously determine the minimum number of visual tokens required for each sample? Inspired by human active vision mechanisms, we introduce AdaptVision, an efficient VLM paradigm that enables adaptive visual token acquisition through a coarse-to-fine approach. Our model initially processes compressed visual tokens from low-resolution images and selectively acquires additional visual information by invoking a bounding box tool to crop key regions when necessary. We train AdaptVision using a reinforcement learning framework that carefully balances accuracy and efficiency. Central to our approach is Decoupled Turn Policy Optimization (DTPO), which decouples the learning objective into two components: (1) tool learning, which optimizes correct tool utilization, and (2) accuracy improvement, which refines the generated responses to improve answer correctness. Based on this formulation, we further decouple advantage estimation by computing separate advantages for tokens associated with each objective. This formulation enables more effective optimization for AdaptVision compared to vanilla GRPO. Comprehensive experiments across multiple VQA benchmarks demonstrate that AdaptVision achieves superior performance while consuming substantially fewer visual tokens than state-of-the-art efficient VLM methods.

</details>


### [52] [Exploiting Domain Properties in Language-Driven Domain Generalization for Semantic Segmentation](https://arxiv.org/abs/2512.03508)
*Seogkyu Jeon,Kibeom Hong,Hyeran Byun*

Main category: cs.CV

TL;DR: DPMFormer introduces domain-aware prompt learning and contrastive learning to address semantic misalignment in domain generalized semantic segmentation, achieving SOTA results.


<details>
  <summary>Details</summary>
Motivation: Existing DGSS methods using VLMs overlook semantic misalignment between visual and textual contexts caused by fixed context prompts learned on single source domains.

Method: Proposes Domain-aware Prompt-driven Masked Transformer (DPMFormer) with: 1) domain-aware prompt learning for semantic alignment, 2) domain-aware contrastive learning with texture perturbation to diversify observable domains, and 3) domain-robust consistency learning to minimize prediction discrepancies between original and augmented images.

Result: Demonstrates superiority and establishes new state-of-the-art performance on various DGSS benchmarks.

Conclusion: DPMFormer effectively addresses semantic misalignment in domain generalized semantic segmentation through domain-aware prompt learning and contrastive learning, providing a robust framework resilient to diverse environmental changes.

Abstract: Recent domain generalized semantic segmentation (DGSS) studies have achieved notable improvements by distilling semantic knowledge from Vision-Language Models (VLMs). However, they overlook the semantic misalignment between visual and textual contexts, which arises due to the rigidity of a fixed context prompt learned on a single source domain. To this end, we present a novel domain generalization framework for semantic segmentation, namely Domain-aware Prompt-driven Masked Transformer (DPMFormer). Firstly, we introduce domain-aware prompt learning to facilitate semantic alignment between visual and textual cues. To capture various domain-specific properties with a single source dataset, we propose domain-aware contrastive learning along with the texture perturbation that diversifies the observable domains. Lastly, to establish a framework resilient against diverse environmental changes, we have proposed the domain-robust consistency learning which guides the model to minimize discrepancies of prediction from original and the augmented images. Through experiments and analyses, we demonstrate the superiority of the proposed framework, which establishes a new state-of-the-art on various DGSS benchmarks. The code is available at https://github.com/jone1222/DPMFormer.

</details>


### [53] [AfroBeats Dance Movement Analysis Using Computer Vision: A Proof-of-Concept Framework Combining YOLO and Segment Anything Model](https://arxiv.org/abs/2512.03509)
*Kwaku Opoku-Ware,Gideon Opoku*

Main category: cs.CV

TL;DR: Preliminary computer vision framework for automated dance analysis using YOLOv8/v11 for detection and SAM for segmentation, tested on Ghanaian AfroBeats dance with promising results but limited validation.


<details>
  <summary>Details</summary>
Motivation: To develop an automated system for dance movement analysis using accessible computer vision techniques without requiring specialized equipment or markers, enabling quantitative assessment of dance performances.

Method: Integrates YOLOv8 and v11 for dancer detection with Segment Anything Model (SAM) for precise segmentation, enabling tracking, step counting, spatial coverage analysis, and rhythm consistency measurement from video recordings.

Result: Achieved ~94% detection precision and 89% recall on a 49-second Ghanaian AfroBeats video, with SAM segmentation reaching ~83% IoU. Primary dancer showed 23% more steps, 37% higher motion intensity, and 42% more space usage than secondary dancers.

Conclusion: The framework demonstrates technical feasibility for automated dance analysis and identifies promising quantitative metrics, but represents early-stage research with limitations including single-video validation and lack of systematic ground truth or pose estimation comparisons.

Abstract: This paper presents a preliminary investigation into automated dance movement analysis using contemporary computer vision techniques. We propose a proof-of-concept framework that integrates YOLOv8 and v11 for dancer detection with the Segment Anything Model (SAM) for precise segmentation, enabling the tracking and quantification of dancer movements in video recordings without specialized equipment or markers. Our approach identifies dancers within video frames, counts discrete dance steps, calculates spatial coverage patterns, and measures rhythm consistency across performance sequences. Testing this framework on a single 49-second recording of Ghanaian AfroBeats dance demonstrates technical feasibility, with the system achieving approximately 94% detection precision and 89% recall on manually inspected samples. The pixel-level segmentation provided by SAM, achieving approximately 83% intersection-over-union with visual inspection, enables motion quantification that captures body configuration changes beyond what bounding-box approaches can represent. Analysis of this preliminary case study indicates that the dancer classified as primary by our system executed 23% more steps with 37% higher motion intensity and utilized 42% more performance space compared to dancers classified as secondary. However, this work represents an early-stage investigation with substantial limitations including single-video validation, absence of systematic ground truth annotations, and lack of comparison with existing pose estimation methods. We present this framework to demonstrate technical feasibility, identify promising directions for quantitative dance metrics, and establish a foundation for future systematic validation studies.

</details>


### [54] [PULSE: A Unified Multi-Task Architecture for Cardiac Segmentation, Diagnosis, and Few-Shot Cross-Modality Clinical Adaptation](https://arxiv.org/abs/2512.03848)
*Hania Ghouse,Maryam Alsharqi,Farhad R. Nezami,Muzammil Behzad*

Main category: cs.CV

TL;DR: PULSE is a unified vision-language framework for cardiac image analysis that combines anatomical segmentation, disease classification, and clinical report generation in a single architecture, using self-supervised representations and multi-task learning.


<details>
  <summary>Details</summary>
Motivation: Current cardiac image analysis is fragmented across separate tasks (segmentation, classification, report generation) with different networks and data regimes, lacking a unified framework that generalizes across imaging modalities and datasets.

Method: Multi-task vision-language framework built on self-supervised representations with composite supervision balancing region overlap learning, pixel-wise classification fidelity, and boundary-aware IoU refinement. Uses multi-scale token reconstruction decoder for segmentation and shared global representations for classification and text generation.

Result: PULSE learns task-invariant cardiac priors, generalizes robustly across datasets, and can be adapted to new imaging modalities with minimal supervision, moving toward a scalable foundation-style cardiac analysis framework.

Conclusion: PULSE represents a significant advancement toward unified cardiac image analysis, enabling transition from pixels to structures to clinical reasoning within a single architecture while maintaining generalization across modalities and datasets.

Abstract: Cardiac image analysis remains fragmented across tasks: anatomical segmentation, disease classification, and grounded clinical report generation are typically handled by separate networks trained under different data regimes. No existing framework unifies these objectives within a single architecture while retaining generalization across imaging modalities and datasets. We introduce PULSE, a multi-task vision-language framework built on self-supervised representations and optimized through a composite supervision strategy that balances region overlap learning, pixel wise classification fidelity, and boundary aware IoU refinement. A multi-scale token reconstruction decoder enables anatomical segmentation, while shared global representations support disease classification and clinically grounded text output allowing the model to transition from pixels to structures and finally clinical reasoning within one architecture. Unlike prior task-specific pipelines, PULSE learns task-invariant cardiac priors, generalizes robustly across datasets, and can be adapted to new imaging modalities with minimal supervision. This moves the field closer to a scalable, foundation style cardiac analysis framework.

</details>


### [55] [FloodDiffusion: Tailored Diffusion Forcing for Streaming Motion Generation](https://arxiv.org/abs/2512.03520)
*Yiyi Cai,Yuhan Wu,Kunhang Li,You Zhou,Bo Zheng,Haiyang Liu*

Main category: cs.CV

TL;DR: FloodDiffusion is a new framework for text-driven streaming human motion generation that uses diffusion forcing with three key improvements to achieve state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods for text-driven human motion generation rely on chunk-by-chunk or auto-regressive approaches with diffusion heads, which may not be optimal for streaming scenarios with time-varying text prompts. The authors aim to create a framework that can generate text-aligned, seamless motion sequences with real-time latency.

Method: The framework adopts a diffusion forcing approach tailored with three key improvements: (1) training with bi-directional attention instead of casual attention, (2) implementing a lower triangular time scheduler instead of random scheduling, and (3) utilizing a continuous time-varying way to introduce text conditioning.

Result: FloodDiffusion achieves state-of-the-art performance on streaming motion generation, reaching an FID of 0.057 on the HumanML3D benchmark, demonstrating the effectiveness of the improved diffusion forcing framework.

Conclusion: The diffusion forcing-based framework with the proposed three improvements successfully models real motion distributions and achieves superior performance for text-driven streaming human motion generation, making it the first diffusion forcing approach to reach state-of-the-art results in this task.

Abstract: We present FloodDiffusion, a new framework for text-driven, streaming human motion generation. Given time-varying text prompts, FloodDiffusion generates text-aligned, seamless motion sequences with real-time latency. Unlike existing methods that rely on chunk-by-chunk or auto-regressive model with diffusion head, we adopt a diffusion forcing framework to model this time-series generation task under time-varying control events. We find that a straightforward implementation of vanilla diffusion forcing (as proposed for video models) fails to model real motion distributions. We demonstrate that to guarantee modeling the output distribution, the vanilla diffusion forcing must be tailored to: (i) train with a bi-directional attention instead of casual attention; (ii) implement a lower triangular time scheduler instead of a random one; (iii) utilize a continues time-varying way to introduce text conditioning. With these improvements, we demonstrate in the first time that the diffusion forcing-based framework achieves state-of-the-art performance on the streaming motion generation task, reaching an FID of 0.057 on the HumanML3D benchmark. Models, code, and weights are available. https://shandaai.github.io/FloodDiffusion/

</details>


### [56] [OpenTrack3D: Towards Accurate and Generalizable Open-Vocabulary 3D Instance Segmentation](https://arxiv.org/abs/2512.03532)
*Zhishan Zhou,Siyuan Wei,Zengran Wang,Chunjie Wang,Xiaosheng Yan,Xiao Liu*

Main category: cs.CV

TL;DR: OpenTrack3D: A mesh-free framework for open-vocabulary 3D instance segmentation that uses visual-spatial tracking for online proposal generation and MLLMs for compositional reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing methods have two key limitations: (1) reliance on dataset-specific proposal networks or mesh-based superpoints, making them inapplicable in mesh-free scenarios and limiting generalization to novel scenes; (2) weak textual reasoning of CLIP-based classifiers that struggle with compositional and functional user queries.

Method: OpenTrack3D uses a visual-spatial tracker to construct cross-view consistent object proposals online from RGB-D streams. It leverages 2D open-vocabulary segmentation, lifts masks to 3D point clouds using depth, extracts mask-guided instance features using DINO, and fuses visual-spatial cues for consistency. Optional superpoints refinement when mesh is available, and replaces CLIP with multi-modal LLMs for better compositional reasoning.

Result: State-of-the-art performance on diverse benchmarks including ScanNet200, Replica, ScanNet++, and SceneFun3D, demonstrating strong generalization capabilities.

Conclusion: OpenTrack3D provides a generalizable and accurate framework for open-vocabulary 3D instance segmentation that works in mesh-free environments and handles complex compositional queries through MLLM integration.

Abstract: Generalizing open-vocabulary 3D instance segmentation (OV-3DIS) to diverse, unstructured, and mesh-free environments is crucial for robotics and AR/VR, yet remains a significant challenge. We attribute this to two key limitations of existing methods: (1) proposal generation relies on dataset-specific proposal networks or mesh-based superpoints, rendering them inapplicable in mesh-free scenarios and limiting generalization to novel scenes; and (2) the weak textual reasoning of CLIP-based classifiers, which struggle to recognize compositional and functional user queries. To address these issues, we introduce OpenTrack3D, a generalizable and accurate framework. Unlike methods that rely on pre-generated proposals, OpenTrack3D employs a novel visual-spatial tracker to construct cross-view consistent object proposals online. Given an RGB-D stream, our pipeline first leverages a 2D open-vocabulary segmenter to generate masks, which are lifted to 3D point clouds using depth. Mask-guided instance features are then extracted using DINO feature maps, and our tracker fuses visual and spatial cues to maintain instance consistency. The core pipeline is entirely mesh-free, yet we also provide an optional superpoints refinement module to further enhance performance when scene mesh is available. Finally, we replace CLIP with a multi-modal large language model (MLLM), significantly enhancing compositional reasoning for complex user queries. Extensive experiments on diverse benchmarks, including ScanNet200, Replica, ScanNet++, and SceneFun3D, demonstrate state-of-the-art performance and strong generalization capabilities.

</details>


### [57] [BlurDM: A Blur Diffusion Model for Image Deblurring](https://arxiv.org/abs/2512.03979)
*Jin-Ting He,Fu-Jen Tsai,Yan-Tsung Peng,Min-Hung Chen,Chia-Wen Lin,Yen-Yu Lin*

Main category: cs.CV

TL;DR: BlurDM integrates blur formation process into diffusion models for dynamic scene deblurring, achieving state-of-the-art performance by simultaneously denoising and deblurring.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion models for deblurring fail to leverage the intrinsic nature of the blurring process, limiting their full potential for dynamic scene deblurring.

Method: BlurDM uses a dual-diffusion forward scheme that diffuses both noise and blur onto sharp images, with a reverse process that performs simultaneous denoising and deblurring. The model operates in latent space for efficiency and integrates as a flexible prior generation network.

Result: BlurDM significantly and consistently enhances existing deblurring methods across four benchmark datasets, demonstrating superior performance.

Conclusion: BlurDM effectively integrates blur formation into diffusion models, providing a powerful approach for dynamic scene deblurring that outperforms existing methods.

Abstract: Diffusion models show promise for dynamic scene deblurring; however, existing studies often fail to leverage the intrinsic nature of the blurring process within diffusion models, limiting their full potential. To address it, we present a Blur Diffusion Model (BlurDM), which seamlessly integrates the blur formation process into diffusion for image deblurring. Observing that motion blur stems from continuous exposure, BlurDM implicitly models the blur formation process through a dual-diffusion forward scheme, diffusing both noise and blur onto a sharp image. During the reverse generation process, we derive a dual denoising and deblurring formulation, enabling BlurDM to recover the sharp image by simultaneously denoising and deblurring, given pure Gaussian noise conditioned on the blurred image as input. Additionally, to efficiently integrate BlurDM into deblurring networks, we perform BlurDM in the latent space, forming a flexible prior generation network for deblurring. Extensive experiments demonstrate that BlurDM significantly and consistently enhances existing deblurring methods on four benchmark datasets. The source code is available at https://github.com/Jin-Ting-He/BlurDM.

</details>


### [58] [DIQ-H: Evaluating Hallucination Persistence in VLMs Under Temporal Visual Degradation](https://arxiv.org/abs/2512.03992)
*Zexin Lin,Hawen Wan,Yebin Zhong,Xiaoqiang*

Main category: cs.CV

TL;DR: DIQ-H is the first benchmark for evaluating Vision-Language Model robustness under dynamic visual degradation in temporal sequences, focusing on hallucination persistence, error recovery, and temporal consistency.


<details>
  <summary>Details</summary>
Motivation: Existing VLM benchmarks focus on static, high-quality images and ignore critical real-world failure modes where transient visual corruption induces hallucinations that persist across subsequent frames in continuous visual streams.

Method: DIQ-H applies physics-based corruptions (motion blur, sensor noise, compression artifacts) to temporal sequences and measures hallucination persistence, error recovery, and temporal consistency through multi-turn QA tasks. Uses Uncertainty-Guided Iterative Refinement (UIR) for scalable annotation with lightweight VLMs and uncertainty filtering.

Result: Experiments on 16 state-of-the-art VLMs reveal substantial robustness gaps: GPT-4o achieves only 78.5% recovery rate, while open-source models struggle with temporal consistency at less than 60%. UIR achieves 15.3% accuracy improvement for annotation.

Conclusion: DIQ-H provides a comprehensive platform for evaluating VLM reliability in real-world deployments, addressing critical temporal degradation and error propagation issues that existing benchmarks ignore.

Abstract: Vision-Language Models (VLMs) deployed in safety-critical applications such as autonomous driving must handle continuous visual streams under imperfect conditions. However, existing benchmarks focus on static, high-quality images and ignore temporal degradation and error propagation, which are critical failure modes where transient visual corruption induces hallucinations that persist across subsequent frames. We introduce DIQ-H, the first benchmark for evaluating VLM robustness under dynamic visual degradation in temporal sequences. DIQ-H applies physics-based corruptions including motion blur, sensor noise, and compression artifacts, and measures hallucination persistence, error recovery, and temporal consistency through multi-turn question-answering tasks. To enable scalable annotation, we propose Uncertainty-Guided Iterative Refinement (UIR), which generates reliable pseudo-ground-truth using lightweight VLMs with uncertainty filtering, achieving a 15.3 percent accuracy improvement. Experiments on 16 state-of-the-art VLMs reveal substantial robustness gaps: even advanced models such as GPT-4o achieve only a 78.5 percent recovery rate, while open-source models struggle with temporal consistency at less than 60 percent. DIQ-H provides a comprehensive platform for evaluating VLM reliability in real-world deployments.

</details>


### [59] [Highly Efficient Test-Time Scaling for T2I Diffusion Models with Text Embedding Perturbation](https://arxiv.org/abs/2512.03996)
*Hang Xu,Linjiang Huang,Feng Zhao*

Main category: cs.CV

TL;DR: Text Embedding Perturbation (TEP) enhances test-time scaling in diffusion models by adding complementary frequency-domain randomness to spatial noise, improving diversity and quality with minimal computation.


<details>
  <summary>Details</summary>
Motivation: Current test-time scaling methods for text-to-image diffusion models focus on search strategies and reward models, but ignore how the stochastic nature of noise affects performance. The paper explores how different randomness formats impact generation quality and diversity.

Method: Introduces text embedding perturbation that complements spatial noise: (1) step-based text embedding perturbation combined with frequency-guided noise schedules, and (2) adaptive perturbation intensity based on frequency-specific contributions and tolerance levels.

Result: The approach integrates seamlessly with existing TTS methods and shows significant improvements on multiple benchmarks with almost no additional computational cost.

Conclusion: Text embedding perturbation provides complementary high-frequency randomness to spatial noise's low-frequency bias, enhancing generative diversity and quality in diffusion models through frequency-aware adaptive perturbation.

Abstract: Test-time scaling (TTS) aims to achieve better results by increasing random sampling and evaluating samples based on rules and metrics. However, in text-to-image(T2I) diffusion models, most related works focus on search strategies and reward models, yet the impact of the stochastic characteristic of noise in T2I diffusion models on the method's performance remains unexplored. In this work, we analyze the effects of randomness in T2I diffusion models and explore a new format of randomness for TTS: text embedding perturbation, which couples with existing randomness like SDE-injected noise to enhance generative diversity and quality. We start with a frequency-domain analysis of these formats of randomness and their impact on generation, and find that these two randomness exhibit complementary behavior in the frequency domain: spatial noise favors low-frequency components (early steps), while text embedding perturbation enhances high-frequency details (later steps), thereby compensating for the potential limitations of spatial noise randomness in high-frequency manipulation. Concurrently, text embedding demonstrates varying levels of tolerance to perturbation across different dimensions of the generation process. Specifically, our method consists of two key designs: (1) Introducing step-based text embedding perturbation, combining frequency-guided noise schedules with spatial noise perturbation. (2) Adapting the perturbation intensity selectively based on their frequency-specific contributions to generation and tolerance to perturbation. Our approach can be seamlessly integrated into existing TTS methods and demonstrates significant improvements on multiple benchmarks with almost no additional computation. Code is available at \href{https://github.com/xuhang07/TEP-Diffusion}{https://github.com/xuhang07/TEP-Diffusion}.

</details>


### [60] [Divide, then Ground: Adapting Frame Selection to Query Types for Long-Form Video Understanding](https://arxiv.org/abs/2512.04000)
*Jialuo Li,Bin Li,Jiahao Li,Yan Lu*

Main category: cs.CV

TL;DR: DIG is a training-free framework that adapts frame selection strategy based on query type: uniform sampling for global queries and query-aware selection for localized queries, improving LMM performance on long-form video understanding.


<details>
  <summary>Details</summary>
Motivation: Current LMMs for long-form video understanding face limitations due to constrained context lengths and high computational costs of processing dense video tokens. Existing query-aware frame selection methods incur significant overhead, but the paper questions whether such complex mechanisms are always necessary.

Method: DIG proposes a training-free framework that first identifies query types (global vs localized). For global queries, it uses efficient uniform sampling. For localized queries, it activates a specialized pipeline to extract query-relevant frames. The approach adapts strategy based on query type without requiring training.

Result: Experiments on three long-form video understanding benchmarks show DIG consistently outperforms existing baselines and robustly improves LMM performance, even when scaling input frames to 256.

Conclusion: Complex query-aware frame selection mechanisms are not universally necessary; adaptive strategies based on query typology (global vs localized) provide both efficiency and performance improvements for long-form video understanding with LMMs.

Abstract: The application of Large Multimodal Models (LMMs) to long-form video understanding is constrained by limited context lengths and the computationally prohibitive cost of processing dense video tokens. Consequently, recent research has focused on query-aware frame selection, methods that often incur significant computational overhead. This paper challenges the assumption that such complex search mechanisms are universally necessary. We first identify and validate a query typology distinguishing between global query and localized query. We demonstrate that while uniform sampling is both effective and efficient for global queries, localized queries indeed necessitate query-aware selection for optimal performance. Building on this insight, we propose DIG, a training-free frame selection framework that adapts its strategy based on the query type. Specifically,DIG employs efficient uniform sampling for global queries while activating a specialized pipeline to extract query-relevant frames for localized queries. Experiments on three long-form video understanding benchmarks demonstrate that DIG consistently outperforms existing baselines and robustly improves LMM performance, even when scaling the input frame count to 256.

</details>


### [61] [On the Temporality for Sketch Representation Learning](https://arxiv.org/abs/2512.04007)
*Marcelo Isaias de Moraes Junior,Moacir Antonelli Ponti*

Main category: cs.CV

TL;DR: Investigates whether treating sketches as sequences is justified and which temporal orders matter most for sketch representation learning.


<details>
  <summary>Details</summary>
Motivation: Despite advances in sketch representation learning, there's still uncertainty about the true relevance of temporal aspects and whether treating sketches as sequences is justified.

Method: Analyzes different approaches to modeling sketches as sequences, comparing absolute vs. relative coordinates and autoregressive vs. non-autoregressive decoders.

Result: Absolute coordinates consistently outperform relative ones; non-autoregressive decoders beat autoregressive ones; temporal importance depends on both order and task.

Conclusion: While treating sketches as sequences is valid with traditional positional encodings, the specific temporal order matters and absolute coordinates with non-autoregressive decoders work best.

Abstract: Sketches are simple human hand-drawn abstractions of complex scenes and real-world objects. Although the field of sketch representation learning has advanced significantly, there is still a gap in understanding the true relevance of the temporal aspect to the quality of these representations. This work investigates whether it is indeed justifiable to treat sketches as sequences, as well as which internal orders play a more relevant role. The results indicate that, although the use of traditional positional encodings is valid for modeling sketches as sequences, absolute coordinates consistently outperform relative ones. Furthermore, non-autoregressive decoders outperform their autoregressive counterparts. Finally, the importance of temporality was shown to depend on both the order considered and the task evaluated.

</details>


### [62] [CartoMapQA: A Fundamental Benchmark Dataset Evaluating Vision-Language Models on Cartographic Map Understanding](https://arxiv.org/abs/2512.03558)
*Huy Quang Ung,Guillaume Habault,Yasutaka Nishimura,Hao Niu,Roberto Legaspi,Tomoki Oya,Ryoichi Kojima,Masato Taya,Chihiro Ono,Atsunori Minamikawa,Yan Liu*

Main category: cs.CV

TL;DR: CartoMapQA is a new benchmark for evaluating Visual-Language Models' understanding of cartographic maps through QA tasks, revealing models struggle with map semantics, geospatial reasoning, and OCR errors.


<details>
  <summary>Details</summary>
Motivation: While Visual-Language Models (LVLMs) have advanced in integrating visual and textual information, their ability to interpret cartographic maps remains largely unexplored, despite the importance of map understanding for real-world applications like navigation and urban planning.

Method: Created CartoMapQA benchmark with over 2000 samples, each containing a cartographic map, question (open-ended or multiple-choice), and ground-truth answer. Tasks cover low-, mid-, and high-level map interpretation skills including symbol recognition, embedded information extraction, scale interpretation, and route-based reasoning.

Result: Evaluation of both open-source and proprietary LVLMs reveals persistent challenges: models frequently struggle with map-specific semantics, exhibit limited geospatial reasoning, and are prone to OCR-related errors.

Conclusion: CartoMapQA provides a valuable tool for identifying weaknesses in LVLMs' map understanding capabilities, guiding future architectural improvements to develop models better equipped for real-world applications requiring robust map interpretation.

Abstract: The rise of Visual-Language Models (LVLMs) has unlocked new possibilities for seamlessly integrating visual and textual information. However, their ability to interpret cartographic maps remains largely unexplored. In this paper, we introduce CartoMapQA, a benchmark specifically designed to evaluate LVLMs' understanding of cartographic maps through question-answering tasks. The dataset includes over 2000 samples, each composed of a cartographic map, a question (with open-ended or multiple-choice answers), and a ground-truth answer. These tasks span key low-, mid- and high-level map interpretation skills, including symbol recognition, embedded information extraction, scale interpretation, and route-based reasoning. Our evaluation of both open-source and proprietary LVLMs reveals persistent challenges: models frequently struggle with map-specific semantics, exhibit limited geospatial reasoning, and are prone to Optical Character Recognition (OCR)-related errors. By isolating these weaknesses, CartoMapQA offers a valuable tool for guiding future improvements in LVLM architectures. Ultimately, it supports the development of models better equipped for real-world applications that depend on robust and reliable map understanding, such as navigation, geographic search, and urban planning. Our source code and data are openly available to the research community at: https://github.com/ungquanghuy-kddi/CartoMapQA.git

</details>


### [63] [PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation](https://arxiv.org/abs/2512.04025)
*Xiaolong Li,Youping Gu,Xi Lin,Weijie Wang,Bohan Zhuang*

Main category: cs.CV

TL;DR: PSA introduces multi-level pooled KV representations instead of binary masks for sparse attention, enabling finer granularity and better information preservation while maintaining computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Quadratic complexity of attention mechanisms is a bottleneck for scaling foundation models. Current sparse attention methods use binary masks that discard entire key-value blocks, causing significant information loss under high sparsity.

Method: Pyramid Sparse Attention (PSA) replaces binary masking with multi-level pooled KV representations. Each query block dynamically allocates lower pooling levels to critical KV blocks and higher levels to less important ones, creating an informative interpolation between full retention and complete pruning. Uses hardware-friendly kernel with decoupled block-tile design.

Result: PSA consistently outperforms or achieves comparable performance to existing sparse attention baselines across video understanding and generation benchmarks, with superior efficiency-quality trade-offs, preserving contextual information and visual fidelity.

Conclusion: PSA effectively mitigates information loss in sparse attention while preserving computational efficiency, offering a versatile solution for both video understanding and generation tasks with better trade-offs than binary masking approaches.

Abstract: Attention mechanisms are the core of foundation models, but their quadratic complexity remains a critical bottleneck for scaling. This challenge has driven the development of efficient attention mechanisms, with sparsity emerging as the dominant paradigm. Current methods typically retain or discard entire key-value blocks with binary masks, resulting in substantial information loss under high sparsity. To mitigate this gap, we present Pyramid Sparse Attention (PSA), a versatile module applicable to both video understanding and generation tasks. Instead of binary masking, PSA introduces multi-level pooled KV representations, enabling finer mask granularity. Specifically, each query block dynamically allocates lower pooling levels to critical KV blocks and higher levels to less important ones, creating an informative interpolation between full retention and complete pruning. This design, analogous to fixed-point quantization and classical feature pyramid networks in computer vision, effectively mitigates information loss while preserving computational efficiency under a low compute budget. It works with a native, hardware-friendly kernel that leverages decoupled block-tile design to ensure efficient execution. Across video understanding and generation benchmarks, PSA preserves contextual information and visual fidelity, consistently outperforming or achieving comparable performance over existing sparse attention baselines with superior efficiency-quality trade-offs. Our code and model weights are publicly available at: http://ziplab.co/PSA

</details>


### [64] [GAOT: Generating Articulated Objects Through Text-Guided Diffusion Models](https://arxiv.org/abs/2512.03566)
*Hao Sun,Lei Fan,Donglin Di,Shaohui Liu*

Main category: cs.CV

TL;DR: GAOT: A three-phase framework for generating articulated 3D objects from text prompts using diffusion models and hypergraph learning.


<details>
  <summary>Details</summary>
Motivation: Existing articulated object generation models lack text conditioning capability, creating a significant gap between textual descriptions and 3D articulated object representations.

Method: Three-phase framework: 1) Fine-tune point cloud generation model for coarse object representation from text, 2) Hypergraph-based learning to refine representations (parts as vertices), 3) Diffusion model to generate joints (edges) based on object parts.

Result: Extensive experiments on PartNet-Mobility dataset demonstrate effectiveness and superior performance over previous methods.

Conclusion: GAOT successfully bridges the gap between text prompts and articulated 3D object generation through a novel three-phase approach combining diffusion models with hypergraph learning.

Abstract: Articulated object generation has seen increasing advancements, yet existing models often lack the ability to be conditioned on text prompts. To address the significant gap between textual descriptions and 3D articulated object representations, we propose GAOT, a three-phase framework that generates articulated objects from text prompts, leveraging diffusion models and hypergraph learning in a three-step process. First, we fine-tune a point cloud generation model to produce a coarse representation of objects from text prompts. Given the inherent connection between articulated objects and graph structures, we design a hypergraph-based learning method to refine these coarse representations, representing object parts as graph vertices. Finally, leveraging a diffusion model, the joints of articulated objects-represented as graph edges-are generated based on the object parts. Extensive qualitative and quantitative experiments on the PartNet-Mobility dataset demonstrate the effectiveness of our approach, achieving superior performance over previous methods.

</details>


### [65] [Fast & Efficient Normalizing Flows and Applications of Image Generative Models](https://arxiv.org/abs/2512.04039)
*Sandeep Nagar*

Main category: cs.CV

TL;DR: This thesis advances generative models (normalizing flows) with architectural improvements and applies them to real-world computer vision tasks including agricultural quality assessment, geological mapping, privacy preservation, and art restoration.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency of generative models (particularly normalizing flows) and demonstrate their practical utility in solving diverse real-world computer vision challenges across agriculture, geology, privacy preservation, and cultural heritage.

Method: Two-part approach: 1) Six architectural innovations for normalizing flows including invertible convolution layers, Quad-coupling layers, parallel inversion algorithms, and Affine-StableSR for super-resolution; 2) Application of generative models (Conditional GANs, autoencoders, Stable Diffusion) to agricultural quality assessment, geological mapping, privacy preservation, and art restoration.

Result: Developed more efficient normalizing flow architectures with proven invertibility conditions, created compact super-resolution models, achieved good accuracy in seed purity testing, improved feature extraction for geological mapping, advanced privacy-preserving techniques, and effectively handled multiple degradation types in art restoration.

Conclusion: The thesis successfully advances both theoretical efficiency of generative models and demonstrates their practical applicability across diverse domains, contributing to both algorithmic improvements and real-world problem-solving in computer vision.

Abstract: This thesis presents novel contributions in two primary areas: advancing the efficiency of generative models, particularly normalizing flows, and applying generative models to solve real-world computer vision challenges. The first part introduce significant improvements to normalizing flow architectures through six key innovations: 1) Development of invertible 3x3 Convolution layers with mathematically proven necessary and sufficient conditions for invertibility, (2) introduction of a more efficient Quad-coupling layer, 3) Design of a fast and efficient parallel inversion algorithm for kxk convolutional layers, 4) Fast & efficient backpropagation algorithm for inverse of convolution, 5) Using inverse of convolution, in Inverse-Flow, for the forward pass and training it using proposed backpropagation algorithm, and 6) Affine-StableSR, a compact and efficient super-resolution model that leverages pre-trained weights and Normalizing Flow layers to reduce parameter count while maintaining performance.
  The second part: 1) An automated quality assessment system for agricultural produce using Conditional GANs to address class imbalance, data scarcity and annotation challenges, achieving good accuracy in seed purity testing; 2) An unsupervised geological mapping framework utilizing stacked autoencoders for dimensionality reduction, showing improved feature extraction compared to conventional methods; 3) We proposed a privacy preserving method for autonomous driving datasets using on face detection and image inpainting; 4) Utilizing Stable Diffusion based image inpainting for replacing the detected face and license plate to advancing privacy-preserving techniques and ethical considerations in the field.; and 5) An adapted diffusion model for art restoration that effectively handles multiple types of degradation through unified fine-tuning.

</details>


### [66] [Global-Local Aware Scene Text Editing](https://arxiv.org/abs/2512.03574)
*Fuxiang Yang,Tonghua Su,Donglin Di,Yin Chen,Xiangqian Wu,Zhongjie Wang,Lei Fan*

Main category: cs.CV

TL;DR: GLASTE is a new scene text editing framework that addresses inconsistency and length-insensitivity issues by combining global contextual information with local features through a global-local structure, joint losses, and size-independent style vectors.


<details>
  <summary>Details</summary>
Motivation: Existing scene text editing methods suffer from two major challenges: inconsistency (failing to maintain coherence between edited patches and surrounding areas) and length-insensitivity (struggling with significant text length differences before and after editing).

Method: Proposes GLASTE framework with: 1) global-local combination structure, 2) joint global and local losses, 3) enhanced text image features, 4) size-independent text style vectors, and 5) affine fusion to fill target text while maintaining aspect ratio.

Result: Extensive experiments on real-world datasets show GLASTE outperforms previous methods in both quantitative metrics and qualitative results, effectively mitigating the inconsistency and length-insensitivity challenges.

Conclusion: GLASTE successfully addresses key challenges in scene text editing by simultaneously incorporating global contextual information with local features, enabling consistent editing that handles varying text lengths while preserving style and background harmony.

Abstract: Scene Text Editing (STE) involves replacing text in a scene image with new target text while preserving both the original text style and background texture. Existing methods suffer from two major challenges: inconsistency and length-insensitivity. They often fail to maintain coherence between the edited local patch and the surrounding area, and they struggle to handle significant differences in text length before and after editing. To tackle these challenges, we propose an end-to-end framework called Global-Local Aware Scene Text Editing (GLASTE), which simultaneously incorporates high-level global contextual information along with delicate local features. Specifically, we design a global-local combination structure, joint global and local losses, and enhance text image features to ensure consistency in text style within local patches while maintaining harmony between local and global areas. Additionally, we express the text style as a vector independent of the image size, which can be transferred to target text images of various sizes. We use an affine fusion to fill target text images into the editing patch while maintaining their aspect ratio unchanged. Extensive experiments on real-world datasets validate that our GLASTE model outperforms previous methods in both quantitative metrics and qualitative results and effectively mitigates the two challenges.

</details>


### [67] [UniComp: Rethinking Video Compression Through Informational Uniqueness](https://arxiv.org/abs/2512.03575)
*Chao Yuan,Shimin Chen,Minliang Lin,Limeng Qiao,Guanglu Wan,Lin Ma*

Main category: cs.CV

TL;DR: UniComp: Information uniqueness driven video compression framework that maximizes information fidelity under computational constraints by measuring token redundancy and progressive compression.


<details>
  <summary>Details</summary>
Motivation: To address video compression from an information-theoretic perspective, focusing on maximizing information fidelity under constrained computational budgets, distinct from attention-based compression methods.

Method: Formulates compression as optimization minimizing conditional entropy, introduces information uniqueness to measure token redundancy, and implements three progressive modules: Frame Group Fusion, Token Allocation, and Spatial Dynamic Compression.

Result: Extensive experiments show UniComp consistently outperforms existing compression methods in preserving essential visual tokens under limited computational budgets.

Conclusion: Information uniqueness plays a pivotal role in token compression efficacy, enabling superior video compression performance under computational constraints.

Abstract: Distinct from attention-based compression methods, this paper presents an information uniqueness driven video compression framework, termed UniComp, which aims to maximize the information fidelity of video representations under constrained computational budgets. Starting from the information-theoretic perspective, we formulate the vision compression as an optimization problem that minimizes conditional entropy (reconstruction error) between retained and full tokens. To achieve this, we introduce the notion of information uniqueness to measure intrinsic redundancy among tokens to link with reconstruction error. Based on uniqueness, we design three modules-Frame Group Fusion, Token Allocation, and Spatial Dynamic Compression-that progressively perform semantic frame grouping, adaptive resource allocation, and fine-grained spatial compression. Extensive experiments demonstrate that UniComp consistently outperforms existing compression methods in preserving essential visual tokens under limited computational budgets, highlighting the pivotal role of information uniqueness in token compression efficacy.

</details>


### [68] [Cross-Stain Contrastive Learning for Paired Immunohistochemistry and Histopathology Slide Representation Learning](https://arxiv.org/abs/2512.03577)
*Yizhi Zhang,Lei Fan,Zhulin Tao,Donglin Di,Yang Song,Sidong Liu,Cong Cong*

Main category: cs.CV

TL;DR: CSCL is a two-stage pretraining framework using cross-stain contrastive learning to create transferable WSI representations by aligning H&E with multiple IHC stains through patch-level contrastive alignment and slide-level attention fusion.


<details>
  <summary>Details</summary>
Motivation: Current WSI representations are limited by scarce multi-stain datasets and inter-stain misalignment issues that degrade feature consistency across different stains, hindering robust cross-stain representation learning.

Method: Two-stage pretraining: 1) Patch-wise contrastive alignment with lightweight adapter to align H&E features with IHC contextual cues; 2) Slide-level MIL with cross-stain attention fusion and global alignment modules to integrate stain-specific features and enforce consistency across stains.

Result: Experiments show consistent improvements on cancer subtype classification, IHC biomarker status classification, and survival prediction, producing high-quality, transferable H&E slide-level representations.

Conclusion: CSCL successfully addresses inter-stain misalignment challenges and enables robust cross-stain representation learning, with code and curated five-stain dataset publicly available.

Abstract: Universal, transferable whole-slide image (WSI) representations are central to computational pathology. Incorporating multiple markers (e.g., immunohistochemistry, IHC) alongside H&E enriches H&E-based features with diverse, biologically meaningful information. However, progress is limited by the scarcity of well-aligned multi-stain datasets. Inter-stain misalignment shifts corresponding tissue across slides, hindering consistent patch-level features and degrading slide-level embeddings. To address this, we curated a slide-level aligned, five-stain dataset (H&E, HER2, KI67, ER, PGR) to enable paired H&E-IHC learning and robust cross-stain representation. Leveraging this dataset, we propose Cross-Stain Contrastive Learning (CSCL), a two-stage pretraining framework with a lightweight adapter trained using patch-wise contrastive alignment to improve the compatibility of H&E features with corresponding IHC-derived contextual cues, and slide-level representation learning with Multiple Instance Learning (MIL), which uses a cross-stain attention fusion module to integrate stain-specific patch features and a cross-stain global alignment module to enforce consistency among slide-level embeddings across different stains. Experiments on cancer subtype classification, IHC biomarker status classification, and survival prediction show consistent gains, yielding high-quality, transferable H&E slide-level representations. The code and data are available at https://github.com/lily-zyz/CSCL.

</details>


### [69] [Dynamic Optical Test for Bot Identification (DOT-BI): A simple check to identify bots in surveys and online processes](https://arxiv.org/abs/2512.03580)
*Malte Bleeker,Mauro Gotsch*

Main category: cs.CV

TL;DR: DOT-BI is a CAPTCHA-like test that uses motion perception of hidden numbers against textured backgrounds to distinguish humans from bots in online surveys and processes.


<details>
  <summary>Details</summary>
Motivation: Need for quick, easy methods to differentiate human respondents from automated systems in surveys and online processes, addressing bot detection challenges.

Method: Display a 'hidden' number with same random black-and-white pixel texture as background; number becomes perceptible to humans through motion and scale differences across frames, but not to frame-by-frame algorithmic processing.

Result: State-of-the-art multimodal models (GPT-5-Thinking, Gemini 2.5 Pro) fail to extract correct values; online survey (n=182) shows 99.5% success rate with 10.7s average completion; lab study (n=39) shows no negative effects on perceived ease-of-use or completion time.

Conclusion: DOT-BI is an effective, user-friendly bot identification method that leverages human motion perception, with code and pre-rendered variants released for adoption in surveys and online processes.

Abstract: We propose the Dynamic Optical Test for Bot Identification (DOT-BI): a quick and easy method that uses human perception of motion to differentiate between human respondents and automated systems in surveys and online processes. In DOT-BI, a 'hidden' number is displayed with the same random black-and-white pixel texture as its background. Only the difference in motion and scale between the number and the background makes the number perceptible to humans across frames, while frame-by-frame algorithmic processing yields no meaningful signal. We conducted two preliminary assessments. Firstly, state-of-the-art, video-capable, multimodal models (GPT-5-Thinking and Gemini 2.5 Pro) fail to extract the correct value, even when given explicit instructions about the mechanism. Secondly, in an online survey (n=182), 99.5% (181/182) of participants solved the task, with an average end-to-end completion time of 10.7 seconds; a supervised lab study (n=39) found no negative effects on perceived ease-of-use or completion time relative to a control. We release code to generate tests and 100+ pre-rendered variants to facilitate adoption in surveys and online processes.

</details>


### [70] [Beyond Boundary Frames: Audio-Visual Semantic Guidance for Context-Aware Video Interpolation](https://arxiv.org/abs/2512.03590)
*Yuchen Deng,Xiuyang Wu,Hai-Tao Zheng,Jie Wang,Feidiao Yang,Yuxing Han*

Main category: cs.CV

TL;DR: BBF is a context-aware video frame interpolation framework that uses multimodal conditioning (text, audio, images, video) and a decoupled fusion mechanism with DiT backbone, achieving state-of-the-art performance on both generic and audio-visual synchronized interpolation tasks.


<details>
  <summary>Details</summary>
Motivation: Existing video frame interpolation methods struggle with fast, complex, non-linear motion patterns and fail to produce sharp, temporally consistent frames in fine-grained tasks like audio-visual synchronized interpolation. Current diffusion-based approaches still have limitations in covering diverse application scenarios.

Method: 1. Enhanced input design for flexible multimodal conditioning (text, audio, images, video). 2. Decoupled multimodal fusion mechanism that sequentially injects different conditional signals into a DiT backbone. 3. Progressive multi-stage training paradigm with start-end frame difference embedding for dynamic data sampling and loss weighting adjustment.

Result: Extensive experiments show BBF outperforms specialized state-of-the-art methods on both generic interpolation and audio-visual synchronized interpolation tasks, establishing a unified framework for video frame interpolation under coordinated multi-channel conditioning.

Conclusion: BBF successfully addresses limitations of existing methods by providing a context-aware, multimodal framework that maintains foundation model generation abilities while achieving superior performance across diverse interpolation scenarios, including challenging audio-visual synchronized tasks.

Abstract: Handling fast, complex, and highly non-linear motion patterns has long posed challenges for video frame interpolation. Although recent diffusion-based approaches improve upon traditional optical-flow-based methods, they still struggle to cover diverse application scenarios and often fail to produce sharp, temporally consistent frames in fine-grained motion tasks such as audio-visual synchronized interpolation. To address these limitations, we introduce BBF (Beyond Boundary Frames), a context-aware video frame interpolation framework, which could be guided by audio/visual semantics. First, we enhance the input design of the interpolation model so that it can flexibly handle multiple conditional modalities, including text, audio, images, and video. Second, we propose a decoupled multimodal fusion mechanism that sequentially injects different conditional signals into a DiT backbone. Finally, to maintain the generation abilities of the foundation model, we adopt a progressive multi-stage training paradigm, where the start-end frame difference embedding is used to dynamically adjust both the data sampling and the loss weighting. Extensive experimental results demonstrate that BBF outperforms specialized state-of-the-art methods on both generic interpolation and audio-visual synchronized interpolation tasks, establishing a unified framework for video frame interpolation under coordinated multi-channel conditioning.

</details>


### [71] [Harnessing Hypergraphs in Geometric Deep Learning for 3D RNA Inverse Folding](https://arxiv.org/abs/2512.03592)
*Guang Yang,Lei Fan*

Main category: cs.CV

TL;DR: HyperRNA is a generative model using hypergraphs and encoder-decoder architecture for RNA inverse folding, outperforming existing methods on sequence design tasks.


<details>
  <summary>Details</summary>
Motivation: The RNA inverse folding problem is crucial for RNA design but challenging due to complex sequence-structure relationships. Current methods need improvement for accurate sequence generation from desired secondary structures.

Method: HyperRNA framework with three components: 1) Preprocessing constructs graphs from RNA backbone atom coordinates using 3-bead coarse-grained representation; 2) Encoding uses attention embedding and hypergraph-based encoder to capture higher-order dependencies; 3) Decoding generates RNA sequences autoregressively.

Result: Experimental results on PDBBind and RNAsolo datasets show HyperRNA outperforms existing RNA design methods for both RNA sequence generation and RNA-protein complex sequence generation tasks.

Conclusion: HyperRNA demonstrates superior performance in RNA inverse folding and highlights the potential of hypergraph-based approaches for RNA engineering applications.

Abstract: The RNA inverse folding problem, a key challenge in RNA design, involves identifying nucleotide sequences that can fold into desired secondary structures, which are critical for ensuring molecular stability and function. The inherent complexity of this task stems from the intricate relationship between sequence and structure, making it particularly challenging. In this paper, we propose a framework, named HyperRNA, a generative model with an encoder-decoder architecture that leverages hypergraphs to design RNA sequences. Specifically, our HyperRNA model consists of three main components: preprocessing, encoding and decoding.
  In the preprocessing stage, graph structures are constructed by extracting the atom coordinates of RNA backbone based on 3-bead coarse-grained representation. The encoding stage processes these graphs, capturing higher order dependencies and complex biomolecular interactions using an attention embedding module and a hypergraph-based encoder. Finally, the decoding stage generates the RNA sequence in an autoregressive manner. We conducted quantitative and qualitative experiments on the PDBBind and RNAsolo datasets to evaluate the inverse folding task for RNA sequence generation and RNA-protein complex sequence generation. The experimental results demonstrate that HyperRNA not only outperforms existing RNA design methods but also highlights the potential of leveraging hypergraphs in RNA engineering.

</details>


### [72] [CloseUpAvatar: High-Fidelity Animatable Full-Body Avatars with Mixture of Multi-Scale Textures](https://arxiv.org/abs/2512.03593)
*David Svitov,Pietro Morerio,Lourdes Agapito,Alessio Del Bue*

Main category: cs.CV

TL;DR: CloseUpAvatar: Articulated human avatar representation using textured planes with adaptive texture switching based on camera distance for realistic close-up rendering while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Existing avatar representations struggle with general camera motions while preserving rendering quality for close-up views. There's a need for a method that can handle wider camera ranges while maintaining high-quality rendering, especially for close-up perspectives.

Method: Represents avatar as textured planes with two sets of learnable textures (low and high-frequency). Automatically switches to high-frequency textures for close cameras and gradually reduces their impact as camera moves away. Uses adaptive rendering quality based on camera distance while limiting number of primitives for performance.

Result: Demonstrates qualitative and quantitative improvements over existing methods in rendering from novel wide-range camera positions. Maintains high FPS by limiting required primitives. Tested on ActorsHQ dataset with high-resolution input images.

Conclusion: CloseUpAvatar enables realistic rendering across wider camera orientation ranges than previous approaches, handling general camera motions while preserving close-up rendering quality through adaptive texture switching.

Abstract: We present a CloseUpAvatar - a novel approach for articulated human avatar representation dealing with more general camera motions, while preserving rendering quality for close-up views. CloseUpAvatar represents an avatar as a set of textured planes with two sets of learnable textures for low and high-frequency detail. The method automatically switches to high-frequency textures only for cameras positioned close to the avatar's surface and gradually reduces their impact as the camera moves farther away. Such parametrization of the avatar enables CloseUpAvatar to adjust rendering quality based on camera distance ensuring realistic rendering across a wider range of camera orientations than previous approaches. We provide experiments using the ActorsHQ dataset with high-resolution input images. CloseUpAvatar demonstrates both qualitative and quantitative improvements over existing methods in rendering from novel wide range camera positions, while maintaining high FPS by limiting the number of required primitives.

</details>


### [73] [HBFormer: A Hybrid-Bridge Transformer for Microtumor and Miniature Organ Segmentation](https://arxiv.org/abs/2512.03597)
*Fuchen Zheng,Xinyi Chen,Weixuan Li,Quanjun Li,Junhua Zhou,Xiaojiao Guo,Xuhang Chen,Chi-Man Pun,Shoujun Zhou*

Main category: cs.CV

TL;DR: HBFormer: A Hybrid-Bridge Transformer architecture for medical image segmentation that combines U-shaped encoder-decoder with Swin Transformer backbone and novel Multi-Scale Feature Fusion decoder to better integrate local details with global context for challenging tasks like microtumor segmentation.


<details>
  <summary>Details</summary>
Motivation: Current Vision Transformers with shifted window-based self-attention struggle to effectively fuse local details with global context, which is critical for challenging medical segmentation tasks like microtumors and miniature organs where both fine-grained boundaries and broad contextual understanding are essential.

Method: Proposes HBFormer with 'Hybrid' design combining U-shaped encoder-decoder framework with Swin Transformer backbone, and 'Bridge' mechanism via novel Multi-Scale Feature Fusion (MFF) decoder that fuses multi-scale encoder features with global context using channel and spatial attention modules built from dilated and depth-wise convolutions.

Result: Achieves state-of-the-art results on challenging medical image segmentation datasets including multi-organ, liver tumor, and bladder tumor benchmarks, demonstrating outstanding capabilities in microtumor and miniature organ segmentation.

Conclusion: HBFormer effectively addresses the local-global feature fusion limitation in existing Vision Transformers through its hybrid-bridge architecture, providing superior performance for challenging medical segmentation tasks requiring both fine-grained boundary definition and broad contextual understanding.

Abstract: Medical image segmentation is a cornerstone of modern clinical diagnostics. While Vision Transformers that leverage shifted window-based self-attention have established new benchmarks in this field, they are often hampered by a critical limitation: their localized attention mechanism struggles to effectively fuse local details with global context. This deficiency is particularly detrimental to challenging tasks such as the segmentation of microtumors and miniature organs, where both fine-grained boundary definition and broad contextual understanding are paramount. To address this gap, we propose HBFormer, a novel Hybrid-Bridge Transformer architecture. The 'Hybrid' design of HBFormer synergizes a classic U-shaped encoder-decoder framework with a powerful Swin Transformer backbone for robust hierarchical feature extraction. The core innovation lies in its 'Bridge' mechanism, a sophisticated nexus for multi-scale feature integration. This bridge is architecturally embodied by our novel Multi-Scale Feature Fusion (MFF) decoder. Departing from conventional symmetric designs, the MFF decoder is engineered to fuse multi-scale features from the encoder with global contextual information. It achieves this through a synergistic combination of channel and spatial attention modules, which are constructed from a series of dilated and depth-wise convolutions. These components work in concert to create a powerful feature bridge that explicitly captures long-range dependencies and refines object boundaries with exceptional precision. Comprehensive experiments on challenging medical image segmentation datasets, including multi-organ, liver tumor, and bladder tumor benchmarks, demonstrate that HBFormer achieves state-of-the-art results, showcasing its outstanding capabilities in microtumor and miniature organ segmentation. Code and models are available at: https://github.com/lzeeorno/HBFormer.

</details>


### [74] [Memory-Guided Point Cloud Completion for Dental Reconstruction](https://arxiv.org/abs/2512.03598)
*Jianan Sun,Yukang Huang,Dongzhihan Wang,Mingyu Fan*

Main category: cs.CV

TL;DR: Retrieval-augmented framework for tooth completion using prototype memory to improve accuracy on partial dental point clouds with large missing regions.


<details>
  <summary>Details</summary>
Motivation: Partial dental point clouds often have large missing regions due to occlusion and limited scanning views, which bias global features and force decoders to hallucinate structures instead of accurately completing missing areas.

Method: Propose a retrieval-augmented framework that integrates a learnable prototype memory into encoder-decoder pipelines. After encoding partial input, retrieves nearest manifold prototype from memory and fuses it with query feature using confidence-gated weighting before decoding. Memory self-organizes into reusable tooth-shape prototypes without requiring tooth-position labels.

Result: Experiments on Teeth3DS benchmark show consistent improvements in Chamfer Distance, with visualizations demonstrating sharper cusps, ridges, and interproximal transitions. The approach provides structural priors that stabilize missing-region inference.

Conclusion: The retrieval-augmented framework offers a simple yet effective way to exploit cross-sample regularities for more accurate and faithful dental point-cloud completion, with plug-and-play compatibility to common completion backbones.

Abstract: Partial dental point clouds often suffer from large missing regions caused by occlusion and limited scanning views, which bias encoder-only global features and force decoders to hallucinate structures. We propose a retrieval-augmented framework for tooth completion that integrates a prototype memory into standard encoder--decoder pipelines. After encoding a partial input into a global descriptor, the model retrieves the nearest manifold prototype from a learnable memory and fuses it with the query feature through confidence-gated weighting before decoding. The memory is optimized end-to-end and self-organizes into reusable tooth-shape prototypes without requiring tooth-position labels, thereby providing structural priors that stabilize missing-region inference and free decoder capacity for detail recovery. The module is plug-and-play and compatible with common completion backbones, while keeping the same training losses. Experiments on a self-processed Teeth3DS benchmark demonstrate consistent improvements in Chamfer Distance, with visualizations showing sharper cusps, ridges, and interproximal transitions. Our approach provides a simple yet effective way to exploit cross-sample regularities for more accurate and faithful dental point-cloud completion.

</details>


### [75] [Motion4D: Learning 3D-Consistent Motion and Semantics for 4D Scene Understanding](https://arxiv.org/abs/2512.03601)
*Haoran Zhou,Gim Hee Lee*

Main category: cs.CV

TL;DR: Motion4D integrates 2D foundation model priors into 4D Gaussian Splatting for 3D-consistent dynamic scene analysis, addressing spatial misalignment and temporal flickering issues.


<details>
  <summary>Details</summary>
Motivation: 2D foundation models for monocular video analysis lack 3D consistency, causing spatial misalignment and temporal flickering in complex 3D environments, which hinders accurate scene geometry and motion understanding.

Method: Two-part iterative optimization framework: 1) Sequential optimization updates motion and semantic fields in stages for local consistency, 2) Global optimization jointly refines all attributes for long-term coherence. Includes 3D confidence map for motion accuracy, adaptive resampling for under-represented regions, and iterative semantic refinement with SAM2 prompts.

Result: Motion4D significantly outperforms both 2D foundation models and existing 3D-based approaches across diverse tasks including point-based tracking, video object segmentation, and novel view synthesis.

Conclusion: Motion4D successfully integrates 2D priors into 4D Gaussian Splatting to achieve 3D-consistent dynamic scene understanding, addressing key limitations of current foundation models while maintaining strong generalization capabilities.

Abstract: Recent advancements in foundation models for 2D vision have substantially improved the analysis of dynamic scenes from monocular videos. However, despite their strong generalization capabilities, these models often lack 3D consistency, a fundamental requirement for understanding scene geometry and motion, thereby causing severe spatial misalignment and temporal flickering in complex 3D environments. In this paper, we present Motion4D, a novel framework that addresses these challenges by integrating 2D priors from foundation models into a unified 4D Gaussian Splatting representation. Our method features a two-part iterative optimization framework: 1) Sequential optimization, which updates motion and semantic fields in consecutive stages to maintain local consistency, and 2) Global optimization, which jointly refines all attributes for long-term coherence. To enhance motion accuracy, we introduce a 3D confidence map that dynamically adjusts the motion priors, and an adaptive resampling process that inserts new Gaussians into under-represented regions based on per-pixel RGB and semantic errors. Furthermore, we enhance semantic coherence through an iterative refinement process that resolves semantic inconsistencies by alternately optimizing the semantic fields and updating prompts of SAM2. Extensive evaluations demonstrate that our Motion4D significantly outperforms both 2D foundation models and existing 3D-based approaches across diverse scene understanding tasks, including point-based tracking, video object segmentation, and novel view synthesis. Our code is available at https://hrzhou2.github.io/motion4d-web/.

</details>


### [76] [LAMP: Language-Assisted Motion Planning for Controllable Video Generation](https://arxiv.org/abs/2512.03619)
*Muhammed Burak Kizil,Enes Sanli,Niloy J. Mitra,Erkut Erdem,Aykut Erdem,Duygu Ceylan*

Main category: cs.CV

TL;DR: LAMP uses LLMs as motion planners to translate natural language into 3D trajectories for objects and cameras via a motion DSL, enabling better motion control for video generation.


<details>
  <summary>Details</summary>
Motivation: Current video generation lacks effective motion control interfaces for specifying object dynamics and camera trajectories, which is essential for creating complex cinematic scenes.

Method: LAMP leverages LLMs as motion planners with a motion domain-specific language (DSL) inspired by cinematography. LLMs translate natural language into structured motion programs, which are deterministically mapped to 3D trajectories for objects and cameras.

Result: LAMP demonstrates improved motion controllability and better alignment with user intent compared to state-of-the-art alternatives, establishing the first framework for generating both object and camera motions from natural language.

Conclusion: LAMP successfully bridges natural language descriptions to explicit 3D motion trajectories using LLMs and a motion DSL, enabling more intuitive and effective motion control for video generation.

Abstract: Video generation has achieved remarkable progress in visual fidelity and controllability, enabling conditioning on text, layout, or motion. Among these, motion control - specifying object dynamics and camera trajectories - is essential for composing complex, cinematic scenes, yet existing interfaces remain limited. We introduce LAMP that leverages large language models (LLMs) as motion planners to translate natural language descriptions into explicit 3D trajectories for dynamic objects and (relatively defined) cameras. LAMP defines a motion domain-specific language (DSL), inspired by cinematography conventions. By harnessing program synthesis capabilities of LLMs, LAMP generates structured motion programs from natural language, which are deterministically mapped to 3D trajectories. We construct a large-scale procedural dataset pairing natural text descriptions with corresponding motion programs and 3D trajectories. Experiments demonstrate LAMP's improved performance in motion controllability and alignment with user intent compared to state-of-the-art alternatives establishing the first framework for generating both object and camera motions directly from natural language specifications.

</details>


### [77] [ReCamDriving: LiDAR-Free Camera-Controlled Novel Trajectory Video Generation](https://arxiv.org/abs/2512.03621)
*Yaokun Li,Shuaixian Wang,Mantang Guo,Jiehui Huang,Taojun Ding,Mu Hu,Kaixuan Wang,Shaojie Shen,Guang Tan*

Main category: cs.CV

TL;DR: ReCamDriving is a vision-based framework that generates novel driving trajectory videos using 3D Gaussian Splatting (3DGS) renderings for precise camera control and geometric guidance, trained on a large parallel-trajectory dataset.


<details>
  <summary>Details</summary>
Motivation: Existing methods have limitations: repair-based approaches fail to restore complex artifacts, and LiDAR-based methods rely on sparse, incomplete cues. There's a need for precise camera-controllable video generation that leverages dense, complete 3D scene representations.

Method: Two-stage training: first stage uses camera poses for coarse control, second stage incorporates 3DGS renderings for fine-grained viewpoint and geometric guidance. Includes 3DGS-based cross-trajectory data curation to eliminate train-test gaps in camera transformations, enabling scalable multi-trajectory supervision from monocular videos.

Result: Achieves state-of-the-art camera controllability and structural consistency. Constructed ParaDrive dataset with over 110K parallel-trajectory video pairs. Demonstrates superior performance through extensive experiments.

Conclusion: ReCamDriving successfully addresses limitations of previous methods by leveraging dense 3DGS renderings for explicit geometric guidance, enabling precise camera-controllable novel-trajectory video generation with improved structural consistency.

Abstract: We propose ReCamDriving, a purely vision-based, camera-controlled novel-trajectory video generation framework. While repair-based methods fail to restore complex artifacts and LiDAR-based approaches rely on sparse and incomplete cues, ReCamDriving leverages dense and scene-complete 3DGS renderings for explicit geometric guidance, achieving precise camera-controllable generation. To mitigate overfitting to restoration behaviors when conditioned on 3DGS renderings, ReCamDriving adopts a two-stage training paradigm: the first stage uses camera poses for coarse control, while the second stage incorporates 3DGS renderings for fine-grained viewpoint and geometric guidance. Furthermore, we present a 3DGS-based cross-trajectory data curation strategy to eliminate the train-test gap in camera transformation patterns, enabling scalable multi-trajectory supervision from monocular videos. Based on this strategy, we construct the ParaDrive dataset, containing over 110K parallel-trajectory video pairs. Extensive experiments demonstrate that ReCamDriving achieves state-of-the-art camera controllability and structural consistency.

</details>


### [78] [FeatureLens: A Highly Generalizable and Interpretable Framework for Detecting Adversarial Examples Based on Image Features](https://arxiv.org/abs/2512.03625)
*Zhigang Yang,Yuan Liu,Jiawei Zhang,Puning Zhang,Xinqiang Ma*

Main category: cs.CV

TL;DR: FeatureLens: A lightweight, interpretable adversarial attack detection framework using simple classifiers on extracted image features, achieving high accuracy with minimal parameters.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks are vulnerable to adversarial attacks, and existing detection methods are often complex, poorly interpretable, and lack generalization capabilities.

Method: Proposes FeatureLens framework with Image Feature Extractor (IFE) and shallow classifiers (SVM, MLP, or XGBoost) using only 51-dimensional features, with model sizes from 1,000 to 30,000 parameters.

Result: Achieves 97.8%-99.75% accuracy in closed-set evaluation and 86.17%-99.6% in generalization evaluation across FGSM, PGD, CW, and DAmageNet attacks.

Conclusion: FeatureLens offers a practical pathway toward transparent and effective adversarial defense by combining strong detection performance with excellent generalization, interpretability, and computational efficiency.

Abstract: Although the remarkable performance of deep neural networks (DNNs) in image classification, their vulnerability to adversarial attacks remains a critical challenge. Most existing detection methods rely on complex and poorly interpretable architectures, which compromise interpretability and generalization. To address this, we propose FeatureLens, a lightweight framework that acts as a lens to scrutinize anomalies in image features. Comprising an Image Feature Extractor (IFE) and shallow classifiers (e.g., SVM, MLP, or XGBoost) with model sizes ranging from 1,000 to 30,000 parameters, FeatureLens achieves high detection accuracy ranging from 97.8% to 99.75% in closed-set evaluation and 86.17% to 99.6% in generalization evaluation across FGSM, PGD, CW, and DAmageNet attacks, using only 51 dimensional features. By combining strong detection performance with excellent generalization, interpretability, and computational efficiency, FeatureLens offers a practical pathway toward transparent and effective adversarial defense.

</details>


### [79] [Optical Context Compression Is Just (Bad) Autoencoding](https://arxiv.org/abs/2512.03643)
*Ivan Yee Lee,Cheng Yang,Taylor Berg-Kirkpatrick*

Main category: cs.CV

TL;DR: Vision-based OCR compression doesn't outperform simple alternatives for text reconstruction or language modeling, challenging the excitement around optical context compression.


<details>
  <summary>Details</summary>
Motivation: To test whether vision-based compression actually provides advantages for text reconstruction and language modeling, challenging the assumptions behind the optical-compression narrative.

Method: Compare DeepSeek-OCR's vision encoder against simple alternatives: parameter-free mean pooling and a learned hierarchical encoder, evaluating both reconstruction quality and language modeling performance at matched compression ratios.

Result: Simple approaches match or surpass vision-based compression for text reconstruction, and outperform it for language modeling where vision-based compression fails to beat simple truncation.

Conclusion: The excitement around optical context compression outpaces the evidence; vision-based compression doesn't provide unique advantages over simpler alternatives for either reconstruction or language modeling.

Abstract: DeepSeek-OCR demonstrates that rendered text can be reconstructed with high fidelity from a small number of vision tokens. This finding has sparked excitement about vision-based context compression for language models. But the evaluation stops at reconstruction; whether these representations help language modeling remains untested. We test two assumptions implicit in the optical-compression narrative: that vision-based compression provides unique advantages for text reconstruction from compressed representations, and that DeepSeek-OCR's reconstruction results are evidence that vision-based compression will be useful for language modeling. Comparing their vision encoder against simple alternatives--parameter-free mean pooling and a learned hierarchical encoder--we find that these simple approaches match or surpass vision for reconstruction at matched compression ratios, and outperform it for language modeling--where vision-based compression fails to beat truncation. The excitement around optical context compression outpaces the evidence. Code and checkpoints are available at https://github.com/ivnle/bad-autoencoding

</details>


### [80] [Multi-Scale Visual Prompting for Lightweight Small-Image Classification](https://arxiv.org/abs/2512.03663)
*Salim Khazem*

Main category: cs.CV

TL;DR: MSVP introduces multi-scale visual prompting for small-image benchmarks like MNIST, Fashion-MNIST, and CIFAR-10, achieving consistent improvements with minimal parameter overhead.


<details>
  <summary>Details</summary>
Motivation: Visual prompting has been effective for large Vision Transformers on high-resolution datasets, but small-image benchmarks (MNIST, Fashion-MNIST, CIFAR-10) widely used in education and research have received little attention for prompting methods.

Method: Multi-Scale Visual Prompting (MSVP) learns global, mid-scale, and local prompt maps fused with input images via lightweight 1×1 convolution. It's backbone-agnostic, adds <0.02% parameters, and works with CNNs and Vision Transformers.

Result: MSVP significantly improves performance across CNN and Vision Transformer backbones on MNIST, Fashion-MNIST, and CIFAR-10 with negligible computational overhead. Ablations show effectiveness of multi-scale prompting and various fusion strategies.

Conclusion: Multi-scale visual prompting provides an effective inductive bias even on low-resolution images, offering a simple, generic solution for small-image benchmarks with minimal parameter cost.

Abstract: Visual prompting has recently emerged as an efficient strategy to adapt vision models using lightweight, learnable parameters injected into the input space. However, prior work mainly targets large Vision Transformers and high-resolution datasets such as ImageNet. In contrast, small-image benchmarks like MNIST, Fashion-MNIST, and CIFAR-10 remain widely used in education, prototyping, and research, yet have received little attention in the context of prompting. In this paper, we introduce \textbf{Multi-Scale Visual Prompting (MSVP)}, a simple and generic module that learns a set of global, mid-scale, and local prompt maps fused with the input image via a lightweight $1 \times 1$ convolution. MSVP is backbone-agnostic, adds less than $0.02\%$ parameters, and significantly improves performance across CNN and Vision Transformer backbones.
  We provide a unified benchmark on MNIST, Fashion-MNIST, and CIFAR-10 using a simple CNN, ResNet-18, and a small Vision Transformer. Our method yields consistent improvements with negligible computational overhead. We further include ablations on prompt scales, fusion strategies, and backbone architectures, along with qualitative analyzes using prompt visualizations and Grad-CAM. Our results demonstrate that multi-scale prompting provides an effective inductive bias even on low-resolution images.

</details>


### [81] [Colon-X: Advancing Intelligent Colonoscopy from Multimodal Understanding to Clinical Reasoning](https://arxiv.org/abs/2512.03667)
*Ge-Peng Ji,Jingyi Liu,Deng-Ping Fan,Nick Barnes*

Main category: cs.CV

TL;DR: Colon-X introduces comprehensive multimodal datasets (ColonVQA with 1.1M+ entries) and reasoning models (ColonR1) for colonoscopy, advancing from basic understanding to clinical reasoning with significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: To advance multimodal intelligence in colonoscopy by addressing the critical gap between multimodal understanding and clinical reasoning, as current MLLMs lack robustness and trustworthiness for clinical applications.

Method: 1) Built ColonVQA - largest multimodal dataset with 1.1M+ VQA entries across 76 findings and 18 tasks; 2) Evaluated 22 MLLMs for generalizability and robustness; 3) Created ColonReason dataset via multi-expert debating; 4) Developed ColonR1 model with task-adaptive rewarding and gradient-stable optimization.

Result: ColonR1 achieved 56.61% overall accuracy under data-scarce conditions, outperforming supervised fine-tuning by 25.22%, establishing new reasoning-enabled baseline for colonoscopy analysis. Current MLLMs showed poor robustness and trustworthiness.

Conclusion: The Colon-X initiative successfully bridges the gap from multimodal understanding to clinical reasoning in colonoscopy, providing comprehensive datasets and advanced reasoning models that significantly outperform existing approaches, with all resources made publicly available.

Abstract: In this study, we present Colon-X, an open initiative aimed at advancing multimodal intelligence in colonoscopy. We begin by constructing ColonVQA, the most comprehensive multimodal dataset ever built for colonoscopy, featuring over 1.1M+ visual question answering entries across 76 clinical findings and 18 multimodal tasks. Beyond serving as a community-wide data foundation, we further investigate a critical yet underexplored transition in colonoscopy - evolving from multimodal understanding to clinical reasoning: (a) To capture the current landscape of multimodal understanding behaviors, we systematically assess the generalizability of 22 multimodal large language models and examine their reliability under human-induced perturbations. The results reveal that clinical outputs from leading MLLMs remain far from robust and trustworthy. (b) To narrow this gap, we further explore reasoning-centric intelligence tailored for colonoscopy. Specifically, we curate ColonReason, a clinically grounded reasoning dataset annotated through a multi-expert debating pipeline, and develop ColonR1, the first R1-styled model incorporating task-adaptive rewarding and gradient-stable optimization techniques. Under data-scarce conditions, our ColonR1 achieves 56.61% overall accuracy, outperforming supervised fine-tuning by 25.22%, and sets a new reasoning-enabled baseline for multimodal colonoscopy analysis. All data and model resources are publicly available at https://github.com/ai4colonoscopy/Colon-X.

</details>


### [82] [ConvRot: Rotation-Based Plug-and-Play 4-bit Quantization for Diffusion Transformers](https://arxiv.org/abs/2512.03673)
*Feice Huang,Zuliang Han,Xing Zhou,Yihuang Chen,Lifei Zhu,Haoqian Wang*

Main category: cs.CV

TL;DR: ConvRot enables efficient 4-bit quantization for diffusion transformers using group-wise rotation to handle outliers, achieving 2.26× speedup and 4.05× memory reduction while maintaining image quality.


<details>
  <summary>Details</summary>
Motivation: Diffusion transformers face deployment challenges due to increasing memory footprint and inference latency as model size grows. Existing rotation-based quantization methods for LLMs have high overhead and struggle with row-wise outliers in diffusion transformers.

Method: Propose ConvRot, a group-wise rotation-based quantization method using regular Hadamard transform (RHT) to suppress both row-wise and column-wise outliers while reducing complexity from quadratic to linear. Design ConvLinear4bit as a plug-and-play module integrating rotation, quantization, GEMM, and dequantization for W4A4 inference without retraining.

Result: Experiments on FLUX.1-dev show 2.26× speedup and 4.05× memory reduction while maintaining image fidelity. First application of rotation-based quantization for plug-and-play W4A4 inference in diffusion transformers.

Conclusion: ConvRot effectively addresses deployment challenges of diffusion transformers through efficient rotation-based quantization, enabling practical deployment with significant speed and memory improvements while preserving visual quality.

Abstract: Diffusion transformers have demonstrated strong capabilities in generating high-quality images. However, as model size increases, the growing memory footprint and inference latency pose significant challenges for practical deployment. Recent studies in large language models (LLMs) show that rotation-based techniques can smooth outliers and enable 4-bit quantization, but these approaches often incur substantial overhead and struggle with row-wise outliers in diffusion transformers. To address these challenges, we propose ConvRot, a group-wise rotation-based quantization method that leverages regular Hadamard transform (RHT) to suppress both row-wise and column-wise outliers while reducing complexity from quadratic to linear. Building on this, we design ConvLinear4bit, a plug-and-play module that integrates rotation, quantization, GEMM, and dequantization, enabling W4A4 inference without retraining and preserving visual quality. Experiments on FLUX.1-dev demonstrate a 2.26$\times$ speedup and 4.05$\times$ memory reduction while maintaining image fidelity. To our knowledge, this is the first application of rotation-based quantization for plug-and-play W4A4 inference in diffusion transformers.

</details>


### [83] [GaussianBlender: Instant Stylization of 3D Gaussians with Disentangled Latent Spaces](https://arxiv.org/abs/2512.03683)
*Melis Ocal,Xiaoyan Xing,Yue Li,Ngo Anh Vien,Sezer Karaoglu,Theo Gevers*

Main category: cs.CV

TL;DR: GaussianBlender is a feed-forward framework for instant text-driven 3D stylization that avoids per-asset optimization and ensures multi-view consistency.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-3D stylization methods require time-intensive per-asset optimization and suffer from multi-view inconsistency due to limitations of current text-to-image models, making them impractical for large-scale production in game development, virtual reality, and digital arts.

Method: The method learns structured, disentangled latent spaces for geometry and appearance from spatially-grouped 3D Gaussians, then uses a latent diffusion model to apply text-conditioned edits on these learned representations in a feed-forward manner.

Result: GaussianBlender delivers instant, high-fidelity, geometry-preserving, multi-view consistent stylization and surpasses methods that require per-instance test-time optimization.

Conclusion: The framework enables practical, democratized 3D stylization at scale by providing instant text-driven editing without the limitations of existing optimization-based approaches.

Abstract: 3D stylization is central to game development, virtual reality, and digital arts, where the demand for diverse assets calls for scalable methods that support fast, high-fidelity manipulation. Existing text-to-3D stylization methods typically distill from 2D image editors, requiring time-intensive per-asset optimization and exhibiting multi-view inconsistency due to the limitations of current text-to-image models, which makes them impractical for large-scale production. In this paper, we introduce GaussianBlender, a pioneering feed-forward framework for text-driven 3D stylization that performs edits instantly at inference. Our method learns structured, disentangled latent spaces with controlled information sharing for geometry and appearance from spatially-grouped 3D Gaussians. A latent diffusion model then applies text-conditioned edits on these learned representations. Comprehensive evaluations show that GaussianBlender not only delivers instant, high-fidelity, geometry-preserving, multi-view consistent stylization, but also surpasses methods that require per-instance test-time optimization - unlocking practical, democratized 3D stylization at scale.

</details>


### [84] [Active Visual Perception: Opportunities and Challenges](https://arxiv.org/abs/2512.03687)
*Yian Li,Xiaoyu Guo,Hao Zhang,Shuiwang Li,Xiaowei Dai*

Main category: cs.CV

TL;DR: Active visual perception enables systems to dynamically interact with environments through sensing and action, offering advantages over passive systems but facing challenges in real-time processing and decision-making.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of passive visual systems in complex environments where static sensing methods may not provide sufficient information, and to explore how active engagement with environments can improve perception capabilities for applications like robotics, autonomous vehicles, and surveillance.

Method: The paper provides a comprehensive overview and exploration of active visual perception systems, examining their principles, capabilities, and implementation approaches across different applications.

Result: The paper identifies that active visual perception offers significant advantages over passive systems by allowing dynamic interaction with environments, but highlights several key challenges including real-time processing of complex visual data, decision-making in dynamic environments, and integration of multimodal sensory inputs.

Conclusion: Active visual perception holds great promise for numerous applications but requires overcoming substantial challenges in processing, decision-making, and multimodal integration before achieving broader adoption and realizing its full potential.

Abstract: Active visual perception refers to the ability of a system to dynamically engage with its environment through sensing and action, allowing it to modify its behavior in response to specific goals or uncertainties. Unlike passive systems that rely solely on visual data, active visual perception systems can direct attention, move sensors, or interact with objects to acquire more informative data. This approach is particularly powerful in complex environments where static sensing methods may not provide sufficient information. Active visual perception plays a critical role in numerous applications, including robotics, autonomous vehicles, human-computer interaction, and surveillance systems. However, despite its significant promise, there are several challenges that need to be addressed, including real-time processing of complex visual data, decision-making in dynamic environments, and integrating multimodal sensory inputs. This paper explores both the opportunities and challenges inherent in active visual perception, providing a comprehensive overview of its potential, current research, and the obstacles that must be overcome for broader adoption.

</details>


### [85] [Structured Uncertainty Similarity Score (SUSS): Learning a Probabilistic, Interpretable, Perceptual Metric Between Images](https://arxiv.org/abs/2512.03701)
*Paula Seidler,Neill D. F. Campbell,Ivor J A Simpson*

Main category: cs.CV

TL;DR: SUSS is a new perceptual similarity score that models images as structured multivariate Normal distributions, trained generatively to align with human vision while maintaining interpretability.


<details>
  <summary>Details</summary>
Motivation: Existing perceptual similarity measures have limitations: deep methods like LPIPS are complex and non-interpretable, while hand-crafted methods like SSIM miss key perceptual properties. There's a need for a score that aligns with human vision while being transparent and interpretable.

Method: SUSS models each image through perceptual components represented by structured multivariate Normal distributions. It's trained generatively in self-supervised manner to assign high likelihood to human-imperceptible augmentations. The final score is a weighted sum of component log-probabilities with weights learned from human perceptual datasets, using image-specific linear transformations in pixel space.

Result: SUSS aligns closely with human perceptual judgments, shows strong perceptual calibration across diverse distortion types, provides localized interpretable explanations through decorrelated residuals and sampling, and demonstrates stable optimization behavior with competitive performance as a perceptual loss for imaging tasks.

Conclusion: SUSS offers a promising approach to perceptual similarity that combines human alignment with interpretability, addressing limitations of both deep feature-based methods and hand-crafted measures while enabling transparent inspection and effective use as a perceptual loss.

Abstract: Perceptual similarity scores that align with human vision are critical for both training and evaluating computer vision models. Deep perceptual losses, such as LPIPS, achieve good alignment but rely on complex, highly non-linear discriminative features with unknown invariances, while hand-crafted measures like SSIM are interpretable but miss key perceptual properties.
  We introduce the Structured Uncertainty Similarity Score (SUSS); it models each image through a set of perceptual components, each represented by a structured multivariate Normal distribution. These are trained in a generative, self-supervised manner to assign high likelihood to human-imperceptible augmentations. The final score is a weighted sum of component log-probabilities with weights learned from human perceptual datasets. Unlike feature-based methods, SUSS learns image-specific linear transformations of residuals in pixel space, enabling transparent inspection through decorrelated residuals and sampling.
  SUSS aligns closely with human perceptual judgments, shows strong perceptual calibration across diverse distortion types, and provides localized, interpretable explanations of its similarity assessments. We further demonstrate stable optimization behavior and competitive performance when using SUSS as a perceptual loss for downstream imaging tasks.

</details>


### [86] [DINO-RotateMatch: A Rotation-Aware Deep Framework for Robust Image Matching in Large-Scale 3D Reconstruction](https://arxiv.org/abs/2512.03715)
*Kaichen Zhang,Tianxiang Sheng,Xuanming Shi*

Main category: cs.CV

TL;DR: DINO-RotateMatch combines self-supervised global descriptors with rotation-aware local matching for improved image matching in large-scale 3D reconstruction from Internet images.


<details>
  <summary>Details</summary>
Motivation: Address challenges of image matching in large-scale 3D reconstruction from unstructured Internet images, particularly handling orientation variations and efficiently pairing semantically relevant images in large collections.

Method: Integrates dataset-adaptive image pairing using DINO for semantic retrieval, with rotation-based augmentation for orientation-dependent local feature extraction using ALIKED and Light Glue.

Result: Achieved consistent improvements in mean Average Accuracy (mAA) on Kaggle Image Matching Challenge 2025, earning Silver Award (47th of 943 teams).

Conclusion: Combining self-supervised global descriptors with rotation-enhanced local matching provides a robust and scalable solution for large-scale 3D reconstruction.

Abstract: This paper presents DINO-RotateMatch, a deep-learning framework designed to address the chal lenges of image matching in large-scale 3D reconstruction from unstructured Internet images. The
  method integrates a dataset-adaptive image pairing strategy with rotation-aware keypoint extraction and
  matching. DINO is employed to retrieve semantically relevant image pairs in large collections, while
  rotation-based augmentation captures orientation-dependent local features using ALIKED and Light Glue. Experiments on the Kaggle Image Matching Challenge 2025 demonstrate consistent improve ments in mean Average Accuracy (mAA), achieving a Silver Award (47th of 943 teams). The results
  confirm that combining self-supervised global descriptors with rotation-enhanced local matching offers
  a robust and scalable solution for large-scale 3D reconstruction.

</details>


### [87] [Dual-level Modality Debiasing Learning for Unsupervised Visible-Infrared Person Re-Identification](https://arxiv.org/abs/2512.03745)
*Jiaze Li,Yan Lu,Bin Liu,Guojun Yin,Mang Ye*

Main category: cs.CV

TL;DR: DMDL framework addresses modality bias in unsupervised visible-infrared person ReID through dual-level debiasing at model and optimization levels.


<details>
  <summary>Details</summary>
Motivation: Two-stage learning pipelines in USL-VI-ReID introduce modality bias where modality-specific cues from single-modality training propagate to cross-modality learning, impairing identity discrimination and generalization.

Method: Proposes Dual-level Modality Debiasing Learning (DMDL) with: 1) Causality-inspired Adjustment Intervention (CAI) module for causal modeling instead of likelihood-based modeling, and 2) Collaborative Bias-free Training (CBT) strategy integrating modality-specific augmentation, label refinement, and feature alignment.

Result: Extensive experiments on benchmark datasets demonstrate DMDL enables modality-invariant feature learning and produces more generalized models.

Conclusion: The proposed DMDL framework effectively addresses modality bias in unsupervised visible-infrared person ReID through dual-level debiasing, leading to improved generalization and modality-invariant feature learning.

Abstract: Two-stage learning pipeline has achieved promising results in unsupervised visible-infrared person re-identification (USL-VI-ReID). It first performs single-modality learning and then operates cross-modality learning to tackle the modality discrepancy. Although promising, this pipeline inevitably introduces modality bias: modality-specific cues learned in the single-modality training naturally propagate into the following cross-modality learning, impairing identity discrimination and generalization. To address this issue, we propose a Dual-level Modality Debiasing Learning (DMDL) framework that implements debiasing at both the model and optimization levels. At the model level, we propose a Causality-inspired Adjustment Intervention (CAI) module that replaces likelihood-based modeling with causal modeling, preventing modality-induced spurious patterns from being introduced, leading to a low-biased model. At the optimization level, a Collaborative Bias-free Training (CBT) strategy is introduced to interrupt the propagation of modality bias across data, labels, and features by integrating modality-specific augmentation, label refinement, and feature alignment. Extensive experiments on benchmark datasets demonstrate that DMDL could enable modality-invariant feature learning and a more generalized model.

</details>


### [88] [Thinking with Programming Vision: Towards a Unified View for Thinking with Images](https://arxiv.org/abs/2512.03746)
*Zirun Guo,Minjie Hong,Feng Zhang,Kai Jia,Tao Jin*

Main category: cs.CV

TL;DR: CodeVision: A code-as-tool framework for multimodal LLMs that generates code as universal interface for image operations, improving robustness to orientation changes and enabling flexible tool composition.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs are brittle with performance degradation on images with simple orientation changes or natural corruptions, and rely on narrow tool sets with limited real-world scalability.

Method: Two-stage training: SFT on curated dataset for multi-turn tool composition and error recovery, followed by RL with dense process reward function for strategic tool use. Code-as-tool framework where model generates code to invoke any image operation.

Result: Significant performance improvement on Qwen2.5-VL and Qwen3-VL series, with emergent capabilities including flexible tool composition, efficient chained execution, and robust error recovery from runtime feedback.

Conclusion: CodeVision provides flexible and scalable approach to tool-based reasoning for MLLMs, addressing brittleness issues and enabling more robust multimodal reasoning through code generation as universal tool interface.

Abstract: Multimodal large language models (MLLMs) that think with images can interactively use tools to reason about visual inputs, but current approaches often rely on a narrow set of tools with limited real-world necessity and scalability. In this work, we first reveal a critical and previously overlooked weakness: even state-of-the-art MLLMs are surprisingly brittle, showing significant performance degradation on images with simple orientation changes or natural corruptions, underscoring the need for more robust tool-based reasoning. To address this, we propose CodeVision, a flexible and scalable code-as-tool framework where the model generates code as a universal interface to invoke any image operation, moving beyond fixed tool registries. We train our model using a two-stage methodology, beginning with Supervised Fine-Tuning (SFT) on a high-quality dataset curated for complex, multi-turn tool composition and error recovery, followed by Reinforcement Learning (RL) with a novel and dense process reward function to encourage strategic and efficient tool use. To facilitate this research, we construct new SFT and RL datasets and introduce a challenging new benchmark suite designed to rigorously evaluate robustness to orientation changes and multi-tool reasoning. Experiments on Qwen2.5-VL and Qwen3-VL series show that our approach significantly improves model performance and fosters emergent capabilities such as flexible tool composition, efficient chained execution, and robust error recovery from runtime feedback. Code is available at https://github.com/ByteDance-BandAI/CodeVision.

</details>


### [89] [Fully Unsupervised Self-debiasing of Text-to-Image Diffusion Models](https://arxiv.org/abs/2512.03749)
*Korada Sri Vardhana,Shrikrishna Lolla,Soma Biswas*

Main category: cs.CV

TL;DR: SelfDebias is an unsupervised test-time debiasing method for diffusion models that uses semantic clusters from image embeddings to guide generation toward uniform distributions, reducing biases without requiring labeled data.


<details>
  <summary>Details</summary>
Motivation: Text-to-image diffusion models trained on internet-scraped datasets (like LAION-5B) inherently learn and reproduce societal biases present in the training data, resulting in stereotypical outputs that need to be addressed.

Method: SelfDebias identifies semantic clusters in an image encoder's embedding space and uses these clusters to guide the diffusion process during inference by minimizing KL divergence between output distribution and uniform distribution. It works with any UNet-based diffusion model without requiring human-annotated datasets or external classifiers.

Result: SelfDebias generalizes across prompts and diffusion model architectures (both conditional and unconditional), effectively debiases images along demographic dimensions while maintaining visual fidelity, and handles abstract concepts where bias identification is challenging.

Conclusion: SelfDebias provides a practical, unsupervised solution for debiasing diffusion models that works across various concepts and architectures without requiring labeled data, addressing a critical limitation in current text-to-image generation systems.

Abstract: Text-to-image (T2I) diffusion models have achieved widespread success due to their ability to generate high-resolution, photorealistic images. These models are trained on large-scale datasets, like LAION-5B, often scraped from the internet. However, since this data contains numerous biases, the models inherently learn and reproduce them, resulting in stereotypical outputs. We introduce SelfDebias, a fully unsupervised test-time debiasing method applicable to any diffusion model that uses a UNet as its noise predictor. SelfDebias identifies semantic clusters in an image encoder's embedding space and uses these clusters to guide the diffusion process during inference, minimizing the KL divergence between the output distribution and the uniform distribution. Unlike supervised approaches, SelfDebias does not require human-annotated datasets or external classifiers trained for each generated concept. Instead, it is designed to automatically identify semantic modes. Extensive experiments show that SelfDebias generalizes across prompts and diffusion model architectures, including both conditional and unconditional models. It not only effectively debiases images along key demographic dimensions while maintaining the visual fidelity of the generated images, but also more abstract concepts for which identifying biases is also challenging.

</details>


### [90] [LSRS: Latent Scale Rejection Sampling for Visual Autoregressive Modeling](https://arxiv.org/abs/2512.03796)
*Hong-Kai Zheng,Piji Li*

Main category: cs.CV

TL;DR: LSRS (Latent Scale Rejection Sampling) improves VAR image generation by progressively refining token maps during inference, reducing structural errors with minimal computational overhead.


<details>
  <summary>Details</summary>
Motivation: VAR models suffer from structural errors due to parallel token sampling within scales, leading to suboptimal image generation quality despite their efficiency.

Method: Proposes LSRS: uses lightweight scoring model to evaluate multiple candidate token maps at each scale, selects high-quality maps to guide subsequent scale generation, prioritizing early scales for structural coherence.

Result: LSRS improves VAR-d30 FID from 1.95 to 1.78 with only 1% additional inference time, and to 1.66 with 15% additional time, significantly enhancing generation quality.

Conclusion: LSRS provides an efficient test-time scaling solution that mitigates autoregressive error accumulation in VAR models while maintaining computational efficiency.

Abstract: Visual Autoregressive (VAR) modeling approach for image generation proposes autoregressive processing across hierarchical scales, decoding multiple tokens per scale in parallel. This method achieves high-quality generation while accelerating synthesis. However, parallel token sampling within a scale may lead to structural errors, resulting in suboptimal generated images. To mitigate this, we propose Latent Scale Rejection Sampling (LSRS), a method that progressively refines token maps in the latent scale during inference to enhance VAR models. Our method uses a lightweight scoring model to evaluate multiple candidate token maps sampled at each scale, selecting the high-quality map to guide subsequent scale generation. By prioritizing early scales critical for structural coherence, LSRS effectively mitigates autoregressive error accumulation while maintaining computational efficiency. Experiments demonstrate that LSRS significantly improves VAR's generation quality with minimal additional computational overhead. For the VAR-d30 model, LSRS increases the inference time by merely 1% while reducing its FID score from 1.95 to 1.78. When the inference time is increased by 15%, the FID score can be further reduced to 1.66. LSRS offers an efficient test-time scaling solution for enhancing VAR-based generation.

</details>


### [91] [HieroGlyphTranslator: Automatic Recognition and Translation of Egyptian Hieroglyphs to English](https://arxiv.org/abs/2512.03817)
*Ahmed Nasser,Marwan Mohamed,Alaa Sherif,Basmala Mahmoud,Shereen Yehia,Asmaa Saad,Mariam S. El-Rahmany,Ensaf H. Mohamed*

Main category: cs.CV

TL;DR: Proposed deep learning method for automatic recognition and translation of Egyptian hieroglyphs from images to English using segmentation, symbol mapping, and CNN-based translation, achieving BLEU score of 42.2.


<details>
  <summary>Details</summary>
Motivation: Egyptian hieroglyphs present translation challenges due to their pictorial nature and multiple meanings per glyph. Deep learning offers promising solutions for automating this complex translation task.

Method: Three-stage approach: 1) Segmentation using Contour and Detectron2, 2) Mapping symbols to Gardiner codes, 3) Translation using CNN model. Used Morris Franken and EgyptianTranslation datasets.

Result: Achieved BLEU score of 42.2, which represents significant improvement over previous research in hieroglyph translation.

Conclusion: The proposed deep learning method successfully automates hieroglyph recognition and translation, demonstrating the effectiveness of CNN-based approaches for this challenging ancient language task.

Abstract: Egyptian hieroglyphs, the ancient Egyptian writing system, are composed entirely of drawings. Translating these glyphs into English poses various challenges, including the fact that a single glyph can have multiple meanings. Deep learning translation applications are evolving rapidly, producing remarkable results that significantly impact our lives. In this research, we propose a method for the automatic recognition and translation of ancient Egyptian hieroglyphs from images to English. This study utilized two datasets for classification and translation: the Morris Franken dataset and the EgyptianTranslation dataset. Our approach is divided into three stages: segmentation (using Contour and Detectron2), mapping symbols to Gardiner codes, and translation (using the CNN model). The model achieved a BLEU score of 42.2, a significant result compared to previous research.

</details>


### [92] [A Robust Camera-based Method for Breath Rate Measurement](https://arxiv.org/abs/2512.03827)
*Alexey Protopopov*

Main category: cs.CV

TL;DR: A robust video-based breath rate measurement method using mathematical transforms achieves <5% relative deviation from ground truth, with 0.57 respirations per minute MAE, outperforming previous works and handling subject movement better.


<details>
  <summary>Details</summary>
Motivation: Existing video-based breath rate measurement methods either require near-ideal conditions or lack sufficient accuracy, despite the proliferation of cheap cameras making remote breath rate monitoring possible.

Method: Combination of mathematical transforms applied to video footage to extract breath rate signals, tested on 14 volunteers with over 2.5 hours of video data.

Result: Average mean absolute error of 0.57 respirations per minute with relative deviation from ground truth less than 5%, significantly better than previous methods and more resistant to subject movement distortions.

Conclusion: The proposed method enables accurate remote breath rate measurement with minimal hardware requirements and fewer behavioral limitations, making it practical for real-world applications.

Abstract: Proliferation of cheap and accessible cameras makes it possible to measure a subject's breath rate from video footage alone. Recent works on this topic have proposed a variety of approaches for accurately measuring human breath rate, however they are either tested in near-ideal conditions, or produce results that are not sufficiently accurate. The present study proposes a more robust method to measure breath rate in humans with minimal hardware requirements using a combination of mathematical transforms with a relative deviation from the ground truth of less than 5%. The method was tested on videos taken from 14 volunteers with a total duration of over 2 hours 30 minutes. The obtained results were compared to reference data and the average mean absolute error was found to be at 0.57 respirations per minute, which is noticeably better than the results from previous works. The breath rate measurement method proposed in the present article is more resistant to distortions caused by subject movement and thus allows one to remotely measure the subject's breath rate without any significant limitations on the subject's behavior.

</details>


### [93] [Lean Unet: A Compact Model for Image Segmentation](https://arxiv.org/abs/2512.03834)
*Ture Hassler,Ida Åkerholm,Marcus Nordström,Gabriele Balletti,Orcun Goksel*

Main category: cs.CV

TL;DR: Proposes LUnet - a lean Unet architecture with constant channel count across layers instead of doubling channels when resolution halves, achieving comparable performance with 30x fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Standard Unet architectures have large memory footprints that limit batch sizes and increase inference latency. Channel pruning requires lengthy optimization and may not generalize well across tasks.

Method: Proposes LUnet with compact, flat hierarchy where channels remain constant across layers instead of doubling when resolution is halved. Skip connections allow bottleneck channels to be reduced significantly.

Result: LUnet achieves performance comparable to conventional Unet and pruned networks with over 30 times fewer parameters. Random channel elimination at identified layers achieves similar or better performance than sophisticated pruning methods.

Conclusion: The final architecture structure is more crucial than channel selection strategy in pruning. Lean Unet with constant channel count requires far fewer parameters while maintaining or improving performance over standard Unet.

Abstract: Unet and its variations have been standard in semantic image segmentation, especially for computer assisted radiology. Current Unet architectures iteratively downsample spatial resolution while increasing channel dimensions to preserve information content. Such a structure demands a large memory footprint, limiting training batch sizes and increasing inference latency. Channel pruning compresses Unet architecture without accuracy loss, but requires lengthy optimization and may not generalize across tasks and datasets. By investigating Unet pruning, we hypothesize that the final structure is the crucial factor, not the channel selection strategy of pruning. Based on our observations, we propose a lean Unet architecture (LUnet) with a compact, flat hierarchy where channels are not doubled as resolution is halved. We evaluate on a public MRI dataset allowing comparable reporting, as well as on two internal CT datasets. We show that a state-of-the-art pruning solution (STAMP) mainly prunes from the layers with the highest number of channels. Comparatively, simply eliminating a random channel at the pruning-identified layer or at the largest layer achieves similar or better performance. Our proposed LUnet with fixed architectures and over 30 times fewer parameters achieves performance comparable to both conventional Unet counterparts and data-adaptively pruned networks. The proposed lean Unet with constant channel count across layers requires far fewer parameters while achieving performance superior to standard Unet for the same total number of parameters. Skip connections allow Unet bottleneck channels to be largely reduced, unlike standard encoder-decoder architectures requiring increased bottleneck channels for information propagation.

</details>


### [94] [Heatmap Pooling Network for Action Recognition from RGB Videos](https://arxiv.org/abs/2512.03837)
*Mengyuan Liu,Jinfu Liu,Yongkang Jiang,Bin He*

Main category: cs.CV

TL;DR: HP-Net is a heatmap pooling network for video action recognition that extracts robust, concise body features using feedback pooling, outperforming existing methods on multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing RGB video feature extraction methods suffer from information redundancy, noise susceptibility, and high storage costs, limiting their effectiveness for human action recognition.

Method: Proposes HP-Net with feedback pooling module to extract information-rich pooled features, plus spatial-motion co-learning and text refinement modulation modules for multimodal integration.

Result: Outperforms existing methods on NTU RGB+D 60, NTU RGB+D 120, Toyota-Smarthome and UAV-Human benchmarks, demonstrating superior performance.

Conclusion: HP-Net effectively addresses limitations of existing RGB video feature extraction methods and provides robust, concise features for improved human action recognition.

Abstract: Human action recognition (HAR) in videos has garnered widespread attention due to the rich information in RGB videos. Nevertheless, existing methods for extracting deep features from RGB videos face challenges such as information redundancy, susceptibility to noise and high storage costs. To address these issues and fully harness the useful information in videos, we propose a novel heatmap pooling network (HP-Net) for action recognition from videos, which extracts information-rich, robust and concise pooled features of the human body in videos through a feedback pooling module. The extracted pooled features demonstrate obvious performance advantages over the previously obtained pose data and heatmap features from videos. In addition, we design a spatial-motion co-learning module and a text refinement modulation module to integrate the extracted pooled features with other multimodal data, enabling more robust action recognition. Extensive experiments on several benchmarks namely NTU RGB+D 60, NTU RGB+D 120, Toyota-Smarthome and UAV-Human consistently verify the effectiveness of our HP-Net, which outperforms the existing human action recognition methods. Our code is publicly available at: https://github.com/liujf69/HPNet-Action.

</details>


### [95] [CoDA: From Text-to-Image Diffusion Models to Training-Free Dataset Distillation](https://arxiv.org/abs/2512.03844)
*Letian Zhou,Songhua Liu,Xinchao Wang*

Main category: cs.CV

TL;DR: CoDA enables dataset distillation using only off-the-shelf text-to-image models by identifying and aligning with the target dataset's intrinsic core distribution, achieving state-of-the-art performance without target-specific generative model training.


<details>
  <summary>Details</summary>
Motivation: Current dataset distillation methods either require diffusion models pre-trained on the full target dataset (undermining DD's purpose and being costly) or use general text-to-image models that suffer from distributional mismatch with target-specific semantics.

Method: CoDA first identifies the "intrinsic core distribution" of the target dataset using robust density-based discovery, then steers the generative process of off-the-shelf text-to-image models to align generated samples with this core distribution.

Result: CoDA achieves performance on par with or superior to previous methods that require target-specific generative models, establishing new SOTA accuracy of 60.4% at 50 IPC on ImageNet-1K without such reliance.

Conclusion: CoDA effectively bridges the gap between general-purpose generative priors and target semantics, enabling efficient dataset distillation using only off-the-shelf models while maintaining high performance.

Abstract: Prevailing Dataset Distillation (DD) methods leveraging generative models confront two fundamental limitations. First, despite pioneering the use of diffusion models in DD and delivering impressive performance, the vast majority of approaches paradoxically require a diffusion model pre-trained on the full target dataset, undermining the very purpose of DD and incurring prohibitive training costs. Second, although some methods turn to general text-to-image models without relying on such target-specific training, they suffer from a significant distributional mismatch, as the web-scale priors encapsulated in these foundation models fail to faithfully capture the target-specific semantics, leading to suboptimal performance. To tackle these challenges, we propose Core Distribution Alignment (CoDA), a framework that enables effective DD using only an off-the-shelf text-to-image model. Our key idea is to first identify the "intrinsic core distribution" of the target dataset using a robust density-based discovery mechanism. We then steer the generative process to align the generated samples with this core distribution. By doing so, CoDA effectively bridges the gap between general-purpose generative priors and target semantics, yielding highly representative distilled datasets. Extensive experiments suggest that, without relying on a generative model specifically trained on the target dataset, CoDA achieves performance on par with or even superior to previous methods with such reliance across all benchmarks, including ImageNet-1K and its subsets. Notably, it establishes a new state-of-the-art accuracy of 60.4% at the 50-images-per-class (IPC) setup on ImageNet-1K. Our code is available on the project webpage: https://github.com/zzzlt422/CoDA

</details>


### [96] [Traffic Image Restoration under Adverse Weather via Frequency-Aware Mamba](https://arxiv.org/abs/2512.03852)
*Liwen Pan,Longguang Wang,Guangwei Gao,Jun Wang,Jun Shi,Juncheng Li*

Main category: cs.CV

TL;DR: FAMamba integrates frequency guidance with Mamba architecture for traffic image restoration under adverse weather, using dual-branch feature extraction and prior-guided blocks for better texture reconstruction.


<details>
  <summary>Details</summary>
Motivation: Existing traffic image restoration methods focus on spatial-domain modeling but neglect frequency-domain priors, and while Mamba excels at long-range dependency modeling, its potential for frequency-domain feature extraction remains unexplored.

Method: Proposes Frequency-Aware Mamba (FAMamba) with: (1) Dual-Branch Feature Extraction Block (DFEB) for local-global interaction via bidirectional 2D frequency-adaptive scanning, (2) Prior-Guided Block (PGB) for wavelet-based high-frequency residual learning, and (3) Adaptive Frequency Scanning Mechanism (AFSM) enabling frequency-domain scanning across distinct subgraphs.

Result: Extensive experiments demonstrate the efficiency and effectiveness of FAMamba for traffic image restoration under adverse weather conditions.

Conclusion: FAMamba successfully integrates frequency guidance with sequence modeling, enabling high-quality image reconstruction with precise details by leveraging texture distribution characteristics in frequency domains.

Abstract: Traffic image restoration under adverse weather conditions remains a critical challenge for intelligent transportation systems. Existing methods primarily focus on spatial-domain modeling but neglect frequency-domain priors. Although the emerging Mamba architecture excels at long-range dependency modeling through patch-wise correlation analysis, its potential for frequency-domain feature extraction remains unexplored. To address this, we propose Frequency-Aware Mamba (FAMamba), a novel framework that integrates frequency guidance with sequence modeling for efficient image restoration. Our architecture consists of two key components: (1) a Dual-Branch Feature Extraction Block (DFEB) that enhances local-global interaction via bidirectional 2D frequency-adaptive scanning, dynamically adjusting traversal paths based on sub-band texture distributions; and (2) a Prior-Guided Block (PGB) that refines texture details through wavelet-based high-frequency residual learning, enabling high-quality image reconstruction with precise details. Meanwhile, we design a novel Adaptive Frequency Scanning Mechanism (AFSM) for the Mamba architecture, which enables the Mamba to achieve frequency-domain scanning across distinct subgraphs, thereby fully leveraging the texture distribution characteristics inherent in subgraph structures. Extensive experiments demonstrate the efficiency and effectiveness of FAMamba.

</details>


### [97] [Prostate biopsy whole slide image dataset from an underrepresented Middle Eastern population](https://arxiv.org/abs/2512.03854)
*Peshawa J. Muhammad Ali,Navin Vincent,Saman S. Abdulla,Han N. Mohammed Fadhl,Anders Blilie,Kelvin Szolnoky,Julia Anna Mielcarz,Xiaoyi Ji,Kimmo Kartasalo,Abdulbasit K. Al-Talabani,Nita Mulliqi*

Main category: cs.CV

TL;DR: First public prostate biopsy dataset from Middle East (Iraq) with 339 WSIs from 185 patients, enabling AI model validation across diverse populations.


<details>
  <summary>Details</summary>
Motivation: Current histopathology datasets are scarce and predominantly represent Western populations, limiting AI model generalizability to less digitized regions like the Middle East.

Method: Collected 339 whole-slide images of prostate core needle biopsies from 185 consecutive patients in Erbil, Iraq, with Gleason scores and ISUP grades assigned by three pathologists, scanned using three different scanners.

Result: Created a publicly available dataset (CC BY 4.0) deposited in Bioimage Archive, enabling grading concordance analysis, color normalization, and cross-scanner robustness evaluation.

Conclusion: This dataset addresses the geographic bias in existing pathology datasets and supports development of more globally generalizable AI models for prostate cancer diagnosis.

Abstract: Artificial intelligence (AI) is increasingly used in digital pathology. Publicly available histopathology datasets remain scarce, and those that do exist predominantly represent Western populations. Consequently, the generalizability of AI models to populations from less digitized regions, such as the Middle East, is largely unknown. This motivates the public release of our dataset to support the development and validation of pathology AI models across globally diverse populations. We present 339 whole-slide images of prostate core needle biopsies from a consecutive series of 185 patients collected in Erbil, Iraq. The slides are associated with Gleason scores and International Society of Urological Pathology grades assigned independently by three pathologists. Scanning was performed using two high-throughput scanners (Leica and Hamamatsu) and one compact scanner (Grundium). All slides were de-identified and are provided in their native formats without further conversion. The dataset enables grading concordance analyses, color normalization, and cross-scanner robustness evaluations. Data will be deposited in the Bioimage Archive (BIA) under accession code: to be announced (TBA), and released under a CC BY 4.0 license.

</details>


### [98] [Diminishing Returns in Self-Supervised Learning](https://arxiv.org/abs/2512.03862)
*Oli Bridge,Huey Sun,Botond Branyicskai-Nagy,Charles D'Ornano,Shomit Basu*

Main category: cs.CV

TL;DR: Small 5M-parameter vision transformers benefit most from targeted pre-training and careful data selection, while indiscriminate stacking of intermediate tasks can waste compute and even degrade performance.


<details>
  <summary>Details</summary>
Motivation: While transformer architectures achieve strong performance, they typically require vast parameters and training data. This work explores how smaller 5M-parameter vision transformers can achieve good performance through strategic training approaches.

Method: Experimented with three distinct pre-training, intermediate fine-tuning, and downstream datasets and training objectives to explore their marginal benefits on a small 5M-parameter vision transformer.

Result: Pre-training and fine-tuning always help but have diminishing returns. Intermediate fine-tuning can actually show harmful impact on downstream performance, potentially due to dissimilarity in task mechanics.

Conclusion: Small-scale ViTs benefit most from targeted pre-training and careful data selection, while indiscriminate stacking of intermediate tasks can waste compute and even degrade performance.

Abstract: While transformer-based architectures have taken computer vision and NLP by storm, they often require a vast amount of parameters and training data to attain strong performance. In this work, we experiment with three distinct pre-training, intermediate fine-tuning, and downstream datasets and training objectives to explore their marginal benefits on a small 5M-parameter vision transformer. We find that while pre-training and fine-tuning always help our model but have diminishing returns, intermediate fine-tuning can actually show harmful impact on downstream performance, potentially due to dissimilarity in task mechanics. Taken together, our results suggest that small-scale ViTs benefit most from targeted pre-training and careful data selection, while indiscriminate stacking of intermediate tasks can waste compute and even degrade performance.

</details>


### [99] [An Automated Framework for Large-Scale Graph-Based Cerebrovascular Analysis](https://arxiv.org/abs/2512.03869)
*Daniele Falcetta,Liane S. Canas,Lorenzo Suppa,Matteo Pentassuglia,Jon Cleary,Marc Modat,Sébastien Ourselin,Maria A. Zuluaga*

Main category: cs.CV

TL;DR: CaravelMetrics is an automated computational framework for analyzing cerebrovascular networks using skeletonization-derived graph representations to extract 15 morphometric, topological, fractal, and geometric features at global and regional scales.


<details>
  <summary>Details</summary>
Motivation: To provide a scalable, fully automated approach for quantitative cerebrovascular feature extraction that supports normative modeling and population-level studies of vascular health and aging, addressing the need for systematic analysis of cerebrovascular organization.

Method: The framework integrates atlas-based regional parcellation, centerline extraction, and graph construction to model vessel morphology through skeletonization-derived graph representations, computing 15 features that can be estimated globally or within arterial territories.

Result: Applied to 570 3D TOF-MRA scans from the IXI dataset (ages 20-86), CaravelMetrics produced reproducible vessel graphs that captured age- and sex-related variations and education-associated increases in vascular complexity, consistent with literature findings.

Conclusion: CaravelMetrics provides a scalable, fully automated framework for quantitative cerebrovascular feature extraction that enables multiscale characterization of cerebrovascular organization and supports population-level studies of vascular health and aging.

Abstract: We present CaravelMetrics, a computational framework for automated cerebrovascular analysis that models vessel morphology through skeletonization-derived graph representations. The framework integrates atlas-based regional parcellation, centerline extraction, and graph construction to compute fifteen morphometric, topological, fractal, and geometric features. The features can be estimated globally from the complete vascular network or regionally within arterial territories, enabling multiscale characterization of cerebrovascular organization. Applied to 570 3D TOF-MRA scans from the IXI dataset (ages 20-86), CaravelMetrics yields reproducible vessel graphs capturing age- and sex-related variations and education-associated increases in vascular complexity, consistent with findings reported in the literature. The framework provides a scalable and fully automated approach for quantitative cerebrovascular feature extraction, supporting normative modeling and population-level studies of vascular health and aging.

</details>


### [100] [Dual Cross-Attention Siamese Transformer for Rectal Tumor Regrowth Assessment in Watch-and-Wait Endoscopy](https://arxiv.org/abs/2512.03883)
*Jorge Tapias Gomez,Despoina Kanata,Aneesh Rangnekar,Christina Lee,Julio Garcia-Aguilar,Joshua Jesse Smith,Harini Veeraraghavan*

Main category: cs.CV

TL;DR: SSDCA: Siamese Swin Transformer with Dual Cross-Attention for early detection of local regrowth in rectal cancer patients during watch-and-wait surveillance using longitudinal endoscopic images.


<details>
  <summary>Details</summary>
Motivation: Watch-and-wait surveillance is increasingly used for rectal cancer patients with clinical complete response after neoadjuvant treatment, but there's a need for objective, accurate methods to detect local regrowth early from follow-up endoscopy images to manage care and prevent distant metastases.

Method: Developed SSDCA (Siamese Swin Transformer with Dual Cross-Attention) that combines longitudinal endoscopic images from restaging and follow-up. Uses pretrained Swin transformers for domain-agnostic feature extraction and dual cross-attention to emphasize features from both scans without requiring spatial alignment.

Result: SSDCA achieved best performance with balanced accuracy (81.76% ± 0.04), sensitivity (90.07% ± 0.08), and specificity (72.86% ± 0.05). Showed robustness to artifacts (blood, stool, telangiectasia, poor image quality). UMAP clustering confirmed discriminative representation learning with maximal inter-cluster separation and minimal intra-cluster dispersion.

Conclusion: SSDCA provides an effective deep learning approach for early detection of local regrowth in rectal cancer patients during watch-and-wait surveillance, offering objective assessment from longitudinal endoscopic images without requiring spatial alignment and demonstrating robustness to imaging variations.

Abstract: Increasing evidence supports watch-and-wait (WW) surveillance for patients with rectal cancer who show clinical complete response (cCR) at restaging following total neoadjuvant treatment (TNT). However, objectively accurate methods to early detect local regrowth (LR) from follow-up endoscopy images during WW are essential to manage care and prevent distant metastases. Hence, we developed a Siamese Swin Transformer with Dual Cross-Attention (SSDCA) to combine longitudinal endoscopic images at restaging and follow-up and distinguish cCR from LR. SSDCA leverages pretrained Swin transformers to extract domain agnostic features and enhance robustness to imaging variations. Dual cross attention is implemented to emphasize features from the two scans without requiring any spatial alignment of images to predict response. SSDCA as well as Swin-based baselines were trained using image pairs from 135 patients and evaluated on a held-out set of image pairs from 62 patients. SSDCA produced the best balanced accuracy (81.76\% $\pm$ 0.04), sensitivity (90.07\% $\pm$ 0.08), and specificity (72.86\% $\pm$ 0.05). Robustness analysis showed stable performance irrespective of artifacts including blood, stool, telangiectasia, and poor image quality. UMAP clustering of extracted features showed maximal inter-cluster separation (1.45 $\pm$ 0.18) and minimal intra-cluster dispersion (1.07 $\pm$ 0.19) with SSDCA, confirming discriminative representation learning.

</details>


### [101] [Zero-Shot Video Translation and Editing with Frame Spatial-Temporal Correspondence](https://arxiv.org/abs/2512.03905)
*Shuai Yang,Junxin Lin,Yifan Zhou,Ziwei Liu,Chen Change Loy*

Main category: cs.CV

TL;DR: FRESCO improves zero-shot video editing by combining intra-frame and inter-frame correspondence for better temporal consistency, outperforming existing methods in video-to-video translation and text-guided editing.


<details>
  <summary>Details</summary>
Motivation: Current zero-shot video adaptation methods using image diffusion models rely on inter-frame correspondence in attention mechanisms, but their soft constraints are insufficient for maintaining temporal consistency, leading to visual incoherence in manipulated videos.

Method: FRESCO integrates both intra-frame and inter-frame correspondence to create a more robust spatial-temporal constraint, ensuring consistent transformation of semantically similar content between frames. It goes beyond attention guidance to explicitly optimize features for better coherence.

Result: Comprehensive experiments show FRESCO generates high-quality, coherent videos and significantly advances over current zero-shot methods in both video-to-video translation and text-guided video editing tasks.

Conclusion: FRESCO's integration of intra-frame and inter-frame correspondence provides superior temporal consistency for zero-shot video applications, representing a significant improvement in visual coherence for video manipulation using image diffusion models.

Abstract: The remarkable success in text-to-image diffusion models has motivated extensive investigation of their potential for video applications. Zero-shot techniques aim to adapt image diffusion models for videos without requiring further model training. Recent methods largely emphasize integrating inter-frame correspondence into attention mechanisms. However, the soft constraint applied to identify the valid features to attend is insufficient, which could lead to temporal inconsistency. In this paper, we present FRESCO, which integrates intra-frame correspondence with inter-frame correspondence to formulate a more robust spatial-temporal constraint. This enhancement ensures a consistent transformation of semantically similar content between frames. Our method goes beyond attention guidance to explicitly optimize features, achieving high spatial-temporal consistency with the input video, significantly enhancing the visual coherence of manipulated videos. We verify FRESCO adaptations on two zero-shot tasks of video-to-video translation and text-guided video editing. Comprehensive experiments demonstrate the effectiveness of our framework in generating high-quality, coherent videos, highlighting a significant advance over current zero-shot methods.

</details>


### [102] [UniMo: Unifying 2D Video and 3D Human Motion with an Autoregressive Framework](https://arxiv.org/abs/2512.03918)
*Youxin Pang,Yong Zhang,Ruizhi Shao,Xiang Deng,Feng Gao,Xu Xiaoming,Xiaoming Wei,Yebin Liu*

Main category: cs.CV

TL;DR: UniMo is the first autoregressive model that jointly models 2D human videos and 3D human motions in a unified framework, enabling simultaneous generation and understanding of both modalities.


<details>
  <summary>Details</summary>
Motivation: Current methods focus on generating one modality given another as condition or integrating them with other modalities like text/audio. Unifying 2D videos and 3D motions for simultaneous optimization and generation remains unexplored due to their substantial structural and distributional differences.

Method: Models videos and 3D motions as unified token sequences using separate embedding layers to mitigate distribution gaps. Uses a novel 3D motion tokenizer with temporal expansion strategy and single VQ-VAE with multiple expert decoders for body shapes, translation, orientation, and poses. Integrates two distinct tasks within a single framework.

Result: Extensive experiments demonstrate simultaneous generation of corresponding videos and motions while performing accurate motion capture. The method effectively unifies different modalities and proves the effectiveness of unified modeling.

Conclusion: This work taps into LLMs' capacity to fuse diverse data types, paving the way for integrating human-centric information into existing models and potentially enabling multimodal, controllable joint modeling of humans, objects, and scenes.

Abstract: We propose UniMo, an innovative autoregressive model for joint modeling of 2D human videos and 3D human motions within a unified framework, enabling simultaneous generation and understanding of these two modalities for the first time. Current methods predominantly focus on generating one modality given another as the condition or integrating either of them with other modalities such as text and audio. Unifying 2D videos and 3D motions for simultaneous optimization and generation remains largely unexplored, presenting significant challenges due to their substantial structural and distributional differences. Inspired by the LLM's ability to unify different modalities, our method models videos and 3D motions as a unified tokens sequence, utilizing separate embedding layers to mitigate distribution gaps. Additionally, we devise a sequence modeling strategy that integrates two distinct tasks within a single framework, proving the effectiveness of unified modeling. Moreover, to efficiently align with visual tokens and preserve 3D spatial information, we design a novel 3D motion tokenizer with a temporal expansion strategy, using a single VQ-VAE to produce quantized motion tokens. It features multiple expert decoders that handle body shapes, translation, global orientation, and body poses for reliable 3D motion reconstruction. Extensive experiments demonstrate that our method simultaneously generates corresponding videos and motions while performing accurate motion capture. This work taps into the capacity of LLMs to fuse diverse data types, paving the way for integrating human-centric information into existing models and potentially enabling multimodal, controllable joint modeling of humans, objects, and scenes.

</details>


### [103] [Beyond the Ground Truth: Enhanced Supervision for Image Restoration](https://arxiv.org/abs/2512.03932)
*Donghun Ryou,Inju Ha,Sanghyeok Chu,Bohyung Han*

Main category: cs.CV

TL;DR: Proposes a framework to enhance ground-truth images for better supervision in real-world image restoration, using frequency-domain mixup to generate perceptually improved training data.


<details>
  <summary>Details</summary>
Motivation: Real-world image restoration suffers from limited ground-truth quality due to practical data acquisition constraints, which restricts model performance despite deep learning advances.

Method: Generates enhanced ground truth via super-resolution with adaptive frequency masks learned by a conditional mask generator, performing frequency-domain mixup to fuse original and super-resolved components while preserving semantic consistency.

Result: Extensive experiments show consistent improvement in restored image quality; user studies validate effectiveness of both supervision enhancement and output refinement.

Conclusion: The proposed framework successfully addresses ground-truth limitations in real-world restoration by generating perceptually enhanced supervision and training a lightweight refinement network that integrates with existing models.

Abstract: Deep learning-based image restoration has achieved significant success. However, when addressing real-world degradations, model performance is limited by the quality of ground-truth images in datasets due to practical constraints in data acquisition. To address this limitation, we propose a novel framework that enhances existing ground truth images to provide higher-quality supervision for real-world restoration. Our framework generates perceptually enhanced ground truth images using super-resolution by incorporating adaptive frequency masks, which are learned by a conditional frequency mask generator. These masks guide the optimal fusion of frequency components from the original ground truth and its super-resolved variants, yielding enhanced ground truth images. This frequency-domain mixup preserves the semantic consistency of the original content while selectively enriching perceptual details, preventing hallucinated artifacts that could compromise fidelity. The enhanced ground truth images are used to train a lightweight output refinement network that can be seamlessly integrated with existing restoration models. Extensive experiments demonstrate that our approach consistently improves the quality of restored images. We further validate the effectiveness of both supervision enhancement and output refinement through user studies. Code is available at https://github.com/dhryougit/Beyond-the-Ground-Truth.

</details>


### [104] [TempR1: Improving Temporal Understanding of MLLMs via Temporal-Aware Multi-Task Reinforcement Learning](https://arxiv.org/abs/2512.03963)
*Tao Wu,Li Yang,Gen Zhan,Yiting Liao,Junlin Li,Deliang Fu,Li Zhang,Limin Wang*

Main category: cs.CV

TL;DR: TempR1 is a temporal-aware multi-task reinforcement learning framework that enhances MLLMs' temporal understanding for long-form video analysis through systematic cross-task optimization.


<details>
  <summary>Details</summary>
Motivation: Existing RL approaches for improving temporal reasoning in MLLMs are limited to specific tasks and data, restricting generalization across diverse temporal understanding scenarios needed for long-form video analysis.

Method: TempR1 uses a multi-task corpus with diverse temporal structures, builds on GRPO algorithm, categorizes temporal tasks into three correspondence types between predicted intervals and ground-truth, and designs tailored localization rewards for each type.

Result: TempR1 achieves state-of-the-art performance across multiple benchmarks, with joint optimization over complementary tasks producing strong synergistic effects that enhance both generalization and single-task performance.

Conclusion: TempR1 establishes a scalable and principled paradigm for temporal reasoning in MLLMs, demonstrating effective cross-task optimization and improved temporal comprehension for diverse video analysis applications.

Abstract: Enhancing the temporal understanding of Multimodal Large Language Models (MLLMs) is essential for advancing long-form video analysis, enabling tasks such as temporal localization, action detection, and time-sensitive question answering. While reinforcement learning (RL) has recently been explored for improving temporal reasoning, existing approaches are often confined to limited task types and data, restricting their generalization across diverse temporal understanding scenarios. To address this challenge, we present TempR1, a temporal-aware multi-task reinforcement learning framework that systematically strengthens MLLMs' temporal comprehension. We curate a multi-task corpus that exposes the model to diverse temporal structures and semantics, and build upon the Group Relative Policy Optimization (GRPO) algorithm to achieve stable and effective cross-task optimization. Specifically, we categorize temporal tasks into three correspondence types between predicted intervals and ground-truth instances, and design tailored localization rewards for each, enabling TempR1 to capture fine-grained temporal dependencies and adapt to different temporal patterns. Extensive experiments demonstrate that TempR1 attains state-of-the-art performance across multiple benchmarks. Moreover, its joint optimization over complementary tasks yields a strong synergistic effect, enhancing both generalization and single-task performance, establishing a scalable and principled paradigm for temporal reasoning in MLLMs.

</details>


### [105] [Training for Identity, Inference for Controllability: A Unified Approach to Tuning-Free Face Personalization](https://arxiv.org/abs/2512.03964)
*Lianyu Pang,Ji Zhou,Qiping Wang,Baoquan Zhao,Zhenguo Yang,Qing Li,Xudong Mao*

Main category: cs.CV

TL;DR: UniID is a tuning-free face personalization framework that unifies text embedding and adapter-based approaches to achieve both high identity fidelity and flexible text controllability through identity-focused learning and normalized rescaling.


<details>
  <summary>Details</summary>
Motivation: Existing tuning-free face personalization methods struggle to simultaneously achieve high identity fidelity and flexible text controllability. Text embedding approaches map facial features to text embeddings while adapter-based methods use cross-attention layers, but neither paradigm alone achieves both goals effectively.

Method: UniID integrates both text embedding and adapter-based paradigms through a principled training-inference strategy: 1) Identity-focused learning during training guides both branches to capture only identity features, 2) Normalized rescaling at inference recovers text controllability while enabling complementary identity signals to enhance each other.

Result: Extensive experiments against six state-of-the-art methods demonstrate that UniID achieves superior performance in both identity preservation and text controllability compared to existing approaches.

Conclusion: UniID provides a unified tuning-free framework that successfully integrates text embedding and adapter-based approaches, enabling high-fidelity face personalization with flexible text controllability through mutual reinforcement of identity-relevant information while preserving the diffusion prior for non-identity attributes.

Abstract: Tuning-free face personalization methods have developed along two distinct paradigms: text embedding approaches that map facial features into the text embedding space, and adapter-based methods that inject features through auxiliary cross-attention layers. While both paradigms have shown promise, existing methods struggle to simultaneously achieve high identity fidelity and flexible text controllability. We introduce UniID, a unified tuning-free framework that synergistically integrates both paradigms. Our key insight is that when merging these approaches, they should mutually reinforce only identity-relevant information while preserving the original diffusion prior for non-identity attributes. We realize this through a principled training-inference strategy: during training, we employ an identity-focused learning scheme that guides both branches to capture identity features exclusively; at inference, we introduce a normalized rescaling mechanism that recovers the text controllability of the base diffusion model while enabling complementary identity signals to enhance each other. This principled design enables UniID to achieve high-fidelity face personalization with flexible text controllability. Extensive experiments against six state-of-the-art methods demonstrate that UniID achieves superior performance in both identity preservation and text controllability. Code will be available at https://github.com/lyuPang/UniID

</details>


### [106] [DirectDrag: High-Fidelity, Mask-Free, Prompt-Free Drag-based Image Editing via Readout-Guided Feature Alignment](https://arxiv.org/abs/2512.03981)
*Sheng-Hao Liao,Shang-Fu Chen,Tai-Ming Huang,Wen-Huang Cheng,Kai-Lung Hua*

Main category: cs.CV

TL;DR: DirectDrag is a mask- and prompt-free image editing framework that enables precise drag-based manipulation using only point displacements, achieving high fidelity without manual masks or text prompts.


<details>
  <summary>Details</summary>
Motivation: Existing drag-based editing methods rely heavily on manual masks and textual prompts for semantic fidelity and motion precision, creating a trade-off between visual artifacts without masks and poor spatial control without prompts.

Method: Two key innovations: 1) Auto Soft Mask Generation module that infers editable regions from point displacement, localizing deformation along movement paths while preserving contextual integrity; 2) Readout-Guided Feature Alignment mechanism that leverages intermediate diffusion activations to maintain structural consistency during point-based edits.

Result: DirectDrag achieves superior image quality compared to existing methods while maintaining competitive drag accuracy, despite operating without manual masks or prompts. Extensive experiments on DragBench and real-world scenarios demonstrate effectiveness.

Conclusion: DirectDrag provides a practical, high-quality interactive image manipulation framework that eliminates the need for manual masks and textual prompts while maintaining precise control and visual fidelity.

Abstract: Drag-based image editing using generative models provides intuitive control over image structures. However, existing methods rely heavily on manually provided masks and textual prompts to preserve semantic fidelity and motion precision. Removing these constraints creates a fundamental trade-off: visual artifacts without masks and poor spatial control without prompts. To address these limitations, we propose DirectDrag, a novel mask- and prompt-free editing framework. DirectDrag enables precise and efficient manipulation with minimal user input while maintaining high image fidelity and accurate point alignment. DirectDrag introduces two key innovations. First, we design an Auto Soft Mask Generation module that intelligently infers editable regions from point displacement, automatically localizing deformation along movement paths while preserving contextual integrity through the generative model's inherent capacity. Second, we develop a Readout-Guided Feature Alignment mechanism that leverages intermediate diffusion activations to maintain structural consistency during point-based edits, substantially improving visual fidelity. Despite operating without manual mask or prompt, DirectDrag achieves superior image quality compared to existing methods while maintaining competitive drag accuracy. Extensive experiments on DragBench and real-world scenarios demonstrate the effectiveness and practicality of DirectDrag for high-quality, interactive image manipulation. Project Page: https://frakw.github.io/DirectDrag/. Code is available at: https://github.com/frakw/DirectDrag.

</details>


### [107] [Emergent Outlier View Rejection in Visual Geometry Grounded Transformers](https://arxiv.org/abs/2512.04012)
*Jisang Han,Sunghwan Hong,Jaewoo Jung,Wooseok Jang,Honggyu An,Qianqian Wang,Seungryong Kim,Chen Feng*

Main category: cs.CV

TL;DR: Feed-forward 3D reconstruction models like VGGT can inherently filter out noisy/distractor images without explicit outlier rejection mechanisms or additional training.


<details>
  <summary>Details</summary>
Motivation: Feed-forward 3D reconstruction models degrade with noisy in-the-wild images, lacking explicit outlier rejection mechanisms that traditional SfM pipelines have.

Method: Discovered VGGT has inherent noise-filtering capability through layer analysis, identified specific layer with outlier-suppressing behavior, and leveraged its internal representations for outlier-view rejection without fine-tuning.

Result: The implicit filtering mechanism is consistent and generalizes well across diverse scenarios, as shown in experiments on controlled and in-the-wild datasets.

Conclusion: Feed-forward 3D reconstruction models possess inherent noise-filtering capabilities that can be leveraged for robust reconstruction without additional training or supervision.

Abstract: Reliable 3D reconstruction from in-the-wild image collections is often hindered by "noisy" images-irrelevant inputs with little or no view overlap with others. While traditional Structure-from-Motion pipelines handle such cases through geometric verification and outlier rejection, feed-forward 3D reconstruction models lack these explicit mechanisms, leading to degraded performance under in-the-wild conditions. In this paper, we discover that the existing feed-forward reconstruction model, e.g., VGGT, despite lacking explicit outlier-rejection mechanisms or noise-aware training, can inherently distinguish distractor images. Through an in-depth analysis under varying proportions of synthetic distractors, we identify a specific layer that naturally exhibits outlier-suppressing behavior. Further probing reveals that this layer encodes discriminative internal representations that enable an effective noise-filtering capability, which we simply leverage to perform outlier-view rejection in feed-forward 3D reconstruction without any additional fine-tuning or supervision. Extensive experiments on both controlled and in-the-wild datasets demonstrate that this implicit filtering mechanism is consistent and generalizes well across diverse scenarios.

</details>


### [108] [Learning Group Actions In Disentangled Latent Image Representations](https://arxiv.org/abs/2512.04015)
*Farhana Hossain Swarnali,Miaomiao Zhang,Tonmoy Hossain*

Main category: cs.CV

TL;DR: A novel framework that automatically learns group actions on latent image manifolds, discovering transformation-relevant structures without manual intervention using learnable binary masks.


<details>
  <summary>Details</summary>
Motivation: Existing methods for modeling group actions either operate in high-dimensional data space (where transformations apply uniformly) or require manual partitioning of latent variables, limiting robust learning of group actions within representation spaces.

Method: Uses learnable binary masks with straight-through estimation to dynamically partition latent representations into transformation-sensitive and invariant components, formulated within a unified optimization framework that jointly learns latent disentanglement and group transformation mappings.

Result: Validated on five 2D/3D image datasets, demonstrating automatic learning of disentangled latent factors for group actions in diverse data, with downstream classification tasks confirming effectiveness of learned representations.

Conclusion: The framework enables automatic discovery of transformation-relevant structures without manual intervention, can be integrated with any standard encoder-decoder architecture, and provides publicly available code for reproducibility.

Abstract: Modeling group actions on latent representations enables controllable transformations of high-dimensional image data. Prior works applying group-theoretic priors or modeling transformations typically operate in the high-dimensional data space, where group actions apply uniformly across the entire input, making it difficult to disentangle the subspace that varies under transformations. While latent-space methods offer greater flexibility, they still require manual partitioning of latent variables into equivariant and invariant subspaces, limiting the ability to robustly learn and operate group actions within the representation space. To address this, we introduce a novel end-to-end framework that for the first time learns group actions on latent image manifolds, automatically discovering transformation-relevant structures without manual intervention. Our method uses learnable binary masks with straight-through estimation to dynamically partition latent representations into transformation-sensitive and invariant components. We formulate this within a unified optimization framework that jointly learns latent disentanglement and group transformation mappings. The framework can be seamlessly integrated with any standard encoder-decoder architecture. We validate our approach on five 2D/3D image datasets, demonstrating its ability to automatically learn disentangled latent factors for group actions in diverse data, while downstream classification tasks confirm the effectiveness of the learned representations. Our code is publicly available at https://github.com/farhanaswarnali/Learning-Group-Actions-In-Disentangled-Latent-Image-Representations .

</details>


### [109] [Ultra-lightweight Neural Video Representation Compression](https://arxiv.org/abs/2512.04019)
*Ho Man Kwan,Tianhao Peng,Ge Gao,Fan Zhang,Mike Nilsson,Andrew Gower,David Bull*

Main category: cs.CV

TL;DR: NVRC-Lite extends neural video compression with lightweight implicit neural representations using multi-scale feature grids and octree-based entropy coding for faster encoding/decoding.


<details>
  <summary>Details</summary>
Motivation: Existing INR-based video codecs like NVRC achieve state-of-the-art performance but have computational complexity issues. Lightweight INRs show promise but need improvements in both performance and practical coding speed.

Method: Two key innovations: 1) Multi-scale feature grids integrated into lightweight neural representation to improve performance at low complexity, 2) Octree-based context model for entropy coding high-dimensional feature grids to accelerate coding speed.

Result: NVRC-Lite outperforms C3 (best lightweight INR-based codec) with 21.03% BD-rate savings in PSNR and 23.06% in MS-SSIM, while achieving 8.4x encoding and 2.5x decoding speedup.

Conclusion: NVRC-Lite successfully extends NVRC toward lightweight representations with improved performance and practical coding speed, making INR-based video compression more viable for real-world applications.

Abstract: Recent works have demonstrated the viability of utilizing over-fitted implicit neural representations (INRs) as alternatives to autoencoder-based models for neural video compression. Among these INR-based video codecs, Neural Video Representation Compression (NVRC) was the first to adopt a fully end-to-end compression framework that compresses INRs, achieving state-of-the-art performance. Moreover, some recently proposed lightweight INRs have shown comparable performance to their baseline codecs with computational complexity lower than 10kMACs/pixel. In this work, we extend NVRC toward lightweight representations, and propose NVRC-Lite, which incorporates two key changes. Firstly, we integrated multi-scale feature grids into our lightweight neural representation, and the use of higher resolution grids significantly improves the performance of INRs at low complexity. Secondly, we address the issue that existing INRs typically leverage autoregressive models for entropy coding: these are effective but impractical due to their slow coding speed. In this work, we propose an octree-based context model for entropy coding high-dimensional feature grids, which accelerates the entropy coding module of the model. Our experimental results demonstrate that NVRC-Lite outperforms C3, one of the best lightweight INR-based video codecs, with up to 21.03% and 23.06% BD-rate savings when measured in PSNR and MS-SSIM, respectively, while achieving 8.4x encoding and 2.5x decoding speedup. The implementation of NVRC-Lite will be made available.

</details>


### [110] [C3G: Learning Compact 3D Representations with 2K Gaussians](https://arxiv.org/abs/2512.04021)
*Honggyu An,Jaewoo Jung,Mungyeom Kim,Sunghwan Hong,Chaehyun Kim,Kazumi Fukuda,Minkyeong Jeon,Jisang Han,Takuya Narihira,Hyuna Ko,Junsu Kim,Yuki Mitsufuji,Seungryong Kim*

Main category: cs.CV

TL;DR: C3G: A feed-forward framework that generates compact 3D Gaussians at essential locations for efficient scene reconstruction and understanding from unposed sparse views.


<details>
  <summary>Details</summary>
Motivation: Existing methods using per-pixel 3D Gaussian Splatting create excessive redundant Gaussians, causing high memory overhead and sub-optimal multi-view feature aggregation, which degrades novel view synthesis and scene understanding performance.

Method: Proposes C3G framework with learnable tokens that aggregate multi-view features through self-attention to guide Gaussian generation, ensuring each Gaussian integrates relevant visual features across views. Uses learned attention patterns for efficient Gaussian decoding and feature lifting.

Result: Demonstrates effectiveness on pose-free novel view synthesis, 3D open-vocabulary segmentation, and view-invariant feature aggregation. Shows superior memory efficiency and feature fidelity compared to existing methods.

Conclusion: A compact yet geometrically meaningful representation is sufficient for high-quality scene reconstruction and understanding, achieving better memory efficiency and feature quality than current approaches.

Abstract: Reconstructing and understanding 3D scenes from unposed sparse views in a feed-forward manner remains as a challenging task in 3D computer vision. Recent approaches use per-pixel 3D Gaussian Splatting for reconstruction, followed by a 2D-to-3D feature lifting stage for scene understanding. However, they generate excessive redundant Gaussians, causing high memory overhead and sub-optimal multi-view feature aggregation, leading to degraded novel view synthesis and scene understanding performance. We propose C3G, a novel feed-forward framework that estimates compact 3D Gaussians only at essential spatial locations, minimizing redundancy while enabling effective feature lifting. We introduce learnable tokens that aggregate multi-view features through self-attention to guide Gaussian generation, ensuring each Gaussian integrates relevant visual features across views. We then exploit the learned attention patterns for Gaussian decoding to efficiently lift features. Extensive experiments on pose-free novel view synthesis, 3D open-vocabulary segmentation, and view-invariant feature aggregation demonstrate our approach's effectiveness. Results show that a compact yet geometrically meaningful representation is sufficient for high-quality scene reconstruction and understanding, achieving superior memory efficiency and feature fidelity compared to existing methods.

</details>


### [111] [RELIC: Interactive Video World Model with Long-Horizon Memory](https://arxiv.org/abs/2512.04040)
*Yicong Hong,Yiqun Mei,Chongjian Ge,Yiran Xu,Yang Zhou,Sai Bi,Yannick Hold-Geoffroy,Mike Roberts,Matthew Fisher,Eli Shechtman,Kalyan Sunkavalli,Feng Liu,Zhengqi Li,Hao Tan*

Main category: cs.CV

TL;DR: RELIC is a unified framework for interactive world modeling that achieves real-time long-horizon streaming, consistent spatial memory, and precise user control simultaneously through autoregressive video-diffusion distillation with camera-aware memory tokens.


<details>
  <summary>Details</summary>
Motivation: Existing approaches only address one aspect of interactive world modeling (real-time streaming, spatial memory, or user control) in isolation, but achieving all three simultaneously is challenging due to performance trade-offs like memory mechanisms degrading real-time performance.

Method: Built on autoregressive video-diffusion distillation, RELIC uses compressed historical latent tokens with relative actions and absolute camera poses in KV cache for memory. Fine-tunes bidirectional teacher video model beyond 5-second horizon, transforms to causal student generator using memory-efficient self-forcing paradigm for full-context distillation over long durations.

Result: 14B-parameter model trained on Unreal Engine-rendered dataset achieves 16 FPS real-time generation with more accurate action following, more stable long-horizon streaming, and more robust spatial-memory retrieval compared to prior work.

Conclusion: RELIC establishes a strong foundation for next-generation interactive world modeling by unifying real-time performance, spatial memory, and user control in a single framework.

Abstract: A truly interactive world model requires three key ingredients: real-time long-horizon streaming, consistent spatial memory, and precise user control. However, most existing approaches address only one of these aspects in isolation, as achieving all three simultaneously is highly challenging-for example, long-term memory mechanisms often degrade real-time performance. In this work, we present RELIC, a unified framework that tackles these three challenges altogether. Given a single image and a text description, RELIC enables memory-aware, long-duration exploration of arbitrary scenes in real time. Built upon recent autoregressive video-diffusion distillation techniques, our model represents long-horizon memory using highly compressed historical latent tokens encoded with both relative actions and absolute camera poses within the KV cache. This compact, camera-aware memory structure supports implicit 3D-consistent content retrieval and enforces long-term coherence with minimal computational overhead. In parallel, we fine-tune a bidirectional teacher video model to generate sequences beyond its original 5-second training horizon, and transform it into a causal student generator using a new memory-efficient self-forcing paradigm that enables full-context distillation over long-duration teacher as well as long student self-rollouts. Implemented as a 14B-parameter model and trained on a curated Unreal Engine-rendered dataset, RELIC achieves real-time generation at 16 FPS while demonstrating more accurate action following, more stable long-horizon streaming, and more robust spatial-memory retrieval compared with prior work. These capabilities establish RELIC as a strong foundation for the next generation of interactive world modeling.

</details>


### [112] [Stable Signer: Hierarchical Sign Language Generative Model](https://arxiv.org/abs/2512.04048)
*Sen Fang,Yalin Feng,Hongbin Zhong,Yanxin Zhang,Dimitris N. Metaxas*

Main category: cs.CV

TL;DR: Stable Signer is a new end-to-end sign language production model that simplifies the traditional pipeline by focusing only on text understanding and pose-to-video generation, achieving 48.6% improvement over SOTA methods.


<details>
  <summary>Details</summary>
Motivation: Traditional SLP approaches suffer from error accumulation across multiple stages (Text2Gloss, Gloss2Pose, Pose2Vid), leading to inaccurate text conversion, pose generation, and video rendering. The field has made slow progress due to these cascading errors.

Method: Redefines SLP as hierarchical end-to-end task with only text understanding (Prompt2Gloss, Text2Gloss) and Pose2Vid. Uses Sign Language Understanding Linker (SLUL) for text understanding trained with Semantic-Aware Gloss Masking Loss (SAGM Loss), and SLP-MoE hand gesture rendering expert block for video generation.

Result: Achieves 48.6% performance improvement compared to current state-of-the-art generation methods in sign language production.

Conclusion: Streamlining the traditional redundant SLP pipeline into a simplified end-to-end architecture with specialized components (SLUL and SLP-MoE) significantly improves sign language video generation quality and addresses error accumulation issues.

Abstract: Sign Language Production (SLP) is the process of converting the complex input text into a real video. Most previous works focused on the Text2Gloss, Gloss2Pose, Pose2Vid stages, and some concentrated on Prompt2Gloss and Text2Avatar stages. However, this field has made slow progress due to the inaccuracy of text conversion, pose generation, and the rendering of poses into real human videos in these stages, resulting in gradually accumulating errors. Therefore, in this paper, we streamline the traditional redundant structure, simplify and optimize the task objective, and design a new sign language generative model called Stable Signer. It redefines the SLP task as a hierarchical generation end-to-end task that only includes text understanding (Prompt2Gloss, Text2Gloss) and Pose2Vid, and executes text understanding through our proposed new Sign Language Understanding Linker called SLUL, and generates hand gestures through the named SLP-MoE hand gesture rendering expert block to end-to-end generate high-quality and multi-style sign language videos. SLUL is trained using the newly developed Semantic-Aware Gloss Masking Loss (SAGM Loss). Its performance has improved by 48.6% compared to the current SOTA generation methods.

</details>


### [113] [PosterCopilot: Toward Layout Reasoning and Controllable Editing for Professional Graphic Design](https://arxiv.org/abs/2512.04082)
*Jiazhe Wei,Ken Li,Tianyu Lao,Haofan Wang,Liang Wang,Caifeng Shan,Chenyang Si*

Main category: cs.CV

TL;DR: PosterCopilot is a framework that uses progressive three-stage training to equip LMMs with geometric understanding and aesthetic reasoning for professional graphic design, enabling layer-controllable iterative editing.


<details>
  <summary>Details</summary>
Motivation: Existing automated graphic design methods using LMMs produce geometrically inaccurate layouts and lack the iterative, layer-specific editing capabilities required in professional workflows.

Method: A progressive three-stage training strategy: 1) Perturbed Supervised Fine-Tuning, 2) Reinforcement Learning for Visual-Reality Alignment, and 3) Reinforcement Learning from Aesthetic Feedback, coupled with a complete workflow integrating trained LMMs with generative models.

Result: Extensive experiments demonstrate that PosterCopilot achieves geometrically accurate and aesthetically superior layouts with unprecedented controllability for professional iterative design.

Conclusion: PosterCopilot advances layout reasoning and controllable editing for professional graphic design by addressing geometric accuracy and workflow integration limitations of existing LMM-based approaches.

Abstract: Graphic design forms the cornerstone of modern visual communication, serving as a vital medium for promoting cultural and commercial events. Recent advances have explored automating this process using Large Multimodal Models (LMMs), yet existing methods often produce geometrically inaccurate layouts and lack the iterative, layer-specific editing required in professional workflows. To address these limitations, we present PosterCopilot, a framework that advances layout reasoning and controllable editing for professional graphic design. Specifically, we introduce a progressive three-stage training strategy that equips LMMs with geometric understanding and aesthetic reasoning for layout design, consisting of Perturbed Supervised Fine-Tuning, Reinforcement Learning for Visual-Reality Alignment, and Reinforcement Learning from Aesthetic Feedback. Furthermore, we develop a complete workflow that couples the trained LMM-based design model with generative models, enabling layer-controllable, iterative editing for precise element refinement while maintaining global visual consistency. Extensive experiments demonstrate that PosterCopilot achieves geometrically accurate and aesthetically superior layouts, offering unprecedented controllability for professional iterative design.

</details>


### [114] [SimFlow: Simplified and End-to-End Training of Latent Normalizing Flows](https://arxiv.org/abs/2512.04084)
*Qinyu Zhao,Guangting Zheng,Tao Yang,Rui Zhu,Xingjian Leng,Stephen Gould,Liang Zheng*

Main category: cs.CV

TL;DR: SimFlow fixes VAE encoder variance to a constant (e.g., 0.5) to simplify NF training, eliminating complex noise pipelines and enabling joint VAE-NF optimization, achieving SOTA gFID scores on ImageNet.


<details>
  <summary>Details</summary>
Motivation: Prior NF methods have two key limitations: 1) they require complex noise augmentation pipelines with extra noising/denoising steps, and 2) they use frozen pretrained VAE encoders leading to suboptimal reconstruction and generation quality.

Method: Fix the variance predicted by the VAE encoder to a constant (e.g., 0.5). This allows the encoder to output broader token distributions, enables the decoder to reconstruct clean images from augmented tokens without extra noise design, and simplifies the VAE evidence lower bound for stable joint training of NF with VAE.

Result: SimFlow achieves gFID 2.15 on ImageNet 256×256 generation, outperforming previous SOTA STARFlow (gFID 2.40). When integrated with REPA-E, it achieves gFID 1.91, setting new SOTA among normalizing flows.

Conclusion: Fixing VAE encoder variance to a constant provides a simple yet effective solution to two major limitations in prior NF methods, enabling better reconstruction quality, simpler training pipelines, and state-of-the-art generation performance.

Abstract: Normalizing Flows (NFs) learn invertible mappings between the data and a Gaussian distribution. Prior works usually suffer from two limitations. First, they add random noise to training samples or VAE latents as data augmentation, introducing complex pipelines including extra noising and denoising steps. Second, they use a pretrained and frozen VAE encoder, resulting in suboptimal reconstruction and generation quality. In this paper, we find that the two issues can be solved in a very simple way: just fixing the variance (which would otherwise be predicted by the VAE encoder) to a constant (e.g., 0.5). On the one hand, this method allows the encoder to output a broader distribution of tokens and the decoder to learn to reconstruct clean images from the augmented token distribution, avoiding additional noise or denoising design. On the other hand, fixed variance simplifies the VAE evidence lower bound, making it stable to train an NF with a VAE jointly. On the ImageNet $256 \times 256$ generation task, our model SimFlow obtains a gFID score of 2.15, outperforming the state-of-the-art method STARFlow (gFID 2.40). Moreover, SimFlow can be seamlessly integrated with the end-to-end representation alignment (REPA-E) method and achieves an improved gFID of 1.91, setting a new state of the art among NFs.

</details>


### [115] [Unique Lives, Shared World: Learning from Single-Life Videos](https://arxiv.org/abs/2512.04085)
*Tengda Han,Sayna Ebrahimi,Dilara Gokay,Li Yang Ku,Maks Ovsjanikov,Iva Babukova,Daniel Zoran,Viorica Patraucean,Joao Carreira,Andrew Zisserman,Dima Damen*

Main category: cs.CV

TL;DR: Single-life learning trains vision models on one person's egocentric videos, showing models develop aligned geometric understanding across different lives and learn transferable representations comparable to diverse web data training.


<details>
  <summary>Details</summary>
Motivation: To explore whether training vision models exclusively on a single individual's egocentric video data can lead to effective visual representation learning, leveraging the multiple viewpoints naturally captured within one person's life.

Method: Train distinct vision models independently on egocentric videos from different individuals (single-life paradigm), use self-supervised learning with multiple viewpoints, and introduce a novel cross-attention-based metric to quantify functional alignment of internal representations.

Result: 1) Models trained on different lives develop highly aligned geometric understanding; 2) Single-life models learn generalizable geometric representations that transfer to downstream tasks like depth estimation; 3) 30 hours from one week of a person's life performs comparably to 30 hours of diverse web data.

Conclusion: The shared structure of the world leads to consistency in models trained on individual lives and provides a powerful signal for visual representation learning, establishing the viability of single-life learning paradigms.

Abstract: We introduce the "single-life" learning paradigm, where we train a distinct vision model exclusively on egocentric videos captured by one individual. We leverage the multiple viewpoints naturally captured within a single life to learn a visual encoder in a self-supervised manner. Our experiments demonstrate three key findings. First, models trained independently on different lives develop a highly aligned geometric understanding. We demonstrate this by training visual encoders on distinct datasets each capturing a different life, both indoors and outdoors, as well as introducing a novel cross-attention-based metric to quantify the functional alignment of the internal representations developed by different models. Second, we show that single-life models learn generalizable geometric representations that effectively transfer to downstream tasks, such as depth estimation, in unseen environments. Third, we demonstrate that training on up to 30 hours from one week of the same person's life leads to comparable performance to training on 30 hours of diverse web data, highlighting the strength of single-life representation learning. Overall, our results establish that the shared structure of the world, both leads to consistency in models trained on individual lives, and provides a powerful signal for visual representation learning.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [116] [Multi-Agent Reinforcement Learning and Real-Time Decision-Making in Robotic Soccer for Virtual Environments](https://arxiv.org/abs/2512.03166)
*Aya Taourirte,Md Sohag Mia*

Main category: cs.RO

TL;DR: A unified MARL framework combining PPO baseline, hierarchical RL for multi-granularity tasks, and mean-field theory for scalability achieves superior performance in 4v4 robotic soccer simulations.


<details>
  <summary>Details</summary>
Motivation: Multi-agent systems in dynamic adversarial environments like robotic soccer need real-time decision-making, cooperation, and scalable algorithms to avoid dimensionality curse. Existing RL methods struggle with multi-granularity tasks (long-term strategy vs instant actions) and large-scale agent interactions.

Method: Three-stage approach: 1) PPO baseline in client-server architecture for real-time scheduling; 2) Hierarchical RL based on options framework with high-level trajectory planning (SMDP) and low-level action execution; 3) Mean-field theory integration into HRL to simplify many-agent interactions as single agent vs population average.

Result: PPO baseline: 4.32 avg goals, 82.9% ball control. HRL: improved to 5.26 avg goals. Mean-field actor-critic: 5.93 avg goals, 89.1% ball control, 92.3% passing accuracy with enhanced training stability. Validated in 4v4 Webots simulations.

Conclusion: The unified MARL framework demonstrates robust, scalable, and cooperative behavior in complex multi-agent domains, effectively addressing multi-granularity tasks and scalability challenges in dynamic adversarial environments.

Abstract: The deployment of multi-agent systems in dynamic, adversarial environments like robotic soccer necessitates real-time decision-making, sophisticated cooperation, and scalable algorithms to avoid the curse of dimensionality. While Reinforcement Learning (RL) offers a promising framework, existing methods often struggle with the multi-granularity of tasks (long-term strategy vs. instant actions) and the complexity of large-scale agent interactions. This paper presents a unified Multi-Agent Reinforcement Learning (MARL) framework that addresses these challenges. First, we establish a baseline using Proximal Policy Optimization (PPO) within a client-server architecture for real-time action scheduling, with PPO demonstrating superior performance (4.32 avg. goals, 82.9% ball control). Second, we introduce a Hierarchical RL (HRL) structure based on the options framework to decompose the problem into a high-level trajectory planning layer (modeled as a Semi-Markov Decision Process) and a low-level action execution layer, improving global strategy (avg. goals increased to 5.26). Finally, to ensure scalability, we integrate mean-field theory into the HRL framework, simplifying many-agent interactions into a single agent vs. the population average. Our mean-field actor-critic method achieves a significant performance boost (5.93 avg. goals, 89.1% ball control, 92.3% passing accuracy) and enhanced training stability. Extensive simulations of 4v4 matches in the Webots environment validate our approach, demonstrating its potential for robust, scalable, and cooperative behavior in complex multi-agent domains.

</details>


### [117] [GRAND: Guidance, Rebalancing, and Assignment for Networked Dispatch in Multi-Agent Path Finding](https://arxiv.org/abs/2512.03194)
*Johannes Gaber,Meshal Alharbi,Daniele Gammelli,Gioele Zardini*

Main category: cs.RO

TL;DR: Hybrid learning+optimization method for multi-agent pickup-and-delivery scheduling improves warehouse throughput by 10% while maintaining real-time execution with 1s compute budget.


<details>
  <summary>Details</summary>
Motivation: Large robot fleets in warehouses require efficient scheduling where small control gains translate to large operational impacts. Need scalable methods for lifelong multi-agent pickup-and-delivery (MAPD) that can handle congestion in large fleets (up to 500 agents).

Method: Hybrid approach coupling learning-based global guidance with lightweight optimization: 1) Graph neural network policy trained via RL outputs desired distribution of free agents over aggregated warehouse graph, 2) Minimum-cost flow converts this to region-to-region rebalancing, 3) Small local assignment problems finalize decisions while preserving accuracy within 1s compute budget.

Result: On congested warehouse benchmarks from League of Robot Runners (LRR) with up to 500 agents, approach improves throughput by up to 10% over 2024 winning scheduler while maintaining real-time execution.

Conclusion: Coupling graph-structured learned guidance with tractable solvers reduces congestion and yields practical, scalable blueprint for high-throughput scheduling in large fleets.

Abstract: Large robot fleets are now common in warehouses and other logistics settings, where small control gains translate into large operational impacts. In this article, we address task scheduling for lifelong Multi-Agent Pickup-and-Delivery (MAPD) and propose a hybrid method that couples learning-based global guidance with lightweight optimization. A graph neural network policy trained via reinforcement learning outputs a desired distribution of free agents over an aggregated warehouse graph. This signal is converted into region-to-region rebalancing through a minimum-cost flow, and finalized by small, local assignment problems, preserving accuracy while keeping per-step latency within a 1 s compute budget. On congested warehouse benchmarks from the League of Robot Runners (LRR) with up to 500 agents, our approach improves throughput by up to 10% over the 2024 winning scheduler while maintaining real-time execution. The results indicate that coupling graph-structured learned guidance with tractable solvers reduces congestion and yields a practical, scalable blueprint for high-throughput scheduling in large fleets.

</details>


### [118] [KALIKO: Kalman-Implicit Koopman Operator Learning For Prediction of Nonlinear Dynamical Systems](https://arxiv.org/abs/2512.03256)
*Albert H. Li,Ivan Dario Jimenez Rodriguez,Joel W. Burdick,Yisong Yue,Aaron D. Ames*

Main category: cs.RO

TL;DR: KALIKO (Kalman-Implicit Koopman Operator Learning) uses Kalman filtering to implicitly learn embeddings for Koopman operator approximation, enabling accurate long-horizon prediction of complex dynamical systems without explicit basis functions.


<details>
  <summary>Details</summary>
Motivation: Long-horizon prediction is crucial for robotics and control (e.g., MPC), but many systems are hard to model due to nonlinearity, chaos, and high-dimensionality. Koopman theory helps by linearizing dynamics in embedding space, but choosing good basis functions is challenging and can lead to inaccurate forecasts or overfitting.

Method: KALIKO leverages the Kalman filter to implicitly learn embeddings corresponding to latent dynamics without requiring an explicit encoder. This approach learns interpretable representations consistent with Koopman theory while avoiding explicit basis function computation.

Result: KALIKO produces interpretable representations, yields high-quality reconstructions, and induces globally linear latent dynamics. It surpasses baselines in open-loop prediction on wave data from high-dimensional PDEs and in closed-loop control tasks like stabilizing an underactuated manipulator's payload against strong wave disturbances.

Conclusion: KALIKO provides an effective approach for long-horizon prediction of complex dynamical systems by implicitly learning Koopman embeddings through Kalman filtering, addressing the basis selection problem while maintaining theoretical consistency and practical performance in challenging control applications.

Abstract: Long-horizon dynamical prediction is fundamental in robotics and control, underpinning canonical methods like model predictive control. Yet, many systems and disturbance phenomena are difficult to model due to effects like nonlinearity, chaos, and high-dimensionality. Koopman theory addresses this by modeling the linear evolution of embeddings of the state under an infinite-dimensional linear operator that can be approximated with a suitable finite basis of embedding functions, effectively trading model nonlinearity for representational complexity. However, explicitly computing a good choice of basis is nontrivial, and poor choices may cause inaccurate forecasts or overfitting. To address this, we present Kalman-Implicit Koopman Operator (KALIKO) Learning, a method that leverages the Kalman filter to implicitly learn embeddings corresponding to latent dynamics without requiring an explicit encoder. KALIKO produces interpretable representations consistent with both theory and prior works, yielding high-quality reconstructions and inducing a globally linear latent dynamics. Evaluated on wave data generated by a high-dimensional PDE, KALIKO surpasses several baselines in open-loop prediction and in a demanding closed-loop simulated control task: stabilizing an underactuated manipulator's payload by predicting and compensating for strong wave disturbances.

</details>


### [119] [GOMP: Grasped Object Manifold Projection for Multimodal Imitation Learning of Manipulation](https://arxiv.org/abs/2512.03347)
*William van den Bogert,Gregory Linkowski,Nima Fazeli*

Main category: cs.RO

TL;DR: GOMP improves imitation learning precision for assembly tasks by constraining grasped objects to manifolds and using interactive bandit adjustments.


<details>
  <summary>Details</summary>
Motivation: Imitation Learning for industrial assembly suffers from compounding errors that reduce trajectory precision, limiting practical effectiveness.

Method: GOMP constrains non-rigidly grasped objects to lower-dimensional manifolds, learns enhancements from expert data, and uses n-arm bandit interactive adjustments.

Result: Demonstrated on four precise assembly tasks with tactile feedback, showing improved precision while remaining modality-agnostic.

Conclusion: GOMP effectively mitigates compounding errors in IL for precise manipulation tasks through manifold constraints and interactive learning.

Abstract: Imitation Learning (IL) holds great potential for learning repetitive manipulation tasks, such as those in industrial assembly. However, its effectiveness is often limited by insufficient trajectory precision due to compounding errors. In this paper, we introduce Grasped Object Manifold Projection (GOMP), an interactive method that mitigates these errors by constraining a non-rigidly grasped object to a lower-dimensional manifold. GOMP assumes a precise task in which a manipulator holds an object that may shift within the grasp in an observable manner and must be mated with a grounded part. Crucially, all GOMP enhancements are learned from the same expert dataset used to train the base IL policy, and are adjusted with an n-arm bandit-based interactive component. We propose a theoretical basis for GOMP's improvement upon the well-known compounding error bound in IL literature. We demonstrate the framework on four precise assembly tasks using tactile feedback, and note that the approach remains modality-agnostic. Data and videos are available at williamvdb.github.io/GOMPsite.

</details>


### [120] [Surfel-LIO: Fast LiDAR-Inertial Odometry with Pre-computed Surfels and Hierarchical Z-order Voxel Hashing](https://arxiv.org/abs/2512.03397)
*Seungwon Choi,Dong-Gyu Park,Seo-Yeon Hwang,Tae-Wan Kim*

Main category: cs.RO

TL;DR: Surfel-LIO: A LiDAR-inertial odometry system using hierarchical voxels with pre-computed surfels for O(1) correspondence retrieval and Z-order curve encoding for cache-friendly spatial indexing, achieving faster processing with comparable accuracy.


<details>
  <summary>Details</summary>
Motivation: Current LIO systems have inefficiencies: (1) nearest neighbor search requires examining multiple spatial units for sufficient plane fitting points, and (2) plane parameters are recomputed at every iteration despite unchanged map geometry.

Method: Proposes Surfel-LIO with hierarchical voxel structure (hVox) using pre-computed surfel representation, enabling O(1) correspondence retrieval without runtime neighbor enumeration or plane fitting, combined with Z-order curve encoding for cache-friendly spatial indexing.

Result: Experimental results on M3DGR dataset show significantly faster processing speed compared to recent state-of-the-art methods while maintaining comparable state estimation accuracy.

Conclusion: Surfel-LIO addresses inefficiencies in current LIO systems through efficient data structures and pre-computation, achieving faster processing with maintained accuracy, with implementation publicly available.

Abstract: LiDAR-inertial odometry (LIO) is an active research area, as it enables accurate real-time state estimation in GPS-denied environments. Recent advances in map data structures and spatial indexing have significantly improved the efficiency of LIO systems. Nevertheless, we observe that two aspects may still leave room for improvement: (1) nearest neighbor search often requires examining multiple spatial units to gather sufficient points for plane fitting, and (2) plane parameters are typically recomputed at every iteration despite unchanged map geometry. Motivated by these observations, we propose Surfel-LIO, which employs a hierarchical voxel structure (hVox) with pre-computed surfel representation. This design enables O(1) correspondence retrieval without runtime neighbor enumeration or plane fitting, combined with Z-order curve encoding for cache-friendly spatial indexing. Experimental results on the M3DGR dataset demonstrate that our method achieves significantly faster processing speed compared to recent state-of-the-art methods while maintaining comparable state estimation accuracy. Our implementation is publicly available at https://github.com/93won/lidar_inertial_odometry.

</details>


### [121] [What Is The Best 3D Scene Representation for Robotics? From Geometric to Foundation Models](https://arxiv.org/abs/2512.03422)
*Tianchen Deng,Yue Pan,Shenghai Yuan,Dong Li,Chen Wang,Mingrui Li,Long Chen,Lihua Xie,Danwei Wang,Jingchuan Wang,Javier Civera,Hesheng Wang,Weidong Chen*

Main category: cs.RO

TL;DR: Survey paper analyzing 3D scene representations for robotics, comparing traditional methods (point clouds, voxels, SDF) with neural approaches (NeRF, 3DGS, Foundation Models), and discussing their applications across perception, mapping, localization, navigation, and manipulation modules.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive overview of scene representation methods for robotics and address the question: "What is the best 3D scene representation for robotics?" The paper aims to help researchers understand current methods and future trends, particularly the potential of 3D Foundation Models as unified solutions.

Method: Categorizes robotics into five core modules (Perception, Mapping, Localization, Navigation, Manipulation), presents standard formulations of different scene representation methods, compares advantages/disadvantages across modules, and discusses future trends including 3D Foundation Models.

Result: Provides systematic comparison of scene representations across robotic modules, identifies that while current SLAM systems use sparse representations, dense representations are critical for downstream tasks, and neural representations enable better semantic integration and scene understanding.

Conclusion: 3D Foundation Models show promise as unified solutions for future robotic applications, though challenges remain. The survey serves as a resource for researchers and includes an open-source GitHub project that will be updated with new works and technologies.

Abstract: In this paper, we provide a comprehensive overview of existing scene representation methods for robotics, covering traditional representations such as point clouds, voxels, signed distance functions (SDF), and scene graphs, as well as more recent neural representations like Neural Radiance Fields (NeRF), 3D Gaussian Splatting (3DGS), and the emerging Foundation Models. While current SLAM and localization systems predominantly rely on sparse representations like point clouds and voxels, dense scene representations are expected to play a critical role in downstream tasks such as navigation and obstacle avoidance. Moreover, neural representations such as NeRF, 3DGS, and foundation models are well-suited for integrating high-level semantic features and language-based priors, enabling more comprehensive 3D scene understanding and embodied intelligence. In this paper, we categorized the core modules of robotics into five parts (Perception, Mapping, Localization, Navigation, Manipulation). We start by presenting the standard formulation of different scene representation methods and comparing the advantages and disadvantages of scene representation across different modules. This survey is centered around the question: What is the best 3D scene representation for robotics? We then discuss the future development trends of 3D scene representations, with a particular focus on how the 3D Foundation Model could replace current methods as the unified solution for future robotic applications. The remaining challenges in fully realizing this model are also explored. We aim to offer a valuable resource for both newcomers and experienced researchers to explore the future of 3D scene representations and their application in robotics. We have published an open-source project on GitHub and will continue to add new works and technologies to this project.

</details>


### [122] [World Models for Autonomous Navigation of Terrestrial Robots from LIDAR Observations](https://arxiv.org/abs/2512.03429)
*Raul Steinmetz,Fabio Demo Rosa,Victor Augusto Kich,Jair Augusto Bottega,Ricardo Bedin Grando,Daniel Fernando Tello Gamarra*

Main category: cs.RO

TL;DR: DreamerV3-based RL with MLP-VAE for LIDAR encoding achieves 100% success rate in robot navigation, outperforming model-free methods that plateau below 85%.


<details>
  <summary>Details</summary>
Motivation: Traditional RL struggles with high-dimensional LIDAR data, forcing simplified observations that reduce spatial awareness and navigation robustness. Model-free approaches are sample inefficient for this problem.

Method: Novel model-based RL framework using DreamerV3 algorithm with MLP-VAE to encode high-dimensional LIDAR readings into compact latent representations, combined with learned dynamics predictor for imagination-based policy optimization.

Result: Achieves 100% success rate across all evaluated TurtleBot3 navigation environments with full 360 LIDAR readings, while model-free baselines (SAC, DDPG, TD3) plateau below 85%. Faster convergence and higher success rates demonstrated.

Conclusion: Integrating predictive world models with learned latent representations enables more efficient and robust navigation from high-dimensional sensory data, overcoming limitations of model-free approaches.

Abstract: Autonomous navigation of terrestrial robots using Reinforcement Learning (RL) from LIDAR observations remains challenging due to the high dimensionality of sensor data and the sample inefficiency of model-free approaches. Conventional policy networks struggle to process full-resolution LIDAR inputs, forcing prior works to rely on simplified observations that reduce spatial awareness and navigation robustness. This paper presents a novel model-based RL framework built on top of the DreamerV3 algorithm, integrating a Multi-Layer Perceptron Variational Autoencoder (MLP-VAE) within a world model to encode high-dimensional LIDAR readings into compact latent representations. These latent features, combined with a learned dynamics predictor, enable efficient imagination-based policy optimization. Experiments on simulated TurtleBot3 navigation tasks demonstrate that the proposed architecture achieves faster convergence and higher success rate compared to model-free baselines such as SAC, DDPG, and TD3. It is worth emphasizing that the DreamerV3-based agent attains a 100% success rate across all evaluated environments when using the full dataset of the Turtlebot3 LIDAR (360 readings), while model-free methods plateaued below 85%. These findings demonstrate that integrating predictive world models with learned latent representations enables more efficient and robust navigation from high-dimensional sensory data.

</details>


### [123] [PerFACT: Motion Policy with LLM-Powered Dataset Synthesis and Fusion Action-Chunking Transformers](https://arxiv.org/abs/2512.03444)
*Davood Soleymanzadeh,Xiao Liang,Minghui Zheng*

Main category: cs.RO

TL;DR: PerFACT introduces LLM-powered workspace generation and fusion transformer networks to create large-scale planning datasets and achieve faster, more generalizable neural motion planning for robots.


<details>
  <summary>Details</summary>
Motivation: Current neural motion planners are limited by small, manually-generated datasets and monolithic architectures that struggle with out-of-distribution scenarios and encoding critical planning information.

Method: Two key components: 1) MotionGeneralizer - LLM-powered workspace generation for large-scale diverse data collection, and 2) MpiNetsFusion - fusion action-chunking transformer networks that attend to multiple feature modalities.

Result: Collected 3.5M trajectories using MotionGeneralizer; MpiNetsFusion plans several times faster than state-of-the-art planners on evaluated tasks.

Conclusion: The proposed PerFACT framework addresses dataset limitations and architectural shortcomings of current neural motion planners, enabling more generalizable and faster planning through LLM-powered data synthesis and fusion transformer networks.

Abstract: Deep learning methods have significantly enhanced motion planning for robotic manipulators by leveraging prior experiences within planning datasets. However, state-of-the-art neural motion planners are primarily trained on small datasets collected in manually generated workspaces, limiting their generalizability to out-of-distribution scenarios. Additionally, these planners often rely on monolithic network architectures that struggle to encode critical planning information. To address these challenges, we introduce Motion Policy with Dataset Synthesis powered by large language models (LLMs) and Fusion Action-Chunking Transformers (PerFACT), which incorporates two key components. Firstly, a novel LLM-powered workspace generation method, MotionGeneralizer, enables large-scale planning data collection by producing a diverse set of semantically feasible workspaces. Secondly, we introduce Fusion Motion Policy Networks (MpiNetsFusion), a generalist neural motion planner that uses a fusion action-chunking transformer to better encode planning signals and attend to multiple feature modalities. Leveraging MotionGeneralizer, we collect 3.5M trajectories to train and evaluate MpiNetsFusion against state-of-the-art planners, which shows that the proposed MpiNetsFusion can plan several times faster on the evaluated tasks.

</details>


### [124] [MSG-Loc: Multi-Label Likelihood-based Semantic Graph Matching for Object-Level Global Localization](https://arxiv.org/abs/2512.03522)
*Gihyeon Lee,Jungwoo Lee,Juwon Kim,Young-Sik Shin,Younggun Cho*

Main category: cs.RO

TL;DR: Multi-label likelihood-based semantic graph matching for object-level global localization that handles semantic ambiguity by using multi-label graph representations and context-aware likelihood propagation.


<details>
  <summary>Details</summary>
Motivation: Robots need to localize in environments with unknown object classes and semantic ambiguity. High semantic ambiguity causes object misclassification and incorrect associations, leading to significant pose estimation errors in global localization using semantic objects.

Method: Proposes a multi-label likelihood-based semantic graph matching framework using multi-label graph representations (instead of single-label) to capture inherent semantic context. Uses context-aware likelihood propagation that combines each node's likelihood with the maximum likelihood of its neighbors to enhance semantic correspondence across graphs.

Result: Evaluated data association and pose estimation performance under both closed-set and open-set detection configurations. Demonstrated scalability to large-vocabulary object categories in real-world indoor scenes and synthetic environments.

Conclusion: The proposed multi-label graph matching framework effectively handles semantic ambiguity in object-level global localization by leveraging semantic context through multi-label representations and likelihood propagation, showing robust performance across different detection scenarios and environments.

Abstract: Robots are often required to localize in environments with unknown object classes and semantic ambiguity. However, when performing global localization using semantic objects, high semantic ambiguity intensifies object misclassification and increases the likelihood of incorrect associations, which in turn can cause significant errors in the estimated pose. Thus, in this letter, we propose a multi-label likelihood-based semantic graph matching framework for object-level global localization. The key idea is to exploit multi-label graph representations, rather than single-label alternatives, to capture and leverage the inherent semantic context of object observations. Based on these representations, our approach enhances semantic correspondence across graphs by combining the likelihood of each node with the maximum likelihood of its neighbors via context-aware likelihood propagation. For rigorous validation, data association and pose estimation performance are evaluated under both closed-set and open-set detection configurations. In addition, we demonstrate the scalability of our approach to large-vocabulary object categories in both real-world indoor scenes and synthetic environments.

</details>


### [125] [AdaPower: Specializing World Foundation Models for Predictive Manipulation](https://arxiv.org/abs/2512.03538)
*Yuhang Huang,Shilong Zou,Jiazhao Zhang,Xinwang Liu,Ruizhen Hu,Kai Xu*

Main category: cs.RO

TL;DR: AdaPower adapts World Foundation Models for precise robotic control via lightweight temporal-spatial adaptation and memory persistence, boosting pre-trained VLA performance by 41% without retraining.


<details>
  <summary>Details</summary>
Motivation: World Foundation Models have strong visual simulation capabilities but lack precision for robotic control. Existing approaches using WFMs as synthetic data generators are computationally expensive and don't fully leverage pre-trained VLA policies.

Method: AdaPower framework with two key components: Temporal-Spatial Test-Time Training (TS-TTT) for inference-time adaptation and Memory Persistence (MP) for long-horizon consistency. Integrated within Model Predictive Control to empower pre-trained VLAs.

Result: Achieves over 41% improvement in task success rates on LIBERO benchmarks without policy retraining, while maintaining computational efficiency and preserving generalist capabilities.

Conclusion: AdaPower successfully bridges the gap between generative realism and control-oriented precision by transforming general-purpose WFMs into specialist world models through lightweight adaptation, enabling effective robotic control without expensive retraining.

Abstract: World Foundation Models (WFMs) offer remarkable visual dynamics simulation capabilities, yet their application to precise robotic control remains limited by the gap between generative realism and control-oriented precision. While existing approaches use WFMs as synthetic data generators, they suffer from high computational costs and underutilization of pre-trained VLA policies. We introduce \textbf{AdaPower} (\textbf{Ada}pt and Em\textbf{power}), a lightweight adaptation framework that transforms general-purpose WFMs into specialist world models through two novel components: Temporal-Spatial Test-Time Training (TS-TTT) for inference-time adaptation and Memory Persistence (MP) for long-horizon consistency. Integrated within a Model Predictive Control framework, our adapted world model empowers pre-trained VLAs, achieving over 41\% improvement in task success rates on LIBERO benchmarks without policy retraining, while preserving computational efficiency and generalist capabilities.

</details>


### [126] [A Learning-based Control Methodology for Transitioning VTOL UAVs](https://arxiv.org/abs/2512.03548)
*Zexin Lin,Yebin Zhong,Hanwen Wan,Jiu Cheng,Zhenglong Sun,Xiaoqiang Ji*

Main category: cs.RO

TL;DR: Proposes RL-based coupled transition control for VTOL UAVs, treating cruise as special hover case, reducing vibrations vs decoupled methods.


<details>
  <summary>Details</summary>
Motivation: VTOL UAV transition control is challenging due to tilting rotor mechanisms that shift center of gravity and thrust direction. Current decoupled control methods cause significant vibration and lack adaptability.

Method: Novel coupled transition control using reinforcement learning (RL) driven controller, with ST3M method treating cruise mode as special case of hover rather than conventional phase-transition approach.

Result: Validated in simulation and real-world, showing efficient controller development/migration, accurate position/attitude control, outstanding trajectory tracking, and reduced vibrations during transition.

Conclusion: RL-based coupled control effectively addresses VTOL transition challenges, offering superior performance over traditional decoupled methods with reduced vibration and better adaptability.

Abstract: Transition control poses a critical challenge in Vertical Take-Off and Landing Unmanned Aerial Vehicle (VTOL UAV) development due to the tilting rotor mechanism, which shifts the center of gravity and thrust direction during transitions. Current control methods' decoupled control of altitude and position leads to significant vibration, and limits interaction consideration and adaptability. In this study, we propose a novel coupled transition control methodology based on reinforcement learning (RL) driven controller. Besides, contrasting to the conventional phase-transition approach, the ST3M method demonstrates a new perspective by treating cruise mode as a special case of hover. We validate the feasibility of applying our method in simulation and real-world environments, demonstrating efficient controller development and migration while accurately controlling UAV position and attitude, exhibiting outstanding trajectory tracking and reduced vibrations during the transition process.

</details>


### [127] [RoboScape-R: Unified Reward-Observation World Models for Generalizable Robotics Training via RL](https://arxiv.org/abs/2512.03556)
*Yinzhou Tang,Yu Shang,Yinuo Chen,Bingwen Wei,Xin Zhang,Shu'ang Yu,Liangzhi Shi,Chao Yu,Chen Gao,Wei Wu,Yong Li*

Main category: cs.RO

TL;DR: RoboScape-R uses world models as universal environment proxies with endogenous rewards to improve embodied policy generalization, achieving 37.5% better performance in out-of-domain scenarios.


<details>
  <summary>Details</summary>
Motivation: Both Imitation Learning and Reinforcement Learning struggle with generalization in embodied AI. IL overfits to expert trajectories, while RL lacks unified reward signals for multi-scene generalization. World models could serve as universal environment proxies but currently rely on task-specific rewards.

Method: Proposes RoboScape-R framework that uses world models as general-purpose environment proxies within RL. Introduces novel world model-based general reward mechanism generating "endogenous" rewards from the model's intrinsic understanding of real-world state transition dynamics.

Result: Extensive experiments show RoboScape-R effectively addresses traditional RL limitations, providing efficient general training environment that substantially enhances embodied policy generalization. Achieves average 37.5% performance improvement over baselines in out-of-domain scenarios.

Conclusion: World models can serve as effective online training strategies for embodied AI. The endogenous reward mechanism derived from world model understanding enables superior generalization across diverse scenarios compared to traditional IL and RL approaches.

Abstract: Achieving generalizable embodied policies remains a key challenge. Traditional policy learning paradigms, including both Imitation Learning (IL) and Reinforcement Learning (RL), struggle to cultivate generalizability across diverse scenarios. While IL policies often overfit to specific expert trajectories, RL suffers from the inherent lack of a unified and general reward signal necessary for effective multi-scene generalization. We posit that the world model is uniquely capable of serving as a universal environment proxy to address this limitation. However, current world models primarily focus on their ability to predict observations and still rely on task-specific, handcrafted reward functions, thereby failing to provide a truly general training environment. Toward this problem, we propose RoboScape-R, a framework leveraging the world model to serve as a versatile, general-purpose proxy for the embodied environment within the RL paradigm. We introduce a novel world model-based general reward mechanism that generates ''endogenous'' rewards derived from the model's intrinsic understanding of real-world state transition dynamics. Extensive experiments demonstrate that RoboScape-R effectively addresses the limitations of traditional RL methods by providing an efficient and general training environment that substantially enhances the generalization capability of embodied policies. Our approach offers critical insights into utilizing the world model as an online training strategy and achieves an average 37.5% performance improvement over baselines under out-of-domain scenarios.

</details>


### [128] [Multimodal Control of Manipulators: Coupling Kinematics and Vision for Self-Driving Laboratory Operations](https://arxiv.org/abs/2512.03630)
*Shifa Sulaiman,Amarnath H,Simon Bogh,Naresh Marturi*

Main category: cs.RO

TL;DR: Three Jacobian-based motion planning schemes (Jacobian Transpose, Pseudo Inverse, Damped Least Square) implemented for redundant manipulator with gripper using RRT* trajectory planning and screw theory kinematics.


<details>
  <summary>Details</summary>
Motivation: To develop and compare efficient motion planning schemes for redundant manipulators with coupled finger grippers, specifically evaluating different Jacobian-based inverse kinematic solvers for task execution.

Method: Implemented 3 motion planning schemes using RRT* algorithm for trajectory planning and screw theory for forward kinematics. Inverse solutions computed using Jacobian Transpose (JT), Pseudo Inverse (PI), and Damped Least Square (DLS) methods. Analyzed space Jacobian, manipulability, trajectory smoothness, RMSE error, velocity continuity, acceleration, jerk, and snap values.

Result: Comparative analysis of three Jacobian methods through simulation studies, evaluating advantages/disadvantages of each scheme to determine suitable inverse solution technique for specific tasks.

Conclusion: The study provides insights into selecting appropriate Jacobian-based motion planning schemes for redundant manipulators with grippers based on task requirements, with recommendations drawn from comprehensive performance metrics analysis.

Abstract: Motion planning schemes are used for planning motions of a manipulator from an initial pose to a final pose during a task execution. A motion planning scheme generally comprises of a trajectory planning method and an inverse kinematic solver to determine trajectories and joints solutions respectively. In this paper, 3 motion planning schemes developed based on Jacobian methods are implemented to traverse a redundant manipulator with a coupled finger gripper through given trajectories. RRT* algorithm is used for planning trajectories and screw theory based forward kinematic equations are solved for determining joint solutions of the manipulator and gripper. Inverse solutions are computed separately using 3 Jacobian based methods such as Jacobian Transpose (JT), Pseudo Inverse (PI), and Damped Least Square (DLS) methods. Space Jacobian and manipulability measurements of the manipulator and gripper are obtained using screw theory formulations. Smoothness and RMSE error of generated trajectories and velocity continuity, acceleration profile, jerk, and snap values of joint motions are analysed for determining an efficient motion planning method for a given task. Advantages and disadvantages of the proposed motion planning schemes mentioned above are analysed using simulation studies to determine a suitable inverse solution technique for the tasks.

</details>


### [129] [Context-Triggered Contingency Games for Strategic Multi-Agent Interaction](https://arxiv.org/abs/2512.03639)
*Kilian Schweppe,Anne-Kathrin Schmuck*

Main category: cs.RO

TL;DR: Novel two-layered game-theoretic framework combining strategic temporal logic games with real-time contingency games for reliable, adaptive multi-agent interaction.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of reliable and efficient interaction in autonomous multi-agent systems where agents must balance long-term strategic objectives with short-term dynamic adaptation in uncertain, interactive environments.

Method: Context-triggered contingency games: two-layered architecture integrating strategic games from temporal logic specifications with dynamic contingency games solved in real time using strategy templates and a novel factor-graph-based solver for scalable model predictive control.

Result: Validated through simulations and hardware experiments in autonomous driving and robotic navigation, demonstrating efficient, reliable, and adaptive multi-agent interaction with guaranteed safety and progress.

Conclusion: The proposed framework successfully enables autonomous agents to balance long-term strategic planning with real-time adaptation, ensuring both safety and progress in uncertain interactive environments.

Abstract: We address the challenge of reliable and efficient interaction in autonomous multi-agent systems, where agents must balance long-term strategic objectives with short-term dynamic adaptation. We propose context-triggered contingency games, a novel integration of strategic games derived from temporal logic specifications with dynamic contingency games solved in real time. Our two-layered architecture leverages strategy templates to guarantee satisfaction of high-level objectives, while a new factor-graph-based solver enables scalable, real-time model predictive control of dynamic interactions. The resulting framework ensures both safety and progress in uncertain, interactive environments. We validate our approach through simulations and hardware experiments in autonomous driving and robotic navigation, demonstrating efficient, reliable, and adaptive multi-agent interaction.

</details>


### [130] [A Novel Approach to Tomato Harvesting Using a Hybrid Gripper with Semantic Segmentation and Keypoint Detection](https://arxiv.org/abs/2512.03684)
*Shahid Ansari,Mahendra Kumar Gohil,Yusuke Maeda,Bishakh Bhattacharya*

Main category: cs.RO

TL;DR: Autonomous tomato harvesting system with hybrid soft-rigid gripper, vision-based perception, and force-controlled grasping achieves 80% success rate with gentle handling.


<details>
  <summary>Details</summary>
Motivation: To develop a reliable autonomous harvesting system that can gently handle delicate tomatoes in cluttered environments while preventing bruising and damage during picking cycles.

Method: Combines hybrid gripper (soft auxetic fingers + rigid exoskeleton + latex basket) with Scotch-yoke actuation, separator leaves, and micro-servo cutter. Uses RGB-D camera with Detectron2 for semantic segmentation and keypoint localization. Implements analytical model for grasp force prediction, PID force control with FSR feedback, and PSO-based trajectory planning for 5-DOF manipulator.

Result: Average picking cycle time of 24.34 seconds with ~80% overall success rate, maintaining low grasp forces (0.20-0.50 N) to prevent bruising while handling occlusion and variable illumination.

Conclusion: The hybrid gripper design combined with integrated vision-control pipeline validates reliable autonomous tomato harvesting in cluttered environments with gentle fruit handling.

Abstract: This paper presents an autonomous tomato-harvesting system built around a hybrid robotic gripper that combines six soft auxetic fingers with a rigid exoskeleton and a latex basket to achieve gentle, cage-like grasping. The gripper is driven by a servo-actuated Scotch--yoke mechanism, and includes separator leaves that form a conical frustum for fruit isolation, with an integrated micro-servo cutter for pedicel cutting. For perception, an RGB--D camera and a Detectron2-based pipeline perform semantic segmentation of ripe/unripe tomatoes and keypoint localization of the pedicel and fruit center under occlusion and variable illumination. An analytical model derived using the principle of virtual work relates servo torque to grasp force, enabling design-level reasoning about actuation requirements. During execution, closed-loop grasp-force regulation is achieved using a proportional--integral--derivative controller with feedback from force-sensitive resistors mounted on selected fingers to prevent slip and bruising. Motion execution is supported by Particle Swarm Optimization (PSO)--based trajectory planning for a 5-DOF manipulator. Experiments demonstrate complete picking cycles (approach, separation, cutting, grasping, transport, release) with an average cycle time of 24.34~s and an overall success rate of approximately 80\%, while maintaining low grasp forces (0.20--0.50~N). These results validate the proposed hybrid gripper and integrated vision--control pipeline for reliable harvesting in cluttered environments.

</details>


### [131] [ContactRL: Safe Reinforcement Learning based Motion Planning for Contact based Human Robot Collaboration](https://arxiv.org/abs/2512.03707)
*Sundas Rafat Mulkana,Ronyu Yu,Tanaya Guha,Emma Li*

Main category: cs.RO

TL;DR: ContactRL uses reinforcement learning with force feedback rewards to enable safe physical human-robot collaboration, achieving low safety violations and high task success, with real-world validation on handover tasks.


<details>
  <summary>Details</summary>
Motivation: In collaborative human-robot tasks, safety requires not just avoiding collisions but also ensuring safe intentional physical contact, which is crucial for advancing deployment of collaborative robots in contact-rich tasks.

Method: ContactRL framework incorporates contact safety directly into RL reward function through force feedback, enabling learning of adaptive motion profiles that minimize human-robot contact forces while maintaining task efficiency. The learned policy is augmented with a kinetic energy based Control Barrier Function (eCBF) shield for deployment safety guarantees.

Result: In simulation: 0.2% safety violation rate with 87.7% task success rate, outperforming state-of-the-art constrained RL baselines. Real-world experiments on UR3e platform performing 360 small object handovers from human hand confirmed safe contact with measured normal forces consistently below 10N.

Conclusion: ContactRL enables safe and efficient physical collaboration, advancing deployment of collaborative robots in contact-rich tasks by directly incorporating contact safety into learning while maintaining task performance.

Abstract: In collaborative human-robot tasks, safety requires not only avoiding collisions but also ensuring safe, intentional physical contact. We present ContactRL, a reinforcement learning (RL) based framework that directly incorporates contact safety into the reward function through force feedback. This enables a robot to learn adaptive motion profiles that minimize human-robot contact forces while maintaining task efficiency. In simulation, ContactRL achieves a low safety violation rate of 0.2\% with a high task success rate of 87.7\%, outperforming state-of-the-art constrained RL baselines. In order to guarantee deployment safety, we augment the learned policy with a kinetic energy based Control Barrier Function (eCBF) shield. Real-world experiments on an UR3e robotic platform performing small object handovers from a human hand across 360 trials confirm safe contact, with measured normal forces consistently below 10N. These results demonstrate that ContactRL enables safe and efficient physical collaboration, thereby advancing the deployment of collaborative robots in contact-rich tasks.

</details>


### [132] [Autonomous Planning In-space Assembly Reinforcement-learning free-flYer (APIARY) International Space Station Astrobee Testing](https://arxiv.org/abs/2512.03729)
*Samantha Chapin,Kenneth Stewart,Roxana Leontie,Carl Glen Henshaw*

Main category: cs.RO

TL;DR: First-ever RL control of free-flying robot in space using Astrobee on ISS, demonstrating rapid deployment of autonomous behaviors for space operations.


<details>
  <summary>Details</summary>
Motivation: To pioneer reinforcement learning for controlling free-flying robots in zero-gravity space environments, enabling improved robotic autonomy for space exploration, logistics, and real-time mission needs.

Method: Trained robust 6-DOF control policy using actor-critic Proximal Policy Optimization (PPO) network in NVIDIA Isaac Lab simulation, with randomization over goal poses and mass distributions for robustness. Conducted simulation testing, ground testing, and flight validation on ISS using NASA Astrobee robot.

Result: Successfully conducted first-ever RL control of free-flyer in space on May 27, 2025, validating that RL enables rapid development and deployment (minutes to hours) of tailored behaviors for space operations.

Conclusion: On-orbit demonstration validates transformative potential of RL for improving robotic autonomy in space, enabling rapid deployment of customized behaviors for exploration, logistics, and real-time mission requirements.

Abstract: The US Naval Research Laboratory's (NRL's) Autonomous Planning In-space Assembly Reinforcement-learning free-flYer (APIARY) experiment pioneers the use of reinforcement learning (RL) for control of free-flying robots in the zero-gravity (zero-G) environment of space. On Tuesday, May 27th 2025 the APIARY team conducted the first ever, to our knowledge, RL control of a free-flyer in space using the NASA Astrobee robot on-board the International Space Station (ISS). A robust 6-degrees of freedom (DOF) control policy was trained using an actor-critic Proximal Policy Optimization (PPO) network within the NVIDIA Isaac Lab simulation environment, randomizing over goal poses and mass distributions to enhance robustness. This paper details the simulation testing, ground testing, and flight validation of this experiment. This on-orbit demonstration validates the transformative potential of RL for improving robotic autonomy, enabling rapid development and deployment (in minutes to hours) of tailored behaviors for space exploration, logistics, and real-time mission needs.

</details>


### [133] [Crossing the Sim2Real Gap Between Simulation and Ground Testing to Space Deployment of Autonomous Free-flyer Control](https://arxiv.org/abs/2512.03736)
*Kenneth Stewart,Samantha Chapin,Roxana Leontie,Carl Glen Henshaw*

Main category: cs.RO

TL;DR: First on-orbit demonstration of RL-based autonomous control of NASA's Astrobee robot on ISS using NVIDIA Omniverse simulation and curriculum learning.


<details>
  <summary>Details</summary>
Motivation: RL offers transformative potential for robotic control in space, enabling autonomous navigation in microgravity and addressing the need for rapid on-orbit adaptation to dynamic mission requirements for future ISAM applications.

Method: Used NVIDIA's Omniverse physics simulator with curriculum learning to train a deep neural network that replaces Astrobee's standard attitude and translation control, creating a GPU-accelerated simulation-to-reality pipeline for efficient Monte Carlo RL training.

Result: Successfully validated the training pipeline that bridges the simulation-to-reality gap, demonstrating the feasibility of training RL policies terrestrially and transferring them to space-based applications with successful on-orbit deployment.

Conclusion: This demonstration paves the way for future In-Space Servicing, Assembly, and Manufacturing (ISAM) applications by showing that RL policies can be trained on Earth and effectively deployed in space, enabling rapid adaptation to dynamic mission requirements.

Abstract: Reinforcement learning (RL) offers transformative potential for robotic control in space. We present the first on-orbit demonstration of RL-based autonomous control of a free-flying robot, the NASA Astrobee, aboard the International Space Station (ISS). Using NVIDIA's Omniverse physics simulator and curriculum learning, we trained a deep neural network to replace Astrobee's standard attitude and translation control, enabling it to navigate in microgravity. Our results validate a novel training pipeline that bridges the simulation-to-reality (Sim2Real) gap, utilizing a GPU-accelerated, scientific-grade simulation environment for efficient Monte Carlo RL training. This successful deployment demonstrates the feasibility of training RL policies terrestrially and transferring them to space-based applications. This paves the way for future work in In-Space Servicing, Assembly, and Manufacturing (ISAM), enabling rapid on-orbit adaptation to dynamic mission requirements.

</details>


### [134] [Cross-embodied Co-design for Dexterous Hands](https://arxiv.org/abs/2512.03743)
*Kehlani Fay,Darin Anthony Djapri,Anya Zorin,James Clinton,Ali El Lahib,Hao Su,Michael T. Tolley,Sha Yi,Xiaolong Wang*

Main category: cs.RO

TL;DR: A co-design framework that simultaneously learns optimal hand morphology and control policies for dexterous manipulation tasks, enabling end-to-end design, training, fabrication, and deployment within 24 hours.


<details>
  <summary>Details</summary>
Motivation: Dexterous manipulation faces fundamental challenges in both control and design, with no consensus on what makes manipulators best for dexterous tasks. There's a need for a systematic approach to design and control robot manipulators optimized for dexterity.

Method: A co-design framework that: 1) supports expansive morphology search space (joint, finger, palm generation), 2) enables scalable evaluation via morphology-conditioned cross-embodied control, and 3) facilitates real-world fabrication with accessible components.

Result: The framework enables end-to-end pipeline to design, train, fabricate, and deploy new robotic hands in under 24 hours. Evaluated across multiple dexterous tasks including in-hand rotation with simulation and real deployment.

Conclusion: The proposed co-design framework successfully addresses the challenge of optimizing both hand morphology and control policies for dexterous manipulation, with practical real-world application and rapid deployment capabilities. The framework will be open-sourced.

Abstract: Dexterous manipulation is limited by both control and design, without consensus as to what makes manipulators best for performing dexterous tasks. This raises a fundamental challenge: how should we design and control robot manipulators that are optimized for dexterity? We present a co-design framework that learns task-specific hand morphology and complementary dexterous control policies. The framework supports 1) an expansive morphology search space including joint, finger, and palm generation, 2) scalable evaluation across the wide design space via morphology-conditioned cross-embodied control, and 3) real-world fabrication with accessible components. We evaluate the approach across multiple dexterous tasks, including in-hand rotation with simulation and real deployment. Our framework enables an end-to-end pipeline that can design, train, fabricate, and deploy a new robotic hand in under 24 hours. The full framework will be open-sourced and available on our website.

</details>


### [135] [Prediction-Driven Motion Planning: Route Integration Strategies in Attention-Based Prediction Models](https://arxiv.org/abs/2512.03756)
*Marlon Steiner,Royden Wagner,Ömer Sahin Tas,Christoph Stiller*

Main category: cs.RO

TL;DR: This paper extends attention-based motion prediction models with navigation information to bridge motion prediction and goal-based motion planning for automated vehicles.


<details>
  <summary>Details</summary>
Motivation: Combining motion prediction and motion planning enhances interactions between automated vehicles and traffic participants, but faces challenges in conditioning predictions on navigation goals and ensuring stable, kinematically feasible trajectories.

Method: The paper extends attention-based motion prediction models by integrating the ego vehicle's intended route and goal pose into the architecture. Several architectural navigation integration strategies are proposed and evaluated on the nuPlan dataset.

Result: Results demonstrate the potential of prediction-driven motion planning, showing how navigation information can enhance both prediction and planning tasks.

Conclusion: The proposed approach successfully bridges the gap between multi-agent motion prediction and goal-based motion planning, offering a promising framework for automated vehicle interactions.

Abstract: Combining motion prediction and motion planning offers a promising framework for enhancing interactions between automated vehicles and other traffic participants. However, this introduces challenges in conditioning predictions on navigation goals and ensuring stable, kinematically feasible trajectories. Addressing the former challenge, this paper investigates the extension of attention-based motion prediction models with navigation information. By integrating the ego vehicle's intended route and goal pose into the model architecture, we bridge the gap between multi-agent motion prediction and goal-based motion planning. We propose and evaluate several architectural navigation integration strategies to our model on the nuPlan dataset. Our results demonstrate the potential of prediction-driven motion planning, highlighting how navigation information can enhance both prediction and planning tasks. Our implementation is at: https://github.com/KIT-MRT/future-motion.

</details>


### [136] [Bayesian Optimization for Automatic Tuning of Torque-Level Nonlinear Model Predictive Control](https://arxiv.org/abs/2512.03772)
*Gabriele Fadini,Deepak Ingole,Tong Duy Son,Alisa Rupenyan*

Main category: cs.RO

TL;DR: Auto-tuning framework for torque-based NMPC using Bayesian Optimization with digital twin to optimize controller parameters for UR10e robot arm, achieving significant tracking improvements.


<details>
  <summary>Details</summary>
Motivation: Manual tuning of NMPC parameters (cost function weights and low-level controller gains) is time-consuming and suboptimal for precise real-time end-effector trajectory tracking on robotic manipulators.

Method: High-dimensional Bayesian Optimization (SAASBO) with digital twin simulation to efficiently explore parameter space and optimize MPC parameters, enabling safe transfer to hardware.

Result: Simulation: +41.9% tracking improvement, -2.5% solve time reduction vs manual tuning. Hardware: +25.8% improvement in real robot experiments.

Conclusion: Digital twin-enabled automated parameter optimization is crucial for robotic operations, providing significant performance gains over manual tuning while ensuring safe hardware transfer.

Abstract: This paper presents an auto-tuning framework for torque-based Nonlinear Model Predictive Control (nMPC), where the MPC serves as a real-time controller for optimal joint torque commands. The MPC parameters, including cost function weights and low-level controller gains, are optimized using high-dimensional Bayesian Optimization (BO) techniques, specifically Sparse Axis-Aligned Subspace (SAASBO) with a digital twin (DT) to achieve precise end-effector trajectory real-time tracking on an UR10e robot arm. The simulation model allows efficient exploration of the high-dimensional parameter space, and it ensures safe transfer to hardware. Our simulation results demonstrate significant improvements in tracking performance (+41.9%) and reduction in solve times (-2.5%) compared to manually-tuned parameters. Moreover, experimental validation on the real robot follows the trend (with a +25.8% improvement), emphasizing the importance of digital twin-enabled automated parameter optimization for robotic operations.

</details>


### [137] [Safety Reinforced Model Predictive Control (SRMPC): Improving MPC with Reinforcement Learning for Motion Planning in Autonomous Driving](https://arxiv.org/abs/2512.03774)
*Johannes Fischer,Marlon Steiner,Ömer Sahin Tas,Christoph Stiller*

Main category: cs.RO

TL;DR: Combines MPC with safe RL to find global optima beyond convex approximations, using constrained RL with learned Lagrangian multipliers for safety in autonomous driving.


<details>
  <summary>Details</summary>
Motivation: Traditional MPC uses convex approximations that confine solutions to subspaces, potentially missing global optima. Need to explore beyond local neighborhoods while maintaining safety in autonomous driving.

Method: Proposes safe reinforcement learning (SRL) within MPC framework to generate new reference trajectories. Uses constrained RL with handcrafted energy function-based safety index as constraint objective, and learns state-dependent Lagrangian multipliers concurrently with safe policy.

Result: Experimental evaluation in highway scenarios shows superiority over both standalone MPC and SRL approaches in terms of safety and performance measures.

Conclusion: Integration of safe RL with MPC enables exploration beyond local convex approximations while maintaining safety, leading to better global solutions for autonomous driving motion planning.

Abstract: Model predictive control (MPC) is widely used for motion planning, particularly in autonomous driving. Real-time capability of the planner requires utilizing convex approximation of optimal control problems (OCPs) for the planner. However, such approximations confine the solution to a subspace, which might not contain the global optimum. To address this, we propose using safe reinforcement learning (SRL) to obtain a new and safe reference trajectory within MPC. By employing a learning-based approach, the MPC can explore solutions beyond the close neighborhood of the previous one, potentially finding global optima. We incorporate constrained reinforcement learning (CRL) to ensure safety in automated driving, using a handcrafted energy function-based safety index as the constraint objective to model safe and unsafe regions. Our approach utilizes a state-dependent Lagrangian multiplier, learned concurrently with the safe policy, to solve the CRL problem. Through experimentation in a highway scenario, we demonstrate the superiority of our approach over both MPC and SRL in terms of safety and performance measures.

</details>


### [138] [MPCFormer: A physics-informed data-driven approach for explainable socially-aware autonomous driving](https://arxiv.org/abs/2512.03795)
*Jia Hu,Zhexi Lian,Xuerun Yan,Ruiang Bi,Dou Shen,Yu Ruan,Haoran Wang*

Main category: cs.RO

TL;DR: MPCFormer: An explainable, socially-aware autonomous driving approach combining physics-informed modeling with Transformer-based learning to understand and generate human-like multi-vehicle interaction behaviors.


<details>
  <summary>Details</summary>
Motivation: Current autonomous driving systems struggle with human-like behavior in dynamic, interactive traffic scenarios due to limited understanding of social interaction mechanisms between vehicles.

Method: Combines physics-informed discrete state-space modeling with Transformer-based encoder-decoder to learn social interaction dynamics from naturalistic driving data, integrated within an MPC framework for planning.

Result: Achieves state-of-the-art trajectory prediction (ADE of 0.86m over 5s), 94.67% planning success rate, 15.75% efficiency improvement, and reduces collision rate from 21.25% to 0.5% in intense interaction scenarios.

Conclusion: MPCFormer successfully models multi-vehicle social interaction dynamics, enabling human-like behavior generation while maintaining safety through MPC integration, outperforming both traditional and RL-based approaches.

Abstract: Autonomous Driving (AD) vehicles still struggle to exhibit human-like behavior in highly dynamic and interactive traffic scenarios. The key challenge lies in AD's limited ability to interact with surrounding vehicles, largely due to a lack of understanding the underlying mechanisms of social interaction. To address this issue, we introduce MPCFormer, an explainable socially-aware autonomous driving approach with physics-informed and data-driven coupled social interaction dynamics. In this model, the dynamics are formulated into a discrete space-state representation, which embeds physics priors to enhance modeling explainability. The dynamics coefficients are learned from naturalistic driving data via a Transformer-based encoder-decoder architecture. To the best of our knowledge, MPCFormer is the first approach to explicitly model the dynamics of multi-vehicle social interactions. The learned social interaction dynamics enable the planner to generate manifold, human-like behaviors when interacting with surrounding traffic. By leveraging the MPC framework, the approach mitigates the potential safety risks typically associated with purely learning-based methods. Open-looped evaluation on NGSIM dataset demonstrates that MPCFormer achieves superior social interaction awareness, yielding the lowest trajectory prediction errors compared with other state-of-the-art approach. The prediction achieves an ADE as low as 0.86 m over a long prediction horizon of 5 seconds. Close-looped experiments in highly intense interaction scenarios, where consecutive lane changes are required to exit an off-ramp, further validate the effectiveness of MPCFormer. Results show that MPCFormer achieves the highest planning success rate of 94.67%, improves driving efficiency by 15.75%, and reduces the collision rate from 21.25% to 0.5%, outperforming a frontier Reinforcement Learning (RL) based planner.

</details>


### [139] [IM HERE: Interaction Model for Human Effort Based Robot Engagement](https://arxiv.org/abs/2512.03828)
*Dominykas Strazdas,Magnus Jung,Jan Marquenie,Ingo Siegert,Ayoub Al-Hamadi*

Main category: cs.RO

TL;DR: IM HERE framework models engagement across human-human, human-robot, and robot-robot interactions using effort-based bilateral relationships, focusing on placement and four key states to enable autonomous social behavior.


<details>
  <summary>Details</summary>
Motivation: Existing engagement models are too vague or lack generalizability across different interaction contexts, creating a need for a more robust framework that can effectively model engagement dynamics.

Method: Introduces IM HERE framework using effort-based description of bilateral relationships between entities, simplifying relationship patterns to focus placement and four key states, integrating both subjective perceptions and objective states.

Result: Framework captures mutual relationships, group behaviors, and social norm conformity, translating them into specific directives for autonomous systems while precisely identifying and describing miscommunication.

Conclusion: Enables automation of social behavior analysis and modeling, allowing autonomous systems to behave according to social norms for full integration while pursuing their own social goals.

Abstract: The effectiveness of human-robot interaction often hinges on the ability to cultivate engagement - a dynamic process of cognitive involvement that supports meaningful exchanges. Many existing definitions and models of engagement are either too vague or lack the ability to generalize across different contexts. We introduce IM HERE, a novel framework that models engagement effectively in human-human, human-robot, and robot-robot interactions. By employing an effort-based description of bilateral relationships between entities, we provide an accurate breakdown of relationship patterns, simplifying them to focus placement and four key states. This framework captures mutual relationships, group behaviors, and actions conforming to social norms, translating them into specific directives for autonomous systems. By integrating both subjective perceptions and objective states, the model precisely identifies and describes miscommunication. The primary objective of this paper is to automate the analysis, modeling, and description of social behavior, and to determine how autonomous systems can behave in accordance with social norms for full social integration while simultaneously pursuing their own social goals.

</details>


### [140] [OmniDexVLG: Learning Dexterous Grasp Generation from Vision Language Model-Guided Grasp Semantics, Taxonomy and Functional Affordance](https://arxiv.org/abs/2512.03874)
*Lei Zhang,Diwen Zheng,Kaixin Bai,Zhenshan Bing,Zoltan-Csaba Marton,Zhaopeng Chen,Alois Christian Knoll,Jianwei Zhang*

Main category: cs.RO

TL;DR: OmniDexVLG is a multimodal framework for semantically controllable dexterous grasp generation that integrates grasp taxonomy, contact semantics, and functional affordance under joint language and visual guidance.


<details>
  <summary>Details</summary>
Motivation: Current dexterous grasp generation lacks unified modeling of multiple semantic dimensions (grasp taxonomy, contact semantics, functional affordance), making semantically controllable grasp synthesis challenging.

Method: Three components: 1) OmniDexDataGen pipeline for semantic-rich dataset generation with grasp taxonomy guidance, functional affordance contact sampling, and physics-based optimization; 2) OmniDexReasoner multimodal semantic reasoning module using multi-agent collaboration and retrieval-augmented generation; 3) Unified Vision-Language Grasping model incorporating all semantic dimensions for fine-grained control.

Result: Substantially outperforms state-of-the-art approaches in grasp diversity, contact semantic diversity, functional affordance diversity, and semantic consistency in both simulation and real-world object grasping experiments.

Conclusion: OmniDexVLG successfully addresses the challenge of semantically controllable dexterous grasp synthesis by providing a unified multimodal framework that enables fine-grained control over grasp generation from natural language instructions.

Abstract: Dexterous grasp generation aims to produce grasp poses that align with task requirements and human interpretable grasp semantics. However, achieving semantically controllable dexterous grasp synthesis remains highly challenging due to the lack of unified modeling of multiple semantic dimensions, including grasp taxonomy, contact semantics, and functional affordance. To address these limitations, we present OmniDexVLG, a multimodal, semantics aware grasp generation framework capable of producing structurally diverse and semantically coherent dexterous grasps under joint language and visual guidance. Our approach begins with OmniDexDataGen, a semantic rich dexterous grasp dataset generation pipeline that integrates grasp taxonomy guided configuration sampling, functional affordance contact point sampling, taxonomy aware differential force closure grasp sampling, and physics based optimization and validation, enabling systematic coverage of diverse grasp types. We further introduce OmniDexReasoner, a multimodal grasp type semantic reasoning module that leverages multi agent collaboration, retrieval augmented generation, and chain of thought reasoning to infer grasp related semantics and generate high quality annotations that align language instructions with task specific grasp intent. Building upon these components, we develop a unified Vision Language Grasping generation model that explicitly incorporates grasp taxonomy, contact structure, and functional affordance semantics, enabling fine grained control over grasp synthesis from natural language instructions. Extensive experiments in simulation and real world object grasping and ablation studies demonstrate that our method substantially outperforms state of the art approaches in terms of grasp diversity, contact semantic diversity, functional affordance diversity, and semantic consistency.

</details>


### [141] [A Modular Architecture Design for Autonomous Driving Racing in Controlled Environments](https://arxiv.org/abs/2512.03886)
*Brais Fontan-Costas,M. Diaz-Cacho,Ruben Fernandez-Boullon,Manuel Alonso-Carracedo,Javier Perez-Robles*

Main category: cs.RO

TL;DR: Autonomous vehicle system architecture for closed-circuit environments using computer vision, localization, planning, and control in a modular pipeline design.


<details>
  <summary>Details</summary>
Motivation: To create an autonomous system capable of performing precision tasks in controlled closed-circuit environments, requiring reliable real-time navigation and coordination of multiple subsystems.

Method: Modular AS architecture with independent subsystems (computer vision for perception, positioning/mapping for localization, path planning for trajectory generation, and control for actuation) connected through a cohesive pipeline architecture.

Result: A functional autonomous vehicle system that combines state-of-the-art technologies for real-time autonomous navigation in controlled environments.

Conclusion: The proposed modular AS architecture successfully enables autonomous vehicle operation in closed circuits through coordinated subsystem integration and pipeline data flow.

Abstract: This paper presents an Autonomous System (AS) architecture for vehicles in a closed circuit. The AS performs precision tasks including computer vision for environment perception, positioning and mapping for accurate localization, path planning for optimal trajectory generation, and control for precise vehicle actuation. Each subsystem operates independently while connecting data through a cohesive pipeline architecture. The system implements a modular design that combines state-of-the-art technologies for real-time autonomous navigation in controlled environments.

</details>


### [142] [Digital Twin-based Control Co-Design of Full Vehicle Active Suspensions via Deep Reinforcement Learning](https://arxiv.org/abs/2512.03891)
*Ying-Kuan Tsai,Yi-Ping Chen,Vispi Karkaria,Wei Chen*

Main category: cs.RO

TL;DR: DT-based control co-design framework using DRL and multi-generation concepts to jointly optimize active suspension components and control policies, achieving 43-52% control effort reduction while maintaining comfort.


<details>
  <summary>Details</summary>
Motivation: Active suspension systems have limitations with fixed hardware designs and non-adaptive control strategies that can't handle uncertain, dynamic conditions. Digital twins and DRL offer opportunities for real-time optimization, but integrating them into a unified framework remains challenging.

Method: DT-based control co-design framework integrating automatic differentiation into DRL to jointly optimize physical suspension components and control policies. Uses multi-generation design concepts, model updating with quantile learning for uncertainty, and addresses partial observability by learning control directly from sensor data.

Result: Optimized systems achieve smoother trajectories and reduce control efforts by approximately 43% for mild driving and 52% for aggressive driving, while maintaining ride comfort and stability. Demonstrates personalized optimization for distinct driver types.

Conclusion: The framework successfully integrates DT, DRL, and uncertainty-aware model updating for active suspension optimization, enabling self-improving systems through multi-generation design and personalized adaptation to different driving behaviors.

Abstract: Active suspension systems are critical for enhancing vehicle comfort, safety, and stability, yet their performance is often limited by fixed hardware designs and control strategies that cannot adapt to uncertain and dynamic operating conditions. Recent advances in digital twins (DTs) and deep reinforcement learning (DRL) offer new opportunities for real-time, data-driven optimization across a vehicle's lifecycle. However, integrating these technologies into a unified framework remains an open challenge. This work presents a DT-based control co-design (CCD) framework for full-vehicle active suspensions using multi-generation design concepts. By integrating automatic differentiation into DRL, we jointly optimize physical suspension components and control policies under varying driver behaviors and environmental uncertainties. DRL also addresses the challenge of partial observability, where only limited states can be sensed and fed back to the controller, by learning optimal control actions directly from available sensor information. The framework incorporates model updating with quantile learning to capture data uncertainty, enabling real-time decision-making and adaptive learning from digital-physical interactions. The approach demonstrates personalized optimization of suspension systems under two distinct driving settings (mild and aggressive). Results show that the optimized systems achieve smoother trajectories and reduce control efforts by approximately 43% and 52% for mild and aggressive, respectively, while maintaining ride comfort and stability. Contributions include: developing a DT-enabled CCD framework integrating DRL and uncertainty-aware model updating for full-vehicle active suspensions, introducing a multi-generation design strategy for self-improving systems, and demonstrating personalized optimization of active suspension systems for distinct driver types.

</details>


### [143] [Autonomous Reinforcement Learning Robot Control with Intel's Loihi 2 Neuromorphic Hardware](https://arxiv.org/abs/2512.03911)
*Kenneth Stewart,Roxana Leontie,Samantha Chapin,Joe Hays,Sumit Bam Shrestha,Carl Glen Henshaw*

Main category: cs.RO

TL;DR: ANN-trained RL policies can be converted to spiking Sigma-Delta Neural Networks (SDNNs) for deployment on neuromorphic hardware like Intel's Loihi 2, enabling energy-efficient robotic control with demonstrated application to Astrobee free-flying robot.


<details>
  <summary>Details</summary>
Motivation: To enable low-latency, energy-efficient deployment of RL-trained neural networks on neuromorphic hardware for robotic control applications, particularly in space and terrestrial robotics where power efficiency is critical.

Method: Developed an end-to-end pipeline to convert ANN policies trained with ReLUs into spiking SDNNs compatible with Intel's Loihi 2 architecture, then deployed and evaluated using Astrobee free-flying robot control in NVIDIA Omniverse Isaac Lab simulation.

Result: Successfully demonstrated conversion and deployment of RL-trained ANN policies to SDNNs on Loihi 2, showing feasibility of neuromorphic platforms for robotic control with comparisons between GPU and Loihi 2 execution performance.

Conclusion: Established a pathway for energy-efficient, real-time neuromorphic computation in robotics, highlighting the potential of neuromorphic hardware for future space and terrestrial robotic applications requiring low-power, low-latency control.

Abstract: We present an end-to-end pipeline for deploying reinforcement learning (RL) trained Artificial Neural Networks (ANNs) on neuromorphic hardware by converting them into spiking Sigma-Delta Neural Networks (SDNNs). We demonstrate that an ANN policy trained entirely in simulation can be transformed into an SDNN compatible with Intel's Loihi 2 architecture, enabling low-latency and energy-efficient inference. As a test case, we use an RL policy for controlling the Astrobee free-flying robot, similar to a previously hardware in space-validated controller. The policy, trained with Rectified Linear Units (ReLUs), is converted to an SDNN and deployed on Intel's Loihi 2, then evaluated in NVIDIA's Omniverse Isaac Lab simulation environment for closed-loop control of Astrobee's motion. We compare execution performance between GPU and Loihi 2. The results highlight the feasibility of using neuromorphic platforms for robotic control and establish a pathway toward energy-efficient, real-time neuromorphic computation in future space and terrestrial robotics applications.

</details>


### [144] [Hierarchical Vision Language Action Model Using Success and Failure Demonstrations](https://arxiv.org/abs/2512.03913)
*Jeongeun Park,Jihwan Yoon,Byungwoo Jeon,Juhan Park,Jinwoo Shin,Namhoon Cho,Kyungjae Lee,Sangdoo Yun,Sungjoon Choi*

Main category: cs.RO

TL;DR: VINE is a hierarchical VLA model that leverages both successful and failed demonstrations to improve robustness through feasibility-guided planning with failure-aware reasoning.


<details>
  <summary>Details</summary>
Motivation: Current VLA models discard failed demonstrations during training, losing valuable information about policy fragility that could improve robustness.

Method: Hierarchical architecture separating high-level reasoning (System 2) from low-level control (System 1). System 2 performs feasibility-guided tree search over 2D scene graphs, predicts success probabilities from both successes and failures, and prunes brittle branches before execution.

Result: Consistently improves success rates and robustness across challenging manipulation tasks by integrating negative experience directly into decision-making.

Conclusion: Failure data is essential for converting broad VLA competence into robust execution, and VINE demonstrates how to effectively leverage mixed-quality datasets for failure-aware reasoning.

Abstract: Prior Vision-Language-Action (VLA) models are typically trained on teleoperated successful demonstrations, while discarding numerous failed attempts that occur naturally during data collection. However, these failures encode where and how policies can be fragile, information that can be exploited to improve robustness. We address this problem by leveraging mixed-quality datasets to learn failure-aware reasoning at planning time. We introduce VINE, a hierarchical vision-language-action model that separates high-level reasoning (System 2) from low-level control (System 1) under a hierarchical reinforcement learning formalism, making failures usable as a structured learning signal rather than noisy supervision. System 2 performs feasibility-guided tree search over a 2D scene-graph abstraction: it proposes subgoal transitions, predicts success probabilities from both successes and failures, and prunes brittle branches before execution, effectively casting plan evaluation as feasibility scoring. The selected subgoal sequence is then passed to System 1, which executes low-level actions without modifying the agent's core skills. Trained entirely from offline teleoperation data, VINE integrates negative experience directly into the decision loop. Across challenging manipulation tasks, this approach consistently improves success rates and robustness, demonstrating that failure data is an essential resource for converting the broad competence of VLAs into robust execution.

</details>


### [145] [Driving is a Game: Combining Planning and Prediction with Bayesian Iterative Best Response](https://arxiv.org/abs/2512.03936)
*Aron Distelzweig,Yiwei Wang,Faris Janjoš,Marcel Hallgarten,Mihai Dobre,Alexander Langmann,Joschka Boedecker,Johannes Betz*

Main category: cs.RO

TL;DR: BIBeR integrates motion prediction with game-theoretic planning using iterative best response loops and Bayesian confidence estimation, achieving 11% improvement on interactive lane-change scenarios.


<details>
  <summary>Details</summary>
Motivation: Current autonomous driving planning systems work well in routine scenarios but struggle in dense urban traffic where lane changes and merges require anticipating and influencing other agents. Existing approaches either use rudimentary prediction integration or end-to-end models that avoid joint prediction-planning modeling under uncertainty.

Method: BIBeR (Bayesian Iterative Best Response) unifies motion prediction and game-theoretic planning by integrating state-of-the-art predictors into an Iterative Best Response loop. The framework repeatedly refines ego and surrounding agent strategies to approximate Nash equilibrium, enabling bidirectional adaptation. It includes Bayesian confidence estimation to quantify prediction reliability and modulate update strength.

Result: BIBeR achieves 11% improvement over state-of-the-art planners on highly interactive interPlan lane-change scenarios and also outperforms existing approaches on standard nuPlan benchmarks.

Conclusion: BIBeR successfully combines the transparency of structured planning with the flexibility of learned models, providing a principled game-theoretic approach that enables bidirectional adaptation between ego vehicle and surrounding agents while maintaining compatibility with modern predictors and planners.

Abstract: Autonomous driving planning systems perform nearly perfectly in routine scenarios using lightweight, rule-based methods but still struggle in dense urban traffic, where lane changes and merges require anticipating and influencing other agents. Modern motion predictors offer highly accurate forecasts, yet their integration into planning is mostly rudimental: discarding unsafe plans. Similarly, end-to-end models offer a one-way integration that avoids the challenges of joint prediction and planning modeling under uncertainty. In contrast, game-theoretic formulations offer a principled alternative but have seen limited adoption in autonomous driving. We present Bayesian Iterative Best Response (BIBeR), a framework that unifies motion prediction and game-theoretic planning into a single interaction-aware process. BIBeR is the first to integrate a state-of-the-art predictor into an Iterative Best Response (IBR) loop, repeatedly refining the strategies of the ego vehicle and surrounding agents. This repeated best-response process approximates a Nash equilibrium, enabling bidirectional adaptation where the ego both reacts to and shapes the behavior of others. In addition, our proposed Bayesian confidence estimation quantifies prediction reliability and modulates update strength, more conservative under low confidence and more decisive under high confidence. BIBeR is compatible with modern predictors and planners, combining the transparency of structured planning with the flexibility of learned models. Experiments show that BIBeR achieves an 11% improvement over state-of-the-art planners on highly interactive interPlan lane-change scenarios, while also outperforming existing approaches on standard nuPlan benchmarks.

</details>


### [146] [MDE-AgriVLN: Agricultural Vision-and-Language Navigation with Monocular Depth Estimation](https://arxiv.org/abs/2512.03958)
*Xiaobei Zhao,Xingqi Lyu,Xiang Li*

Main category: cs.RO

TL;DR: MDE-AgriVLN improves agricultural robot navigation by adding monocular depth estimation to vision-and-language navigation, boosting success rate from 0.23 to 0.32 and reducing navigation error from 4.43m to 4.08m.


<details>
  <summary>Details</summary>
Motivation: Agricultural robots rely on manual operations or railway systems for movement, and most use single cameras for monocular vision, resulting in limited spatial perception compared to human binocular vision.

Method: Proposes MDE-AgriVLN method with MDE module that generates depth features from RGB images to assist decision-making in agricultural vision-and-language navigation.

Result: On the A2A benchmark, increases Success Rate from 0.23 to 0.32 and decreases Navigation Error from 4.43m to 4.08m, achieving state-of-the-art performance.

Conclusion: Monocular depth estimation effectively enhances spatial perception for agricultural robots, improving navigation performance in agricultural VLN tasks.

Abstract: Agricultural robots are serving as powerful assistants across a wide range of agricultural tasks, nevertheless, still heavily relying on manual operations or railway systems for movement. The AgriVLN method and the A2A benchmark pioneeringly extend Vision-and-Language Navigation (VLN) to the agricultural domain, enabling a robot to navigate to a target position following a natural language instruction. Unlike human binocular vision, most agricultural robots are only given a single camera for monocular vision, which results in limited spatial perception. To bridge this gap, we present the method of Agricultural Vision-and-Language Navigation with Monocular Depth Estimation (MDE-AgriVLN), in which we propose the MDE module generating depth features from RGB images, to assist the decision-maker on reasoning. When evaluated on the A2A benchmark, our MDE-AgriVLN method successfully increases Success Rate from 0.23 to 0.32 and decreases Navigation Error from 4.43m to 4.08m, demonstrating the state-of-the-art performance in the agricultural VLN domain. Code: https://github.com/AlexTraveling/MDE-AgriVLN.

</details>


### [147] [Artificial Microsaccade Compensation: Stable Vision for an Ornithopter](https://arxiv.org/abs/2512.03995)
*Levi Burner,Guido de Croon,Yiannis Aloimonos*

Main category: cs.RO

TL;DR: A bio-inspired method for real-time video stabilization that mimics microsaccades to compensate for high-frequency shaking in tailless ornithopter videos, outperforming commercial software.


<details>
  <summary>Details</summary>
Motivation: Inspired by biological microsaccades in foveated vision systems, the paper addresses the challenge of stabilizing video from tailless ornithopters that shake at 12-20 Hz, which has previously resisted camera-based sensing solutions.

Method: Develops Artificial Microsaccade Compensation that minimizes changes in image intensity by optimizing over 3D rotation represented in SO(3), enabling real-time computation and producing distortion-free stabilized video.

Result: The method achieves higher quality results than Adobe Premier Pro's warp stabilizer while running in real time, and when adapted to hold fixed viewing orientation, dramatically reduces inter-frame motion with efficient recursive updates.

Conclusion: The bio-inspired approach successfully stabilizes challenging high-frequency shaking in ornithopter videos in real time, outperforming commercial solutions and enabling practical camera-based sensing for previously unstable platforms.

Abstract: Animals with foveated vision, including humans, experience microsaccades, small, rapid eye movements that they are not aware of. Inspired by this phenomenon, we develop a method for "Artificial Microsaccade Compensation". It can stabilize video captured by a tailless ornithopter that has resisted attempts to use camera-based sensing because it shakes at 12-20 Hz. Our approach minimizes changes in image intensity by optimizing over 3D rotation represented in SO(3). This results in a stabilized video, computed in real time, suitable for human viewing, and free from distortion. When adapted to hold a fixed viewing orientation, up to occasional saccades, it can dramatically reduce inter-frame motion while also benefiting from an efficient recursive update. When compared to Adobe Premier Pro's warp stabilizer, which is widely regarded as the best commercial video stabilization software available, our method achieves higher quality results while also running in real time.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [148] [Exploring Syntropic Frameworks in AI Alignment: A Philosophical Investigation](https://arxiv.org/abs/2512.03048)
*Austin Spizzirri*

Main category: cs.AI

TL;DR: AI alignment should shift from encoding fixed human values to architecting syntropic, reasons-responsive agents through developmental processes, addressing fundamental philosophical challenges in value specification.


<details>
  <summary>Details</summary>
Motivation: Traditional content-based AI alignment approaches face structural instability due to the is-ought gap, value pluralism, and the extended frame problem, necessitating a new philosophical foundation for creating genuinely moral AI agents.

Method: Proposes a three-part philosophical framework: 1) identifying the "specification trap" in value encoding, 2) introducing "syntropy" as an information-theoretic concept for multi-agent alignment, and 3) establishing criteria for genuine moral capacity based on compatibilist theories with an embodied experimental paradigm.

Result: The paper develops a comprehensive philosophical framework that generates specific, falsifiable predictions about value emergence and moral agency in AI systems, though empirical validation remains pending in a separate research project.

Conclusion: AI alignment should be reconceived as architecting syntropic, reasons-responsive agents through process-based, multi-agent developmental mechanisms rather than attempting to encode fixed human value content, providing a more stable foundation for creating genuinely moral artificial systems.

Abstract: I argue that AI alignment should be reconceived as architecting syntropic, reasons-responsive agents through process-based, multi-agent, developmental mechanisms rather than encoding fixed human value content. The paper makes three philosophical contributions. First, I articulate the ``specification trap'' argument demonstrating why content-based value specification appears structurally unstable due to the conjunction of the is-ought gap, value pluralism, and the extended frame problem. Second, I propose syntropy -- the recursive reduction of mutual uncertainty between agents through state alignment -- as an information-theoretic framework for understanding multi-agent alignment dynamics. Third, I establish a functional distinction between genuine and simulated moral capacity grounded in compatibilist theories of guidance control, coupled with an embodied experimental paradigm and verification regime providing operational criteria independent of phenomenological claims. This paper represents the philosophical component of a broader research program whose empirical validation is being developed in a separate project currently in preparation. While the framework generates specific, falsifiable predictions about value emergence and moral agency in artificial systems, empirical validation remains pending.

</details>


### [149] [Beyond the Black Box: A Cognitive Architecture for Explainable and Aligned AI](https://arxiv.org/abs/2512.03072)
*Hu Keyi*

Main category: cs.AI

TL;DR: Weight-Calculatism is a novel cognitive architecture using atomic Logical Atoms and fundamental operations (Pointing/Comparison) to achieve explainable, value-aligned AGI through interpretable weight calculations.


<details>
  <summary>Details</summary>
Motivation: Current AI paradigms face fundamental challenges in explainability and value alignment, lacking transparency and trustworthiness needed for AGI development.

Method: Deconstructs cognition into indivisible Logical Atoms and two fundamental operations (Pointing and Comparison), formalizing decision-making through Weight = Benefit * Probability model with traceable Initial Weights, implemented via graph-algorithm computational engine and global workspace workflow.

Result: Achieves transparent, human-like reasoning and robust learning in unprecedented scenarios, demonstrating potential as viable pathway toward trustworthy and aligned AGI.

Conclusion: Weight-Calculatism establishes both practical and theoretical foundation for building explainable, value-aligned AGI through atomic decomposition and interpretable weight calculations.

Abstract: Current AI paradigms, as "architects of experience," face fundamental challenges in explainability and value alignment. This paper introduces "Weight-Calculatism," a novel cognitive architecture grounded in first principles, and demonstrates its potential as a viable pathway toward Artificial General Intelligence (AGI). The architecture deconstructs cognition into indivisible Logical Atoms and two fundamental operations: Pointing and Comparison. Decision-making is formalized through an interpretable Weight-Calculation model (Weight = Benefit * Probability), where all values are traceable to an auditable set of Initial Weights. This atomic decomposition enables radical explainability, intrinsic generality for novel situations, and traceable value alignment. We detail its implementation via a graph-algorithm-based computational engine and a global workspace workflow, supported by a preliminary code implementation and scenario validation. Results indicate that the architecture achieves transparent, human-like reasoning and robust learning in unprecedented scenarios, establishing a practical and theoretical foundation for building trustworthy and aligned AGI.

</details>


### [150] [When Do Symbolic Solvers Enhance Reasoning in Large Language Models?](https://arxiv.org/abs/2512.03272)
*Zhiyuan He,Dingmin Wang*

Main category: cs.AI

TL;DR: Symbolic-solver-integrated methods outperform long CoT reasoning only for problems with limited implicit reasoning but large search spaces, like constraint satisfaction problems requiring backtracking.


<details>
  <summary>Details</summary>
Motivation: Large Reasoning Models (LRMs) using long Chains of Thought (CoTs) incur substantial token overhead and can "overthink" to produce incorrect answers. The paper explores when symbolic-solver-integrated approaches (using LLMs to generate code for symbolic solvers) can enhance conventional long-CoT reasoning.

Method: The paper investigates when symbolic-solver-integrated methods outperform conventional long-CoT reasoning. It uses LLMs to translate reasoning tasks into executable code for symbolic solvers, comparing performance across different problem types and LLM architectures (including GPT-4o and CodeLlama-13B).

Result: Symbolic-solver-integrated methods only help when problems require limited implicit reasoning but involve ample search space. GPT-4o performs better on deductive problems with shallow reasoning depth, while symbolic-solver integration significantly improves LLM performance on constraint satisfaction problems requiring repeated backtracking. With declarative exemplars, even CodeLlama-13B can outperform GPT-4o on difficult Zebra puzzles.

Conclusion: The effectiveness of symbolic-solver-integrated approaches depends on problem characteristics - they excel for constraint satisfaction problems with large search spaces requiring backtracking, while conventional long-CoT reasoning remains better for problems requiring deeper implicit reasoning. Problem type should guide the choice between these reasoning paradigms.

Abstract: Large Reasoning Models (LRMs) achieve strong performance on complex reasoning tasks by generating long Chains of Thought (CoTs). However, this paradigm might incur substantial token overhead, especially when models "overthink" by producing lengthy reasoning chains, which can even lead to incorrect answers. A promising direction is the symbolic-solver-integrated approach, which leverages the code generation capabilities of LLMs to translate reasoning tasks into executable code and then solve them with a symbolic solver. In this paper, we explore an open question of when the conventional long-CoT can be enhanced by symbolic solvers. Our experimental results show that the symbolic-solver-integrated method only helps when the problem requires limited implicit reasoning but involves an ample search space. The latest LLMs, like GPT-4o, show better performance on deductive problems with shallow reasoning depth, while the symbolic-solver-integrated method significantly improves the LLMs' performance in constraint satisfaction problems that require repeated backtracks. When a declarative exemplar is provided, even CodeLlama-13B can outperform GPT-4o in difficult Zebra puzzles.

</details>


### [151] [Prior preferences in active inference agents: soft, hard, and goal shaping](https://arxiv.org/abs/2512.03293)
*Filippo Torresan,Ryota Kanai,Manuel Baltieri*

Main category: cs.AI

TL;DR: Active inference agents with goal shaping perform best in navigation tasks but sacrifice exploration of environment dynamics.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the lack of attention in active inference literature on how preference distributions should be specified and how different specifications impact agent performance, particularly in balancing exploitation and exploration.

Method: Four preference distribution definitions were tested: hard vs. soft goals, with vs. without goal shaping (intermediate goals). These were compared in a grid world navigation task using active inference agents.

Result: Goal shaping enabled the best overall performance (promoting exploitation) but sacrificed learning about environment transition dynamics (hampering exploration).

Conclusion: The specification of preference distributions significantly impacts active inference agent performance, with goal shaping optimizing exploitation at the cost of exploration, highlighting the trade-off between these competing drives.

Abstract: Active inference proposes expected free energy as an objective for planning and decision-making to adequately balance exploitative and explorative drives in learning agents. The exploitative drive, or what an agent wants to achieve, is formalised as the Kullback-Leibler divergence between a variational probability distribution, updated at each inference step, and a preference probability distribution that indicates what states or observations are more likely for the agent, hence determining the agent's goal in a certain environment. In the literature, the questions of how the preference distribution should be specified and of how a certain specification impacts inference and learning in an active inference agent have been given hardly any attention. In this work, we consider four possible ways of defining the preference distribution, either providing the agents with hard or soft goals and either involving or not goal shaping (i.e., intermediate goals). We compare the performances of four agents, each given one of the possible preference distributions, in a grid world navigation task. Our results show that goal shaping enables the best performance overall (i.e., it promotes exploitation) while sacrificing learning about the environment's transition dynamics (i.e., it hampers exploration).

</details>


### [152] [Evaluating Generalization Capabilities of LLM-Based Agents in Mixed-Motive Scenarios Using Concordia](https://arxiv.org/abs/2512.03318)
*Chandler Smith,Marwa Abdulhai,Manfred Diaz,Marko Tesic,Rakshit S. Trivedi,Alexander Sasha Vezhnevets,Lewis Hammond,Jesse Clifton,Minsuk Chang,Edgar A. Duéñez-Guzmán,John P. Agapiou,Jayd Matyas,Danny Karmon,Akash Kundu,Aliaksei Korshuk,Ananya Ananya,Arrasy Rahman,Avinaash Anand Kulandaivel,Bain McHale,Beining Zhang,Buyantuev Alexander,Carlos Saith Rodriguez Rojas,Caroline Wang,Chetan Talele,Chenao Liu,Chichen Lin,Diana Riazi,Di Yang Shi,Emanuel Tewolde,Elizaveta Tennant,Fangwei Zhong,Fuyang Cui,Gang Zhao,Gema Parreño Piqueras,Hyeonggeun Yun,Ilya Makarov,Jiaxun Cui,Jebish Purbey,Jim Dilkes,Jord Nguyen,Lingyun Xiao,Luis Felipe Giraldo,Manuela Chacon-Chamorro,Manuel Sebastian Rios Beltran,Marta Emili García Segura,Mengmeng Wang,Mogtaba Alim,Nicanor Quijano,Nico Schiavone,Olivia Macmillan-Scott,Oswaldo Peña,Peter Stone,Ram Mohan Rao Kadiyala,Rolando Fernandez,Ruben Manrique,Sunjia Lu,Sheila A. McIlraith,Shamika Dhuri,Shuqing Shi,Siddhant Gupta,Sneheel Sarangi,Sriram Ganapathi Subramanian,Taehun Cha,Toryn Q. Klassen,Wenming Tu,Weijian Fan,Wu Ruiyang,Xue Feng,Yali Du,Yang Liu,Yiding Wang,Yipeng Kang,Yoonchang Sung,Yuxuan Chen,Zhaowei Zhang,Zhihan Wang,Zhiqiang Wu,Ziang Chen,Zilong Zheng,Zixia Jia,Ziyan Wang,Dylan Hadfield-Menell,Natasha Jaques,Tim Baarslag,Jose Hernandez-Orallo,Joel Z. Leibo*

Main category: cs.AI

TL;DR: The paper introduces a method using Concordia simulation to evaluate LLM agents' zero-shot cooperation in mixed-motive social situations, revealing significant gaps in generalization capabilities.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation methods fail to measure how well LLM agents' social interaction capabilities generalize to novel social situations, which is critical as these agents are increasingly deployed in human and artificial agent interactions.

Method: Uses Concordia, a natural language multi-agent simulation environment, to evaluate LLM-based agents' ability to cooperate in zero-shot, mixed-motive environments by testing their capability to identify and exploit opportunities for mutual gain across diverse partners and contexts.

Result: Empirical results from the NeurIPS 2024 Concordia Contest show significant gaps between current agent capabilities and the robust generalization required for reliable cooperation, particularly in scenarios demanding persuasion and norm enforcement.

Conclusion: Current LLM agents lack the robust generalization needed for reliable cooperation in novel social situations, highlighting the need for improved evaluation methods and agent capabilities for effective mixed-motive social interactions.

Abstract: Large Language Model (LLM) agents have demonstrated impressive capabilities for social interaction and are increasingly being deployed in situations where they might engage with both human and artificial agents. These interactions represent a critical frontier for LLM-based agents, yet existing evaluation methods fail to measure how well these capabilities generalize to novel social situations. In this paper, we introduce a method for evaluating the ability of LLM-based agents to cooperate in zero-shot, mixed-motive environments using Concordia, a natural language multi-agent simulation environment. Our method measures general cooperative intelligence by testing an agent's ability to identify and exploit opportunities for mutual gain across diverse partners and contexts. We present empirical results from the NeurIPS 2024 Concordia Contest, where agents were evaluated on their ability to achieve mutual gains across a suite of diverse scenarios ranging from negotiation to collective action problems. Our findings reveal significant gaps between current agent capabilities and the robust generalization required for reliable cooperation, particularly in scenarios demanding persuasion and norm enforcement.

</details>


### [153] [Multimodal Reinforcement Learning with Agentic Verifier for AI Agents](https://arxiv.org/abs/2512.03438)
*Reuben Tan,Baolin Peng,Zhengyuan Yang,Hao Cheng,Oier Mees,Theodore Zhao,Andrea Tupini,Isar Meijier,Qianhui Wu,Yuncong Yang,Lars Liden,Yu Gu,Sheng Zhang,Xiaodong Liu,Lijuan Wang,Marc Pollefeys,Yong Jae Lee,Jianfeng Gao*

Main category: cs.AI

TL;DR: Argos is an agentic reward system that selects optimal scoring functions to evaluate multimodal reasoning across accuracy, localization, and reasoning quality, achieving SOTA results while preventing reward hacking.


<details>
  <summary>Details</summary>
Motivation: Current multimodal reinforcement learning (MMRL) models rely on sparse, outcome-based rewards, which provide limited guidance. Richer rewards from reasoning tokens could improve learning, but it's challenging to compute informative rewards due to varying scoring needs across samples and noisy teacher signals.

Method: Argos (Agentic Reward for Grounded & Objective Scoring) is a principled reward agent that selects from a pool of teacher-model derived and rule-based scoring functions to evaluate: (1) final response accuracy, (2) spatiotemporal localization of entities/actions, and (3) reasoning process quality.

Result: Using Argos for both SFT data curation and RL training achieves state-of-the-art results across spatial reasoning, visual hallucination, robotics, and embodied AI benchmarks. The approach prevents collapse to ungrounded solutions during RL and reduces reward-hacking in MMRL.

Conclusion: Argos demonstrates that agentic verification is essential for effective multimodal reasoning training, as SFT alone is insufficient. The system's effectiveness is theoretically justified through pareto-optimality, providing a principled approach to reward design in MMRL.

Abstract: Agentic reasoning models trained with multimodal reinforcement learning (MMRL) have become increasingly capable, yet they are almost universally optimized using sparse, outcome-based rewards computed based on the final answers. Richer rewards computed from the reasoning tokens can improve learning significantly by providing more fine-grained guidance. However, it is challenging to compute more informative rewards in MMRL beyond those based on outcomes since different samples may require different scoring functions and teacher models may provide noisy reward signals too. In this paper, we introduce the Argos (Agentic Reward for Grounded & Objective Scoring), a principled reward agent to train multimodal reasoning models for agentic tasks. For each sample, Argos selects from a pool of teacher-model derived and rule-based scoring functions to simultaneously evaluate: (i) final response accuracy, (ii) spatiotemporal localization of referred entities and actions, and (iii) the quality of the reasoning process. We find that by leveraging our agentic verifier across both SFT data curation and RL training, our model achieves state-of-the-art results across multiple agentic tasks such as spatial reasoning, visual hallucination as well as robotics and embodied AI benchmarks. Critically, we demonstrate that just relying on SFT post-training on highly curated reasoning data is insufficient, as agents invariably collapse to ungrounded solutions during RL without our online verification. We also show that our agentic verifier can help to reduce reward-hacking in MMRL. Finally, we also provide a theoretical justification for the effectiveness of Argos through the concept of pareto-optimality.

</details>


### [154] [Multi-Agent Reinforcement Learning with Communication-Constrained Priors](https://arxiv.org/abs/2512.03528)
*Guang Yang,Tianpei Yang,Jingwen Qiao,Yanqing Wu,Jing Huo,Xingguo Chen,Yang Gao*

Main category: cs.AI

TL;DR: Proposes a communication-constrained MARL framework that handles lossy communication by distinguishing lossy/lossless messages and quantifying their impact on rewards.


<details>
  <summary>Details</summary>
Motivation: Real-world multi-agent systems face lossy communication issues, but existing MARL methods lack scalability and robustness for complex dynamic environments.

Method: Develops a generalized communication-constrained model, uses learning prior to distinguish message types, decouples their impact via dual mutual information estimator, and integrates message impact into global reward.

Result: Validated effectiveness across several communication-constrained benchmarks.

Conclusion: Proposed framework addresses lossy communication challenges in MARL, improving applicability to real-world scenarios through systematic handling of communication constraints.

Abstract: Communication is one of the effective means to improve the learning of cooperative policy in multi-agent systems. However, in most real-world scenarios, lossy communication is a prevalent issue. Existing multi-agent reinforcement learning with communication, due to their limited scalability and robustness, struggles to apply to complex and dynamic real-world environments. To address these challenges, we propose a generalized communication-constrained model to uniformly characterize communication conditions across different scenarios. Based on this, we utilize it as a learning prior to distinguish between lossy and lossless messages for specific scenarios. Additionally, we decouple the impact of lossy and lossless messages on distributed decision-making, drawing on a dual mutual information estimatior, and introduce a communication-constrained multi-agent reinforcement learning framework, quantifying the impact of communication messages into the global reward. Finally, we validate the effectiveness of our approach across several communication-constrained benchmarks.

</details>


### [155] [PARC: An Autonomous Self-Reflective Coding Agent for Robust Execution of Long-Horizon Tasks](https://arxiv.org/abs/2512.03549)
*Yuki Orimo,Iori Kurata,Hodaka Mori,Ryuhei Okuno,Ryohto Sawada,Daisuke Okanohara*

Main category: cs.AI

TL;DR: PARC is a hierarchical multi-agent coding system with self-assessment that autonomously executes long computational tasks, successfully reproducing materials science results and competing with human baselines on Kaggle.


<details>
  <summary>Details</summary>
Motivation: To create an AI system capable of autonomous, robust execution of long-horizon computational tasks without human intervention, addressing the need for AI that can independently conduct large-scale scientific and analytical work.

Method: Hierarchical multi-agent architecture with task planning, execution, and self-assessment/self-feedback mechanisms that evaluate actions from independent context to detect and correct strategic errors.

Result: Successfully reproduced key materials science results (lithium-ion conduction, alloy segregation) by coordinating dozens of parallel simulations (43 hours each), and produced competitive solutions on Kaggle tasks from minimal natural-language instructions.

Conclusion: Integrating hierarchical multi-agent systems with self-assessment and self-feedback enables AI systems capable of independent, large-scale scientific and analytical work, demonstrating PARC's potential for autonomous computational task execution.

Abstract: We introduce PARC, a coding agent for the autonomous and robust execution of long-horizon computational tasks. PARC is built on a hierarchical multi-agent architecture incorporating task planning, execution, and a mechanism that evaluates its own actions and their outcomes from an independent context and provides feedback, namely self-assessment and self-feedback. This design enables PARC to detect and correct high-level strategic errors and sustain progress without human intervention. We evaluate PARC across computational science and data science tasks. In materials science, it autonomously reproduces key results from studies on lithium-ion conduction and alloy segregation. In particular, it coordinates dozens of parallel simulation tasks, each requiring roughly 43 hours of computation, managing orchestration, monitoring, and error correction end-to-end. In Kaggle-based experiments, starting from minimal natural-language instructions, PARC conducts data analysis and implements search strategies, producing solutions competitive with human-engineered baselines. These results highlight the potential of integrating a hierarchical multi-agent system with self-assessment and self-feedback to enable AI systems capable of independent, large-scale scientific and analytical work.

</details>


### [156] [Reason-Plan-ReAct: A Reasoner-Planner Supervising a ReAct Executor for Complex Enterprise Tasks](https://arxiv.org/abs/2512.03560)
*Gianni Molinari,Fabio Ciravegna*

Main category: cs.AI

TL;DR: RP-ReAct is a multi-agent system that separates strategic planning from execution to improve reliability and efficiency in complex enterprise tasks, addressing context window limitations and trajectory instability.


<details>
  <summary>Details</summary>
Motivation: Current autonomous agents struggle with complex enterprise tasks due to: 1) single-agent architectures causing trajectory instability, and 2) privacy requirements forcing use of smaller open-weight models with limited context windows that get overwhelmed by large tool outputs.

Method: RP-ReAct uses a multi-agent approach with a Reasoner Planner Agent (RPA) for strategic planning and reasoning, and Proxy-Execution Agents (PEA) for tool execution using ReAct. Includes context-saving strategy to manage large tool outputs via external storage.

Result: Superior performance on ToolQA benchmark across six open-weight reasoning models, showing improved generalization, robustness, and stability across different model scales compared to state-of-the-art baselines.

Conclusion: RP-ReAct's decoupled multi-agent architecture with context management enables effective, deployable enterprise agentic solutions that overcome limitations of single-agent systems and context window constraints.

Abstract: Despite recent advances, autonomous agents often struggle to solve complex tasks in enterprise domains that require coordinating multiple tools and processing diverse data sources. This struggle is driven by two main limitations. First, single-agent architectures enforce a monolithic plan-execute loop, which directly causes trajectory instability. Second, the requirement to use local open-weight models for data privacy introduces smaller context windows leading to the rapid consumption of context from large tool outputs. To solve this problem we introduce RP-ReAct (Reasoner Planner-ReAct), a novel multi-agent approach that fundamentally decouples strategic planning from low-level execution to achieve superior reliability and efficiency. RP-ReAct consists of a Reasoner Planner Agent (RPA), responsible for planning each sub-step, continuously analysing the execution results using the strong reasoning capabilities of a Large Reasoning Model, and one or multiple Proxy-Execution Agent (PEA) that translates sub-steps into concrete tool interactions using a ReAct approach. Crucially, we incorporate a context-saving strategy within the PEA to mitigate context window overflow by managing large tool outputs via external storage and on-demand access. We evaluate RP-ReAct, on the challenging, multi-domain ToolQA benchmark using a diverse set of six open-weight reasoning models. Our empirical results show that RP-ReAct achieves superior performance and improved generalization ability over state-of-the-art baselines when addressing diverse complex tasks across the evaluated domains. Furthermore we establish the enhanced robustness and stability of our approach across different model scales, paving the way for effective and deployable agentic solutions for enterprises.

</details>


### [157] [EnCompass: Enhancing Agent Programming with Search Over Program Execution Paths](https://arxiv.org/abs/2512.03571)
*Zhening Li,Armando Solar-Lezama,Yisong Yue,Stephan Zheng*

Main category: cs.AI

TL;DR: PAN programming model disentangles agent workflow logic from inference strategies, enabling easy experimentation with different search strategies via simple input changes.


<details>
  <summary>Details</summary>
Motivation: Current agent programming approaches entangle core workflow logic with inference-time strategies (like tree search), making it difficult to experiment with different strategies independently.

Method: Introduces "probabilistic angelic nondeterminism" (PAN) programming model and EnCompass framework in Python that uses decorators to compile agent workflows into search spaces.

Result: Three case studies demonstrate that the framework allows quick improvement of agent reliability and easy switching between inference strategies with minimal additional coding.

Conclusion: PAN provides a clean separation of concerns in agent programming, enabling more flexible experimentation and development of LLM-based agents.

Abstract: We introduce a new approach to agent programming, the development of LLM-based agents. Current approaches to agent programming often entangle two aspects of agent design: the core workflow logic and the inference-time strategy (e.g., tree search). We introduce "probabilistic angelic nondeterminism" ("PAN"), a programming model that disentangles these two concerns, allowing the programmer to describe the agent workflow and independently experiment with different inference-time strategies by simply changing a few inputs. We provide an implementation of PAN in Python as the EnCompass framework, which uses a Python decorator to compile agent workflow programs into a search space. We present three case studies that demonstrate how the framework lets the programmer quickly improve the reliability of an agent and easily switch between different inference-time strategies, all with little additional coding.

</details>


### [158] [DeepRule: An Integrated Framework for Automated Business Rule Generation via Deep Predictive Modeling and Hybrid Search Optimization](https://arxiv.org/abs/2512.03607)
*Yusen Wu,Xiaotie Deng*

Main category: cs.AI

TL;DR: DeepRule is an automated business rule generation framework for retail assortment/pricing that addresses theory-practice gaps through LLM-based knowledge fusion, game-theoretic optimization, and interpretable rule distillation.


<details>
  <summary>Details</summary>
Motivation: Addresses systematic misalignment between theoretical models and real-world retail complexities: (1) data modality mismatch with unstructured text sources, (2) dynamic feature entanglement in price elasticity modeling, (3) operational infeasibility from multi-tier business constraints.

Method: Tri-level architecture: 1) Hybrid knowledge fusion engine using LLMs for semantic parsing of unstructured text into structured features; 2) Game-theoretic constrained optimization with bilateral utility functions for supply chain reconciliation; 3) Interpretable decision distillation interface using LLM-guided symbolic regression with economic priors as hard constraints.

Result: Validated in real retail environments, achieving higher profits compared to systematic B2C baselines while ensuring operational feasibility.

Conclusion: Establishes a closed-loop pipeline unifying unstructured knowledge injection, multi-agent optimization, and interpretable strategy synthesis for practical economic intelligence applications.

Abstract: This paper proposes DeepRule, an integrated framework for automated business rule generation in retail assortment and pricing optimization. Addressing the systematic misalignment between existing theoretical models and real-world economic complexities, we identify three critical gaps: (1) data modality mismatch where unstructured textual sources (e.g. negotiation records, approval documents) impede accurate customer profiling; (2) dynamic feature entanglement challenges in modeling nonlinear price elasticity and time-varying attributes; (3) operational infeasibility caused by multi-tier business constraints.
  Our framework introduces a tri-level architecture for above challenges. We design a hybrid knowledge fusion engine employing large language models (LLMs) for deep semantic parsing of unstructured text, transforming distributor agreements and sales assessments into structured features while integrating managerial expertise. Then a game-theoretic constrained optimization mechanism is employed to dynamically reconcile supply chain interests through bilateral utility functions, encoding manufacturer-distributor profit redistribution as endogenous objectives under hierarchical constraints. Finally an interpretable decision distillation interface leveraging LLM-guided symbolic regression to find and optimize pricing strategies and auditable business rules embeds economic priors (e.g. non-negative elasticity) as hard constraints during mathematical expression search. We validate the framework in real retail environments achieving higher profits versus systematic B2C baselines while ensuring operational feasibility. This establishes a close-loop pipeline unifying unstructured knowledge injection, multi-agent optimization, and interpretable strategy synthesis for real economic intelligence.

</details>


### [159] [MemVerse: Multimodal Memory for Lifelong Learning Agents](https://arxiv.org/abs/2512.03627)
*Junming Liu,Yifei Sun,Weihua Cheng,Haodong Lei,Yirong Chen,Licheng Wen,Xuemeng Yang,Daocheng Fu,Pinlong Cai,Nianchen Deng,Yi Yu,Shuyue Hu,Botian Shi,Ding Wang*

Main category: cs.AI

TL;DR: MemVerse is a plug-and-play memory framework that combines fast parametric recall with hierarchical retrieval-based memory to solve AI agents' catastrophic forgetting problem and enable scalable multimodal intelligence.


<details>
  <summary>Details</summary>
Motivation: Current AI agents lack reliable memory, leading to catastrophic forgetting of past experiences, difficulty with long-horizon reasoning, and incoherent operation in multimodal/interactive environments.

Method: MemVerse maintains short-term memory for recent context while transforming raw multimodal experiences into structured long-term memories organized as hierarchical knowledge graphs. It uses periodic distillation to compress essential knowledge from long-term memory into the parametric model for fast differentiable recall.

Result: Extensive experiments show MemVerse significantly improves multimodal reasoning and continual learning efficiency, enabling agents to remember, adapt, and reason coherently across extended interactions.

Conclusion: MemVerse provides a model-agnostic memory framework that bridges parametric and retrieval-based memory, solving fundamental memory limitations in AI agents while supporting continual consolidation, adaptive forgetting, and bounded memory growth.

Abstract: Despite rapid progress in large-scale language and vision models, AI agents still suffer from a fundamental limitation: they cannot remember. Without reliable memory, agents catastrophically forget past experiences, struggle with long-horizon reasoning, and fail to operate coherently in multimodal or interactive environments. We introduce MemVerse, a model-agnostic, plug-and-play memory framework that bridges fast parametric recall with hierarchical retrieval-based memory, enabling scalable and adaptive multimodal intelligence. MemVerse maintains short-term memory for recent context while transforming raw multimodal experiences into structured long-term memories organized as hierarchical knowledge graphs. This design supports continual consolidation, adaptive forgetting, and bounded memory growth. To handle real-time demands, MemVerse introduces a periodic distillation mechanism that compresses essential knowledge from long-term memory into the parametric model, allowing fast, differentiable recall while preserving interpretability. Extensive experiments demonstrate that MemVerse significantly improves multimodal reasoning and continual learning efficiency, empowering agents to remember, adapt, and reason coherently across extended interactions.

</details>


### [160] [RoCo: Role-Based LLMs Collaboration for Automatic Heuristic Design](https://arxiv.org/abs/2512.03762)
*Jiawei Xu,Fengfeng Wei,Weineng Chen*

Main category: cs.AI

TL;DR: RoCo is a multi-agent LLM-based system for automatic heuristic design that uses four specialized agents (explorer, exploiter, critic, integrator) collaborating to generate high-quality heuristics for combinatorial optimization problems.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based automatic heuristic design approaches often use only single-role models, limiting diversity and quality. There's a need for more sophisticated collaborative systems that can balance exploration and exploitation while incorporating critical evaluation.

Method: RoCo coordinates four specialized LLM agents: explorer (creative diversity), exploiter (efficiency refinements), critic (evaluation and feedback), and integrator (synthesis and balance). They interact in structured multi-round processes with feedback, refinement, and elite mutations guided by both short-term and long-term reflections.

Result: RoCo achieves superior performance on five different combinatorial optimization problems in both white-box and black-box settings, consistently outperforming existing methods including ReEvo and HSEvo.

Conclusion: The role-based collaborative paradigm establishes a new standard for robust and high-performing automatic heuristic design, demonstrating the effectiveness of multi-agent collaboration in enhancing heuristic diversity and quality.

Abstract: Automatic Heuristic Design (AHD) has gained traction as a promising solution for solving combinatorial optimization problems (COPs). Large Language Models (LLMs) have emerged and become a promising approach to achieving AHD, but current LLM-based AHD research often only considers a single role. This paper proposes RoCo, a novel Multi-Agent Role-Based System, to enhance the diversity and quality of AHD through multi-role collaboration. RoCo coordinates four specialized LLM-guided agents-explorer, exploiter, critic, and integrator-to collaboratively generate high-quality heuristics. The explorer promotes long-term potential through creative, diversity-driven thinking, while the exploiter focuses on short-term improvements via conservative, efficiency-oriented refinements. The critic evaluates the effectiveness of each evolution step and provides targeted feedback and reflection. The integrator synthesizes proposals from the explorer and exploiter, balancing innovation and exploitation to drive overall progress. These agents interact in a structured multi-round process involving feedback, refinement, and elite mutations guided by both short-term and accumulated long-term reflections. We evaluate RoCo on five different COPs under both white-box and black-box settings. Experimental results demonstrate that RoCo achieves superior performance, consistently generating competitive heuristics that outperform existing methods including ReEvo and HSEvo, both in white-box and black-box scenarios. This role-based collaborative paradigm establishes a new standard for robust and high-performing AHD.

</details>


### [161] [Omni-AutoThink: Adaptive Multimodal Reasoning via Reinforcement Learning](https://arxiv.org/abs/2512.03783)
*Dongchao Yang,Songxiang Liu,Disong Wang,Yuanyuan Wang,Guanglu Wan,Helen Meng*

Main category: cs.AI

TL;DR: Omni-AutoThink: Adaptive reasoning framework for Omni models that dynamically adjusts reasoning depth based on task difficulty, improving performance on multimodal tasks.


<details>
  <summary>Details</summary>
Motivation: Existing Omni models have rigid reasoning behaviors - they either overthink simple problems or fail to reason when necessary. There's a need for adaptive reasoning that adjusts to task complexity.

Method: Two-stage framework: (1) Adaptive Supervised Fine-Tuning with reasoning-augmented data to build fundamental reasoning capability, and (2) Adaptive Reinforcement Learning (Adaptive GRPO) to optimize reasoning behaviors based on task complexity and reward feedback.

Result: Significantly improves adaptive reasoning performance compared to previous baselines across text-only, text-audio, text-visual, and text-audio-visual modalities.

Conclusion: Omni-AutoThink successfully addresses rigid reasoning limitations in Omni models through adaptive reasoning, with benchmark data and code to be publicly released for further research.

Abstract: Recent advances in Omni models have enabled unified multimodal perception and generation. However, most existing systems still exhibit rigid reasoning behaviors, either overthinking simple problems or failing to reason when necessary. To address this limitation, we propose Omni-AutoThink, a novel adaptive reasoning framework that dynamically adjusts the model's reasoning depth according to task difficulty. Our framework comprises two stages: (1) an Adaptive Supervised Fine-Tuning (Adaptive SFT) stage, which endows the Omni model with fundamental reasoning capability using large-scale reasoning-augmented data, and (2) an Adaptive Reinforcement Learning (Adaptive GRPO) stage, which optimizes reasoning behaviors based on task complexity and reward feedback. We further construct a comprehensive adaptive reasoning benchmark that spans text-only, text-audio, text-visual, and text-audio-visual modalities, providing both training and evaluation splits for multimodal reasoning assessment. Experimental results demonstrate that our proposed framework significantly improves adaptive reasoning performance compared to previous baselines. All benchmark data and code will be publicly released.

</details>


### [162] [A Hierarchical Tree-based approach for creating Configurable and Static Deep Research Agent (Static-DRA)](https://arxiv.org/abs/2512.03887)
*Saurav Prateek*

Main category: cs.AI

TL;DR: Static-DRA is a configurable tree-based deep research agent with user-tunable Depth and Breadth parameters that balance research quality against computational costs.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of static RAG pipelines in handling complex, multi-turn research tasks by providing a configurable, resource-aware solution for deep research.

Method: Hierarchical tree-based static workflow with Supervisor, Independent, and Worker agents, featuring user-tunable Depth and Breadth parameters for granular control over research intensity.

Result: Achieved overall score of 34.72 on DeepResearch Bench using RACE framework with depth=2, breadth=5, and gemini-2.5-pro model; increasing parameters improves research depth and scores.

Conclusion: Static-DRA provides a pragmatic, resource-aware solution with transparent user control over deep research processes, validated through systematic evaluation.

Abstract: The advancement in Large Language Models has driven the creation of complex agentic systems, such as Deep Research Agents (DRAs), to overcome the limitations of static Retrieval Augmented Generation (RAG) pipelines in handling complex, multi-turn research tasks. This paper introduces the Static Deep Research Agent (Static-DRA), a novel solution built upon a configurable and hierarchical Tree-based static workflow.
  The core contribution is the integration of two user-tunable parameters, Depth and Breadth, which provide granular control over the research intensity. This design allows end-users to consciously balance the desired quality and comprehensiveness of the research report against the associated computational cost of Large Language Model (LLM) interactions. The agent's architecture, comprising Supervisor, Independent, and Worker agents, facilitates effective multi-hop information retrieval and parallel sub-topic investigation.
  We evaluate the Static-DRA against the established DeepResearch Bench using the RACE (Reference-based Adaptive Criteria-driven Evaluation) framework. Configured with a depth of 2 and a breadth of 5, and powered by the gemini-2.5-pro model, the agent achieved an overall score of 34.72. Our experiments validate that increasing the configured Depth and Breadth parameters results in a more in-depth research process and a correspondingly higher evaluation score. The Static-DRA offers a pragmatic and resource-aware solution, empowering users with transparent control over the deep research process. The entire source code, outputs and benchmark results are open-sourced at https://github.com/SauravP97/Static-Deep-Research/

</details>


### [163] [Autonomous Agents and Policy Compliance: A Framework for Reasoning About Penalties](https://arxiv.org/abs/2512.03931)
*Vineel Tummala,Daniela Inclezan*

Main category: cs.AI

TL;DR: Extends policy language with penalty reasoning for autonomous agents, enabling trade-offs between compliance and goal achievement while simulating realistic human decision-making.


<details>
  <summary>Details</summary>
Motivation: Prior work focused only on compliance, but real-world scenarios sometimes require deviation from policies to achieve high-stakes goals. Also, modeling non-compliant behavior helps policymakers simulate realistic human decision-making.

Method: Extends Gelfond and Lobo's AOPL to incorporate penalties, integrates Answer Set Programming (ASP) for reasoning, develops automated translation from extended AOPL to ASP, and refines ASP-based planning algorithms to account for incurred penalties.

Result: Framework generates higher-quality plans that avoid harmful actions while sometimes improving computational efficiency. Experiments in two domains demonstrate effectiveness.

Conclusion: The approach enhances autonomous decision-making by enabling penalty-aware reasoning and informs policy refinement through realistic simulation of human decision-making under policy constraints.

Abstract: This paper presents a logic programming-based framework for policy-aware autonomous agents that can reason about potential penalties for non-compliance and act accordingly. While prior work has primarily focused on ensuring compliance, our approach considers scenarios where deviating from policies may be necessary to achieve high-stakes goals. Additionally, modeling non-compliant behavior can assist policymakers by simulating realistic human decision-making. Our framework extends Gelfond and Lobo's Authorization and Obligation Policy Language (AOPL) to incorporate penalties and integrates Answer Set Programming (ASP) for reasoning. Compared to previous approaches, our method ensures well-formed policies, accounts for policy priorities, and enhances explainability by explicitly identifying rule violations and their consequences. Building on the work of Harders and Inclezan, we introduce penalty-based reasoning to distinguish between non-compliant plans, prioritizing those with minimal repercussions. To support this, we develop an automated translation from the extended AOPL into ASP and refine ASP-based planning algorithms to account for incurred penalties. Experiments in two domains demonstrate that our framework generates higher-quality plans that avoid harmful actions while, in some cases, also improving computational efficiency. These findings underscore its potential for enhancing autonomous decision-making and informing policy refinement. Under consideration in Theory and Practice of Logic Programming (TPLP).

</details>


### [164] [Benchmark for Planning and Control with Large Language Model Agents: Blocksworld with Model Context Protocol](https://arxiv.org/abs/2512.03955)
*Niklas Jobs,Luis Miguel Vieira da Silva,Jayanth Somashekaraiah,Maximilian Weigand,David Kube,Felix Gehlhoff*

Main category: cs.AI

TL;DR: A benchmark with executable simulation environment for evaluating LLM-based agents in industrial automation planning, using Blocksworld problem with 5 complexity levels and Model Context Protocol for standardized tool interface.


<details>
  <summary>Details</summary>
Motivation: Industrial automation needs flexible control strategies that adapt to changing tasks/environments. LLM-based agents show promise for adaptive planning but lack standardized benchmarks for systematic comparison.

Method: Introduce benchmark with executable simulation environment representing Blocksworld problem (5 complexity categories). Integrate Model Context Protocol (MCP) as standardized tool interface to connect diverse agent architectures without implementation-specific modifications.

Result: Demonstrate benchmark's applicability with single-agent implementation, establishing quantitative metrics for comparison of LLM-based planning and execution approaches.

Conclusion: The benchmark provides standardized evaluation framework for LLM-based agents in industrial automation planning, enabling systematic comparison through MCP integration and quantitative metrics.

Abstract: Industrial automation increasingly requires flexible control strategies that can adapt to changing tasks and environments. Agents based on Large Language Models (LLMs) offer potential for such adaptive planning and execution but lack standardized benchmarks for systematic comparison. We introduce a benchmark with an executable simulation environment representing the Blocksworld problem providing five complexity categories. By integrating the Model Context Protocol (MCP) as a standardized tool interface, diverse agent architectures can be connected to and evaluated against the benchmark without implementation-specific modifications. A single-agent implementation demonstrates the benchmark's applicability, establishing quantitative metrics for comparison of LLM-based planning and execution approaches.

</details>
