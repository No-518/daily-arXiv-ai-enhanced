<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 109]
- [cs.RO](#cs.RO) [Total: 47]
- [cs.AI](#cs.AI) [Total: 69]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [WorldVQA: Measuring Atomic World Knowledge in Multimodal Large Language Models](https://arxiv.org/abs/2602.02537)
*Runjie Zhou,Youbo Shao,Haoyu Lu,Bowei Xing,Tongtong Bai,Yujie Chen,Jie Zhao,Lin Sui,Haotian Yao,Zijia Zhao,Hao Yang,Haoning Wu,Zaida Zhou,Jinguo Zhu,Zhiqi Huang,Yiping Bao,Yangyang Liu,Y. Charles,Xinyu Zhou*

Main category: cs.CV

TL;DR: WorldVQA is a benchmark that evaluates MLLMs' atomic visual world knowledge by decoupling visual knowledge retrieval from reasoning, focusing on what models memorize about visual entities across a stratified taxonomy.


<details>
  <summary>Details</summary>
Motivation: Current evaluations often conflate visual knowledge retrieval with reasoning, making it difficult to assess what MLLMs actually memorize about the visual world. There's a need for a benchmark that strictly measures atomic visual knowledge to evaluate visual factuality and hallucination rates.

Method: WorldVQA decouples visual knowledge retrieval from reasoning by creating a benchmark that assesses the atomic capability of grounding and naming visual entities. It uses a stratified taxonomy that spans from common head-class objects to long-tail rarities.

Result: The paper introduces WorldVQA as a benchmark, but doesn't present specific experimental results. The benchmark is positioned as a tool for evaluating MLLMs' visual factuality, encyclopedic breadth, and hallucination rates.

Conclusion: WorldVQA serves as a rigorous test for visual factuality in MLLMs, establishing a standard for assessing the encyclopedic breadth and hallucination rates of current and next-generation frontier models.

Abstract: We introduce WorldVQA, a benchmark designed to evaluate the atomic visual world knowledge of Multimodal Large Language Models (MLLMs). Unlike current evaluations, which often conflate visual knowledge retrieval with reasoning, WorldVQA decouples these capabilities to strictly measure "what the model memorizes." The benchmark assesses the atomic capability of grounding and naming visual entities across a stratified taxonomy, spanning from common head-class objects to long-tail rarities. We expect WorldVQA to serve as a rigorous test for visual factuality, thereby establishing a standard for assessing the encyclopedic breadth and hallucination rates of current and next-generation frontier models.

</details>


### [2] [AdaptMMBench: Benchmarking Adaptive Multimodal Reasoning for Mode Selection and Reasoning Process](https://arxiv.org/abs/2602.02676)
*Xintong Zhang,Xiaowen Zhang,Jongrong Wu,Zhi Gao,Shilin Yan,Zhenxin Diao,Kunpeng Gao,Xuanyan Chen,Yuwei Wu,Yunde Jia,Qing Li*

Main category: cs.CV

TL;DR: AdaptMMBench is a new benchmark for evaluating adaptive multimodal reasoning in VLMs, focusing on dynamic difficulty assessment and multi-dimensional process analysis across five domains.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations for adaptive multimodal reasoning rely on static difficulty labels and simplistic metrics, failing to capture dynamic difficulty relative to model capacities and obscuring the distinction between adaptive mode selection and general performance.

Method: Proposes AdaptMMBench with five domains (real-world, OCR, GUI, knowledge, math) using MCC metric to evaluate selection rationality, dynamically identifying task difficulties based on models' capability boundaries, and enabling multi-dimensional process evaluation across key step coverage, tool effectiveness, and computational efficiency.

Result: Evaluation shows adaptive mode selection scales with model capacity but decouples from final accuracy, while key step coverage aligns with performance, and tool effectiveness remains highly inconsistent across model architectures.

Conclusion: AdaptMMBench provides a comprehensive framework for evaluating adaptive multimodal reasoning that isolates meta-cognition ability and enables fine-grained analysis of reasoning processes beyond traditional accuracy metrics.

Abstract: Adaptive multimodal reasoning has emerged as a promising frontier in Vision-Language Models (VLMs), aiming to dynamically modulate between tool-augmented visual reasoning and text reasoning to enhance both effectiveness and efficiency. However, existing evaluations rely on static difficulty labels and simplistic metrics, which fail to capture the dynamic nature of difficulty relative to varying model capacities. Consequently, they obscure the distinction between adaptive mode selection and general performance while neglecting fine-grained process analyses. In this paper, we propose AdaptMMBench, a comprehensive benchmark for adaptive multimodal reasoning across five domains: real-world, OCR, GUI, knowledge, and math, encompassing both direct perception and complex reasoning tasks. AdaptMMBench utilizes a Matthews Correlation Coefficient (MCC) metric to evaluate the selection rationality of different reasoning modes, isolating this meta-cognition ability by dynamically identifying task difficulties based on models' capability boundaries. Moreover, AdaptMMBench facilitates multi-dimensional process evaluation across key step coverage, tool effectiveness, and computational efficiency. Our evaluation reveals that while adaptive mode selection scales with model capacity, it notably decouples from final accuracy. Conversely, key step coverage aligns with performance, though tool effectiveness remains highly inconsistent across model architectures.

</details>


### [3] [End-to-end reconstruction of OCT optical properties and speckle-reduced structural intensity via physics-based learning](https://arxiv.org/abs/2602.02721)
*Jinglun Yu,Yaning Wang,Wenhan Guo,Yuan Gao,Yu Sun,Jin U. Kang*

Main category: cs.CV

TL;DR: Deep learning framework for OCT inverse scattering that jointly recovers optical parameters and structural images using physics-informed supervision.


<details>
  <summary>Details</summary>
Motivation: Inverse scattering in OCT is challenging due to attenuation, speckle noise, and parameter coupling, but recovering both structural images and intrinsic tissue optical properties (refractive index, scattering coefficient, anisotropy) is valuable for quantitative tissue characterization.

Method: Regularized end-to-end deep learning framework trained with Monte Carlo-simulated ground truth, incorporating a physics-based OCT forward model that generates predicted signals from estimated parameters for physics-consistent supervision.

Result: Experiments on synthetic corneal OCT dataset demonstrate robust optical map recovery under noise, improved resolution, and enhanced structural fidelity for layer visualization.

Conclusion: The approach enables quantitative multi-parameter tissue characterization and highlights the benefit of combining physics-informed modeling with deep learning for computational OCT.

Abstract: Inverse scattering in optical coherence tomography (OCT) seeks to recover both structural images and intrinsic tissue optical properties, including refractive index, scattering coefficient, and anisotropy. This inverse problem is challenging due to attenuation, speckle noise, and strong coupling among parameters. We propose a regularized end-to-end deep learning framework that jointly reconstructs optical parameter maps and speckle-reduced OCT structural intensity for layer visualization. Trained with Monte Carlo-simulated ground truth, our network incorporates a physics-based OCT forward model that generates predicted signals from the estimated parameters, providing physics-consistent supervision for parameter recovery and artifact suppression. Experiments on the synthetic corneal OCT dataset demonstrate robust optical map recovery under noise, improved resolution, and enhanced structural fidelity. This approach enables quantitative multi-parameter tissue characterization and highlights the benefit of combining physics-informed modeling with deep learning for computational OCT.

</details>


### [4] [SVD-ViT: Does SVD Make Vision Transformers Attend More to the Foreground?](https://arxiv.org/abs/2602.02765)
*Haruhiko Murata,Kazuhiro Hotta*

Main category: cs.CV

TL;DR: SVD-ViT uses singular value decomposition to help Vision Transformers focus on foreground features instead of background noise, improving classification accuracy.


<details>
  <summary>Details</summary>
Motivation: Vision Transformers lack explicit mechanisms to distinguish foreground from background, causing them to learn unnecessary background features and artifacts that degrade classification performance.

Method: Proposes SVD-ViT with three components: SPC module, SSVA, and ID-RSVD, which use singular value decomposition to extract and aggregate singular vectors capturing object foreground information while suppressing background noise.

Result: Experimental results show improved classification accuracy and effective learning of informative foreground representations while reducing background noise impact.

Conclusion: SVD-ViT successfully addresses ViT's foreground-background discrimination limitation through SVD-based feature prioritization, leading to better classification performance.

Abstract: Vision Transformers (ViT) have been established as large-scale foundation models. However, because self-attention operates globally, they lack an explicit mechanism to distinguish foreground from background. As a result, ViT may learn unnecessary background features and artifacts, leading to degraded classification performance. To address this issue, we propose SVD-ViT, which leverages singular value decomposition (SVD) to prioritize the learning of foreground features. SVD-ViT consists of three components-\textbf{SPC module}, \textbf{SSVA}, and \textbf{ID-RSVD}-and suppresses task-irrelevant factors such as background noise and artifacts by extracting and aggregating singular vectors that capture object foreground information. Experimental results demonstrate that our method improves classification accuracy and effectively learns informative foreground representations while reducing the impact of background noise.

</details>


### [5] [LmPT: Conditional Point Transformer for Anatomical Landmark Detection on 3D Point Clouds](https://arxiv.org/abs/2602.02808)
*Matteo Bastico,Pierre Onghena,David Ryckelynck,Beatriz Marcotegui,Santiago Velasco-Forero,Laurent Corté,Caroline Robine--Decourcelle,Etienne Decencière*

Main category: cs.CV

TL;DR: LmPT is a transformer-based method for automatic anatomical landmark detection on point clouds that enables cross-species learning between human and animal bones.


<details>
  <summary>Details</summary>
Motivation: Manual landmarking is time-consuming and variable, while rule-based methods are limited to specific geometries or landmarks. There's a need for automated methods that can work across different species for translational research.

Method: Landmark Point Transformer (LmPT) uses point cloud representation of anatomical surfaces with a conditioning mechanism for adaptability to different input types, enabling cross-species learning between homologous bones.

Result: The method demonstrates effective generalization across species, evaluated on both human and newly annotated dog femurs, showing successful cross-species landmark detection.

Conclusion: LmPT provides an effective automated solution for anatomical landmark detection that overcomes limitations of traditional methods and enables translational research across species through cross-species learning.

Abstract: Accurate identification of anatomical landmarks is crucial for various medical applications. Traditional manual landmarking is time-consuming and prone to inter-observer variability, while rule-based methods are often tailored to specific geometries or limited sets of landmarks. In recent years, anatomical surfaces have been effectively represented as point clouds, which are lightweight structures composed of spatial coordinates. Following this strategy and to overcome the limitations of existing landmarking techniques, we propose Landmark Point Transformer (LmPT), a method for automatic anatomical landmark detection on point clouds that can leverage homologous bones from different species for translational research. The LmPT model incorporates a conditioning mechanism that enables adaptability to different input types to conduct cross-species learning. We focus the evaluation of our approach on femoral landmarking using both human and newly annotated dog femurs, demonstrating its generalization and effectiveness across species. The code and dog femur dataset will be publicly available at: https://github.com/Pierreoo/LandmarkPointTransformer.

</details>


### [6] [Self-Supervised Uncalibrated Multi-View Video Anonymization in the Operating Room](https://arxiv.org/abs/2602.02850)
*Keqi Chen,Vinkle Srivastav,Armine Vardazaryan,Cindy Rolland,Didier Mutter,Nicolas Padoy*

Main category: cs.CV

TL;DR: Self-supervised multi-view video anonymization framework for operating rooms that achieves over 97% recall without manual annotations or camera calibration.


<details>
  <summary>Details</summary>
Motivation: Privacy preservation is essential for OR video research, but existing methods require manual annotations for each clinical site and camera calibration when cameras are repositioned, creating scalability bottlenecks.

Method: Uses whole-body person detection and pose estimation without annotation/calibration. Retrieves false negatives using temporal and multi-view context, then uses recovered detections as pseudo labels to iteratively fine-tune detectors. Also fine-tunes pose models using their own high-score predictions.

Result: Achieves over 97% recall on 4D-OR dataset and real surgery dataset. Trained real-time whole-body detector using pseudo labels achieves comparable performance, demonstrating practical applicability.

Conclusion: Proposed framework effectively addresses scalability bottlenecks in OR video anonymization by eliminating need for manual annotations and camera calibration while maintaining high detection accuracy.

Abstract: Privacy preservation is a prerequisite for using video data in Operating Room (OR) research. Effective anonymization relies on the exhaustive localization of every individual; even a single missed detection necessitates extensive manual correction. However, existing approaches face two critical scalability bottlenecks: (1) they usually require manual annotations of each new clinical site for high accuracy; (2) while multi-camera setups have been widely adopted to address single-view ambiguity, camera calibration is typically required whenever cameras are repositioned. To address these problems, we propose a novel self-supervised multi-view video anonymization framework consisting of whole-body person detection and whole-body pose estimation, without annotation or camera calibration. Our core strategy is to enhance the single-view detector by "retrieving" false negatives using temporal and multi-view context, and conducting self-supervised domain adaptation. We first run an off-the-shelf whole-body person detector in each view with a low-score threshold to gather candidate detections. Then, we retrieve the low-score false negatives that exhibit consistency with the high-score detections via tracking and self-supervised uncalibrated multi-view association. These recovered detections serve as pseudo labels to iteratively fine-tune the whole-body detector. Finally, we apply whole-body pose estimation on each detected person, and fine-tune the pose model using its own high-score predictions. Experiments on the 4D-OR dataset of simulated surgeries and our dataset of real surgeries show the effectiveness of our approach achieving over 97% recall. Moreover, we train a real-time whole-body detector using our pseudo labels, achieving comparable performance and highlighting our method's practical applicability. Code is available at https://github.com/CAMMA-public/OR_anonymization.

</details>


### [7] [ViThinker: Active Vision-Language Reasoning via Dynamic Perceptual Querying](https://arxiv.org/abs/2602.02873)
*Weihang You,Qingchan Zhu,David Liu,Yi Pan,Geng Yuan,Hanqi Jiang*

Main category: cs.CV

TL;DR: ViThinker enables vision-language models to actively query for task-relevant visual features instead of passively processing pre-computed inputs, improving reasoning accuracy through generative mental simulation.


<details>
  <summary>Details</summary>
Motivation: Current Chain-of-Thought reasoning in vision-language models suffers from premature visual-to-text conversion that discards continuous spatial information, and existing enhancement methods remain passive by processing pre-computed inputs rather than actively seeking task-relevant details.

Method: ViThinker framework enables models to autonomously generate decision tokens that trigger synthesis of expert-aligned visual features on demand. It uses a two-stage curriculum: first distilling frozen experts into model parameters, then learning task-driven querying via sparsity penalties to discover minimal sufficient perception for each reasoning step.

Result: Evaluations across vision-centric benchmarks demonstrate consistent improvements, showing that active query generation outperforms passive approaches in both perceptual grounding and reasoning accuracy.

Conclusion: Active perception through autonomous query generation enables vision-language models to perform generative mental simulation without external tool calls, leading to better reasoning performance than passive approaches.

Abstract: Chain-of-Thought (CoT) reasoning excels in language models but struggles in vision-language models due to premature visual-to-text conversion that discards continuous information such as geometry and spatial layout. While recent methods enhance CoT through static enumeration or attention-based selection, they remain passive, i.e., processing pre-computed inputs rather than actively seeking task-relevant details. Inspired by human active perception, we introduce ViThinker, a framework that enables vision-language models to autonomously generate decision (query) tokens triggering the synthesis of expert-aligned visual features on demand. ViThinker internalizes vision-expert capabilities during training, performing generative mental simulation during inference without external tool calls. Through a two-stage curriculum: first distilling frozen experts into model parameters, then learning task-driven querying via sparsity penalties, i.e., ViThinker discovers minimal sufficient perception for each reasoning step. Evaluations across vision-centric benchmarks demonstrate consistent improvements, validating that active query generation outperforms passive approaches in both perceptual grounding and reasoning accuracy.

</details>


### [8] [DoubleTake: Contrastive Reasoning for Faithful Decision-Making in Medical Imaging](https://arxiv.org/abs/2602.02894)
*Daivik Patel,Shrenik Patel*

Main category: cs.CV

TL;DR: A contrastive document-aware reference selection framework for medical imaging that constructs compact evidence sets optimized for discrimination rather than similarity, improving accuracy by 15% on MediConfusion benchmark.


<details>
  <summary>Details</summary>
Motivation: Existing medical imaging decision-making approaches rely on nearest neighbor retrieval that returns redundant evidence and reinforces single hypotheses, failing to reason over subtle visual differences between confusable conditions.

Method: Contrastive document-aware reference selection framework using ROCO embeddings and metadata to balance visual relevance, embedding diversity, and source-level provenance, plus Counterfactual-Contrastive Inference with structured pairwise visual comparisons and margin-based decision rules.

Result: Achieves state-of-the-art performance on MediConfusion benchmark, improving set-level accuracy by nearly 15% relative to prior methods while reducing confusion and improving individual accuracy.

Conclusion: The proposed framework enables systematic study of contrastive retrieval in medical image reasoning and demonstrates that optimized evidence selection for discrimination rather than similarity significantly improves diagnostic accuracy.

Abstract: Accurate decision making in medical imaging requires reasoning over subtle visual differences between confusable conditions, yet most existing approaches rely on nearest neighbor retrieval that returns redundant evidence and reinforces a single hypothesis. We introduce a contrastive, document-aware reference selection framework that constructs compact evidence sets optimized for discrimination rather than similarity by explicitly balancing visual relevance, embedding diversity, and source-level provenance using ROCO embeddings and metadata. While ROCO provides large-scale image-caption pairs, it does not specify how references should be selected for contrastive reasoning, and naive retrieval frequently yields near-duplicate figures from the same document. To address this gap, we release a reproducible reference selection protocol and curated reference bank that enable a systematic study of contrastive retrieval in medical image reasoning. Building on these contrastive evidence sets, we propose Counterfactual-Contrastive Inference, a confidence-aware reasoning framework that performs structured pairwise visual comparisons and aggregates evidence using margin-based decision rules with faithful abstention. On the MediConfusion benchmark, our approach achieves state-of-the-art performance, improving set-level accuracy by nearly 15% relative to prior methods while reducing confusion and improving individual accuracy.

</details>


### [9] [FaceLinkGen: Rethinking Identity Leakage in Privacy-Preserving Face Recognition with Identity Extraction](https://arxiv.org/abs/2602.02914)
*Wenqi Guo,Shan Du*

Main category: cs.CV

TL;DR: FaceLinkGen attack shows pixel-level reconstruction metrics (PSNR/SSIM) fail to measure real privacy in face recognition systems - identity linkage and regeneration attacks succeed even without pixel recovery.


<details>
  <summary>Details</summary>
Motivation: Current privacy-preserving face recognition (PPFR) evaluations focus on pixel-level reconstruction resistance, but this fails to capture real privacy risks where identity can be extracted without recovering original pixels.

Method: Developed FaceLinkGen attack that performs identity linkage/matching and face regeneration directly from protected templates without pixel recovery. Tested on three recent PPFR systems in both standard and near zero knowledge settings.

Result: FaceLinkGen achieved over 98.5% matching accuracy and above 96% regeneration success on three PPFR systems. Even in near zero knowledge setting, exceeded 92% matching and 94% regeneration, showing pixel distortion metrics don't reflect real privacy.

Conclusion: Visual obfuscation leaves identity information exposed to attackers and untrusted providers. There's a structural gap between current evaluation metrics (PSNR/SSIM) and actual privacy protection needs in face recognition systems.

Abstract: Transformation-based privacy-preserving face recognition (PPFR) aims to verify identities while hiding facial data from attackers and malicious service providers. Existing evaluations mostly treat privacy as resistance to pixel-level reconstruction, measured by PSNR and SSIM. We show that this reconstruction-centric view fails. We present FaceLinkGen, an identity extraction attack that performs linkage/matching and face regeneration directly from protected templates without recovering original pixels. On three recent PPFR systems, FaceLinkGen reaches over 98.5\% matching accuracy and above 96\% regeneration success, and still exceeds 92\% matching and 94\% regeneration in a near zero knowledge setting. These results expose a structural gap between pixel distortion metrics, which are widely used in PPFR evaluation, and real privacy. We show that visual obfuscation leaves identity information broadly exposed to both external intruders and untrusted service providers.

</details>


### [10] [A Multi-scale Linear-time Encoder for Whole-Slide Image Analysis](https://arxiv.org/abs/2602.02918)
*Jagan Mohan Reddy Dwarampudi,Joshua Wong,Hien Van Nguyen,Tania Banerjee*

Main category: cs.CV

TL;DR: MARBLE is the first purely Mamba-based multi-state MIL framework for whole-slide image analysis that processes multiple magnification levels in parallel with linear-time state-space modeling, achieving significant performance improvements over existing methods.


<details>
  <summary>Details</summary>
Motivation: WSI analysis faces challenges due to gigapixel resolutions and hierarchical magnifications. Existing MIL methods typically operate at a single scale, and transformer-based approaches suffer from quadratic attention costs, creating a need for scalable, efficient multi-scale analysis frameworks.

Method: MARBLE uses a purely Mamba-based architecture with parallel multi-scale processing and integrates coarse-to-fine reasoning within a linear-time state-space model. It efficiently captures cross-scale dependencies with minimal parameter overhead by coupling parallel multi-scale processing with linear-time sequence modeling.

Result: Experiments on five public datasets show improvements of up to 6.9% in AUC, 20.3% in accuracy, and 2.3% in C-index, establishing MARBLE as an efficient and generalizable framework for multi-scale WSI analysis.

Conclusion: MARBLE provides a scalable and modular alternative to attention-based architectures for whole-slide image analysis, demonstrating superior performance through its purely Mamba-based multi-state multiple instance learning framework with efficient cross-scale dependency capture.

Abstract: We introduce Multi-scale Adaptive Recurrent Biomedical Linear-time Encoder (MARBLE), the first \textit{purely Mamba-based} multi-state multiple instance learning (MIL) framework for whole-slide image (WSI) analysis. MARBLE processes multiple magnification levels in parallel and integrates coarse-to-fine reasoning within a linear-time state-space model, efficiently capturing cross-scale dependencies with minimal parameter overhead. WSI analysis remains challenging due to gigapixel resolutions and hierarchical magnifications, while existing MIL methods typically operate at a single scale and transformer-based approaches suffer from quadratic attention costs. By coupling parallel multi-scale processing with linear-time sequence modeling, MARBLE provides a scalable and modular alternative to attention-based architectures. Experiments on five public datasets show improvements of up to \textbf{6.9\%} in AUC, \textbf{20.3\%} in accuracy, and \textbf{2.3\%} in C-index, establishing MARBLE as an efficient and generalizable framework for multi-scale WSI analysis.

</details>


### [11] [SRA-Seg: Synthetic to Real Alignment for Semi-Supervised Medical Image Segmentation](https://arxiv.org/abs/2602.02944)
*OFM Riaz Rahman Aranya,Kevin Desai*

Main category: cs.CV

TL;DR: SRA-Seg bridges the domain gap between synthetic and real medical images for segmentation by aligning their feature distributions using DINOv2 embeddings and soft edge blending, achieving strong performance with minimal labeled real data.


<details>
  <summary>Details</summary>
Motivation: Synthetic medical images fail to improve segmentation despite visual realism due to domain gaps between synthetic and real feature spaces that current semi-supervised methods cannot bridge.

Method: SRA-Seg uses similarity-alignment loss with frozen DINOv2 embeddings to pull synthetic features toward real ones, soft edge blending for smooth anatomical transitions, EMA teacher for pseudo-labeling, and soft-segmentation losses for uncertainty handling.

Result: With only 10% labeled real data and 90% synthetic unlabeled data, SRA-Seg achieves 89.34% Dice on ACDC and 84.42% on FIVES, outperforming existing semi-supervised methods and matching performance of methods using real unlabeled data.

Conclusion: SRA-Seg effectively bridges the synthetic-real domain gap for medical image segmentation, enabling synthetic data to meaningfully improve performance with minimal labeled real data.

Abstract: Synthetic data, an appealing alternative to extensive expert-annotated data for medical image segmentation, consistently fails to improve segmentation performance despite its visual realism. The reason being that synthetic and real medical images exist in different semantic feature spaces, creating a domain gap that current semi-supervised learning methods cannot bridge. We propose SRA-Seg, a framework explicitly designed to align synthetic and real feature distributions for medical image segmentation. SRA-Seg introduces a similarity-alignment (SA) loss using frozen DINOv2 embeddings to pull synthetic representations toward their nearest real counterparts in semantic space. We employ soft edge blending to create smooth anatomical transitions and continuous labels, eliminating the hard boundaries from traditional copy-paste augmentation. The framework generates pseudo-labels for synthetic images via an EMA teacher model and applies soft-segmentation losses that respect uncertainty in mixed regions. Our experiments demonstrate strong results: using only 10% labeled real data and 90% synthetic unlabeled data, SRA-Seg achieves 89.34% Dice on ACDC and 84.42% on FIVES, significantly outperforming existing semi-supervised methods and matching the performance of methods using real unlabeled data.

</details>


### [12] [Nüwa: Mending the Spatial Integrity Torn by VLM Token Pruning](https://arxiv.org/abs/2602.02951)
*Yihong Huang,Fei Ma,Yihua Shao,Jingcai Guo,Zitong Yu,Laizhong Cui,Qi Tian*

Main category: cs.CV

TL;DR: Nüwa is a two-stage token pruning framework for Vision Language Models that preserves spatial integrity to improve performance on both VQA and visual grounding tasks.


<details>
  <summary>Details</summary>
Motivation: Existing token pruning methods for VLMs work well for VQA but degrade significantly on visual grounding tasks because they lose global spatial reference frames derived from token positional interactions.

Method: Two-stage framework: 1) After vision encoder, apply separation, alignment, and aggregation operations inspired by swarm intelligence to retain global spatial anchors; 2) Within LLM, perform text-guided pruning to keep task-relevant visual tokens.

Result: Achieves SOTA on VQA benchmarks (94% to 95%) and substantial improvements on visual grounding tasks (7% to 47%).

Conclusion: Nüwa enables efficient feature aggregation while maintaining spatial integrity, addressing limitations of existing pruning methods and improving performance across both VQA and visual grounding tasks.

Abstract: Vision token pruning has proven to be an effective acceleration technique for the efficient Vision Language Model (VLM). However, existing pruning methods demonstrate excellent performance preservation in visual question answering (VQA) and suffer substantial degradation on visual grounding (VG) tasks. Our analysis of the VLM's processing pipeline reveals that strategies utilizing global semantic similarity and attention scores lose the global spatial reference frame, which is derived from the interactions of tokens' positional information. Motivated by these findings, we propose $\text{Nüwa}$, a two-stage token pruning framework that enables efficient feature aggregation while maintaining spatial integrity. In the first stage, after the vision encoder, we apply three operations, namely separation, alignment, and aggregation, which are inspired by swarm intelligence algorithms to retain information-rich global spatial anchors. In the second stage, within the LLM, we perform text-guided pruning to retain task-relevant visual tokens. Extensive experiments demonstrate that $\text{Nüwa}$ achieves SOTA performance on multiple VQA benchmarks (from 94% to 95%) and yields substantial improvements on visual grounding tasks (from 7% to 47%).

</details>


### [13] [TRACE: Temporal Radiology with Anatomical Change Explanation for Grounded X-ray Report Generation](https://arxiv.org/abs/2602.02963)
*OFM Riaz Rahman Aranya,Kevin Desai*

Main category: cs.CV

TL;DR: TRACE is the first model that combines temporal comparison, change classification, and spatial localization for chest X-rays, generating natural language descriptions of interval changes while grounding findings with bounding boxes.


<details>
  <summary>Details</summary>
Motivation: Temporal comparison of chest X-rays is fundamental in clinical radiology for detecting disease progression, treatment response, and new findings. While vision-language models exist for single-image tasks, no current method combines temporal change detection with spatial localization.

Method: TRACE (Temporal Radiology with Anatomical Change Explanation) jointly performs temporal comparison, change classification, and spatial localization. Given prior and current chest X-rays, it generates natural language descriptions of interval changes (worsened, improved, stable) while grounding each finding with bounding box coordinates.

Result: TRACE demonstrates effective spatial localization with over 90% grounding accuracy. The ablation study reveals an emergent capability: change detection only emerges when temporal comparison and spatial grounding are jointly learned, suggesting grounding provides a spatial attention mechanism essential for temporal reasoning.

Conclusion: TRACE establishes a foundation for the challenging task of temporal radiology change detection with spatial grounding, showing that joint learning of temporal comparison and spatial localization is essential for meaningful change detection in medical imaging.

Abstract: Temporal comparison of chest X-rays is fundamental to clinical radiology, enabling detection of disease progression, treatment response, and new findings. While vision-language models have advanced single-image report generation and visual grounding, no existing method combines these capabilities for temporal change detection. We introduce Temporal Radiology with Anatomical Change Explanation (TRACE), the first model that jointly performs temporal comparison, change classification, and spatial localization. Given a prior and current chest X-ray, TRACE generates natural language descriptions of interval changes (worsened, improved, stable) while grounding each finding with bounding box coordinates. TRACE demonstrates effective spatial localization with over 90% grounding accuracy, establishing a foundation for this challenging new task. Our ablation study uncovers an emergent capability: change detection arises only when temporal comparison and spatial grounding are jointly learned, as neither alone enables meaningful change detection. This finding suggests that grounding provides a spatial attention mechanism essential for temporal reasoning.

</details>


### [14] [Dynamic High-frequency Convolution for Infrared Small Target Detection](https://arxiv.org/abs/2602.02969)
*Ruojing Li,Chao Xiao,Qian Yin,Wei An,Nuo Chen,Xinyi Ying,Miao Li,Yingqian Wang*

Main category: cs.CV

TL;DR: Proposes Dynamic High-Frequency Convolution (DHiF) for infrared small target detection, using dynamic local filter banks to discriminatively model high-frequency components and distinguish targets from clutter.


<details>
  <summary>Details</summary>
Motivation: Current learning-based methods for single-frame infrared small target (SIRST) detection neglect explicit modeling and discriminative representation learning of various high-frequency components (HFCs), which is crucial for distinguishing targets from other HFCs like bright corners and broken clouds.

Method: Dynamic High-Frequency Convolution (DHiF) translates discriminative modeling into generation of dynamic local filter banks. DHiF is sensitive to HFCs due to dynamic parameters being symmetrically adjusted within zero-centered range based on Fourier transformation properties. It works with standard convolution to adaptively process different HFC regions and capture distinctive grayscale variation characteristics.

Result: Extensive experiments across different SIRST detection networks on real-scene datasets show DHiF exhibits superior detection performance compared to other state-of-the-art convolution operations, with promising improvement.

Conclusion: DHiF effectively addresses the challenge of distinguishing infrared small targets from other high-frequency components through discriminative representation learning, serving as a drop-in replacement for standard convolution that can be used in arbitrary SIRST detection networks without significant computational efficiency decrease.

Abstract: Infrared small targets are typically tiny and locally salient, which belong to high-frequency components (HFCs) in images. Single-frame infrared small target (SIRST) detection is challenging, since there are many HFCs along with targets, such as bright corners, broken clouds, and other clutters. Current learning-based methods rely on the powerful capabilities of deep networks, but neglect explicit modeling and discriminative representation learning of various HFCs, which is important to distinguish targets from other HFCs. To address the aforementioned issues, we propose a dynamic high-frequency convolution (DHiF) to translate the discriminative modeling process into the generation of a dynamic local filter bank. Especially, DHiF is sensitive to HFCs, owing to the dynamic parameters of its generated filters being symmetrically adjusted within a zero-centered range according to Fourier transformation properties. Combining with standard convolution operations, DHiF can adaptively and dynamically process different HFC regions and capture their distinctive grayscale variation characteristics for discriminative representation learning. DHiF functions as a drop-in replacement for standard convolution and can be used in arbitrary SIRST detection networks without significant decrease in computational efficiency. To validate the effectiveness of our DHiF, we conducted extensive experiments across different SIRST detection networks on real-scene datasets. Compared to other state-of-the-art convolution operations, DHiF exhibits superior detection performance with promising improvement. Codes are available at https://github.com/TinaLRJ/DHiF.

</details>


### [15] [Fisheye Stereo Vision: Depth and Range Error](https://arxiv.org/abs/2602.02973)
*Leaf Jiang,Matthew Holzel,Bernhard Kaplan,Hsiou-Yuan Liu,Sabyasachi Paul,Karen Rankin,Piotr Swierczynski*

Main category: cs.CV

TL;DR: Analytical expressions for depth and range error in fisheye stereo vision, focusing on accuracy at large angles.


<details>
  <summary>Details</summary>
Motivation: Fisheye stereo vision systems have accuracy challenges at large angles; existing models may not adequately address error behavior at extreme viewing angles.

Method: Derived analytical expressions for depth and range error as functions of object distance, specifically accounting for large-angle effects in fisheye stereo systems.

Result: Developed mathematical models that quantify depth and range error behavior, enabling better prediction and compensation of accuracy degradation at large angles.

Conclusion: The derived analytical expressions provide valuable tools for designing and optimizing fisheye stereo vision systems, particularly for applications requiring accurate depth estimation at wide viewing angles.

Abstract: This study derives analytical expressions for the depth and range error of fisheye stereo vision systems as a function of object distance, specifically accounting for accuracy at large angles.

</details>


### [16] [SceneLinker: Compositional 3D Scene Generation via Semantic Scene Graph from RGB Sequences](https://arxiv.org/abs/2602.02974)
*Seok-Young Kim,Dooyoung Kim,Woojin Cho,Hail Song,Suji Kang,Woontack Woo*

Main category: cs.CV

TL;DR: SceneLinker generates compositional 3D scenes from RGB sequences using semantic scene graphs for Mixed Reality applications.


<details>
  <summary>Details</summary>
Motivation: To enable adaptive Mixed Reality content generation that reflects real-world layouts by capturing semantic relationships between objects, addressing limitations of prior works that struggled with contextual relationships and object arrangement alignment.

Method: Uses a graph network with cross-check feature attention for scene graph prediction, and constructs a graph-variational autoencoder (graph-VAE) with joint shape and layout blocks for 3D scene generation.

Result: Outperforms state-of-the-art methods on 3RScan/3DSSG and SG-FRONT datasets in both quantitative and qualitative evaluations, even in complex indoor environments and challenging scene graph constraints.

Conclusion: Enables users to generate consistent 3D spaces from physical environments via scene graphs, facilitating creation of spatial Mixed Reality content.

Abstract: We introduce SceneLinker, a novel framework that generates compositional 3D scenes via semantic scene graph from RGB sequences. To adaptively experience Mixed Reality (MR) content based on each user's space, it is essential to generate a 3D scene that reflects the real-world layout by compactly capturing the semantic cues of the surroundings. Prior works struggled to fully capture the contextual relationship between objects or mainly focused on synthesizing diverse shapes, making it challenging to generate 3D scenes aligned with object arrangements. We address these challenges by designing a graph network with cross-check feature attention for scene graph prediction and constructing a graph-variational autoencoder (graph-VAE), which consists of a joint shape and layout block for 3D scene generation. Experiments on the 3RScan/3DSSG and SG-FRONT datasets demonstrate that our approach outperforms state-of-the-art methods in both quantitative and qualitative evaluations, even in complex indoor environments and under challenging scene graph constraints. Our work enables users to generate consistent 3D spaces from their physical environments via scene graphs, allowing them to create spatial MR content. Project page is https://scenelinker2026.github.io.

</details>


### [17] [Aligning Forest and Trees in Images and Long Captions for Visually Grounded Understanding](https://arxiv.org/abs/2602.02977)
*Byeongju Woo,Zilin Wang,Byeonghyun Pak,Sangwoo Mo,Stella X. Yu*

Main category: cs.CV

TL;DR: CAFT is a hierarchical vision-language model that aligns global and local semantics across images and long captions without pixel-level supervision, achieving SOTA on long-text retrieval benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current vision-language models like CLIP struggle with long captions because they treat images and texts as undifferentiated wholes, lacking fine-grained hierarchical alignment between visual and textual domains.

Method: CAFT couples a fine-to-coarse visual encoder with a hierarchical text transformer, using hierarchical alignment loss that matches whole images with whole captions while biasing region-sentence correspondences, building coarse semantics from fine-grained evidence.

Result: Trained on 30M image-text pairs, CAFT achieves state-of-the-art performance on six long-text retrieval benchmarks and exhibits strong scaling behavior.

Conclusion: Hierarchical cross-domain alignment enables fine-grained, visually grounded image-text representations to emerge without explicit region-level supervision.

Abstract: Large vision-language models such as CLIP struggle with long captions because they align images and texts as undifferentiated wholes. Fine-grained vision-language understanding requires hierarchical semantics capturing both global context and localized details across visual and textual domains. Yet linguistic hierarchies from syntax or semantics rarely match visual organization, and purely visual hierarchies tend to fragment scenes into appearance-driven parts without semantic focus. We propose CAFT (Cross-domain Alignment of Forests and Trees), a hierarchical image-text representation learning framework that aligns global and local semantics across images and long captions without pixel-level supervision. Coupling a fine-to-coarse visual encoder with a hierarchical text transformer, it uses a hierarchical alignment loss that matches whole images with whole captions while biasing region-sentence correspondences, so that coarse semantics are built from fine-grained evidence rather than from aggregation untethered to part-level grounding. Trained on 30M image-text pairs, CAFT achieves state-of-the-art performance on six long-text retrieval benchmarks and exhibits strong scaling behavior. Experiments show that hierarchical cross-domain alignment enables fine-grained, visually grounded image-text representations to emerge without explicit region-level supervision.

</details>


### [18] [SharpTimeGS: Sharp and Stable Dynamic Gaussian Splatting via Lifespan Modulation](https://arxiv.org/abs/2602.02989)
*Zhanfeng Liao,Jiajun Zhang,Hanzhang Tu,Zhixi Wang,Yunqi Gao,Hongwen Zhang,Yebin Liu*

Main category: cs.CV

TL;DR: SharpTimeGS: A lifespan-aware 4D Gaussian framework for dynamic scene reconstruction that achieves temporally adaptive modeling of static and dynamic regions through learnable lifespan parameters and lifespan-velocity-aware densification.


<details>
  <summary>Details</summary>
Motivation: Existing Gaussian-based methods struggle to balance representation and optimization between long-term static and short-term dynamic regions in dynamic scene reconstruction, leading to issues with stability and fidelity.

Method: Introduces learnable lifespan parameters that reformulate temporal visibility from Gaussian decay to flat-top profiles, allowing primitives to remain active over intended durations. Uses lifespan to modulate motion (reducing drift for static points while keeping dynamic motion unrestricted). Implements lifespan-velocity-aware densification strategy to allocate capacity based on motion while keeping static areas compact.

Result: Achieves state-of-the-art performance on multiple benchmarks while supporting real-time rendering up to 4K resolution at 100 FPS on a single RTX 4090 GPU.

Conclusion: SharpTimeGS effectively decouples motion magnitude from temporal duration, improving long-term stability without compromising dynamic fidelity, enabling high-quality 4D reconstruction with balanced static-dynamic region optimization.

Abstract: Novel view synthesis of dynamic scenes is fundamental to achieving photorealistic 4D reconstruction and immersive visual experiences. Recent progress in Gaussian-based representations has significantly improved real-time rendering quality, yet existing methods still struggle to maintain a balance between long-term static and short-term dynamic regions in both representation and optimization. To address this, we present SharpTimeGS, a lifespan-aware 4D Gaussian framework that achieves temporally adaptive modeling of both static and dynamic regions under a unified representation. Specifically, we introduce a learnable lifespan parameter that reformulates temporal visibility from a Gaussian-shaped decay into a flat-top profile, allowing primitives to remain consistently active over their intended duration and avoiding redundant densification. In addition, the learned lifespan modulates each primitives' motion, reducing drift in long-lived static points while retaining unrestricted motion for short-lived dynamic ones. This effectively decouples motion magnitude from temporal duration, improving long-term stability without compromising dynamic fidelity. Moreover, we design a lifespan-velocity-aware densification strategy that mitigates optimization imbalance between static and dynamic regions by allocating more capacity to regions with pronounced motion while keeping static areas compact and stable. Extensive experiments on multiple benchmarks demonstrate that our method achieves state-of-the-art performance while supporting real-time rendering up to 4K resolution at 100 FPS on one RTX 4090.

</details>


### [19] [Video-OPD: Efficient Post-Training of Multimodal Large Language Models for Temporal Video Grounding via On-Policy Distillation](https://arxiv.org/abs/2602.02994)
*Jiaze Li,Hao Yin,Haoran Xu,Boshen Xu,Wenhui Tan,Zewen He,Jianzhong Ju,Zhenbo Luo,Jian Luan*

Main category: cs.CV

TL;DR: Video-OPD: Efficient post-training framework for Temporal Video Grounding using on-policy distillation with dense token-level supervision and Teacher-Validated Disagreement Focusing curriculum.


<details>
  <summary>Details</summary>
Motivation: Existing GRPO-based reinforcement learning methods for Temporal Video Grounding suffer from sparse reward signals and high computational overhead, limiting their effectiveness and efficiency.

Method: Video-OPD uses on-policy distillation with a frontier teacher providing dense token-level supervision via reverse KL divergence. TVDF curriculum prioritizes teacher-reliable, informative trajectories for efficient training.

Result: Video-OPD consistently outperforms GRPO while achieving substantially faster convergence and lower computational cost.

Conclusion: On-policy distillation is an effective alternative to conventional reinforcement learning for Temporal Video Grounding, offering better performance with improved efficiency.

Abstract: Reinforcement learning has emerged as a principled post-training paradigm for Temporal Video Grounding (TVG) due to its on-policy optimization, yet existing GRPO-based methods remain fundamentally constrained by sparse reward signals and substantial computational overhead. We propose Video-OPD, an efficient post-training framework for TVG inspired by recent advances in on-policy distillation. Video-OPD optimizes trajectories sampled directly from the current policy, thereby preserving alignment between training and inference distributions, while a frontier teacher supplies dense, token-level supervision via a reverse KL divergence objective. This formulation preserves the on-policy property critical for mitigating distributional shift, while converting sparse, episode-level feedback into fine-grained, step-wise learning signals. Building on Video-OPD, we introduce Teacher-Validated Disagreement Focusing (TVDF), a lightweight training curriculum that iteratively prioritizes trajectories that are both teacher-reliable and maximally informative for the student, thereby improving training efficiency. Empirical results demonstrate that Video-OPD consistently outperforms GRPO while achieving substantially faster convergence and lower computational cost, establishing on-policy distillation as an effective alternative to conventional reinforcement learning for TVG.

</details>


### [20] [VOILA: Value-of-Information Guided Fidelity Selection for Cost-Aware Multimodal Question Answering](https://arxiv.org/abs/2602.03007)
*Rahul Atul Bhope,K. R. Jayaram,Vinod Muthusamy,Ritesh Kumar,Vatche Isahagian,Nalini Venkatasubramanian*

Main category: cs.CV

TL;DR: VOILA is a framework that optimizes visual fidelity selection before model execution to reduce costs while maintaining accuracy in VQA tasks.


<details>
  <summary>Details</summary>
Motivation: Most multimodal vision-language systems operate at fixed fidelity levels despite significant costs from retrieving and processing high-fidelity visual inputs, creating inefficiency.

Method: Two-stage pipeline: 1) gradient-boosted regressor estimates correctness likelihood at each fidelity from question features alone, 2) isotonic calibrator refines probabilities for reliable decision-making, then selects minimum-cost fidelity maximizing expected utility.

Result: Achieves 50-60% cost reductions while retaining 90-95% of full-resolution accuracy across diverse query types and model architectures (7B-235B parameters) on five datasets.

Conclusion: Pre-retrieval fidelity selection is vital to optimize multimodal inference under resource constraints, demonstrating significant efficiency gains without substantial accuracy loss.

Abstract: Despite significant costs from retrieving and processing high-fidelity visual inputs, most multimodal vision-language systems operate at fixed fidelity levels. We introduce VOILA, a framework for Value-Of-Information-driven adaptive fidelity selection in Visual Question Answering (VQA) that optimizes what information to retrieve before model execution. Given a query, VOILA uses a two-stage pipeline: a gradient-boosted regressor estimates correctness likelihood at each fidelity from question features alone, then an isotonic calibrator refines these probabilities for reliable decision-making. The system selects the minimum-cost fidelity maximizing expected utility given predicted accuracy and retrieval costs. We evaluate VOILA across three deployment scenarios using five datasets (VQA-v2, GQA, TextVQA, LoCoMo, FloodNet) and six Vision-Language Models (VLMs) with 7B-235B parameters. VOILA consistently achieves 50-60% cost reductions while retaining 90-95% of full-resolution accuracy across diverse query types and model architectures, demonstrating that pre-retrieval fidelity selection is vital to optimize multimodal inference under resource constraints.

</details>


### [21] [Thinking inside the Convolution for Image Inpainting: Reconstructing Texture via Structure under Global and Local Side](https://arxiv.org/abs/2602.03013)
*Haipeng Liu,Yang Wang,Biao Qian,Yong Rui,Meng Wang*

Main category: cs.CV

TL;DR: The paper proposes a method to address information loss in image inpainting by using statistical normalization/denormalization to guide structure and texture feature map reconstruction during convolutional downsampling.


<details>
  <summary>Details</summary>
Motivation: Current CNN-based image inpainting methods suffer from information loss of both structure and texture feature maps during convolutional downsampling, leading to non-ideal upsampling outputs. The paper aims to systematically address whether and how structure and texture feature maps can mutually help alleviate this information loss.

Method: The authors adopt statistical normalization and denormalization strategy for reconstruction guidance during the convolutional downsampling process, using structure and texture feature maps to mutually help preserve information.

Result: Extensive experiments show advantages over state-of-the-art methods on images from low-to-high resolutions (256×256 and 512×512), with particularly strong performance when substituting all encoders with their proposed approach.

Conclusion: The proposed statistical normalization/denormalization strategy effectively mitigates information loss during convolutional downsampling in image inpainting, improving reconstruction quality across various resolutions.

Abstract: Image inpainting has earned substantial progress, owing to the encoder-and-decoder pipeline, which is benefited from the Convolutional Neural Networks (CNNs) with convolutional downsampling to inpaint the masked regions semantically from the known regions within the encoder, coupled with an upsampling process from the decoder for final inpainting output. Recent studies intuitively identify the high-frequency structure and low-frequency texture to be extracted by CNNs from the encoder, and subsequently for a desirable upsampling recovery. However, the existing arts inevitably overlook the information loss for both structure and texture feature maps during the convolutional downsampling process, hence suffer from a non-ideal upsampling output. In this paper, we systematically answer whether and how the structure and texture feature map can mutually help to alleviate the information loss during the convolutional downsampling. Given the structure and texture feature maps, we adopt the statistical normalization and denormalization strategy for the reconstruction guidance during the convolutional downsampling process. The extensive experimental results validate its advantages to the state-of-the-arts over the images from low-to-high resolutions including 256*256 and 512*512, especially holds by substituting all the encoders by ours. Our code is available at https://github.com/htyjers/ConvInpaint-TSGL

</details>


### [22] [A Vision-Based Analysis of Congestion Pricing in New York City](https://arxiv.org/abs/2602.03015)
*Mehmet Kerem Turkcan,Jhonatan Tavori,Javad Ghaderi,Gil Zussman,Zoran Kostic,Andrew Smyth*

Main category: cs.CV

TL;DR: Automated analysis of NYC traffic camera data shows systematic changes in vehicle density following congestion pricing implementation from Jan 2025 to Jan 2026.


<details>
  <summary>Details</summary>
Motivation: To empirically examine the impact of New York City's congestion pricing program on traffic patterns using objective, automated data analysis.

Method: Computer vision pipeline processing footage from over 900 traffic cameras throughout Manhattan and NYC, comparing traffic patterns from Nov 2024 (baseline) through program implementation in Jan 2025 until Jan 2026.

Result: Established baseline traffic patterns and identified systematic changes in vehicle density across the monitored region following congestion pricing implementation.

Conclusion: Congestion pricing in NYC led to measurable, systematic changes in traffic density patterns as detected through automated camera analysis.

Abstract: We examine the impact of New York City's congestion pricing program through automated analysis of traffic camera data. Our computer vision pipeline processes footage from over 900 cameras distributed throughout Manhattan and New York, comparing traffic patterns from November 2024 through the program's implementation in January 2025 until January 2026. We establish baseline traffic patterns and identify systematic changes in vehicle density across the monitored region.

</details>


### [23] [MUSE: A Multi-agent Framework for Unconstrained Story Envisioning via Closed-Loop Cognitive Orchestration](https://arxiv.org/abs/2602.03028)
*Wenzhang Sun,Zhenyu Wang,Zhangchi Hu,Chunfeng Wang,Hao Li,Wei Chen*

Main category: cs.CV

TL;DR: MUSE is a multi-agent framework for generating long-form audio-visual stories that uses iterative plan-execute-verify-revise loops to maintain narrative coherence and identity consistency over extended sequences.


<details>
  <summary>Details</summary>
Motivation: Existing approaches to generating long-form audio-visual stories suffer from semantic drift and identity inconsistency as sequences grow longer, creating an intent-execution gap where high-level narrative intent isn't preserved across coherent shot-level multimodal generation.

Method: MUSE formulates storytelling as a closed-loop constraint enforcement problem using a multi-agent framework with iterative plan-execute-verify-revise loops. It translates narrative intent into explicit machine-executable controls over identity, spatial composition, and temporal continuity, applying targeted multimodal feedback to correct violations during generation.

Result: MUSE substantially improves long-horizon narrative coherence, cross-modal identity consistency, and cinematic quality compared with representative baselines, as evaluated through the MUSEBench reference-free evaluation protocol validated by human judgments.

Conclusion: The closed-loop, multi-agent approach of MUSE effectively addresses the intent-execution gap in long-form audio-visual storytelling by enforcing narrative constraints through iterative verification and revision, leading to more coherent and consistent multimodal story generation.

Abstract: Generating long-form audio-visual stories from a short user prompt remains challenging due to an intent-execution gap, where high-level narrative intent must be preserved across coherent, shot-level multimodal generation over long horizons. Existing approaches typically rely on feed-forward pipelines or prompt-only refinement, which often leads to semantic drift and identity inconsistency as sequences grow longer. We address this challenge by formulating storytelling as a closed-loop constraint enforcement problem and propose MUSE, a multi-agent framework that coordinates generation through an iterative plan-execute-verify-revise loop. MUSE translates narrative intent into explicit, machine-executable controls over identity, spatial composition, and temporal continuity, and applies targeted multimodal feedback to correct violations during generation. To evaluate open-ended storytelling without ground-truth references, we introduce MUSEBench, a reference-free evaluation protocol validated by human judgments. Experiments demonstrate that MUSE substantially improves long-horizon narrative coherence, cross-modal identity consistency, and cinematic quality compared with representative baselines.

</details>


### [24] [Bongards at the Boundary of Perception and Reasoning: Programs or Language?](https://arxiv.org/abs/2602.03038)
*Cassidy Langenfeld,Claas Beger,Gloria Geng,Wasu Top Piriyakulkij,Keya Hu,Yewen Pu,Kevin Ellis*

Main category: cs.CV

TL;DR: Neurosymbolic approach combining LLMs and Bayesian optimization solves Bongard problems, classic visual reasoning challenges that test human-like generalization to novel situations.


<details>
  <summary>Details</summary>
Motivation: While VLMs excel at everyday visual tasks, they struggle with the radical generalization required by Bongard problems, which test human-like visual reasoning in completely novel situations. The paper aims to bridge this gap.

Method: Neurosymbolic approach: 1) Use LLMs to generate parameterized programmatic representations of hypothesized solution rules for Bongard problems, 2) Perform parameter fitting using Bayesian optimization to find optimal rule parameters.

Result: The method is evaluated on two tasks: 1) Classifying Bongard problem images given ground truth rules, 2) Solving problems from scratch without prior rule knowledge. Results demonstrate the approach's effectiveness on these challenging visual reasoning tasks.

Conclusion: The neurosymbolic framework combining LLMs for rule generation and Bayesian optimization for parameter fitting provides a promising approach to solving Bongard problems, advancing visual reasoning capabilities toward more human-like generalization.

Abstract: Vision-Language Models (VLMs) have made great strides in everyday visual tasks, such as captioning a natural image, or answering commonsense questions about such images. But humans possess the puzzling ability to deploy their visual reasoning abilities in radically new situations, a skill rigorously tested by the classic set of visual reasoning challenges known as the Bongard problems. We present a neurosymbolic approach to solving these problems: given a hypothesized solution rule for a Bongard problem, we leverage LLMs to generate parameterized programmatic representations for the rule and perform parameter fitting using Bayesian optimization. We evaluate our method on classifying Bongard problem images given the ground truth rule, as well as on solving the problems from scratch.

</details>


### [25] [HP-GAN: Harnessing pretrained networks for GAN improvement with FakeTwins and discriminator consistency](https://arxiv.org/abs/2602.03039)
*Geonhui Son,Jeong Ryong Lee,Dosik Hwang*

Main category: cs.CV

TL;DR: HP-GAN improves GAN image synthesis by using pretrained networks with self-supervised learning (FakeTwins) and discriminator consistency between CNN and ViT features, achieving better FID scores across diverse datasets.


<details>
  <summary>Details</summary>
Motivation: Current GAN methods use pretrained networks for perceptual losses, but don't fully exploit their potential. The paper aims to enhance pretrained network utilization through self-supervised learning and better discriminator coordination.

Method: HP-GAN introduces two key strategies: 1) FakeTwins - uses pretrained networks as encoders to compute self-supervised loss for generator training, and 2) Discriminator consistency - aligns assessments between CNN and ViT feature-based discriminators to promote coherent learning.

Result: Extensive evaluation across 17 datasets (varying sizes and domains) shows HP-GAN consistently outperforms state-of-the-art methods in FID scores, with significant improvements in image diversity and quality.

Conclusion: HP-GAN successfully enhances GAN performance by better exploiting neural network priors through self-supervised learning and discriminator consistency, demonstrating robust improvements across diverse image synthesis scenarios.

Abstract: Generative Adversarial Networks (GANs) have made significant progress in enhancing the quality of image synthesis. Recent methods frequently leverage pretrained networks to calculate perceptual losses or utilize pretrained feature spaces. In this paper, we extend the capabilities of pretrained networks by incorporating innovative self-supervised learning techniques and enforcing consistency between discriminators during GAN training. Our proposed method, named HP-GAN, effectively exploits neural network priors through two primary strategies: FakeTwins and discriminator consistency. FakeTwins leverages pretrained networks as encoders to compute a self-supervised loss and applies this through the generated images to train the generator, thereby enabling the generation of more diverse and high quality images. Additionally, we introduce a consistency mechanism between discriminators that evaluate feature maps extracted from Convolutional Neural Network (CNN) and Vision Transformer (ViT) feature networks. Discriminator consistency promotes coherent learning among discriminators and enhances training robustness by aligning their assessments of image quality. Our extensive evaluation across seventeen datasets-including scenarios with large, small, and limited data, and covering a variety of image domains-demonstrates that HP-GAN consistently outperforms current state-of-the-art methods in terms of Fréchet Inception Distance (FID), achieving significant improvements in image diversity and quality. Code is available at: https://github.com/higun2/HP-GAN.

</details>


### [26] [IVC-Prune: Revealing the Implicit Visual Coordinates in LVLMs for Vision Token Pruning](https://arxiv.org/abs/2602.03060)
*Zhichao Sun,Yidong Ma,Gang Liu,Yibo Chen,Xu Tang,Yao Hu,Yongchao Xu*

Main category: cs.CV

TL;DR: IVC-Prune is a training-free pruning method that reduces visual tokens by ~50% while maintaining ≥99% of original performance by preserving both implicit visual coordinate tokens (essential for spatial reasoning) and semantically relevant foreground tokens.


<details>
  <summary>Details</summary>
Motivation: Large Vision-Language Models have prohibitive inference costs when processing high-resolution visual inputs. Existing visual token pruning methods focus on semantic relevance but discard tokens crucial for spatial reasoning, creating a performance gap.

Method: IVC-Prune identifies implicit visual coordinate (IVC) tokens through theoretical analysis of Rotary Position Embeddings' mathematical properties, targeting positions where rotation matrices approximate identity or 90° rotation. It also identifies foreground tokens via two-stage process: semantic seed discovery followed by contextual refinement using value-vector similarity.

Result: Extensive evaluations across 4 LVLMs and 20 benchmarks show IVC-Prune reduces visual tokens by approximately 50% while maintaining ≥99% of original performance, with even improvements on several benchmarks.

Conclusion: The paper reveals LVLMs establish implicit visual coordinate systems through RoPE, and IVC-Prune effectively preserves both spatial reasoning capability and semantic relevance through training-free, prompt-aware pruning, significantly reducing inference costs without performance degradation.

Abstract: Large Vision-Language Models (LVLMs) achieve impressive performance across multiple tasks. A significant challenge, however, is their prohibitive inference cost when processing high-resolution visual inputs. While visual token pruning has emerged as a promising solution, existing methods that primarily focus on semantic relevance often discard tokens that are crucial for spatial reasoning. We address this gap through a novel insight into \emph{how LVLMs process spatial reasoning}. Specifically, we reveal that LVLMs implicitly establish visual coordinate systems through Rotary Position Embeddings (RoPE), where specific token positions serve as \textbf{implicit visual coordinates} (IVC tokens) that are essential for spatial reasoning. Based on this insight, we propose \textbf{IVC-Prune}, a training-free, prompt-aware pruning strategy that retains both IVC tokens and semantically relevant foreground tokens. IVC tokens are identified by theoretically analyzing the mathematical properties of RoPE, targeting positions at which its rotation matrices approximate identity matrix or the $90^\circ$ rotation matrix. Foreground tokens are identified through a robust two-stage process: semantic seed discovery followed by contextual refinement via value-vector similarity. Extensive evaluations across four representative LVLMs and twenty diverse benchmarks show that IVC-Prune reduces visual tokens by approximately 50\% while maintaining $\geq$ 99\% of the original performance and even achieving improvements on several benchmarks. Source codes are available at https://github.com/FireRedTeam/IVC-Prune.

</details>


### [27] [JRDB-Pose3D: A Multi-person 3D Human Pose and Shape Estimation Dataset for Robotics](https://arxiv.org/abs/2602.03064)
*Sandika Biswas,Kian Izadpanah,Hamid Rezatofighi*

Main category: cs.CV

TL;DR: JRDB-Pose3D is a new dataset for 3D human pose estimation in crowded real-world scenes, captured from a mobile robot platform with rich annotations including SMPL-based poses, track IDs, and social context information.


<details>
  <summary>Details</summary>
Motivation: Existing 3D human pose datasets focus on single-person scenes or controlled lab environments, limiting their relevance to real-world applications like autonomous driving and human-robot interaction where crowded scenes are common.

Method: Created JRDB-Pose3D by capturing multi-human indoor and outdoor environments from a mobile robotic platform, providing SMPL-based 3D pose annotations with consistent body-shape parameters and track IDs for each individual over time.

Result: The dataset contains 5-10 human poses per frame on average, with some scenes featuring up to 35 individuals simultaneously, presenting challenges like occlusions, truncated bodies, and out-of-frame body parts that reflect real-world complexity.

Conclusion: JRDB-Pose3D bridges the gap between controlled lab datasets and real-world applications by providing comprehensive 3D human pose data in crowded environments, with inherited social context annotations making it valuable for various human-centric perception tasks.

Abstract: Real-world scenes are inherently crowded. Hence, estimating 3D poses of all nearby humans, tracking their movements over time, and understanding their activities within social and environmental contexts are essential for many applications, such as autonomous driving, robot perception, robot navigation, and human-robot interaction. However, most existing 3D human pose estimation datasets primarily focus on single-person scenes or are collected in controlled laboratory environments, which restricts their relevance to real-world applications. To bridge this gap, we introduce JRDB-Pose3D, which captures multi-human indoor and outdoor environments from a mobile robotic platform. JRDB-Pose3D provides rich 3D human pose annotations for such complex and dynamic scenes, including SMPL-based pose annotations with consistent body-shape parameters and track IDs for each individual over time. JRDB-Pose3D contains, on average, 5-10 human poses per frame, with some scenes featuring up to 35 individuals simultaneously. The proposed dataset presents unique challenges, including frequent occlusions, truncated bodies, and out-of-frame body parts, which closely reflect real-world environments. Moreover, JRDB-Pose3D inherits all available annotations from the JRDB dataset, such as 2D pose, information about social grouping, activities, and interactions, full-scene semantic masks with consistent human- and object-level tracking, and detailed annotations for each individual, such as age, gender, and race, making it a holistic dataset for a wide range of downstream perception and human-centric understanding tasks.

</details>


### [28] [Finding Optimal Video Moment without Training: Gaussian Boundary Optimization for Weakly Supervised Video Grounding](https://arxiv.org/abs/2602.03071)
*Sunoh Kim,Kimin Yun,Daeho Um*

Main category: cs.CV

TL;DR: GBO is a novel inference framework that optimizes segment boundaries through a principled optimization problem, improving weakly supervised video grounding without requiring training.


<details>
  <summary>Details</summary>
Motivation: Existing Gaussian-based temporal proposal methods rely on heuristic mappings from Gaussian parameters to segment boundaries, leading to suboptimal localization performance in weakly supervised video grounding.

Method: Proposes Gaussian Boundary Optimization (GBO), a training-free inference framework that predicts segment boundaries by solving an optimization problem balancing proposal coverage and segment compactness, with closed-form solutions and compatibility with various proposal architectures.

Result: GBO significantly improves localization performance, achieving state-of-the-art results across standard benchmarks, with demonstrated efficiency and generalizability across different proposal schemes.

Conclusion: GBO provides a theoretically grounded, practical solution for weakly supervised video grounding that outperforms heuristic approaches while being training-free and compatible with existing proposal architectures.

Abstract: Weakly supervised temporal video grounding aims to localize query-relevant segments in untrimmed videos using only video-sentence pairs, without requiring ground-truth segment annotations that specify exact temporal boundaries. Recent approaches tackle this task by utilizing Gaussian-based temporal proposals to represent query-relevant segments. However, their inference strategies rely on heuristic mappings from Gaussian parameters to segment boundaries, resulting in suboptimal localization performance. To address this issue, we propose Gaussian Boundary Optimization (GBO), a novel inference framework that predicts segment boundaries by solving a principled optimization problem that balances proposal coverage and segment compactness. We derive a closed-form solution for this problem and rigorously analyze the optimality conditions under varying penalty regimes. Beyond its theoretical foundations, GBO offers several practical advantages: it is training-free and compatible with both single-Gaussian and mixture-based proposal architectures. Our experiments show that GBO significantly improves localization, achieving state-of-the-art results across standard benchmarks. Extensive experiments demonstrate the efficiency and generalizability of GBO across various proposal schemes. The code is available at \href{https://github.com/sunoh-kim/gbo}{https://github.com/sunoh-kim/gbo}.

</details>


### [29] [A generalizable large-scale foundation model for musculoskeletal radiographs](https://arxiv.org/abs/2602.03076)
*Shinn Kim,Soobin Lee,Kyoungseob Shin,Han-Soo Kim,Yongsung Kim,Minsu Kim,Juhong Nam,Somang Ko,Daeheon Kwon,Wook Huh,Ilkyu Han,Sunghoon Kwon*

Main category: cs.CV

TL;DR: SKELEX is a large-scale foundation model for musculoskeletal radiographs trained on 1.2M images using self-supervised learning, outperforming baselines on 12 diagnostic tasks and enabling zero-shot abnormality localization.


<details>
  <summary>Details</summary>
Motivation: Current AI models for musculoskeletal imaging are task-specific, annotation-dependent, and lack generalizability across diseases and anatomical regions. There's a clinical need for a generalizable foundation model, but public datasets are too limited in size and diversity.

Method: Developed SKELEX using self-supervised learning on 1.2 million diverse musculoskeletal radiographs. Created an interpretable, region-guided model for bone tumor prediction based on SKELEX's zero-shot localization capability.

Result: Outperformed baselines on 12 downstream diagnostic tasks including fracture detection, osteoarthritis grading, and bone tumor classification. Demonstrated zero-shot abnormality localization without task-specific training. Maintained robust performance on independent external datasets and deployed as a publicly accessible web application.

Conclusion: SKELEX provides a scalable, label-efficient, and generalizable AI framework for musculoskeletal imaging, establishing a foundation for clinical translation and data-efficient research in musculoskeletal radiology.

Abstract: Artificial intelligence (AI) has shown promise in detecting and characterizing musculoskeletal diseases from radiographs. However, most existing models remain task-specific, annotation-dependent, and limited in generalizability across diseases and anatomical regions. Although a generalizable foundation model trained on large-scale musculoskeletal radiographs is clinically needed, publicly available datasets remain limited in size and lack sufficient diversity to enable training across a wide range of musculoskeletal conditions and anatomical sites. Here, we present SKELEX, a large-scale foundation model for musculoskeletal radiographs, trained using self-supervised learning on 1.2 million diverse, condition-rich images. The model was evaluated on 12 downstream diagnostic tasks and generally outperformed baselines in fracture detection, osteoarthritis grading, and bone tumor classification. Furthermore, SKELEX demonstrated zero-shot abnormality localization, producing error maps that identified pathologic regions without task-specific training. Building on this capability, we developed an interpretable, region-guided model for predicting bone tumors, which maintained robust performance on independent external datasets and was deployed as a publicly accessible web application. Overall, SKELEX provides a scalable, label-efficient, and generalizable AI framework for musculoskeletal imaging, establishing a foundation for both clinical translation and data-efficient research in musculoskeletal radiology.

</details>


### [30] [Gromov Wasserstein Optimal Transport for Semantic Correspondences](https://arxiv.org/abs/2602.03105)
*Francis Snelgar,Stephen Gould,Ming Xu,Liang Zheng,Akshay Asthana*

Main category: cs.CV

TL;DR: Replaces Stable Diffusion features with optimal transport matching to boost DINOv2's semantic correspondence performance while being 5-10x more efficient.


<details>
  <summary>Details</summary>
Motivation: Current state-of-the-art semantic correspondence methods combine DINOv2 and Stable Diffusion features, but this ensemble approach is computationally expensive. The authors seek a more efficient alternative that maintains spatial consistency without using Stable Diffusion.

Method: Replaces standard nearest neighbor matching with an optimal transport algorithm that includes a Gromov-Wasserstein spatial smoothness prior, working with DINOv2 features alone instead of combining with Stable Diffusion features.

Result: Significantly boosts DINOv2 baseline performance, achieving competitive or superior results compared to state-of-the-art methods using Stable Diffusion features, while being 5-10x more computationally efficient.

Conclusion: Optimal transport with spatial smoothness priors can effectively replace Stable Diffusion features for semantic correspondence, providing similar spatial consistency benefits with much lower computational cost.

Abstract: Establishing correspondences between image pairs is a long studied problem in computer vision. With recent large-scale foundation models showing strong zero-shot performance on downstream tasks including classification and segmentation, there has been interest in using the internal feature maps of these models for the semantic correspondence task. Recent works observe that features from DINOv2 and Stable Diffusion (SD) are complementary, the former producing accurate but sparse correspondences, while the latter produces spatially consistent correspondences. As a result, current state-of-the-art methods for semantic correspondence involve combining features from both models in an ensemble. While the performance of these methods is impressive, they are computationally expensive, requiring evaluating feature maps from large-scale foundation models. In this work we take a different approach, instead replacing SD features with a superior matching algorithm which is imbued with the desirable spatial consistency property. Specifically, we replace the standard nearest neighbours matching with an optimal transport algorithm that includes a Gromov Wasserstein spatial smoothness prior. We show that we can significantly boost the performance of the DINOv2 baseline, and be competitive and sometimes surpassing state-of-the-art methods using Stable Diffusion features, while being 5--10x more efficient. We make code available at https://github.com/fsnelgar/semantic_matching_gwot .

</details>


### [31] [Beyond Cropping and Rotation: Automated Evolution of Powerful Task-Specific Augmentations with Generative Models](https://arxiv.org/abs/2602.03123)
*Judah Goldfeder,Shreyes Kaliyur,Vaibhav Sourirajan,Patrick Minwan Puma,Philippe Martin Wyder,Yuhang Hu,Jiong Lin,Hod Lipson*

Main category: cs.CV

TL;DR: EvoAug is an automated augmentation learning pipeline that uses generative models and evolutionary algorithms to learn optimal task-specific augmentations through hierarchical stochastic augmentation trees.


<details>
  <summary>Details</summary>
Motivation: While generative models offer new possibilities for data augmentation with greater diversity and realism, they risk degrading performance if poorly matched to the task. Traditional automated augmentation methods like AutoAugment don't fully leverage these generative capabilities.

Method: EvoAug combines generative models (conditional diffusion, few-shot NeRFs) with an efficient evolutionary algorithm to learn stochastic augmentation trees that hierarchically compose augmentations for structured, adaptive transformations.

Result: Strong performance across fine-grained classification and few-shot learning tasks, with discovered augmentations aligning with domain knowledge even in low-data settings.

Conclusion: EvoAug demonstrates the potential of learned generative augmentations for robust model training, unlocking new possibilities beyond traditional augmentation methods.

Abstract: Data augmentation has long been a cornerstone for reducing overfitting in vision models, with methods like AutoAugment automating the design of task-specific augmentations. Recent advances in generative models, such as conditional diffusion and few-shot NeRFs, offer a new paradigm for data augmentation by synthesizing data with significantly greater diversity and realism. However, unlike traditional augmentations like cropping or rotation, these methods introduce substantial changes that enhance robustness but also risk degrading performance if the augmentations are poorly matched to the task. In this work, we present EvoAug, an automated augmentation learning pipeline, which leverages these generative models alongside an efficient evolutionary algorithm to learn optimal task-specific augmentations. Our pipeline introduces a novel approach to image augmentation that learns stochastic augmentation trees that hierarchically compose augmentations, enabling more structured and adaptive transformations. We demonstrate strong performance across fine-grained classification and few-shot learning tasks. Notably, our pipeline discovers augmentations that align with domain knowledge, even in low-data settings. These results highlight the potential of learned generative augmentations, unlocking new possibilities for robust model training.

</details>


### [32] [Feature, Alignment, and Supervision in Category Learning: A Comparative Approach with Children and Neural Networks](https://arxiv.org/abs/2602.03124)
*Fanxiao Wani Qiu,Oscar Leong*

Main category: cs.CV

TL;DR: Children and CNNs show different learning patterns in few-shot semi-supervised category learning - children generalize rapidly with feature biases, while CNNs benefit more from added supervision depending on feature structure and alignment.


<details>
  <summary>Details</summary>
Motivation: To understand how humans and machines learn from sparse data by comparing children and convolutional neural networks under identical conditions in a few-shot semi-supervised category learning task.

Method: Species-fair design comparing children and CNNs on few-shot semi-supervised category learning with novel object categories. Both received mixtures of labeled/unlabeled exemplars while varying supervision (1/3/6 labels), target feature (size, shape, pattern), and perceptual alignment (high/low).

Result: Children generalized rapidly from minimal labels but showed strong feature-specific biases and sensitivity to alignment. CNNs showed different interaction patterns: added supervision improved performance, but both alignment and feature structure moderated the impact of additional supervision on learning.

Conclusion: Human-model comparisons must be drawn under the right conditions, emphasizing interactions among supervision, feature structure, and alignment rather than overall accuracy.

Abstract: Understanding how humans and machines learn from sparse data is central to cognitive science and machine learning. Using a species-fair design, we compare children and convolutional neural networks (CNNs) in a few-shot semi-supervised category learning task. Both learners are exposed to novel object categories under identical conditions. Learners receive mixtures of labeled and unlabeled exemplars while we vary supervision (1/3/6 labels), target feature (size, shape, pattern), and perceptual alignment (high/low). We find that children generalize rapidly from minimal labels but show strong feature-specific biases and sensitivity to alignment. CNNs show a different interaction profile: added supervision improves performance, but both alignment and feature structure moderate the impact additional supervision has on learning. These results show that human-model comparisons must be drawn under the right conditions, emphasizing interactions among supervision, feature structure, and alignment rather than overall accuracy.

</details>


### [33] [Flexible Geometric Guidance for Probabilistic Human Pose Estimation with Diffusion Models](https://arxiv.org/abs/2602.03126)
*Francis Snelgar,Ming Xu,Stephen Gould,Liang Zheng,Akshay Asthana*

Main category: cs.CV

TL;DR: Diffusion-based framework for 3D human pose estimation that samples multiple plausible poses from 2D images without requiring paired 2D-3D training data.


<details>
  <summary>Details</summary>
Motivation: Traditional methods assume deterministic mapping from 2D to 3D poses, ignoring inherent ambiguity and occlusion issues. Machine learning approaches require large paired datasets and suffer from generalization problems.

Method: Uses diffusion models with guidance framework: unconditional diffusion model trained only on 3D data, guided by gradients from 2D keypoint detector heatmaps to sample plausible poses consistent with 2D images.

Result: State-of-the-art performance on Human 3.6M under best-of-m evaluation among methods not requiring paired data. Competitive generalization on MPI-INF-3DHP and 3DPW datasets. Framework enables novel tasks like pose generation and completion without additional training.

Conclusion: Diffusion models effectively address ambiguity in 3D pose estimation, eliminate need for paired training data, and provide flexible framework for multiple pose-related tasks while maintaining competitive performance.

Abstract: 3D human pose estimation from 2D images is a challenging problem due to depth ambiguity and occlusion. Because of these challenges the task is underdetermined, where there exists multiple -- possibly infinite -- poses that are plausible given the image. Despite this, many prior works assume the existence of a deterministic mapping and estimate a single pose given an image. Furthermore, methods based on machine learning require a large amount of paired 2D-3D data to train and suffer from generalization issues to unseen scenarios. To address both of these issues, we propose a framework for pose estimation using diffusion models, which enables sampling from a probability distribution over plausible poses which are consistent with a 2D image. Our approach falls under the guidance framework for conditional generation, and guides samples from an unconditional diffusion model, trained only on 3D data, using the gradients of the heatmaps from a 2D keypoint detector. We evaluate our method on the Human 3.6M dataset under best-of-$m$ multiple hypothesis evaluation, showing state-of-the-art performance among methods which do not require paired 2D-3D data for training. We additionally evaluate the generalization ability using the MPI-INF-3DHP and 3DPW datasets and demonstrate competitive performance. Finally, we demonstrate the flexibility of our framework by using it for novel tasks including pose generation and pose completion, without the need to train bespoke conditional models. We make code available at https://github.com/fsnelgar/diffusion_pose .

</details>


### [34] [FinMTM: A Multi-Turn Multimodal Benchmark for Financial Reasoning and Agent Evaluation](https://arxiv.org/abs/2602.03130)
*Chenxi Zhang,Ziliang Gan,Liyun Zhu,Youwei Pang,Qing Zhang,Rongjunchen Zhang*

Main category: cs.CV

TL;DR: FinMTM is a multi-turn multimodal benchmark for financial vision-language models, featuring 11k+ bilingual QA pairs across diverse financial visuals and task formats with specialized evaluation protocols.


<details>
  <summary>Details</summary>
Motivation: Existing financial benchmarks are limited by being single-turn and using narrow question formats, failing to evaluate VLMs comprehensively in realistic financial application scenarios that require complex reasoning.

Method: Created FinMTM benchmark with 11,133 bilingual Chinese/English QA pairs grounded in financial visuals (candlestick charts, statistical plots, report figures). Covers multiple task types: single/multiple-choice, multi-turn dialogues, and agent-based tasks with specialized evaluation protocols.

Result: Evaluation of 22 VLMs reveals significant limitations in fine-grained visual perception, long-context reasoning, and complex agent workflows, demonstrating the need for more robust financial VLMs.

Conclusion: FinMTM addresses critical gaps in financial VLM evaluation by providing a comprehensive multi-turn multimodal benchmark that better reflects real-world financial analysis scenarios and reveals current model limitations.

Abstract: The financial domain poses substantial challenges for vision-language models (VLMs) due to specialized chart formats and knowledge-intensive reasoning requirements. However, existing financial benchmarks are largely single-turn and rely on a narrow set of question formats, limiting comprehensive evaluation in realistic application scenarios. To address this gap, we propose FinMTM, a multi-turn multimodal benchmark that expands diversity along both data and task dimensions. On the data side, we curate and annotate 11{,}133 bilingual (Chinese and English) financial QA pairs grounded in financial visuals, including candlestick charts, statistical plots, and report figures. On the task side, FinMTM covers single- and multiple-choice questions, multi-turn open-ended dialogues, and agent-based tasks. We further design task-specific evaluation protocols, including a set-overlap scoring rule for multiple-choice questions, a weighted combination of turn-level and session-level scores for multi-turn dialogues, and a composite metric that integrates planning quality with final outcomes for agent tasks. Extensive experimental evaluation of 22 VLMs reveal their limitations in fine-grained visual perception, long-context reasoning, and complex agent workflows.

</details>


### [35] [SwiftVLM: Efficient Vision-Language Model Inference via Cross-Layer Token Bypass](https://arxiv.org/abs/2602.03134)
*Chen Qian,Xinran Yu,Danyang Li,Guoxuan Chi,Zheng Yang,Qiang Ma,Xin Miao*

Main category: cs.CV

TL;DR: SwiftVLM introduces a bypass pruning paradigm that preserves unselected visual tokens for later re-evaluation, achieving better accuracy-efficiency trade-offs in vision-language models.


<details>
  <summary>Details</summary>
Motivation: Existing visual token pruning methods suffer from significant performance degradation on tasks requiring fine-grained visual details due to premature pruning decisions that irreversibly lose critical information.

Method: SwiftVLM uses a bypass pruning paradigm that preserves unselected visual tokens and forwards them to subsequent pruning stages for re-evaluation. It performs pruning at model-specific layers with strong visual token selection capability while enabling independent pruning decisions across layers.

Result: Experiments across multiple VLMs and benchmarks show SwiftVLM consistently outperforms existing pruning strategies, achieving superior accuracy-efficiency trade-offs and more faithful visual token selection behavior.

Conclusion: The bypass pruning paradigm effectively addresses the limitations of early pruning by allowing re-evaluation of visual tokens across layers, leading to better preservation of fine-grained visual details while maintaining computational efficiency.

Abstract: Visual token pruning is a promising approach for reducing the computational cost of vision-language models (VLMs), and existing methods often rely on early pruning decisions to improve efficiency. While effective on coarse-grained reasoning tasks, they suffer from significant performance degradation on tasks requiring fine-grained visual details. Through layer-wise analysis, we reveal substantial discrepancies in visual token importance across layers, showing that tokens deemed unimportant at shallow layers can later become highly relevant for text-conditioned reasoning. To avoid irreversible critical information loss caused by premature pruning, we introduce a new pruning paradigm, termed bypass, which preserves unselected visual tokens and forwards them to subsequent pruning stages for re-evaluation. Building on this paradigm, we propose SwiftVLM, a simple and training-free method that performs pruning at model-specific layers with strong visual token selection capability, while enabling independent pruning decisions across layers. Experiments across multiple VLMs and benchmarks demonstrate that SwiftVLM consistently outperforms existing pruning strategies, achieving superior accuracy-efficiency trade-offs and more faithful visual token selection behavior.

</details>


### [36] [FSOD-VFM: Few-Shot Object Detection with Vision Foundation Models and Graph Diffusion](https://arxiv.org/abs/2602.03137)
*Chen-Bin Feng,Youyang Sha,Longfei Liu,Yongjun Yu,Chi Man Vong,Xuanlong Yu,Xi Shen*

Main category: cs.CV

TL;DR: FSOD-VFM is a few-shot object detection framework that leverages vision foundation models (UPN, SAM2, DINOv2) with graph-based confidence reweighting to address overfragmentation issues in bounding box proposals, achieving state-of-the-art performance without additional training.


<details>
  <summary>Details</summary>
Motivation: Vision foundation models have strong generalization capabilities but often produce overfragmented bounding boxes that cover only partial object regions, leading to numerous small false-positive proposals rather than accurate, complete object detections in few-shot settings.

Method: Integrates three foundation models: UPN for category-agnostic bounding boxes, SAM2 for mask extraction, and DINOv2 features. Introduces novel graph-based confidence reweighting where bounding boxes are modeled as nodes in a directed graph, using graph diffusion to propagate confidence scores, assigning higher confidence to whole objects and lower confidence to fragmented parts.

Result: Substantially outperforms existing approaches on Pascal-5^i, COCO-20^i, and CD-FSOD datasets. On challenging CD-FSOD dataset, achieves 31.6 AP in 10-shot setting vs. 21.4 AP for previous training-free methods. Demonstrates superior performance without requiring additional training.

Conclusion: FSOD-VFM effectively addresses the overfragmentation problem in foundation model-based few-shot object detection through graph-based confidence reweighting, achieving state-of-the-art performance across multiple benchmarks while maintaining training-free operation.

Abstract: In this paper, we present FSOD-VFM: Few-Shot Object Detectors with Vision Foundation Models, a framework that leverages vision foundation models to tackle the challenge of few-shot object detection. FSOD-VFM integrates three key components: a universal proposal network (UPN) for category-agnostic bounding box generation, SAM2 for accurate mask extraction, and DINOv2 features for efficient adaptation to new object categories. Despite the strong generalization capabilities of foundation models, the bounding boxes generated by UPN often suffer from overfragmentation, covering only partial object regions and leading to numerous small, false-positive proposals rather than accurate, complete object detections. To address this issue, we introduce a novel graph-based confidence reweighting method. In our approach, predicted bounding boxes are modeled as nodes in a directed graph, with graph diffusion operations applied to propagate confidence scores across the network. This reweighting process refines the scores of proposals, assigning higher confidence to whole objects and lower confidence to local, fragmented parts. This strategy improves detection granularity and effectively reduces the occurrence of false-positive bounding box proposals. Through extensive experiments on Pascal-5$^i$, COCO-20$^i$, and CD-FSOD datasets, we demonstrate that our method substantially outperforms existing approaches, achieving superior performance without requiring additional training. Notably, on the challenging CD-FSOD dataset, which spans multiple datasets and domains, our FSOD-VFM achieves 31.6 AP in the 10-shot setting, substantially outperforming previous training-free methods that reach only 21.4 AP. Code is available at: https://intellindust-ai-lab.github.io/projects/FSOD-VFM.

</details>


### [37] [Diversity-Preserved Distribution Matching Distillation for Fast Visual Synthesis](https://arxiv.org/abs/2602.03139)
*Tianhe Wu,Ruibin Li,Lei Zhang,Kede Ma*

Main category: cs.CV

TL;DR: DP-DMD is a role-separated distillation framework that prevents mode collapse in diffusion model distillation by dedicating the first step to diversity preservation and subsequent steps to quality refinement.


<details>
  <summary>Details</summary>
Motivation: Existing distribution matching distillation (DMD) methods suffer from mode collapse due to reverse-KL formulation's mode-seeking behavior, and current remedies require computationally expensive perceptual/adversarial regularization with training instability issues.

Method: Role-separated distillation framework: first step uses target-prediction objective (e.g., v-prediction) to preserve diversity, subsequent steps use standard DMD loss for quality refinement, with DMD gradients blocked at first step. No perceptual backbone, discriminator, auxiliary networks, or additional ground-truth images needed.

Result: Preserves sample diversity while maintaining visual quality on par with state-of-the-art methods in extensive text-to-image experiments, despite its simplicity.

Conclusion: DP-DMD provides an effective solution to mode collapse in DMD without the computational overhead and instability of existing regularization approaches, enabling high-quality generation with low inference cost.

Abstract: Distribution matching distillation (DMD) aligns a multi-step generator with its few-step counterpart to enable high-quality generation under low inference cost. However, DMD tends to suffer from mode collapse, as its reverse-KL formulation inherently encourages mode-seeking behavior, for which existing remedies typically rely on perceptual or adversarial regularization, thereby incurring substantial computational overhead and training instability. In this work, we propose a role-separated distillation framework that explicitly disentangles the roles of distilled steps: the first step is dedicated to preserving sample diversity via a target-prediction (e.g., v-prediction) objective, while subsequent steps focus on quality refinement under the standard DMD loss, with gradients from the DMD objective blocked at the first step. We term this approach Diversity-Preserved DMD (DP-DMD), which, despite its simplicity -- no perceptual backbone, no discriminator, no auxiliary networks, and no additional ground-truth images -- preserves sample diversity while maintaining visual quality on par with state-of-the-art methods in extensive text-to-image experiments.

</details>


### [38] [Fully Kolmogorov-Arnold Deep Model in Medical Image Segmentation](https://arxiv.org/abs/2602.03156)
*Xingyu Qiu,Xinghua Ma,Dong Liang,Gongning Luo,Wei Wang,Kuanquan Wang,Shuo Li*

Main category: cs.CV

TL;DR: This paper introduces ALL U-KAN, the first fully Kolmogorov-Arnold (KA) based deep model that completely replaces traditional FC and Conv layers with KA-based layers, overcoming training difficulties and memory limitations of deeply stacked KANs.


<details>
  <summary>Details</summary>
Motivation: Deeply stacked KANs are practically impossible due to high training difficulties and substantial memory requirements, limiting existing studies to few KAN layers and hindering comprehensive exploration of KANs' potential.

Method: Proposes Share-activation KAN (SaKAN) with simplified parameterization and denser training samples, introduces Grad-Free Spline to eliminate spline gradient memory overhead, and builds ALL U-KAN with KA and KAonv layers replacing traditional FC and Conv layers.

Result: Achieves 10x parameter reduction and 20x memory reduction compared to deeply stacked KANs, with superior segmentation accuracy on three medical image segmentation tasks compared to partial KA-based and traditional architectures.

Conclusion: KA-based layers can entirely replace traditional architectures in deep learning, achieving superior learning capacity and unlocking new explorations into deep KAN architectures with practical efficiency.

Abstract: Deeply stacked KANs are practically impossible due to high training difficulties and substantial memory requirements. Consequently, existing studies can only incorporate few KAN layers, hindering the comprehensive exploration of KANs. This study overcomes these limitations and introduces the first fully KA-based deep model, demonstrating that KA-based layers can entirely replace traditional architectures in deep learning and achieve superior learning capacity. Specifically, (1) the proposed Share-activation KAN (SaKAN) reformulates Sprecher's variant of Kolmogorov-Arnold representation theorem, which achieves better optimization due to its simplified parameterization and denser training samples, to ease training difficulty, (2) this paper indicates that spline gradients contribute negligibly to training while consuming huge GPU memory, thus proposes the Grad-Free Spline to significantly reduce memory usage and computational overhead. (3) Building on these two innovations, our ALL U-KAN is the first representative implementation of fully KA-based deep model, where the proposed KA and KAonv layers completely replace FC and Conv layers. Extensive evaluations on three medical image segmentation tasks confirm the superiority of the full KA-based architecture compared to partial KA-based and traditional architectures, achieving all higher segmentation accuracy. Compared to directly deeply stacked KAN, ALL U-KAN achieves 10 times reduction in parameter count and reduces memory consumption by more than 20 times, unlocking the new explorations into deep KAN architectures.

</details>


### [39] [Human-in-the-loop Adaptation in Group Activity Feature Learning for Team Sports Video Retrieval](https://arxiv.org/abs/2602.03157)
*Chihiro Nakatani,Hiroaki Kawashima,Norimichi Ukita*

Main category: cs.CV

TL;DR: Human-in-the-loop adaptation for group activity feature learning without group activity annotations, improving video retrieval performance through interactive fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Prior group activity learning methods require supervised classification with predefined classes, which limits flexibility. The paper aims to enable group activity video retrieval without group activity annotations through human-in-the-loop adaptation.

Method: 1) Self-supervised pre-training of Group Activity Feature (GAF) space based on activity similarity. 2) Interactive fine-tuning where users label selected videos as positive/negative relative to queries. 3) Data-efficient video selection process to choose informative videos for labeling. 4) Contrastive learning to update GAF space, moving positive videos closer and negative videos farther from queries.

Result: Significant improvement in retrieval performance on two team sports datasets. Ablation studies confirm contributions of various human-in-the-loop adaptation components.

Conclusion: Human-in-the-loop adaptation enables effective group activity feature learning without group activity annotations, significantly improving video retrieval performance through interactive fine-tuning with minimal user labeling effort.

Abstract: This paper proposes human-in-the-loop adaptation for Group Activity Feature Learning (GAFL) without group activity annotations. This human-in-the-loop adaptation is employed in a group-activity video retrieval framework to improve its retrieval performance. Our method initially pre-trains the GAF space based on the similarity of group activities in a self-supervised manner, unlike prior work that classifies videos into pre-defined group activity classes in a supervised learning manner. Our interactive fine-tuning process updates the GAF space to allow a user to better retrieve videos similar to query videos given by the user. In this fine-tuning, our proposed data-efficient video selection process provides several videos, which are selected from a video database, to the user in order to manually label these videos as positive or negative. These labeled videos are used to update (i.e., fine-tune) the GAF space, so that the positive and negative videos move closer to and farther away from the query videos through contrastive learning. Our comprehensive experimental results on two team sports datasets validate that our method significantly improves the retrieval performance. Ablation studies also demonstrate that several components in our human-in-the-loop adaptation contribute to the improvement of the retrieval performance. Code: https://github.com/chihina/GAFL-FINE-CVIU.

</details>


### [40] [BinaryDemoire: Moiré-Aware Binarization for Image Demoiréing](https://arxiv.org/abs/2602.03176)
*Zheng Chen,Zhi Yang,Xiaoyang Liu,Weihang Zhang,Mengfan Wang,Yifan Fu,Linghe Kong,Yulun Zhang*

Main category: cs.CV

TL;DR: BinaryDemoire: A binarized demoiréing framework that uses moiré-aware binary gates and shuffle-grouped residual adapters to effectively remove moiré patterns with extreme 1-bit compression.


<details>
  <summary>Details</summary>
Motivation: Image demoiréing requires removing frequency-dependent artifacts, but existing deep networks are computationally expensive for deployment. Binarization offers extreme compression (1-bit weights/activations) but performs poorly when naively applied to demoiréing due to the complex frequency structure of moiré patterns.

Method: 1) Moiré-aware binary gate (MABG) extracts lightweight frequency descriptors and activation statistics to predict channel-wise gating coefficients for aggregating binary convolution responses. 2) Shuffle-grouped residual adapter (SGRA) performs structured sparse shortcut alignment with interleaved mixing to promote information exchange across channel partitions.

Result: Extensive experiments on four benchmarks demonstrate that BinaryDemoire surpasses current binarization methods for demoiréing tasks.

Conclusion: BinaryDemoire provides an effective binarized framework for demoiréing that explicitly accommodates the frequency structure of moiré artifacts while achieving extreme compression through 1-bit quantization.

Abstract: Image demoiréing aims to remove structured moiré artifacts in recaptured imagery, where degradations are highly frequency-dependent and vary across scales and directions. While recent deep networks achieve high-quality restoration, their full-precision designs remain costly for deployment. Binarization offers an extreme compression regime by quantizing both activations and weights to 1-bit. Yet, it has been rarely studied for demoiréing and performs poorly when naively applied. In this work, we propose BinaryDemoire, a binarized demoiréing framework that explicitly accommodates the frequency structure of moiré degradations. First, we introduce a moiré-aware binary gate (MABG) that extracts lightweight frequency descriptors together with activation statistics. It predicts channel-wise gating coefficients to condition the aggregation of binary convolution responses. Second, we design a shuffle-grouped residual adapter (SGRA) that performs structured sparse shortcut alignment. It further integrates interleaved mixing to promote information exchange across different channel partitions. Extensive experiments on four benchmarks demonstrate that the proposed BinaryDemoire surpasses current binarization methods. Code: https://github.com/zhengchen1999/BinaryDemoire.

</details>


### [41] [LSGQuant: Layer-Sensitivity Guided Quantization for One-Step Diffusion Real-World Video Super-Resolution](https://arxiv.org/abs/2602.03182)
*Tianxing Wu,Zheng Chen,Cirou Xu,Bowen Chai,Yong Guo,Yutong Liu,Linghe Kong,Yulun Zhang*

Main category: cs.CV

TL;DR: LSGQuant is a layer-sensitivity guided quantization method for one-step diffusion-based video super-resolution that maintains near-original performance while significantly reducing model size and computational cost.


<details>
  <summary>Details</summary>
Motivation: One-step diffusion models show promise for real-world video super-resolution but suffer from large model size and high computational cost due to Diffusion Transformers (DiTs). Existing low-bit quantization approaches struggle with the high dynamic range of input latent and diverse layer behaviors in these models.

Method: LSGQuant introduces three key components: 1) Dynamic Range Adaptive Quantizer (DRAQ) to handle video token activations, 2) Variance-Oriented Layer Training Strategy (VOLTS) based on layer sensitivity analysis, and 3) Quantization-Aware Optimization (QAO) to jointly refine quantized and high-precision branches.

Result: Extensive experiments show LSGQuant achieves nearly the same performance as the original full-precision model and significantly outperforms existing quantization techniques for one-step diffusion-based video super-resolution.

Conclusion: LSGQuant effectively addresses the quantization challenges in diffusion-based VSR models, enabling practical deployment with minimal performance degradation while providing significant computational benefits.

Abstract: One-Step Diffusion Models have demonstrated promising capability and fast inference in video super-resolution (VSR) for real-world. Nevertheless, the substantial model size and high computational cost of Diffusion Transformers (DiTs) limit downstream applications. While low-bit quantization is a common approach for model compression, the effectiveness of quantized models is challenged by the high dynamic range of input latent and diverse layer behaviors. To deal with these challenges, we introduce LSGQuant, a layer-sensitivity guided quantizing approach for one-step diffusion-based real-world VSR. Our method incorporates a Dynamic Range Adaptive Quantizer (DRAQ) to fit video token activations. Furthermore, we estimate layer sensitivity and implement a Variance-Oriented Layer Training Strategy (VOLTS) by analyzing layer-wise statistics in calibration. We also introduce Quantization-Aware Optimization (QAO) to jointly refine the quantized branch and a retained high-precision branch. Extensive experiments demonstrate that our method has nearly performance to origin model with full-precision and significantly exceeds existing quantization techniques. Code is available at: https://github.com/zhengchen1999/LSGQuant.

</details>


### [42] [From Single Scan to Sequential Consistency: A New Paradigm for LIDAR Relocalization](https://arxiv.org/abs/2602.03198)
*Minghang Zhu,Zhijing Wang,Yuxin Guo,Wen Li,Sheng Ao,Cheng Wang*

Main category: cs.CV

TL;DR: TempLoc: A temporal-aware LiDAR relocalization framework that enhances robustness by modeling sequential consistency across scans through uncertainty-guided coordinate fusion.


<details>
  <summary>Details</summary>
Motivation: Existing regression-based LiDAR relocalization methods are prone to dynamic or ambiguous scenarios because they either rely on single-frame inference or neglect spatio-temporal consistency across scans.

Method: Three-module framework: 1) Global Coordinate Estimation predicts point-wise global coordinates with uncertainties, 2) Prior Coordinate Generation estimates inter-frame point correspondences via attention mechanism, 3) Uncertainty-Guided Coordinate Fusion integrates both predictions end-to-end for temporally consistent 6-DoF pose estimation.

Result: Outperforms state-of-the-art methods by a large margin on NCLT and Oxford Robot-Car benchmarks, demonstrating effectiveness of temporal-aware correspondence modeling.

Conclusion: TempLoc effectively enhances LiDAR relocalization robustness through temporal consistency modeling, showing the importance of spatio-temporal correspondence in challenging scenarios.

Abstract: LiDAR relocalization aims to estimate the global 6-DoF pose of a sensor in the environment. However, existing regression-based approaches are prone to dynamic or ambiguous scenarios, as they either solely rely on single-frame inference or neglect the spatio-temporal consistency across scans. In this paper, we propose TempLoc, a new LiDAR relocalization framework that enhances the robustness of localization by effectively modeling sequential consistency. Specifically, a Global Coordinate Estimation module is first introduced to predict point-wise global coordinates and associated uncertainties for each LiDAR scan. A Prior Coordinate Generation module is then presented to estimate inter-frame point correspondences by the attention mechanism. Lastly, an Uncertainty-Guided Coordinate Fusion module is deployed to integrate both predictions of point correspondence in an end-to-end fashion, yielding a more temporally consistent and accurate global 6-DoF pose. Experimental results on the NCLT and Oxford Robot-Car benchmarks show that our TempLoc outperforms stateof-the-art methods by a large margin, demonstrating the effectiveness of temporal-aware correspondence modeling in LiDAR relocalization. Our code will be released soon.

</details>


### [43] [Hand3R: Online 4D Hand-Scene Reconstruction in the Wild](https://arxiv.org/abs/2602.03200)
*Wendi Hu,Haonan Zhou,Wenhao Hu,Gaoang Wang*

Main category: cs.CV

TL;DR: Hand3R is the first online framework for joint 4D hand-scene reconstruction from monocular video, combining hand and scene models via visual prompting for simultaneous reconstruction without offline optimization.


<details>
  <summary>Details</summary>
Motivation: Existing methods for hand reconstruction recover isolated hands in local coordinates, overlooking the surrounding 3D environment, which is crucial for understanding physical interaction in Embodied AI.

Method: Hand3R synergizes a pre-trained hand expert with a 4D scene foundation model via a scene-aware visual prompting mechanism, injecting high-fidelity hand priors into a persistent scene memory for simultaneous reconstruction in a single forward pass.

Result: Experiments demonstrate that Hand3R bypasses reliance on offline optimization and delivers competitive performance in both local hand reconstruction and global positioning.

Conclusion: Hand3R enables simultaneous reconstruction of accurate hand meshes and dense metric-scale scene geometry from monocular video, addressing the gap in joint hand-scene reconstruction for Embodied AI.

Abstract: For Embodied AI, jointly reconstructing dynamic hands and the dense scene context is crucial for understanding physical interaction. However, most existing methods recover isolated hands in local coordinates, overlooking the surrounding 3D environment. To address this, we present Hand3R, the first online framework for joint 4D hand-scene reconstruction from monocular video. Hand3R synergizes a pre-trained hand expert with a 4D scene foundation model via a scene-aware visual prompting mechanism. By injecting high-fidelity hand priors into a persistent scene memory, our approach enables simultaneous reconstruction of accurate hand meshes and dense metric-scale scene geometry in a single forward pass. Experiments demonstrate that Hand3R bypasses the reliance on offline optimization and delivers competitive performance in both local hand reconstruction and global positioning.

</details>


### [44] [VIRAL: Visual In-Context Reasoning via Analogy in Diffusion Transformers](https://arxiv.org/abs/2602.03210)
*Zhiwen Li,Zhongjie Duan,Jinyan Ye,Cen Chen,Daoyuan Chen,Yaliang Li,Yingda Chen*

Main category: cs.CV

TL;DR: VIRAL is a framework that enables visual in-context learning by formulating it as conditional generation via visual analogy, adapting a frozen Diffusion Transformer with role-aware conditioning and Mixture-of-Experts LoRA to handle diverse vision tasks.


<details>
  <summary>Details</summary>
Motivation: Replicating In-Context Learning (ICL) in computer vision is challenging due to task heterogeneity, and current visual context datasets have gaps that need to be addressed.

Method: Formulates ICL as conditional generation via visual analogy (x_s : x_t :: x_q : y_q), adapts a frozen Diffusion Transformer using role-aware multi-image conditioning, and introduces Mixture-of-Experts LoRA to mitigate gradient interference across diverse tasks.

Result: VIRAL outperforms existing methods, demonstrating that a unified visual ICL paradigm can handle the majority of visual tasks including open-domain editing.

Conclusion: The proposed VIRAL framework successfully enables visual in-context learning across diverse vision tasks, validated by experiments and supported by a newly curated large-scale dataset spanning perception, restoration, and editing.

Abstract: Replicating In-Context Learning (ICL) in computer vision remains challenging due to task heterogeneity. We propose \textbf{VIRAL}, a framework that elicits visual reasoning from a pre-trained image editing model by formulating ICL as conditional generation via visual analogy ($x_s : x_t :: x_q : y_q$). We adapt a frozen Diffusion Transformer (DiT) using role-aware multi-image conditioning and introduce a Mixture-of-Experts LoRA to mitigate gradient interference across diverse tasks. Additionally, to bridge the gaps in current visual context datasets, we curate a large-scale dataset spanning perception, restoration, and editing. Experiments demonstrate that VIRAL outperforms existing methods, validating that a unified V-ICL paradigm can handle the majority of visual tasks, including open-domain editing. Our code is available at https://anonymous.4open.science/r/VIRAL-744A

</details>


### [45] [LEVIO: Lightweight Embedded Visual Inertial Odometry for Resource-Constrained Devices](https://arxiv.org/abs/2602.03294)
*Jonas Kühne,Christian Vogt,Michele Magno,Luca Benini*

Main category: cs.CV

TL;DR: LEVIO is an ultra-low-power visual-inertial odometry system optimized for resource-constrained hardware like micro-drones and smart glasses, achieving 20 FPS with <100 mW power consumption.


<details>
  <summary>Details</summary>
Motivation: Existing VIO systems are too computationally demanding for resource-constrained hardware (micro-drones, smart glasses), creating a need for efficient motion tracking solutions for mobile robotics and AR applications.

Method: Hardware-software co-optimized VIO pipeline using ORB feature tracking and bundle adjustment, with emphasis on computational efficiency through parallelization and low memory usage for embedded microcontrollers and low-power SoCs.

Result: Achieves 20 FPS real-time performance while consuming less than 100 mW on a parallel-processing ultra-low-power RISC-V SoC, validated against public VIO datasets with good accuracy-efficiency balance.

Conclusion: LEVIO provides a practical, open-source solution for efficient 6-DoF motion tracking on ultra-low-power platforms, enabling real-time sensing for constrained hardware applications.

Abstract: Accurate, infrastructure-less sensor systems for motion tracking are essential for mobile robotics and augmented reality (AR) applications. The most popular state-of-the-art visual-inertial odometry (VIO) systems, however, are too computationally demanding for resource-constrained hardware, such as micro-drones and smart glasses. This work presents LEVIO, a fully featured VIO pipeline optimized for ultra-low-power compute platforms, allowing six-degrees-of-freedom (DoF) real-time sensing. LEVIO incorporates established VIO components such as Oriented FAST and Rotated BRIEF (ORB) feature tracking and bundle adjustment, while emphasizing a computationally efficient architecture with parallelization and low memory usage to suit embedded microcontrollers and low-power systems-on-chip (SoCs). The paper proposes and details the algorithmic design choices and the hardware-software co-optimization approach, and presents real-time performance on resource-constrained hardware. LEVIO is validated on a parallel-processing ultra-low-power RISC-V SoC, achieving 20 FPS while consuming less than 100 mW, and benchmarked against public VIO datasets, offering a compelling balance between efficiency and accuracy. To facilitate reproducibility and adoption, the complete implementation is released as open-source.

</details>


### [46] [ConsisDrive: Identity-Preserving Driving World Models for Video Generation by Instance Mask](https://arxiv.org/abs/2602.03213)
*Zhuoran Yang,Yanyong Zhang*

Main category: cs.CV

TL;DR: ConsisDrive is an identity-preserving driving world model that prevents object identity drift in generated driving videos using instance-level temporal constraints.


<details>
  <summary>Details</summary>
Motivation: Existing world models for autonomous driving suffer from identity drift - where objects change appearance or category across frames due to lack of instance-level temporal constraints, reducing the quality and usefulness of generated driving data.

Method: Two key components: (1) Instance-Masked Attention that applies identity and trajectory masks to ensure visual tokens only interact with corresponding instance features across space and time; (2) Instance-Masked Loss that adaptively emphasizes foreground regions with probabilistic instance masking to reduce background noise while maintaining scene fidelity.

Result: Achieves state-of-the-art driving video generation quality and demonstrates significant improvements in downstream autonomous driving tasks on the nuScenes dataset.

Conclusion: ConsisDrive provides an effective solution to identity drift in driving world models, enabling generation of temporally consistent, high-quality driving data that benefits autonomous driving applications.

Abstract: Autonomous driving relies on robust models trained on large-scale, high-quality multi-view driving videos. Although world models provide a cost-effective solution for generating realistic driving data, they often suffer from identity drift, where the same object changes its appearance or category across frames due to the absence of instance-level temporal constraints. We introduce ConsisDrive, an identity-preserving driving world model designed to enforce temporal consistency at the instance level. Our framework incorporates two key components: (1) Instance-Masked Attention, which applies instance identity masks and trajectory masks within attention blocks to ensure that visual tokens interact only with their corresponding instance features across spatial and temporal dimensions, thereby preserving object identity consistency; and (2) Instance-Masked Loss, which adaptively emphasizes foreground regions with probabilistic instance masking, reducing background noise while maintaining overall scene fidelity. By integrating these mechanisms, ConsisDrive achieves state-of-the-art driving video generation quality and demonstrates significant improvements in downstream autonomous driving tasks on the nuScenes dataset. Our project page is https://shanpoyang654.github.io/ConsisDrive/page.html.

</details>


### [47] [FARTrack: Fast Autoregressive Visual Tracking with High Performance](https://arxiv.org/abs/2602.03214)
*Guijie Wang,Tong Lin,Yifan Bai,Anjia Cao,Shiyi Liang,Wangbo Zhao,Xing Wei*

Main category: cs.CV

TL;DR: FARTrack is a fast autoregressive tracking framework that achieves real-time performance while maintaining competitive accuracy through task-specific self-distillation and inter-frame sparsification techniques.


<details>
  <summary>Details</summary>
Motivation: Current high-performance visual trackers are too slow for resource-constrained devices, creating a trade-off between tracking performance and inference speed that needs to be addressed.

Method: Proposes FARTrack with two key techniques: 1) Task-Specific Self-Distillation for layer-by-layer compression of task-specific tokens, and 2) Inter-frame Autoregressive Sparsification for sequentially condensing multiple templates with optimal sparsification strategy.

Result: Achieves 70.6% AO on GOT-10k in real-time, with fastest model reaching 343 FPS on GPU and 121 FPS on CPU while maintaining competitive tracking performance.

Conclusion: FARTrack successfully bridges the gap between tracking performance and inference speed, making high-quality visual tracking practical for deployment on resource-constrained devices through efficient autoregressive design.

Abstract: Inference speed and tracking performance are two critical evaluation metrics in the field of visual tracking. However, high-performance trackers often suffer from slow processing speeds, making them impractical for deployment on resource-constrained devices. To alleviate this issue, we propose FARTrack, a Fast Auto-Regressive Tracking framework. Since autoregression emphasizes the temporal nature of the trajectory sequence, it can maintain high performance while achieving efficient execution across various devices. FARTrack introduces Task-Specific Self-Distillation and Inter-frame Autoregressive Sparsification, designed from the perspectives of shallow-yet-accurate distillation and redundant-to-essential token optimization, respectively. Task-Specific Self-Distillation achieves model compression by distilling task-specific tokens layer by layer, enhancing the model's inference speed while avoiding suboptimal manual teacher-student layer pairs assignments. Meanwhile, Inter-frame Autoregressive Sparsification sequentially condenses multiple templates, avoiding additional runtime overhead while learning a temporally-global optimal sparsification strategy. FARTrack demonstrates outstanding speed and competitive performance. It delivers an AO of 70.6% on GOT-10k in real-time. Beyond, our fastest model achieves a speed of 343 FPS on the GPU and 121 FPS on the CPU.

</details>


### [48] [QVLA: Not All Channels Are Equal in Vision-Language-Action Model's Quantization](https://arxiv.org/abs/2602.03782)
*Yuhao Xu,Yantai Yang,Zhenyang Fan,Yufan Liu,Yuming Li,Bing Li,Zhipeng Zhang*

Main category: cs.CV

TL;DR: QVLA introduces an action-centric quantization framework for Vision-Language-Action models that uses channel-wise bit allocation based on action-space sensitivity, achieving significant compression with minimal performance loss.


<details>
  <summary>Details</summary>
Motivation: VLA models have immense computational demands that hinder deployment on resource-constrained robotic platforms. Existing quantization methods from LLMs are flawed for robotics because they prioritize data fidelity over action accuracy, where minor action deviations can lead to catastrophic task failures.

Method: QVLA introduces a granular channel-wise bit allocation strategy that measures action-space sensitivity when quantizing each channel to various bit-widths. This creates per-channel importance metrics that guide global optimization, unifying quantization and pruning (0-bit) into a single framework.

Result: On LIBERO, QVLA reduces OpenVLA-OFT to 29.2% of original VRAM while maintaining 98.9% performance and achieving 1.49x speedup, representing a 22.6% improvement over LLM-derived SmoothQuant.

Conclusion: QVLA establishes a principled foundation for compressing VLA models in robotics, enabling deployment of powerful large-scale models on real-world hardware through action-centric quantization.

Abstract: The advent of Vision-Language-Action (VLA) models represents a significant leap for embodied intelligence, yet their immense computational demands critically hinder deployment on resource-constrained robotic platforms. Intuitively, low-bit quantization is a prevalent and preferred technique for large-scale model compression. However, we find that a systematic analysis of VLA model's quantization is fundamentally lacking. We argue that naively applying uniform-bit quantization from Large Language Models (LLMs) to robotics is flawed, as these methods prioritize passive data fidelity while ignoring how minor action deviations compound into catastrophic task failures. To bridge this gap, we introduce QVLA, the first action-centric quantization framework specifically designed for embodied control. In a sharp departure from the rigid, uniform-bit quantization of LLM-based methods, QVLA introduces a highly granular, channel-wise bit allocation strategy. Its core mechanism is to directly measure the final action-space sensitivity when quantizing each individual channel to various bit-widths. This process yields a precise, per-channel importance metric that guides a global optimization, which elegantly unifies quantization and pruning (0-bit) into a single, cohesive framework. Extensive evaluations on different baselines demonstrate the superiority of our approach. In the LIBERO, the quantization version of OpenVLA-OFT with our method requires only 29.2% of the original model's VRAM while maintaining 98.9% of its original performance and achieving a 1.49x speedup. This translates to a 22.6% performance improvement over the LLM-derived method SmoothQuant. Our work establishes a new, principled foundation for compressing VLA models in robotics, paving the way for deploying powerful, large-scale models on real-world hardware. Code will be released.

</details>


### [49] [PokeFusion Attention: Enhancing Reference-Free Style-Conditioned Generation](https://arxiv.org/abs/2602.03220)
*Jingbang Tang*

Main category: cs.CV

TL;DR: PokeFusion Attention: A lightweight decoder-level cross-attention mechanism for reference-free style-conditioned character generation in diffusion models, enabling stable character structure and consistent style expression without external images.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for stylized character generation either rely on under-specified text-only prompting (causing style drift and geometric inconsistency) or reference-based adapters (adding architectural complexity and limiting deployment flexibility). Need for reference-free, consistent stylized generation.

Method: PokeFusion Attention - a decoder-level cross-attention mechanism that fuses textual semantics with learned style embeddings directly in the diffusion decoder. Decouples text and style conditioning at attention level, trains only decoder cross-attention layers with a compact style projection module, keeping pretrained diffusion backbone frozen.

Result: Experiments on Pokemon-style character generation benchmark show consistent improvements in style fidelity, semantic alignment, and character shape consistency compared to adapter-based baselines, while maintaining low parameter overhead and inference-time simplicity.

Conclusion: PokeFusion Attention provides a parameter-efficient, plug-and-play control component for reference-free stylized character generation that can be easily integrated into existing diffusion pipelines and transferred across different backbones.

Abstract: This paper studies reference-free style-conditioned character generation in text-to-image diffusion models, where high-quality synthesis requires both stable character structure and consistent, fine-grained style expression across diverse prompts. Existing approaches primarily rely on text-only prompting, which is often under-specified for visual style and tends to produce noticeable style drift and geometric inconsistency, or introduce reference-based adapters that depend on external images at inference time, increasing architectural complexity and limiting deployment flexibility.We propose PokeFusion Attention, a lightweight decoder-level cross-attention mechanism that fuses textual semantics with learned style embeddings directly inside the diffusion decoder. By decoupling text and style conditioning at the attention level, our method enables effective reference-free stylized generation while keeping the pretrained diffusion backbone fully frozen.PokeFusion Attention trains only decoder cross-attention layers together with a compact style projection module, resulting in a parameter-efficient and plug-and-play control component that can be easily integrated into existing diffusion pipelines and transferred across different backbones.Experiments on a stylized character generation benchmark (Pokemon-style) demonstrate that our method consistently improves style fidelity, semantic alignment, and character shape consistency compared with representative adapter-based baselines, while maintaining low parameter overhead and inference-time simplicity.

</details>


### [50] [Spiral RoPE: Rotate Your Rotary Positional Embeddings in the 2D Plane](https://arxiv.org/abs/2602.03227)
*Haoyu Liu,Sucheng Ren,Tingyu Zhu,Peng Wang,Cihang Xie,Alan Yuille,Zeyu Zheng,Feng Wang*

Main category: cs.CV

TL;DR: Spiral RoPE extends standard 2D RoPE to enable multi-directional positional encoding in vision transformers, overcoming the limitation of axis-aligned directions and improving performance across vision tasks.


<details>
  <summary>Details</summary>
Motivation: Standard axial 2D RoPE restricts positional encoding to horizontal and vertical directions, which hinders modeling of oblique spatial relationships that naturally exist in images. This directional constraint is identified as a fundamental limitation for vision transformers.

Method: Spiral RoPE partitions embedding channels into multiple groups associated with uniformly distributed directions. Each group is rotated according to the projection of patch positions onto its corresponding direction, enabling spatial encoding beyond just horizontal and vertical axes.

Result: Spiral RoPE consistently improves performance across a wide range of vision tasks including classification, segmentation, and generation. Attention maps show more concentrated activations on semantically relevant objects and better respect local object boundaries.

Conclusion: Multi-directional positional encoding is important for vision transformers, and Spiral RoPE provides an effective solution that overcomes the directional limitations of standard 2D RoPE while maintaining its desirable properties like relative position encoding and length extrapolation.

Abstract: Rotary Position Embedding (RoPE) is the de facto positional encoding in large language models due to its ability to encode relative positions and support length extrapolation. When adapted to vision transformers, the standard axial formulation decomposes two-dimensional spatial positions into horizontal and vertical components, implicitly restricting positional encoding to axis-aligned directions. We identify this directional constraint as a fundamental limitation of the standard axial 2D RoPE, which hinders the modeling of oblique spatial relationships that naturally exist in natural images. To overcome this limitation, we propose Spiral RoPE, a simple yet effective extension that enables multi-directional positional encoding by partitioning embedding channels into multiple groups associated with uniformly distributed directions. Each group is rotated according to the projection of the patch position onto its corresponding direction, allowing spatial relationships to be encoded beyond the horizontal and vertical axes. Across a wide range of vision tasks including classification, segmentation, and generation, Spiral RoPE consistently improves performance. Qualitative analysis of attention maps further show that Spiral RoPE exhibits more concentrated activations on semantically relevant objects and better respects local object boundaries, highlighting the importance of multi-directional positional encoding in vision transformers.

</details>


### [51] [EventFlash: Towards Efficient MLLMs for Event-Based Vision](https://arxiv.org/abs/2602.03230)
*Shaoyu Liu,Jianing Li,Guanghui Zhao,Yunjian Zhang,Wen Jiang,Ming Li,Xiangyang Ji*

Main category: cs.CV

TL;DR: EventFlash is an efficient multimodal LLM for event-based vision that reduces computational cost through spatiotemporal token sparsification, achieving 12.4× throughput improvement while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Current event-based MLLMs use dense image-like processing that ignores the spatiotemporal sparsity of event streams, leading to high computational costs. There's a need for more efficient processing that leverages the inherent sparsity of event data.

Method: 1) Created EventMind dataset with 500k+ instruction sets for curriculum training; 2) Adaptive temporal window aggregation module compresses temporal tokens while preserving key cues; 3) Sparse density-guided attention module selects informative spatial regions and suppresses sparse areas.

Result: Achieves 12.4× throughput improvement over baseline (EventFlash-Zero) while maintaining comparable performance. Supports long-range event stream processing with up to 1,000 bins, significantly outperforming EventGPT's 5-bin limit.

Conclusion: EventFlash serves as an efficient foundation model for event-based vision by effectively leveraging spatiotemporal sparsity to reduce computational costs while maintaining robust perception capabilities.

Abstract: Event-based multimodal large language models (MLLMs) enable robust perception in high-speed and low-light scenarios, addressing key limitations of frame-based MLLMs. However, current event-based MLLMs often rely on dense image-like processing paradigms, overlooking the spatiotemporal sparsity of event streams and resulting in high computational cost. In this paper, we propose EventFlash, a novel and efficient MLLM to explore spatiotemporal token sparsification for reducing data redundancy and accelerating inference. Technically, we build EventMind, a large-scale and scene-diverse dataset with over 500k instruction sets, providing both short and long event stream sequences to support our curriculum training strategy. We then present an adaptive temporal window aggregation module for efficient temporal sampling, which adaptively compresses temporal tokens while retaining key temporal cues. Finally, a sparse density-guided attention module is designed to improve spatial token efficiency by selecting informative regions and suppressing empty or sparse areas. Experimental results show that EventFlash achieves a $12.4\times$ throughput improvement over the baseline (EventFlash-Zero) while maintaining comparable performance. It supports long-range event stream processing with up to 1,000 bins, significantly outperforming the 5-bin limit of EventGPT. We believe EventFlash serves as an efficient foundation model for event-based vision.

</details>


### [52] [InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation](https://arxiv.org/abs/2602.03242)
*Zhuoran Yang,Xi Guo,Chenjing Ding,Chiyu Wang,Wei Wu,Yanyong Zhang*

Main category: cs.CV

TL;DR: InstaDrive enhances driving video realism with instance-level temporal consistency and spatial geometric fidelity using Instance Flow Guider and Spatial Geometric Aligner, improving autonomous driving tasks and enabling safety evaluation through CARLA simulations.


<details>
  <summary>Details</summary>
Motivation: World models for driving video generation struggle with instance-level temporal consistency and spatial geometric fidelity, limiting their realism and usefulness for autonomous driving applications.

Method: Proposes InstaDrive framework with two key components: (1) Instance Flow Guider extracts and propagates instance features across frames for temporal consistency, (2) Spatial Geometric Aligner improves spatial reasoning, ensures precise instance positioning, and models occlusion hierarchies.

Result: Achieves state-of-the-art video generation quality and enhances downstream autonomous driving tasks on nuScenes dataset. Also enables rigorous safety evaluation through CARLA autopilot simulations of rare safety-critical scenarios.

Conclusion: InstaDrive's instance-aware mechanisms significantly improve driving video realism, benefiting both video generation quality and autonomous driving task performance while enabling comprehensive safety evaluation.

Abstract: Autonomous driving relies on robust models trained on high-quality, large-scale multi-view driving videos. While world models offer a cost-effective solution for generating realistic driving videos, they struggle to maintain instance-level temporal consistency and spatial geometric fidelity. To address these challenges, we propose InstaDrive, a novel framework that enhances driving video realism through two key advancements: (1) Instance Flow Guider, which extracts and propagates instance features across frames to enforce temporal consistency, preserving instance identity over time. (2) Spatial Geometric Aligner, which improves spatial reasoning, ensures precise instance positioning, and explicitly models occlusion hierarchies. By incorporating these instance-aware mechanisms, InstaDrive achieves state-of-the-art video generation quality and enhances downstream autonomous driving tasks on the nuScenes dataset. Additionally, we utilize CARLA's autopilot to procedurally and stochastically simulate rare but safety-critical driving scenarios across diverse maps and regions, enabling rigorous safety evaluation for autonomous systems. Our project page is https://shanpoyang654.github.io/InstaDrive/page.html.

</details>


### [53] [LaVPR: Benchmarking Language and Vision for Place Recognition](https://arxiv.org/abs/2602.03253)
*Ofer Idan,Dan Badur,Yosi Keller,Yoli Shavit*

Main category: cs.CV

TL;DR: LaVPR introduces a large-scale VPR benchmark with 650K+ natural language descriptions, enabling multi-modal fusion for robustness and cross-modal retrieval for language-based localization, showing language helps small models rival larger vision-only ones.


<details>
  <summary>Details</summary>
Motivation: Standard VPR fails under extreme environmental changes and perceptual aliasing, and cannot perform "blind" localization from verbal descriptions alone, which is needed for applications like emergency response.

Method: Extends existing VPR datasets with natural language descriptions to create LaVPR benchmark. Investigates two paradigms: 1) Multi-Modal Fusion for enhanced robustness, and 2) Cross-Modal Retrieval for language-based localization using Low-Rank Adaptation (LoRA) and Multi-Similarity loss.

Result: Language descriptions yield consistent gains in visually degraded conditions, with most significant impact on smaller backbones. Adding language allows compact models to rival performance of much larger vision-only architectures. Cross-modal retrieval with LoRA and Multi-Similarity loss substantially outperforms standard contrastive methods across vision-language models.

Conclusion: LaVPR enables a new class of localization systems that are both resilient to real-world stochasticity and practical for resource-constrained deployment, bridging the gap between visual and language-based place recognition.

Abstract: Visual Place Recognition (VPR) often fails under extreme environmental changes and perceptual aliasing. Furthermore, standard systems cannot perform "blind" localization from verbal descriptions alone, a capability needed for applications such as emergency response. To address these challenges, we introduce LaVPR, a large-scale benchmark that extends existing VPR datasets with over 650,000 rich natural-language descriptions. Using LaVPR, we investigate two paradigms: Multi-Modal Fusion for enhanced robustness and Cross-Modal Retrieval for language-based localization. Our results show that language descriptions yield consistent gains in visually degraded conditions, with the most significant impact on smaller backbones. Notably, adding language allows compact models to rival the performance of much larger vision-only architectures. For cross-modal retrieval, we establish a baseline using Low-Rank Adaptation (LoRA) and Multi-Similarity loss, which substantially outperforms standard contrastive methods across vision-language models. Ultimately, LaVPR enables a new class of localization systems that are both resilient to real-world stochasticity and practical for resource-constrained deployment. Our dataset and code are available at https://github.com/oferidan1/LaVPR.

</details>


### [54] [HypCBC: Domain-Invariant Hyperbolic Cross-Branch Consistency for Generalizable Medical Image Analysis](https://arxiv.org/abs/2602.03264)
*Francesco Di Salvo,Sebastian Doerrich,Jonas Alle,Christian Ledig*

Main category: cs.CV

TL;DR: Hyperbolic representation learning outperforms Euclidean methods for medical image analysis, achieving +2.1% AUC gains on domain generalization benchmarks through cross-branch consistency constraints.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks struggle with robust generalization in medical imaging due to data scarcity, covariate shifts from different devices/protocols, and heterogeneous patient populations. Euclidean manifolds fail to capture complex hierarchical structures in clinical data.

Method: Proposes hyperbolic representation learning for medical image analysis with unsupervised domain-invariant hyperbolic cross-branch consistency constraint. Validates across eleven in-distribution datasets and three ViT models.

Result: Achieves statistically significant gains on in-distribution datasets and outperforms state-of-the-art Euclidean methods by average +2.1% AUC on three domain generalization benchmarks (Fitzpatrick17k, Camelyon17-WILDS, cross-dataset retinal imaging).

Conclusion: Hyperbolic manifolds effectively model complex medical data structures and enable better domain generalization than Euclidean approaches, with demonstrated improvements across diverse imaging modalities, data sizes, and label granularities.

Abstract: Robust generalization beyond training distributions remains a critical challenge for deep neural networks. This is especially pronounced in medical image analysis, where data is often scarce and covariate shifts arise from different hardware devices, imaging protocols, and heterogeneous patient populations. These factors collectively hinder reliable performance and slow down clinical adoption. Despite recent progress, existing learning paradigms primarily rely on the Euclidean manifold, whose flat geometry fails to capture the complex, hierarchical structures present in clinical data. In this work, we exploit the advantages of hyperbolic manifolds to model complex data characteristics. We present the first comprehensive validation of hyperbolic representation learning for medical image analysis and demonstrate statistically significant gains across eleven in-distribution datasets and three ViT models. We further propose an unsupervised, domain-invariant hyperbolic cross-branch consistency constraint. Extensive experiments confirm that our proposed method promotes domain-invariant features and outperforms state-of-the-art Euclidean methods by an average of $+2.1\%$ AUC on three domain generalization benchmarks: Fitzpatrick17k, Camelyon17-WILDS, and a cross-dataset setup for retinal imaging. These datasets span different imaging modalities, data sizes, and label granularities, confirming generalization capabilities across substantially different conditions. The code is available at https://github.com/francescodisalvo05/hyperbolic-cross-branch-consistency .

</details>


### [55] [Global Geometry Is Not Enough for Vision Representations](https://arxiv.org/abs/2602.03282)
*Jiwan Chung,Seon Joo Kim*

Main category: cs.CV

TL;DR: Standard geometric metrics fail to predict compositional binding in vision encoders, while functional sensitivity (Jacobian) reliably tracks this capability.


<details>
  <summary>Details</summary>
Motivation: To challenge the assumption that globally well-distributed embeddings are sufficient for representational competence, particularly for compositional binding where geometric metrics may be insensitive to how elements are composed.

Method: Tested 21 vision encoders by comparing standard geometry-based statistics with functional sensitivity (measured by input-output Jacobian) for predicting compositional binding, and provided analytic analysis of objective design.

Result: Geometry-based statistics show near-zero correlation with compositional binding, while functional sensitivity reliably tracks this capability. The disparity arises because existing losses constrain embedding geometry but leave local input-output mapping unconstrained.

Conclusion: Global embedding geometry captures only partial representational competence; functional sensitivity is a critical complementary axis for modeling composite structure.

Abstract: A common assumption in representation learning is that globally well-distributed embeddings support robust and generalizable representations. This focus has shaped both training objectives and evaluation protocols, implicitly treating global geometry as a proxy for representational competence. While global geometry effectively encodes which elements are present, it is often insensitive to how they are composed. We investigate this limitation by testing the ability of geometric metrics to predict compositional binding across 21 vision encoders. We find that standard geometry-based statistics exhibit near-zero correlation with compositional binding. In contrast, functional sensitivity, as measured by the input-output Jacobian, reliably tracks this capability. We further provide an analytic account showing that this disparity arises from objective design, as existing losses explicitly constrain embedding geometry but leave the local input-output mapping unconstrained. These results suggest that global embedding geometry captures only a partial view of representational competence and establish functional sensitivity as a critical complementary axis for modeling composite structure.

</details>


### [56] [A3-TTA: Adaptive Anchor Alignment Test-Time Adaptation for Image Segmentation](https://arxiv.org/abs/2602.03292)
*Jianghao Wu,Xiangde Luo,Yubo Zhou,Lianming Wu,Guotai Wang,Shaoting Zhang*

Main category: cs.CV

TL;DR: A3-TTA is a test-time adaptation framework for image segmentation that uses anchor-guided supervision to generate reliable pseudo-labels, addressing issues of error accumulation and catastrophic forgetting in domain shift scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing pseudo-label-based TTA methods rely on perturbation-ensemble heuristics that lack distributional grounding, causing unstable training signals, error accumulation, and catastrophic forgetting during adaptation to target domains.

Method: Proposes A3-TTA with anchor-guided supervision: identifies well-predicted target images using class compact density metric as anchors, uses them as stable references for pseudo-label generation, regularizes via semantic consistency and boundary-aware entropy minimization, and employs self-adaptive exponential moving average to mitigate label noise.

Result: Improves average Dice scores by 10.40 to 17.68 percentage points compared to source model, outperforms state-of-the-art TTA methods on multi-domain medical images (heart structure and prostate segmentation) and natural images, and excels in continual TTA with strong anti-forgetting ability.

Conclusion: A3-TTA provides an effective TTA framework that addresses limitations of existing pseudo-label methods through anchor-guided supervision, achieving significant performance gains and robustness in both single-domain and continual adaptation scenarios.

Abstract: Test-Time Adaptation (TTA) offers a practical solution for deploying image segmentation models under domain shift without accessing source data or retraining. Among existing TTA strategies, pseudo-label-based methods have shown promising performance. However, they often rely on perturbation-ensemble heuristics (e.g., dropout sampling, test-time augmentation, Gaussian noise), which lack distributional grounding and yield unstable training signals. This can trigger error accumulation and catastrophic forgetting during adaptation. To address this, we propose \textbf{A3-TTA}, a TTA framework that constructs reliable pseudo-labels through anchor-guided supervision. Specifically, we identify well-predicted target domain images using a class compact density metric, under the assumption that confident predictions imply distributional proximity to the source domain. These anchors serve as stable references to guide pseudo-label generation, which is further regularized via semantic consistency and boundary-aware entropy minimization. Additionally, we introduce a self-adaptive exponential moving average strategy to mitigate label noise and stabilize model update during adaptation. Evaluated on both multi-domain medical images (heart structure and prostate segmentation) and natural images, A3-TTA significantly improves average Dice scores by 10.40 to 17.68 percentage points compared to the source model, outperforming several state-of-the-art TTA methods under different segmentation model architectures. A3-TTA also excels in continual TTA, maintaining high performance across sequential target domains with strong anti-forgetting ability. The code will be made publicly available at https://github.com/HiLab-git/A3-TTA.

</details>


### [57] [Full end-to-end diagnostic workflow automation of 3D OCT via foundation model-driven AI for retinal diseases](https://arxiv.org/abs/2602.03302)
*Jinze Zhang,Jian Zhong,Li Lin,Jiaxiong Li,Ke Ma,Naiyang Li,Meng Li,Yuan Pan,Zeyu Meng,Mengyun Zhou,Shang Huang,Shilong Yu,Zhengyu Duan,Sutong Li,Honghui Xia,Juping Liu,Dan Liang,Yantao Wei,Xiaoying Tang,Jin Yuan,Peng Xiao*

Main category: cs.CV

TL;DR: FOCUS is a foundation model-driven framework that automates end-to-end 3D OCT retinal disease diagnosis through quality assessment, abnormality detection, and multi-disease classification with unified adaptive aggregation from 2D slices to 3D patient-level diagnosis.


<details>
  <summary>Details</summary>
Motivation: Current OCT diagnostic automation is limited by multi-stage workflows and conventional single-slice single-task AI models, preventing full diagnostic automation in clinical practice despite OCT's high-resolution 3D imaging capabilities.

Method: FOCUS uses EfficientNetV2-S for image quality assessment, fine-tuned Vision Foundation Model for abnormality detection and multi-disease classification, and a unified adaptive aggregation method to integrate 2D slice predictions into comprehensive 3D patient-level diagnosis.

Result: Achieved high F1 scores: 99.01% for quality assessment, 97.46% for abnormality detection, and 94.39% for patient-level diagnosis. Real-world validation showed stable performance (90.22%-95.24% F1) across centers and devices. Matched expert performance in abnormality detection (95.47% vs 90.91%) and multi-disease diagnosis (93.49% vs 91.35%) with better efficiency.

Conclusion: FOCUS automates the image-to-diagnosis pipeline, representing a critical advance toward unmanned ophthalmology with a validated blueprint for autonomous screening to enhance population-scale retinal care accessibility and efficiency.

Abstract: Optical coherence tomography (OCT) has revolutionized retinal disease diagnosis with its high-resolution and three-dimensional imaging nature, yet its full diagnostic automation in clinical practices remains constrained by multi-stage workflows and conventional single-slice single-task AI models. We present Full-process OCT-based Clinical Utility System (FOCUS), a foundation model-driven framework enabling end-to-end automation of 3D OCT retinal disease diagnosis. FOCUS sequentially performs image quality assessment with EfficientNetV2-S, followed by abnormality detection and multi-disease classification using a fine-tuned Vision Foundation Model. Crucially, FOCUS leverages a unified adaptive aggregation method to intelligently integrate 2D slices-level predictions into comprehensive 3D patient-level diagnosis. Trained and tested on 3,300 patients (40,672 slices), and externally validated on 1,345 patients (18,498 slices) across four different-tier centers and diverse OCT devices, FOCUS achieved high F1 scores for quality assessment (99.01%), abnormally detection (97.46%), and patient-level diagnosis (94.39%). Real-world validation across centers also showed stable performance (F1: 90.22%-95.24%). In human-machine comparisons, FOCUS matched expert performance in abnormality detection (F1: 95.47% vs 90.91%) and multi-disease diagnosis (F1: 93.49% vs 91.35%), while demonstrating better efficiency. FOCUS automates the image-to-diagnosis pipeline, representing a critical advance towards unmanned ophthalmology with a validated blueprint for autonomous screening to enhance population scale retinal care accessibility and efficiency.

</details>


### [58] [PQTNet: Pixel-wise Quantitative Thermography Neural Network for Estimating Defect Depth in Polylactic Acid Parts by Additive Manufacturing](https://arxiv.org/abs/2602.03314)
*Lei Deng,Wenhao Huang,Chao Yang,Haoyuan Zheng,Yinbin Tian,Yue Ma*

Main category: cs.CV

TL;DR: PQT-Net uses thermal imaging and deep learning to accurately measure defect depths in 3D-printed PLA parts with high precision.


<details>
  <summary>Details</summary>
Motivation: Defect depth quantification in additively manufactured components is challenging for non-destructive testing, requiring accurate measurement methods.

Method: Proposes Pixel-wise Quantitative Thermography Neural Network (PQT-Net) with novel data augmentation converting thermal sequences to 2D stripe images, using EfficientNetV2-S backbone and custom Residual Regression Head.

Result: Achieves minimum Mean Absolute Error of 0.0094 mm and coefficient of determination exceeding 99%, outperforming other deep learning models.

Conclusion: PQT-Net shows high precision for robust quantitative defect characterization in additive manufacturing.

Abstract: Defect depth quantification in additively manufactured (AM) components remains a significant challenge for non-destructive testing (NDT). This study proposes a Pixel-wise Quantitative Thermography Neural Network (PQT-Net) to address this challenge for polylactic acid (PLA) parts. A key innovation is a novel data augmentation strategy that reconstructs thermal sequence data into two-dimensional stripe images, preserving the complete temporal evolution of heat diffusion for each pixel. The PQT-Net architecture incorporates a pre-trained EfficientNetV2-S backbone and a custom Residual Regression Head (RRH) with learnable parameters to refine outputs. Comparative experiments demonstrate the superiority of PQT-Net over other deep learning models, achieving a minimum Mean Absolute Error (MAE) of 0.0094 mm and a coefficient of determination (R) exceeding 99%. The high precision of PQT-Net underscores its potential for robust quantitative defect characterization in AM.

</details>


### [59] [Invisible Clean-Label Backdoor Attacks for Generative Data Augmentation](https://arxiv.org/abs/2602.03316)
*Ting Xiang,Jinhui Zhao,Changjian Chen,Zhuo Tang*

Main category: cs.CV

TL;DR: InvLBA: An invisible clean-label backdoor attack method for generative data augmentation using latent perturbation instead of pixel-level triggers, achieving high attack success rates while maintaining clean accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing pixel-level clean-label backdoor attacks (like COMBAT) perform poorly on generated images from data augmentation, showing low attack success rates. This motivates exploring attacks at the latent feature level rather than pixel level.

Method: InvLBA (Invisible clean-label Backdoor Attack) uses latent perturbation for generative data augmentation. Instead of pixel-level triggers, it operates at the latent feature level to create invisible backdoors in generated training images.

Result: Experiments show InvLBA improves attack success rate by 46.43% on average across multiple datasets, with almost no reduction in clean accuracy and high robustness against state-of-the-art defense methods.

Conclusion: Latent-level attacks are more effective than pixel-level attacks for generative data augmentation scenarios, and InvLBA provides a theoretically guaranteed, effective clean-label backdoor attack method that maintains stealthiness and effectiveness.

Abstract: With the rapid advancement of image generative models, generative data augmentation has become an effective way to enrich training images, especially when only small-scale datasets are available. At the same time, in practical applications, generative data augmentation can be vulnerable to clean-label backdoor attacks, which aim to bypass human inspection. However, based on theoretical analysis and preliminary experiments, we observe that directly applying existing pixel-level clean-label backdoor attack methods (e.g., COMBAT) to generated images results in low attack success rates. This motivates us to move beyond pixel-level triggers and focus instead on the latent feature level. To this end, we propose InvLBA, an invisible clean-label backdoor attack method for generative data augmentation by latent perturbation. We theoretically prove that the generalization of the clean accuracy and attack success rates of InvLBA can be guaranteed. Experiments on multiple datasets show that our method improves the attack success rate by 46.43% on average, with almost no reduction in clean accuracy and high robustness against SOTA defense methods.

</details>


### [60] [MedSAM-Agent: Empowering Interactive Medical Image Segmentation with Multi-turn Agentic Reinforcement Learning](https://arxiv.org/abs/2602.03320)
*Shengyuan Liu,Liuxin Bao,Qi Yang,Wanting Geng,Boyun Zheng,Chenxin Li,Wenting Chen,Houwen Peng,Yixuan Yuan*

Main category: cs.CV

TL;DR: MedSAM-Agent is a multi-step autonomous decision-making framework for medical image segmentation that uses hybrid prompting for expert-curated trajectories and two-stage training with process rewards to improve interaction efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Current MLLM-based segmentation approaches use single-turn, rigid interactions and lack process-level supervision, leading to redundant actions and failure to fully exploit interactive tools' dynamic potential.

Method: Reformulates interactive segmentation as multi-step autonomous decision-making with: 1) hybrid prompting strategy for expert-curated trajectory generation, and 2) two-stage training pipeline integrating multi-turn end-to-end outcome verification with clinical-fidelity process reward design.

Result: Achieves state-of-the-art performance across 6 medical modalities and 21 datasets, effectively unifying autonomous medical reasoning with robust, iterative optimization.

Conclusion: MedSAM-Agent bridges the gap in current interactive segmentation approaches by enabling more efficient, adaptive multi-step decision-making with process-level supervision, demonstrating superior performance across diverse medical imaging tasks.

Abstract: Medical image segmentation is evolving from task-specific models toward generalizable frameworks. Recent research leverages Multi-modal Large Language Models (MLLMs) as autonomous agents, employing reinforcement learning with verifiable reward (RLVR) to orchestrate specialized tools like the Segment Anything Model (SAM). However, these approaches often rely on single-turn, rigid interaction strategies and lack process-level supervision during training, which hinders their ability to fully exploit the dynamic potential of interactive tools and leads to redundant actions. To bridge this gap, we propose MedSAM-Agent, a framework that reformulates interactive segmentation as a multi-step autonomous decision-making process. First, we introduce a hybrid prompting strategy for expert-curated trajectory generation, enabling the model to internalize human-like decision heuristics and adaptive refinement strategies. Furthermore, we develop a two-stage training pipeline that integrates multi-turn, end-to-end outcome verification with a clinical-fidelity process reward design to promote interaction parsimony and decision efficiency. Extensive experiments across 6 medical modalities and 21 datasets demonstrate that MedSAM-Agent achieves state-of-the-art performance, effectively unifying autonomous medical reasoning with robust, iterative optimization. Code is available \href{https://github.com/CUHK-AIM-Group/MedSAM-Agent}{here}.

</details>


### [61] [PWAVEP: Purifying Imperceptible Adversarial Perturbations in 3D Point Clouds via Spectral Graph Wavelets](https://arxiv.org/abs/2602.03333)
*Haoran Li,Renyang Liu,Hongjia Liu,Chen Wang,Long Yin,Jian Xu*

Main category: cs.CV

TL;DR: PWAVEP is a plug-and-play spectral domain defense that purifies 3D point clouds by removing adversarial outliers and filtering high-frequency noise using graph wavelet transforms.


<details>
  <summary>Details</summary>
Motivation: Current adversarial attacks on 3D point clouds achieve spatial imperceptibility and high attack performance, while existing defenses are cumbersome, requiring invasive model modifications, expensive training, or auxiliary data access.

Method: PWAVEP computes spectral graph wavelet domain saliency and local sparsity scores for each point, then uses a hierarchical strategy: eliminates highly salient adversarial outliers and applies spectral filtering to moderately salient points by attenuating high-frequency coefficients via graph wavelet transform.

Result: Extensive evaluations show PWAVEP achieves superior accuracy and robustness compared to existing approaches, advancing state-of-the-art in 3D point cloud purification.

Conclusion: PWAVEP provides an effective plug-and-play, non-invasive defense mechanism for 3D point clouds that addresses imperceptible adversarial perturbations through spectral domain analysis and purification.

Abstract: Recent progress in adversarial attacks on 3D point clouds, particularly in achieving spatial imperceptibility and high attack performance, presents significant challenges for defenders. Current defensive approaches remain cumbersome, often requiring invasive model modifications, expensive training procedures or auxiliary data access. To address these threats, in this paper, we propose a plug-and-play and non-invasive defense mechanism in the spectral domain, grounded in a theoretical and empirical analysis of the relationship between imperceptible perturbations and high-frequency spectral components. Building upon these insights, we introduce a novel purification framework, termed PWAVEP, which begins by computing a spectral graph wavelet domain saliency score and local sparsity score for each point. Guided by these values, PWAVEP adopts a hierarchical strategy, it eliminates the most salient points, which are identified as hardly recoverable adversarial outliers. Simultaneously, it applies a spectral filtering process to a broader set of moderately salient points. This process leverages a graph wavelet transform to attenuate high-frequency coefficients associated with the targeted points, thereby effectively suppressing adversarial noise. Extensive evaluations demonstrate that the proposed PWAVEP achieves superior accuracy and robustness compared to existing approaches, advancing the state-of-the-art in 3D point cloud purification. Code and datasets are available at https://github.com/a772316182/pwavep

</details>


### [62] [Composable Visual Tokenizers with Generator-Free Diagnostics of Learnability](https://arxiv.org/abs/2602.03339)
*Bingchen Zhao,Qiushan Guo,Ye Wang,Yixuan Huang,Zhonghua Zhai,Yu Tian*

Main category: cs.CV

TL;DR: CompTok is a framework for training visual tokenizers with enhanced compositionality using token-conditioned diffusion with InfoGAN-style objectives and token swapping for better compositional control.


<details>
  <summary>Details</summary>
Motivation: To create visual tokenizers that enable better compositional control in image generation, allowing tokens to represent meaningful semantic components that can be swapped and combined for high-level image editing.

Method: Uses token-conditioned diffusion decoder with InfoGAN-style objective (training recognition model to predict tokens from decoded images). Includes token swapping between images during training and applies adversarial flow regularizer to keep unpaired swap generations on natural-image distribution.

Result: Achieves state-of-the-art performance on image class-conditioned generation and enables high-level semantic editing through token swapping. Also improves on proposed metrics measuring token space compositionality and learnability.

Conclusion: CompTok successfully creates compositional visual tokenizers that support advanced image editing capabilities while maintaining strong generation performance, with proposed metrics providing useful evaluation of token space properties.

Abstract: We introduce CompTok, a training framework for learning visual tokenizers whose tokens are enhanced for compositionality. CompTok uses a token-conditioned diffusion decoder. By employing an InfoGAN-style objective, where we train a recognition model to predict the tokens used to condition the diffusion decoder using the decoded images, we enforce the decoder to not ignore any of the tokens. To promote compositional control, besides the original images, CompTok also trains on tokens formed by swapping token subsets between images, enabling more compositional control of the token over the decoder. As the swapped tokens between images do not have ground truth image targets, we apply a manifold constraint via an adversarial flow regularizer to keep unpaired swap generations on the natural-image distribution. The resulting tokenizer not only achieves state-of-the-art performance on image class-conditioned generation, but also demonstrates properties such as swapping tokens between images to achieve high level semantic editing of an image. Additionally, we propose two metrics that measures the landscape of the token space that can be useful to describe not only the compositionality of the tokens, but also how easy to learn the landscape is for a generator to be trained on this space. We show in experiments that CompTok can improve on both of the metrics as well as supporting state-of-the-art generators for class conditioned generation.

</details>


### [63] [Tiled Prompts: Overcoming Prompt Underspecification in Image and Video Super-Resolution](https://arxiv.org/abs/2602.03342)
*Bryan Sangwoo Kim,Jonghyun Park,Jong Chul Ye*

Main category: cs.CV

TL;DR: Tiled Prompts addresses prompt underspecification in diffusion-based super-resolution by generating tile-specific prompts for each latent tile, improving local guidance and reducing artifacts.


<details>
  <summary>Details</summary>
Motivation: Current text-conditioned diffusion models for super-resolution use single global captions that cause prompt underspecification - missing localized details (prompt sparsity) and providing irrelevant local guidance (prompt misguidance), especially amplified by classifier-free guidance.

Method: Proposes Tiled Prompts framework that generates tile-specific prompts for each latent tile and performs super-resolution under locally text-conditioned posteriors, providing high-information guidance with minimal overhead.

Result: Experiments on high-resolution real-world images and videos show consistent gains in perceptual quality and text alignment, while reducing hallucinations and tile-level artifacts compared to global-prompt baselines.

Conclusion: Tiled Prompts effectively resolves prompt underspecification in diffusion-based super-resolution by providing localized text guidance, improving both image/video quality and semantic alignment.

Abstract: Text-conditioned diffusion models have advanced image and video super-resolution by using prompts as semantic priors, but modern super-resolution pipelines typically rely on latent tiling to scale to high resolutions, where a single global caption causes prompt underspecification. A coarse global prompt often misses localized details (prompt sparsity) and provides locally irrelevant guidance (prompt misguidance) that can be amplified by classifier-free guidance. We propose Tiled Prompts, a unified framework for image and video super-resolution that generates a tile-specific prompt for each latent tile and performs super-resolution under locally text-conditioned posteriors, providing high-information guidance that resolves prompt underspecification with minimal overhead. Experiments on high resolution real-world images and videos show consistent gains in perceptual quality and text alignment, while reducing hallucinations and tile-level artifacts relative to global-prompt baselines.

</details>


### [64] [Z3D: Zero-Shot 3D Visual Grounding from Images](https://arxiv.org/abs/2602.03361)
*Nikita Drozdov,Andrey Lemeshko,Nikita Gavrilov,Anton Konushin,Danila Rukhovich,Maksim Kolodiazhnyi*

Main category: cs.CV

TL;DR: Z3D is a zero-shot 3D visual grounding method that localizes objects from multi-view images without geometric supervision, using advanced 3D instance segmentation and prompt-based segmentation with VLMs.


<details>
  <summary>Details</summary>
Motivation: To enable 3D visual grounding without requiring geometric supervision or object priors, addressing performance bottlenecks in prior zero-shot methods that degrade when applied to 3D scenes.

Method: Z3D pipeline operates on multi-view images with optional camera poses/depth maps, using: 1) state-of-the-art zero-shot 3D instance segmentation for high-quality bounding box proposals, and 2) prompt-based segmentation leveraging modern VLMs for advanced reasoning.

Result: Achieves state-of-the-art performance among zero-shot methods on ScanRefer and Nr3D benchmarks.

Conclusion: Z3D demonstrates effective zero-shot 3D visual grounding from multi-view images alone, overcoming key bottlenecks in prior methods through improved proposal generation and VLM-based reasoning.

Abstract: 3D visual grounding (3DVG) aims to localize objects in a 3D scene based on natural language queries. In this work, we explore zero-shot 3DVG from multi-view images alone, without requiring any geometric supervision or object priors. We introduce Z3D, a universal grounding pipeline that flexibly operates on multi-view images while optionally incorporating camera poses and depth maps. We identify key bottlenecks in prior zero-shot methods causing significant performance degradation and address them with (i) a state-of-the-art zero-shot 3D instance segmentation method to generate high-quality 3D bounding box proposals and (ii) advanced reasoning via prompt-based segmentation, which utilizes full capabilities of modern VLMs. Extensive experiments on the ScanRefer and Nr3D benchmarks demonstrate that our approach achieves state-of-the-art performance among zero-shot methods. Code is available at https://github.com/col14m/z3d .

</details>


### [65] [Symbol-Aware Reasoning with Masked Discrete Diffusion for Handwritten Mathematical Expression Recognition](https://arxiv.org/abs/2602.03370)
*Takaya Kawakatsu,Ryo Ishiyama*

Main category: cs.CV

TL;DR: Discrete diffusion framework for HMER that refines symbols iteratively instead of sequential generation, achieving state-of-the-art results on MathWriting and CROHME benchmarks.


<details>
  <summary>Details</summary>
Motivation: Autoregressive models for HMER struggle with exposure bias and syntactic inconsistency when dealing with diverse symbols and 2D structural layouts.

Method: Discrete diffusion framework with multi-step remasking that progressively refines symbols and structural relations, plus symbol-aware tokenization and Random-Masking Mutual Learning for enhanced syntactic alignment.

Result: Achieves 5.56% CER and 60.42% EM on MathWriting benchmark, outperforming Transformer and commercial baselines, with consistent gains on CROHME 2014-2023 datasets.

Conclusion: Discrete diffusion provides a new paradigm for structure-aware visual recognition beyond generative modeling, effectively addressing HMER challenges.

Abstract: Handwritten Mathematical Expression Recognition (HMER) requires reasoning over diverse symbols and 2D structural layouts, yet autoregressive models struggle with exposure bias and syntactic inconsistency. We present a discrete diffusion framework that reformulates HMER as iterative symbolic refinement instead of sequential generation. Through multi-step remasking, the proposal progressively refines both symbols and structural relations, removing causal dependencies and improving structural consistency. A symbol-aware tokenization and Random-Masking Mutual Learning further enhance syntactic alignment and robustness to handwriting diversity. On the MathWriting benchmark, the proposal achieves 5.56\% CER and 60.42\% EM, outperforming strong Transformer and commercial baselines. Consistent gains on CROHME 2014--2023 demonstrate that discrete diffusion provides a new paradigm for structure-aware visual recognition beyond generative modeling.

</details>


### [66] [Multi-Resolution Alignment for Voxel Sparsity in Camera-Based 3D Semantic Scene Completion](https://arxiv.org/abs/2602.03371)
*Zhiwen Yang,Yuxin Peng*

Main category: cs.CV

TL;DR: MRA approach uses multi-resolution alignment to address voxel sparsity in camera-based 3D semantic scene completion for autonomous driving.


<details>
  <summary>Details</summary>
Motivation: Existing camera-based 3D SSC methods suffer from voxel sparsity (many empty voxels in driving scenes), which limits optimization efficiency and model performance due to relying only on sparse voxel label supervision.

Method: Proposes Multi-Resolution Alignment (MRA) with three modules: 1) Multi-resolution View Transformer for scene-level alignment of 2D-to-3D features, 2) Cubic Semantic Anisotropy for instance-level semantic significance, 3) Critical Distribution Alignment for auxiliary supervision on critical voxels across resolutions.

Result: The approach mitigates voxel sparsity issues and improves camera-based 3D semantic scene completion performance (code available on GitHub).

Conclusion: MRA effectively addresses voxel sparsity in camera-based 3D SSC by exploiting multi-resolution alignment at both scene and instance levels, providing better optimization and performance for autonomous driving perception systems.

Abstract: Camera-based 3D semantic scene completion (SSC) offers a cost-effective solution for assessing the geometric occupancy and semantic labels of each voxel in the surrounding 3D scene with image inputs, providing a voxel-level scene perception foundation for the perception-prediction-planning autonomous driving systems. Although significant progress has been made in existing methods, their optimization rely solely on the supervision from voxel labels and face the challenge of voxel sparsity as a large portion of voxels in autonomous driving scenarios are empty, which limits both optimization efficiency and model performance. To address this issue, we propose a \textit{Multi-Resolution Alignment (MRA)} approach to mitigate voxel sparsity in camera-based 3D semantic scene completion, which exploits the scene and instance level alignment across multi-resolution 3D features as auxiliary supervision. Specifically, we first propose the Multi-resolution View Transformer module, which projects 2D image features into multi-resolution 3D features and aligns them at the scene level through fusing discriminative seed features. Furthermore, we design the Cubic Semantic Anisotropy module to identify the instance-level semantic significance of each voxel, accounting for the semantic differences of a specific voxel against its neighboring voxels within a cubic area. Finally, we devise a Critical Distribution Alignment module, which selects critical voxels as instance-level anchors with the guidance of cubic semantic anisotropy, and applies a circulated loss for auxiliary supervision on the critical feature distribution consistency across different resolutions. The code is available at https://github.com/PKU-ICST-MIPL/MRA_TIP.

</details>


### [67] [SLIM-Diff: Shared Latent Image-Mask Diffusion with Lp loss for Data-Scarce Epilepsy FLAIR MRI](https://arxiv.org/abs/2602.03372)
*Mario Pascual-González,Ariadna Jiménez-Partinen,R. M. Luque-Baena,Fátima Nagib-Raya,Ezequiel López-Rubio*

Main category: cs.CV

TL;DR: SLIM-Diff is a compact joint diffusion model for generating FCD lesions in epilepsy MRI using a shared-bottleneck U-Net with 2-channel image+mask representation and tunable Lp loss geometry.


<details>
  <summary>Details</summary>
Motivation: Focal cortical dysplasia lesions in epilepsy FLAIR MRI are subtle and scarce, making joint image-mask generative modeling prone to instability and memorization problems.

Method: Proposes SLIM-Diff with: (i) single shared-bottleneck U-Net enforcing tight coupling between anatomy and lesion geometry from 2-channel image+mask representation, (ii) loss-geometry tuning via tunable Lp objective, comparing x₀-prediction vs ε-prediction with L₂ loss.

Result: x₀-prediction is consistently the strongest choice for joint synthesis; fractional sub-quadratic penalties (L₁.₅) improve image fidelity while L₂ better preserves lesion mask morphology.

Conclusion: SLIM-Diff provides an effective compact joint diffusion model for FCD lesion synthesis with optimized prediction parameterization and loss geometry, with code and model weights publicly available.

Abstract: Focal cortical dysplasia (FCD) lesions in epilepsy FLAIR MRI are subtle and scarce, making joint image--mask generative modeling prone to instability and memorization. We propose SLIM-Diff, a compact joint diffusion model whose main contributions are (i) a single shared-bottleneck U-Net that enforces tight coupling between anatomy and lesion geometry from a 2-channel image+mask representation, and (ii) loss-geometry tuning via a tunable $L_p$ objective. As an internal baseline, we include the canonical DDPM-style objective ($ε$-prediction with $L_2$ loss) and isolate the effect of prediction parameterization and $L_p$ geometry under a matched setup. Experiments show that $x_0$-prediction is consistently the strongest choice for joint synthesis, and that fractional sub-quadratic penalties ($L_{1.5}$) improve image fidelity while $L_2$ better preserves lesion mask morphology. Our code and model weights are available in https://github.com/MarioPasc/slim-diff

</details>


### [68] [Unifying Watermarking via Dimension-Aware Mapping](https://arxiv.org/abs/2602.03373)
*Jiale Meng,Runyi Hu,Jie Zhang,Zheming Lu,Ivor Tsang,Tianwei Zhang*

Main category: cs.CV

TL;DR: DiM is a multi-dimensional watermarking framework that unifies existing methods by modeling watermark information as payloads of different dimensionalities (1D binary, 2D spatial, 3D spatiotemporal) and showing that dimensional configuration determines watermarking behavior.


<details>
  <summary>Details</summary>
Motivation: Deep watermarking methods share similar encoder-decoder architectures but differ substantially in functional behaviors. The authors aim to create a unified framework that explains these differences and enables new capabilities.

Method: DiM formulates watermarking as a dimension-aware mapping problem, modeling watermark information as payloads of different dimensionalities. The framework analyzes how same-dimensional vs cross-dimensional mappings affect watermarking behavior, with instantiation in the video domain using spatiotemporal representations.

Result: Experiments show that varying only embedding and extraction dimensions (without architectural changes) leads to different watermarking capabilities, including spatiotemporal tamper localization, local embedding control, and recovery of temporal order under frame disruptions.

Conclusion: DiM provides a unified functional-level understanding of watermarking methods, demonstrating that dimensional configuration is key to determining watermarking behavior and enabling new capabilities through dimension-aware mappings.

Abstract: Deep watermarking methods often share similar encoder-decoder architectures, yet differ substantially in their functional behaviors. We propose DiM, a new multi-dimensional watermarking framework that formulates watermarking as a dimension-aware mapping problem, thereby unifying existing watermarking methods at the functional level. Under DiM, watermark information is modeled as payloads of different dimensionalities, including one-dimensional binary messages, two-dimensional spatial masks, and three-dimensional spatiotemporal structures. We find that the dimensional configuration of embedding and extraction largely determines the resulting watermarking behavior. Same-dimensional mappings preserve payload structure and support fine-grained control, while cross-dimensional mappings enable spatial or spatiotemporal localization. We instantiate DiM in the video domain, where spatiotemporal representations enable a broader set of dimension mappings. Experiments demonstrate that varying only the embedding and extraction dimensions, without architectural changes, leads to different watermarking capabilities, including spatiotemporal tamper localization, local embedding control, and recovery of temporal order under frame disruptions.

</details>


### [69] [Seeing Through the Chain: Mitigate Hallucination in Multimodal Reasoning Models via CoT Compression and Contrastive Preference Optimization](https://arxiv.org/abs/2602.03380)
*Hao Fang,Jinyu Li,Jiawei Kong,Tianqu Zhuang,Kuofeng Gao,Bin Chen,Shu-Tao Xia,Yaowei Wang*

Main category: cs.CV

TL;DR: C3PO is a training framework that reduces hallucinations in multimodal reasoning models through chain-of-thought compression and contrastive preference optimization.


<details>
  <summary>Details</summary>
Motivation: Multimodal reasoning models suffer from hallucinations, and current solutions are insufficient. The authors identify that reasoning mechanisms exacerbate models' reliance on language priors while overlooking visual inputs, leading to hallucinations.

Method: C3PO framework with two components: 1) Chain-of-Thought Compression - selectively filters redundant thinking tokens for compact, signal-efficient CoT representations, and 2) Contrastive Preference Optimization - uses reasoning-enhanced preference tuning with AI feedback and a hallucination-inducing mechanism to create informative negative signals for contrastive correction.

Result: Demonstrates consistent hallucination reduction across diverse multimodal reasoning models and benchmarks, with theoretical justification for effectiveness.

Conclusion: C3PO effectively addresses hallucination in multimodal reasoning models through targeted compression of reasoning traces and contrastive optimization using carefully crafted negative signals.

Abstract: While multimodal reasoning models (MLRMs) have exhibited impressive capabilities, they remain prone to hallucinations, and effective solutions are still underexplored. In this paper, we experimentally analyze the hallucination cause and propose C3PO, a training-based mitigation framework comprising \textbf{C}hain-of-Thought \textbf{C}ompression and \textbf{C}ontrastive \textbf{P}reference \textbf{O}ptimization. Firstly, we identify that introducing reasoning mechanisms exacerbates models' reliance on language priors while overlooking visual inputs, which can produce CoTs with reduced visual cues but redundant text tokens. To this end, we propose to selectively filter redundant thinking tokens for a more compact and signal-efficient CoT representation that preserves task-relevant information while suppressing noise. In addition, we observe that the quality of the reasoning trace largely determines whether hallucination emerges in subsequent responses. To leverage this insight, we introduce a reasoning-enhanced preference tuning scheme that constructs training pairs using high-quality AI feedback. We further design a multimodal hallucination-inducing mechanism that elicits models' inherent hallucination patterns via carefully crafted inducers, yielding informative negative signals for contrastive correction. We provide theoretical justification for the effectiveness and demonstrate consistent hallucination reduction across diverse MLRMs and benchmarks.

</details>


### [70] [From Vicious to Virtuous Cycles: Synergistic Representation Learning for Unsupervised Video Object-Centric Learning](https://arxiv.org/abs/2602.03390)
*Hyun Seok Seong,WonJun Moon,Jae-Pil Heo*

Main category: cs.CV

TL;DR: SRL breaks the vicious cycle in slot-based object-centric learning by establishing mutual refinement between encoder and decoder, achieving SOTA results on video benchmarks.


<details>
  <summary>Details</summary>
Motivation: Slot-based object-centric learning suffers from a fundamental conflict: encoder produces sharp attention maps while decoder produces blurry reconstructions, creating a vicious cycle where noisy encoder features force decoder to average, and blurry reconstructions lack high-frequency details to supervise encoder.

Method: Synergistic Representation Learning (SRL) establishes a virtuous cycle where encoder and decoder mutually refine each other - encoder's sharpness deblurs decoder's semantic boundaries, while decoder's spatial consistency denoises encoder's features, stabilized by warm-up phase with slot regularization.

Result: SRL achieves state-of-the-art results on video object-centric learning benchmarks by bridging the representational gap between encoder and decoder.

Conclusion: The mutual refinement approach in SRL successfully breaks the vicious cycle in slot-based architectures, enabling better object-centric learning through synergistic encoder-decoder interaction.

Abstract: Unsupervised object-centric learning models, particularly slot-based architectures, have shown great promise in decomposing complex scenes. However, their reliance on reconstruction-based training creates a fundamental conflict between the sharp, high-frequency attention maps of the encoder and the spatially consistent but blurry reconstruction maps of the decoder. We identify that this discrepancy gives rise to a vicious cycle: the noisy feature map from the encoder forces the decoder to average over possibilities and produce even blurrier outputs, while the gradient computed from blurry reconstruction maps lacks high-frequency details necessary to supervise encoder features. To break this cycle, we introduce Synergistic Representation Learning (SRL) that establishes a virtuous cycle where the encoder and decoder mutually refine one another. SRL leverages the encoder's sharpness to deblur the semantic boundary within the decoder output, while exploiting the decoder's spatial consistency to denoise the encoder's features. This mutual refinement process is stabilized by a warm-up phase with a slot regularization objective that initially allocates distinct entities per slot. By bridging the representational gap between the encoder and decoder, SRL achieves state-of-the-art results on video object-centric learning benchmarks. Codes are available at https://github.com/hynnsk/SRL.

</details>


### [71] [UnHype: CLIP-Guided Hypernetworks for Dynamic LoRA Unlearning](https://arxiv.org/abs/2602.03410)
*Piotr Wójcik,Maksym Petrenko,Wojciech Gromski,Przemysław Spurek,Maciej Zieba*

Main category: cs.CV

TL;DR: UnHype is a hypernetwork-enhanced LoRA framework for scalable, context-aware machine unlearning in diffusion models, addressing limitations of existing methods in concept semantics and multi-concept removal.


<details>
  <summary>Details</summary>
Motivation: Address concerns about misuse of large-scale diffusion models by developing effective machine unlearning methods that can selectively remove harmful concepts while maintaining overall generative capabilities, overcoming limitations of current LoRA-based approaches in semantic adaptability and multi-concept scalability.

Method: Introduces UnHype framework incorporating hypernetworks into single- and multi-concept LoRA training, where hypernetworks dynamically generate adaptive LoRA weights based on CLIP embeddings during inference for context-aware unlearning; compatible with Stable Diffusion and modern flow-based text-to-image models.

Result: Demonstrates stable training behavior and effective concept control across challenging tasks including object erasure, celebrity erasure, and explicit content removal, showing effectiveness and versatility in machine unlearning.

Conclusion: UnHype provides a scalable, context-aware solution for machine unlearning in diffusion models that addresses semantic adaptability and multi-concept removal challenges while maintaining model performance.

Abstract: Recent advances in large-scale diffusion models have intensified concerns about their potential misuse, particularly in generating realistic yet harmful or socially disruptive content. This challenge has spurred growing interest in effective machine unlearning, the process of selectively removing specific knowledge or concepts from a model without compromising its overall generative capabilities. Among various approaches, Low-Rank Adaptation (LoRA) has emerged as an effective and efficient method for fine-tuning models toward targeted unlearning. However, LoRA-based methods often exhibit limited adaptability to concept semantics and struggle to balance removing closely related concepts with maintaining generalization across broader meanings. Moreover, these methods face scalability challenges when multiple concepts must be erased simultaneously. To address these limitations, we introduce UnHype, a framework that incorporates hypernetworks into single- and multi-concept LoRA training. The proposed architecture can be directly plugged into Stable Diffusion as well as modern flow-based text-to-image models, where it demonstrates stable training behavior and effective concept control. During inference, the hypernetwork dynamically generates adaptive LoRA weights based on the CLIP embedding, enabling more context-aware, scalable unlearning. We evaluate UnHype across several challenging tasks, including object erasure, celebrity erasure, and explicit content removal, demonstrating its effectiveness and versatility. Repository: https://github.com/gmum/UnHype.

</details>


### [72] [Socratic-Geo: Synthetic Data Generation and Geometric Reasoning via Multi-Agent Interaction](https://arxiv.org/abs/2602.03414)
*Zhengbo Jiao,Shaobo Wang,Zifan Zhang,Wei Wang,Bing Zhao,Hu Wei,Linfeng Zhang*

Main category: cs.CV

TL;DR: Socratic-Geo is an autonomous multi-agent framework that dynamically couples data synthesis with model learning to address geometric reasoning bottlenecks in MLLMs, achieving state-of-the-art results with minimal seed data.


<details>
  <summary>Details</summary>
Motivation: MLLMs struggle with geometric reasoning due to extreme scarcity of high-quality image-text pairs. Human annotation is expensive, automated methods lack fidelity, and existing approaches either passively adapt to available images or use inefficient random exploration with filtering.

Method: A three-agent framework: 1) Teacher agent generates parameterized Python scripts with reflective feedback (Reflect for solvability, RePI for visual validity) ensuring image-text pair purity; 2) Solver agent optimizes reasoning through preference learning, with failure paths guiding Teacher's targeted augmentation; 3) Generator learns image generation capabilities on accumulated "image-code-instruction" triplets, distilling programmatic drawing intelligence into visual generation.

Result: Starting from only 108 seed problems, Socratic-Solver achieves 49.11 on six benchmarks using one-quarter of baseline data, surpassing strong baselines by 2.43 points. Socratic-Generator achieves 42.4% on GenExam, establishing new state-of-the-art for open-source models, surpassing Seedream-4.0 (39.8%) and approaching Gemini-2.5-Flash-Image (43.1%).

Conclusion: Socratic-Geo demonstrates that autonomous multi-agent interaction can effectively address geometric reasoning bottlenecks in MLLMs by dynamically coupling data synthesis with model learning, achieving superior performance with minimal seed data and establishing new state-of-the-art results.

Abstract: Multimodal Large Language Models (MLLMs) have significantly advanced vision-language understanding. However, even state-of-the-art models struggle with geometric reasoning, revealing a critical bottleneck: the extreme scarcity of high-quality image-text pairs. Human annotation is prohibitively expensive, while automated methods fail to ensure fidelity and training effectiveness. Existing approaches either passively adapt to available images or employ inefficient random exploration with filtering, decoupling generation from learning needs. We propose Socratic-Geo, a fully autonomous framework that dynamically couples data synthesis with model learning through multi-agent interaction. The Teacher agent generates parameterized Python scripts with reflective feedback (Reflect for solvability, RePI for visual validity), ensuring image-text pair purity. The Solver agent optimizes reasoning through preference learning, with failure paths guiding Teacher's targeted augmentation. Independently, the Generator learns image generation capabilities on accumulated "image-code-instruction" triplets, distilling programmatic drawing intelligence into visual generation. Starting from only 108 seed problems, Socratic-Solver achieves 49.11 on six benchmarks using one-quarter of baseline data, surpassing strong baselines by 2.43 points. Socratic-Generator achieves 42.4% on GenExam, establishing new state-of-the-art for open-source models, surpassing Seedream-4.0 (39.8%) and approaching Gemini-2.5-Flash-Image (43.1%).

</details>


### [73] [ConsistentRFT: Reducing Visual Hallucinations in Flow-based Reinforcement Fine-Tuning](https://arxiv.org/abs/2602.03425)
*Xiaofeng Tan,Jun Liu,Yuanting Fan,Bin-Bin Gao,Xi Jiang,Xiaochen Chen,Jinlong Peng,Chengjie Wang,Hongsong Wang,Feng Zheng*

Main category: cs.CV

TL;DR: ConsistentRFT reduces visual hallucinations in flow-based models by addressing exploration-exploitation issues in reinforcement fine-tuning through dynamic granularity rollouts and consistent policy gradient optimization.


<details>
  <summary>Details</summary>
Motivation: Reinforcement Fine-Tuning (RFT) on flow-based models often introduces visual hallucinations like over-optimized details and semantic misalignment, which degrade model performance and reliability.

Method: Proposes ConsistentRFT framework with two key components: (1) Dynamic Granularity Rollout (DGR) mechanism that balances exploration between global semantics and local details by dynamically scheduling noise sources, and (2) Consistent Policy Gradient Optimization (CPGO) that preserves model consistency by aligning current policy with a stable prior.

Result: Significantly reduces visual hallucinations with average reductions of 49% for low-level and 38% for high-level perceptual hallucinations. Outperforms other RFT methods on out-of-domain metrics, showing 5.1% improvement vs. baseline's -0.4% decrease over FLUX1.dev.

Conclusion: ConsistentRFT effectively mitigates visual hallucinations in flow-based models by addressing core exploration-exploitation problems in RFT, leading to better performance and reduced artifacts while maintaining model consistency.

Abstract: Reinforcement Fine-Tuning (RFT) on flow-based models is crucial for preference alignment. However, they often introduce visual hallucinations like over-optimized details and semantic misalignment. This work preliminarily explores why visual hallucinations arise and how to reduce them. We first investigate RFT methods from a unified perspective, and reveal the core problems stemming from two aspects, exploration and exploitation: (1) limited exploration during stochastic differential equation (SDE) rollouts, leading to an over-emphasis on local details at the expense of global semantics, and (2) trajectory imitation process inherent in policy gradient methods, distorting the model's foundational vector field and its cross-step consistency. Building on this, we propose ConsistentRFT, a general framework to mitigate these hallucinations. Specifically, we design a Dynamic Granularity Rollout (DGR) mechanism to balance exploration between global semantics and local details by dynamically scheduling different noise sources. We then introduce a Consistent Policy Gradient Optimization (CPGO) that preserves the model's consistency by aligning the current policy with a more stable prior. Extensive experiments demonstrate that ConsistentRFT significantly mitigates visual hallucinations, achieving average reductions of 49\% for low-level and 38\% for high-level perceptual hallucinations. Furthermore, ConsistentRFT outperforms other RFT methods on out-of-domain metrics, showing an improvement of 5.1\% (v.s. the baseline's decrease of -0.4\%) over FLUX1.dev. This is \href{https://xiaofeng-tan.github.io/projects/ConsistentRFT}{Project Page}.

</details>


### [74] [Hierarchical Concept-to-Appearance Guidance for Multi-Subject Image Generation](https://arxiv.org/abs/2602.03448)
*Yijia Xu,Zihao Wang,Jinshi Cui*

Main category: cs.CV

TL;DR: CAG framework uses hierarchical guidance from concepts to appearances for better multi-subject image generation with improved identity consistency and compositional control.


<details>
  <summary>Details</summary>
Motivation: Existing multi-subject image generation methods suffer from identity inconsistency and limited compositional control because they rely on diffusion models to implicitly associate text prompts with reference images.

Method: Hierarchical Concept-to-Appearance Guidance (CAG) with two levels: 1) Conceptual level uses VAE dropout training to encourage reliance on VLM semantic signals, 2) Appearance level uses correspondence-aware masked attention in DiT to restrict text tokens to matched reference regions.

Result: Achieves state-of-the-art performance on multi-subject image generation, substantially improving prompt following and subject consistency.

Conclusion: Explicit hierarchical guidance from concepts to appearances enables more faithful multi-subject image generation with better identity preservation and compositional control.

Abstract: Multi-subject image generation aims to synthesize images that faithfully preserve the identities of multiple reference subjects while following textual instructions. However, existing methods often suffer from identity inconsistency and limited compositional control, as they rely on diffusion models to implicitly associate text prompts with reference images. In this work, we propose Hierarchical Concept-to-Appearance Guidance (CAG), a framework that provides explicit, structured supervision from high-level concepts to fine-grained appearances. At the conceptual level, we introduce a VAE dropout training strategy that randomly omits reference VAE features, encouraging the model to rely more on robust semantic signals from a Visual Language Model (VLM) and thereby promoting consistent concept-level generation in the absence of complete appearance cues. At the appearance level, we integrate the VLM-derived correspondences into a correspondence-aware masked attention module within the Diffusion Transformer (DiT). This module restricts each text token to attend only to its matched reference regions, ensuring precise attribute binding and reliable multi-subject composition. Extensive experiments demonstrate that our method achieves state-of-the-art performance on the multi-subject image generation, substantially improving prompt following and subject consistency.

</details>


### [75] [Contextualized Visual Personalization in Vision-Language Models](https://arxiv.org/abs/2602.03454)
*Yeongtak Oh,Sangwon Yu,Junsung Park,Han Cheol Moon,Jisoo Mok,Sungroh Yoon*

Main category: cs.CV

TL;DR: CoViP is a reinforcement-learning framework that enables VLMs to generate personalized responses by associating new images with users' visual-textual history, addressing contextualized visual personalization.


<details>
  <summary>Details</summary>
Motivation: Current VLMs fail to generate personalized responses based on users' specific visual experiences because they cannot associate new visual inputs with accumulated visual-textual context. The paper formalizes this as "contextualized visual personalization."

Method: CoViP treats personalized image captioning as a core task and improves this capability through reinforcement-learning-based post-training and caption-augmented generation. The framework includes diagnostic evaluations to ensure VLMs truly leverage visual context rather than textual shortcuts.

Result: Extensive experiments show existing VLMs have substantial limitations in contextualized visual personalization, while CoViP improves personalized image captioning and yields holistic gains across downstream personalization tasks.

Conclusion: CoViP represents a crucial advancement for enabling robust and generalizable contextualized visual personalization in vision-language models.

Abstract: Despite recent progress in vision-language models (VLMs), existing approaches often fail to generate personalized responses based on the user's specific experiences, as they lack the ability to associate visual inputs with a user's accumulated visual-textual context. We newly formalize this challenge as contextualized visual personalization, which requires the visual recognition and textual retrieval of personalized visual experiences by VLMs when interpreting new images. To address this issue, we propose CoViP, a unified framework that treats personalized image captioning as a core task for contextualized visual personalization and improves this capability through reinforcement-learning-based post-training and caption-augmented generation. We further introduce diagnostic evaluations that explicitly rule out textual shortcut solutions and verify whether VLMs truly leverage visual context. Extensive experiments demonstrate that existing open-source and proprietary VLMs exhibit substantial limitations, while CoViP not only improves personalized image captioning but also yields holistic gains across downstream personalization tasks. These results highlight CoViP as a crucial stage for enabling robust and generalizable contextualized visual personalization.

</details>


### [76] [Inlier-Centric Post-Training Quantization for Object Detection Models](https://arxiv.org/abs/2602.03472)
*Minsu Kim,Dongyeun Lee,Jaemyung Yu,Jiwan Hur,Giseop Kim,Junmo Kim*

Main category: cs.CV

TL;DR: InlierQ: A label-free post-training quantization method for object detection that separates task-irrelevant anomalies from informative inliers using gradient-aware volume saliency and EM algorithm, reducing quantization error with minimal calibration data.


<details>
  <summary>Details</summary>
Motivation: Object detection has high computational demands making deployment slow and power-hungry. Task-irrelevant morphologies like background clutter and sensor noise create redundant activations that expand activation ranges and skew distributions toward irrelevant responses, complicating bit allocation and weakening preservation of informative features.

Method: InlierQ computes gradient-aware volume saliency scores, classifies each volume as inlier or anomaly, and fits a posterior distribution over these scores using Expectation-Maximization (EM) algorithm. This suppresses anomalies while preserving informative features. The approach is label-free, drop-in, and requires only 64 calibration samples.

Result: Experiments on COCO and nuScenes benchmarks show consistent reductions in quantization error for camera-based (2D and 3D) and LiDAR-based (3D) object detection.

Conclusion: InlierQ effectively addresses the challenge of task-irrelevant anomalies in object detection quantization by separating anomalies from informative inliers, enabling more efficient bit allocation and better preservation of important features with minimal calibration requirements.

Abstract: Object detection is pivotal in computer vision, yet its immense computational demands make deployment slow and power-hungry, motivating quantization. However, task-irrelevant morphologies such as background clutter and sensor noise induce redundant activations (or anomalies). These anomalies expand activation ranges and skew activation distributions toward task-irrelevant responses, complicating bit allocation and weakening the preservation of informative features. Without a clear criterion to distinguish anomalies, suppressing them can inadvertently discard useful information. To address this, we present InlierQ, an inlier-centric post-training quantization approach that separates anomalies from informative inliers. InlierQ computes gradient-aware volume saliency scores, classifies each volume as an inlier or anomaly, and fits a posterior distribution over these scores using the Expectation-Maximization (EM) algorithm. This design suppresses anomalies while preserving informative features. InlierQ is label-free, drop-in, and requires only 64 calibration samples. Experiments on the COCO and nuScenes benchmarks show consistent reductions in quantization error for camera-based (2D and 3D) and LiDAR-based (3D) object detection.

</details>


### [77] [Decoupling Skeleton and Flesh: Efficient Multimodal Table Reasoning with Disentangled Alignment and Structure-aware Guidance](https://arxiv.org/abs/2602.03491)
*Yingjie Zhu,Xuefeng Bai,Kehai Chen,Yang Xiang,Youcheng Pan,Xiaoqiang Zhou,Min Zhang*

Main category: cs.CV

TL;DR: DiSCo disentangles table structure from content for better LVLM adaptation, and Table-GLS enables structured reasoning without external tools or heavy supervision.


<details>
  <summary>Details</summary>
Motivation: Table reasoning is hard for LVLMs due to complex layouts and intertwined structure-content info. Existing methods need expensive training, RL, or external tools, limiting efficiency and scalability.

Method: Two-stage framework: 1) DiSCo disentangles structure from content during multimodal alignment; 2) Table-GLS performs global-to-local structured reasoning via exploration and evidence-grounded inference.

Result: Extensive experiments show the framework efficiently enhances LVLM table understanding and reasoning, with strong generalization to unseen table structures.

Conclusion: The proposed approach adapts LVLMs to table reasoning with minimal annotation and no external tools, addressing key challenges in table image understanding.

Abstract: Reasoning over table images remains challenging for Large Vision-Language Models (LVLMs) due to complex layouts and tightly coupled structure-content information. Existing solutions often depend on expensive supervised training, reinforcement learning, or external tools, limiting efficiency and scalability. This work addresses a key question: how to adapt LVLMs to table reasoning with minimal annotation and no external tools? Specifically, we first introduce DiSCo, a Disentangled Structure-Content alignment framework that explicitly separates structural abstraction from semantic grounding during multimodal alignment, efficiently adapting LVLMs to tables structures. Building on DiSCo, we further present Table-GLS, a Global-to-Local Structure-guided reasoning framework that performs table reasoning via structured exploration and evidence-grounded inference. Extensive experiments across diverse benchmarks demonstrate that our framework efficiently enhances LVLM's table understanding and reasoning capabilities, particularly generalizing to unseen table structures.

</details>


### [78] [Semantic Routing: Exploring Multi-Layer LLM Feature Weighting for Diffusion Transformers](https://arxiv.org/abs/2602.03510)
*Bozhou Li,Yushuo Guan,Haolin Li,Bohan Zeng,Yiyan Ji,Yue Ding,Pengfei Wan,Kun Gai,Yuanxing Zhang,Wentao Zhang*

Main category: cs.CV

TL;DR: The paper introduces a normalized convex fusion framework with lightweight gates to dynamically route multi-layer LLM hidden states across diffusion time and network depth, finding depth-wise semantic routing superior for text-to-image generation.


<details>
  <summary>Details</summary>
Motivation: Current DiT-based text-to-image models use static text conditioning with single LLM layers, failing to leverage the semantic hierarchy across LLM layers and non-stationary denoising dynamics over diffusion time and network depth.

Method: A unified normalized convex fusion framework with lightweight gates for systematic organization of multi-layer LLM hidden states via time-wise, depth-wise, and joint fusion strategies.

Result: Depth-wise semantic routing consistently improves text-image alignment and compositional generation (+9.97 on GenAI-Bench Counting), while time-wise fusion degrades visual fidelity due to train-inference trajectory mismatch under classifier-free guidance.

Conclusion: Depth-wise routing is a strong baseline for dynamic text conditioning in DiT models, highlighting the need for trajectory-aware signals to enable robust time-dependent conditioning.

Abstract: Recent DiT-based text-to-image models increasingly adopt LLMs as text encoders, yet text conditioning remains largely static and often utilizes only a single LLM layer, despite pronounced semantic hierarchy across LLM layers and non-stationary denoising dynamics over both diffusion time and network depth. To better match the dynamic process of DiT generation and thereby enhance the diffusion model's generative capability, we introduce a unified normalized convex fusion framework equipped with lightweight gates to systematically organize multi-layer LLM hidden states via time-wise, depth-wise, and joint fusion. Experiments establish Depth-wise Semantic Routing as the superior conditioning strategy, consistently improving text-image alignment and compositional generation (e.g., +9.97 on the GenAI-Bench Counting task). Conversely, we find that purely time-wise fusion can paradoxically degrade visual generation fidelity. We attribute this to a train-inference trajectory mismatch: under classifier-free guidance, nominal timesteps fail to track the effective SNR, causing semantically mistimed feature injection during inference. Overall, our results position depth-wise routing as a strong and effective baseline and highlight the critical need for trajectory-aware signals to enable robust time-dependent conditioning.

</details>


### [79] [Interpretable Logical Anomaly Classification via Constraint Decomposition and Instruction Fine-Tuning](https://arxiv.org/abs/2602.03530)
*Xufei Zhang,Xinjiao Zhou,Ziling Deng,Dongdong Geng,Jianxiong Wang*

Main category: cs.CV

TL;DR: LogiCls is a vision-language framework that decomposes logical constraints into verifiable subqueries for industrial anomaly classification, providing both violation categories and evidence trails.


<details>
  <summary>Details</summary>
Motivation: Existing anomaly detection approaches treat it as binary classification, which doesn't indicate which logical rule is broken, limiting their value for quality assurance in industrial settings.

Method: LogiCls decomposes logical constraints into sequence of verifiable subqueries, uses data-centric instruction synthesis with chain-of-thought supervision, precise grounding annotations, diverse image-text augmentations, and difficulty-aware resampling for training stability.

Result: Extensive experiments show LogiCls delivers robust, interpretable, and accurate industrial logical anomaly classification with predicted violation categories and evidence trails.

Conclusion: The proposed Logical Anomaly Classification (LAC) task and LogiCls framework effectively unify anomaly detection and fine-grained violation classification, providing valuable interpretability for quality assurance.

Abstract: Logical anomalies are violations of predefined constraints on object quantity, spatial layout, and compositional relationships in industrial images. While prior work largely treats anomaly detection as a binary decision, such formulations cannot indicate which logical rule is broken and therefore offer limited value for quality assurance. We introduce Logical Anomaly Classification (LAC), a task that unifies anomaly detection and fine-grained violation classification in a single inference step. To tackle LAC, we propose LogiCls, a vision-language framework that decomposes complex logical constraints into a sequence of verifiable subqueries. We further present a data-centric instruction synthesis pipeline that generates chain-of-thought (CoT) supervision for these subqueries, coupling precise grounding annotations with diverse image-text augmentations to adapt vision language models (VLMs) to logic-sensitive reasoning. Training is stabilized by a difficulty-aware resampling strategy that emphasizes challenging subqueries and long tail constraint types. Extensive experiments demonstrate that LogiCls delivers robust, interpretable, and accurate industrial logical anomaly classification, providing both the predicted violation categories and their evidence trails.

</details>


### [80] [PnP-U3D: Plug-and-Play 3D Framework Bridging Autoregression and Diffusion for Unified Understanding and Generation](https://arxiv.org/abs/2602.03533)
*Yongwei Chen,Tianyi Wei,Yushi Lan,Zhaoyang Lyu,Shangchen Zhou,Xudong Xu,Xingang Pan*

Main category: cs.CV

TL;DR: Unified 3D framework combining autoregression for understanding and diffusion for generation, achieving SOTA across tasks with minimal performance compromise.


<details>
  <summary>Details</summary>
Motivation: Existing unified 3D frameworks using autoregressive paradigms suffer from performance degradation due to forced signal quantization and high training costs. The authors aim to create a unified 3D framework that effectively combines understanding and generation while preserving model capabilities and reducing training overhead.

Method: Combines autoregressive next-token prediction for 3D understanding with continuous diffusion for 3D generation. Uses a lightweight transformer to bridge feature spaces between large language models and 3D diffusion models, enabling cross-modal information exchange while preserving pretrained model priors.

Result: Achieves state-of-the-art performance across diverse 3D understanding and generation benchmarks, and excels in 3D editing tasks, demonstrating the effectiveness of the unified AR+diffusion approach.

Conclusion: The unified AR+diffusion framework represents a promising direction for building general-purpose 3D intelligence by effectively combining understanding and generation capabilities while minimizing performance compromise and training costs.

Abstract: The rapid progress of large multimodal models has inspired efforts toward unified frameworks that couple understanding and generation. While such paradigms have shown remarkable success in 2D, extending them to 3D remains largely underexplored. Existing attempts to unify 3D tasks under a single autoregressive (AR) paradigm lead to significant performance degradation due to forced signal quantization and prohibitive training cost. Our key insight is that the essential challenge lies not in enforcing a unified autoregressive paradigm, but in enabling effective information interaction between generation and understanding while minimally compromising their inherent capabilities and leveraging pretrained models to reduce training cost. Guided by this perspective, we present the first unified framework for 3D understanding and generation that combines autoregression with diffusion. Specifically, we adopt an autoregressive next-token prediction paradigm for 3D understanding, and a continuous diffusion paradigm for 3D generation. A lightweight transformer bridges the feature space of large language models and the conditional space of 3D diffusion models, enabling effective cross-modal information exchange while preserving the priors learned by standalone models. Extensive experiments demonstrate that our framework achieves state-of-the-art performance across diverse 3D understanding and generation benchmarks, while also excelling in 3D editing tasks. These results highlight the potential of unified AR+diffusion models as a promising direction for building more general-purpose 3D intelligence.

</details>


### [81] [Constrained Dynamic Gaussian Splatting](https://arxiv.org/abs/2602.03538)
*Zihan Zheng,Zhenglong Wu,Xuanxuan Wang,Houqiang Zhong,Xiaoyun Zhang,Qiang Hu,Guangtao Zhai,Wenjun Zhang*

Main category: cs.CV

TL;DR: CDGS is a budget-constrained dynamic Gaussian splatting framework that enforces strict Gaussian limits during training via a differentiable budget controller, achieving optimal rendering quality under hardware constraints with 3x compression over SOTA.


<details>
  <summary>Details</summary>
Motivation: Dynamic Gaussian Splatting faces a deployment dilemma: unconstrained densification causes excessive memory use incompatible with edge devices, while heuristic pruning fails to achieve optimal rendering quality under preset Gaussian budgets.

Method: Proposes Constrained Dynamic Gaussian Splatting (CDGS) with: 1) Differentiable budget controller using multi-modal unified importance score (geometric, motion, perceptual cues), 2) Decoupled optimization of static/dynamic elements with adaptive allocation, 3) Three-phase training strategy, 4) Dual-mode hybrid compression scheme.

Result: CDGS strictly adheres to hardware constraints (error < 2%), pushes Pareto frontier of rate-distortion performance, achieves over 3x compression compared to state-of-the-art methods, and delivers optimal rendering quality under varying capacity limits.

Conclusion: CDGS successfully formulates dynamic scene reconstruction as a budget-constrained optimization problem, enabling high-fidelity 4D reconstruction while strictly adhering to hardware constraints through novel differentiable budget control and adaptive allocation mechanisms.

Abstract: While Dynamic Gaussian Splatting enables high-fidelity 4D reconstruction, its deployment is severely hindered by a fundamental dilemma: unconstrained densification leads to excessive memory consumption incompatible with edge devices, whereas heuristic pruning fails to achieve optimal rendering quality under preset Gaussian budgets. In this work, we propose Constrained Dynamic Gaussian Splatting (CDGS), a novel framework that formulates dynamic scene reconstruction as a budget-constrained optimization problem to enforce a strict, user-defined Gaussian budget during training. Our key insight is to introduce a differentiable budget controller as the core optimization driver. Guided by a multi-modal unified importance score, this controller fuses geometric, motion, and perceptual cues for precise capacity regulation. To maximize the utility of this fixed budget, we further decouple the optimization of static and dynamic elements, employing an adaptive allocation mechanism that dynamically distributes capacity based on motion complexity. Furthermore, we implement a three-phase training strategy to seamlessly integrate these constraints, ensuring precise adherence to the target count. Coupled with a dual-mode hybrid compression scheme, CDGS not only strictly adheres to hardware constraints (error < 2%}) but also pushes the Pareto frontier of rate-distortion performance. Extensive experiments demonstrate that CDGS delivers optimal rendering quality under varying capacity limits, achieving over 3x compression compared to state-of-the-art methods.

</details>


### [82] [Cut to the Mix: Simple Data Augmentation Outperforms Elaborate Ones in Limited Organ Segmentation Datasets](https://arxiv.org/abs/2602.03555)
*Chang Liu,Fuxin Fan,Annette Schwarz,Andreas Maier*

Main category: cs.CV

TL;DR: The paper investigates four inter-image data augmentation strategies (CutMix, CarveMix, ObjectAug, AnatoMix) for multi-organ segmentation with limited data, finding that CutMix, CarveMix, and AnatoMix improve segmentation performance over baseline nnUNet.


<details>
  <summary>Details</summary>
Motivation: Deep learning segmentation models require large annotated datasets, but clinical data is often scarce. Traditional data augmentation uses intra-image operations, while inter-image strategies that combine elements from different images are underexplored for multi-organ segmentation.

Method: The authors investigated four inter-image data augmentation strategies: CutMix, CarveMix, ObjectAug, and AnatoMix. These were tested on two organ segmentation datasets and compared against state-of-the-art nnUNet without data augmentation. Additional experiments combined these with traditional data augmentation strategies.

Result: CutMix, CarveMix, and AnatoMix improved average dice scores by 4.9, 2.0, and 1.9 respectively compared to nnUNet without data augmentation. Performance further improved when combined with traditional data augmentation. CutMix proved particularly robust despite producing intuitively 'wrong' images.

Conclusion: Inter-image data augmentation strategies, especially CutMix, effectively improve multi-organ segmentation performance with limited data. CutMix is a robust and simple strategy that enhances segmentation even when it creates unrealistic images. The findings provide valuable benchmarks for future research.

Abstract: Multi-organ segmentation is a widely applied clinical routine and automated organ segmentation tools dramatically improve the pipeline of the radiologists. Recently, deep learning (DL) based segmentation models have shown the capacity to accomplish such a task. However, the training of the segmentation networks requires large amount of data with manual annotations, which is a major concern due to the data scarcity from clinic. Working with limited data is still common for researches on novel imaging modalities. To enhance the effectiveness of DL models trained with limited data, data augmentation (DA) is a crucial regularization technique. Traditional DA (TDA) strategies focus on basic intra-image operations, i.e. generating images with different orientations and intensity distributions. In contrast, the interimage and object-level DA operations are able to create new images from separate individuals. However, such DA strategies are not well explored on the task of multi-organ segmentation. In this paper, we investigated four possible inter-image DA strategies: CutMix, CarveMix, ObjectAug and AnatoMix, on two organ segmentation datasets. The result shows that CutMix, CarveMix and AnatoMix can improve the average dice score by 4.9, 2.0 and 1.9, compared with the state-of-the-art nnUNet without DA strategies. These results can be further improved by adding TDA strategies. It is revealed in our experiments that Cut-Mix is a robust but simple DA strategy to drive up the segmentation performance for multi-organ segmentation, even when CutMix produces intuitively 'wrong' images. Our implementation is publicly available for future benchmarks.

</details>


### [83] [ELIQ: A Label-Free Framework for Quality Assessment of Evolving AI-Generated Images](https://arxiv.org/abs/2602.03558)
*Xinyue Li,Zhiming Xu,Zhichao Zhang,Zhaolin Cai,Sijing Wu,Xiongkuo Min,Yitong Chen,Guangtao Zhai*

Main category: cs.CV

TL;DR: ELIQ is a label-free framework for assessing quality of evolving AI-generated images, focusing on visual quality and prompt-image alignment without human annotations.


<details>
  <summary>Details</summary>
Motivation: Generative text-to-image models are advancing rapidly, making previously collected quality labels unreliable for newer generations, creating a need for scalable, label-free quality assessment methods.

Method: Automatically constructs positive and aspect-specific negative pairs to cover conventional and AIGC-specific distortions, adapts pre-trained multimodal model via instruction tuning, and uses lightweight gated fusion with Quality Query Transformer for two-dimensional quality prediction.

Result: ELIQ consistently outperforms existing label-free methods across multiple benchmarks and generalizes from AIGC to user-generated content scenarios without modification.

Conclusion: ELIQ paves the way for scalable, label-free quality assessment under continuously evolving generative models, addressing the challenge of unreliable labels for newer AI-generated content.

Abstract: Generative text-to-image models are advancing at an unprecedented pace, continuously shifting the perceptual quality ceiling and rendering previously collected labels unreliable for newer generations. To address this, we present ELIQ, a Label-free Framework for Quality Assessment of Evolving AI-generated Images. Specifically, ELIQ focuses on visual quality and prompt-image alignment, automatically constructs positive and aspect-specific negative pairs to cover both conventional distortions and AIGC-specific distortion modes, enabling transferable supervision without human annotations. Building on these pairs, ELIQ adapts a pre-trained multimodal model into a quality-aware critic via instruction tuning and predicts two-dimensional quality using lightweight gated fusion and a Quality Query Transformer. Experiments across multiple benchmarks demonstrate that ELIQ consistently outperforms existing label-free methods, generalizes from AI-generated content (AIGC) to user-generated content (UGC) scenarios without modification, and paves the way for scalable and label-free quality assessment under continuously evolving generative models. The code will be released upon publication.

</details>


### [84] [SlowFocus: Enhancing Fine-grained Temporal Understanding in Video LLM](https://arxiv.org/abs/2602.03589)
*Ming Nie,Dan Ding,Chunwei Wang,Yuanfan Guo,Jianhua Han,Hang Xu,Li Zhang*

Main category: cs.CV

TL;DR: SlowFocus mechanism enhances video LLMs by dynamically focusing on query-relevant temporal segments with dense sampling while maintaining global context, improving fine-grained video understanding without compromising token quality.


<details>
  <summary>Details</summary>
Motivation: Current Video LLMs struggle to balance high-quality frame-level semantics (enough tokens per frame) with comprehensive video-level temporal information (enough sampled frames), limiting fine-grained video understanding capabilities.

Method: Introduces SlowFocus mechanism that: 1) Identifies query-related temporal segments, 2) Performs dense sampling on these segments for local high-frequency features, 3) Uses multi-frequency mixing attention to aggregate local details with global low-frequency contexts, plus training strategies for temporal grounding and reasoning.

Result: Demonstrates superiority across existing public video understanding benchmarks and the proposed FineAction-CGR benchmark for fine-grained temporal understanding tasks.

Conclusion: SlowFocus effectively addresses the trade-off between frame-level detail and temporal coverage in Video LLMs, enabling enhanced fine-grained video understanding through dynamic temporal focusing and multi-frequency feature aggregation.

Abstract: Large language models (LLMs) have demonstrated exceptional capabilities in text understanding, which has paved the way for their expansion into video LLMs (Vid-LLMs) to analyze video data. However, current Vid-LLMs struggle to simultaneously retain high-quality frame-level semantic information (i.e., a sufficient number of tokens per frame) and comprehensive video-level temporal information (i.e., an adequate number of sampled frames per video). This limitation hinders the advancement of Vid-LLMs towards fine-grained video understanding. To address this issue, we introduce the SlowFocus mechanism, which significantly enhances the equivalent sampling frequency without compromising the quality of frame-level visual tokens. SlowFocus begins by identifying the query-related temporal segment based on the posed question, then performs dense sampling on this segment to extract local high-frequency features. A multi-frequency mixing attention module is further leveraged to aggregate these local high-frequency details with global low-frequency contexts for enhanced temporal comprehension. Additionally, to tailor Vid-LLMs to this innovative mechanism, we introduce a set of training strategies aimed at bolstering both temporal grounding and detailed temporal reasoning capabilities. Furthermore, we establish FineAction-CGR, a benchmark specifically devised to assess the ability of Vid-LLMs to process fine-grained temporal understanding tasks. Comprehensive experiments demonstrate the superiority of our mechanism across both existing public video understanding benchmarks and our proposed FineAction-CGR.

</details>


### [85] [High-Resolution Underwater Camouflaged Object Detection: GBU-UCOD Dataset and Topology-Aware and Frequency-Decoupled Networks](https://arxiv.org/abs/2602.03591)
*Wenji Wu,Shuo Ye,Yiyu Liu,Jiguang He,Zhuo Wang,Zitong Yu*

Main category: cs.CV

TL;DR: DeepTopo-Net: A novel framework for Underwater Camouflaged Object Detection that integrates topology-aware modeling with frequency-decoupled perception to address challenges in marine environments.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with topological fragmentation of slender creatures in deep sea and subtle feature extraction of transparent organisms due to extreme visual similarity between targets and backgrounds across varying marine depths.

Method: Proposes DeepTopo-Net with two key components: 1) Water-Conditioned Adaptive Perceptor (WCAP) using Riemannian metric tensors to dynamically deform convolutional sampling fields to address physical degradation, and 2) Abyssal-Topology Refinement Module (ATRM) to maintain structural connectivity of spindly targets through skeletal priors. Also introduces GBU-UCOD, the first high-resolution (2K) benchmark for marine vertical zonation.

Result: Extensive experiments on MAS3K, RMAS, and the proposed GBU-UCOD datasets demonstrate state-of-the-art performance, particularly in preserving morphological integrity of complex underwater patterns.

Conclusion: DeepTopo-Net effectively addresses challenges in underwater camouflaged object detection through topology-aware modeling and frequency-decoupled perception, with the new GBU-UCOD benchmark filling data gaps for hadal and abyssal zones.

Abstract: Underwater Camouflaged Object Detection (UCOD) is a challenging task due to the extreme visual similarity between targets and backgrounds across varying marine depths. Existing methods often struggle with topological fragmentation of slender creatures in the deep sea and the subtle feature extraction of transparent organisms. In this paper, we propose DeepTopo-Net, a novel framework that integrates topology-aware modeling with frequency-decoupled perception. To address physical degradation, we design the Water-Conditioned Adaptive Perceptor (WCAP), which employs Riemannian metric tensors to dynamically deform convolutional sampling fields. Furthermore, the Abyssal-Topology Refinement Module (ATRM) is developed to maintain the structural connectivity of spindly targets through skeletal priors. Specifically, we first introduce GBU-UCOD, the first high-resolution (2K) benchmark tailored for marine vertical zonation, filling the data gap for hadal and abyssal zones. Extensive experiments on MAS3K, RMAS, and our proposed GBU-UCOD datasets demonstrate that DeepTopo-Net achieves state-of-the-art performance, particularly in preserving the morphological integrity of complex underwater patterns. The datasets and codes will be released at https://github.com/Wuwenji18/GBU-UCOD.

</details>


### [86] [TIPS Over Tricks: Simple Prompts for Effective Zero-shot Anomaly Detection](https://arxiv.org/abs/2602.03594)
*Alireza Salehi,Ehsan Karami,Sepehr Noey,Sahand Noey,Makoto Yamada,Reshad Hosseini,Mohammad Sabokrou*

Main category: cs.CV

TL;DR: TIPS-based zero-shot anomaly detection pipeline improves both image-level detection and pixel-level localization by addressing CLIP's spatial misalignment and weak sensitivity to fine-grained anomalies through decoupled prompts and local evidence injection.


<details>
  <summary>Details</summary>
Motivation: CLIP-based zero-shot anomaly detection suffers from spatial misalignment and weak sensitivity to fine-grained anomalies, limiting both localization and detection performance. Prior work focuses on complex auxiliary modules while overlooking backbone choice.

Method: Uses TIPS VLM as backbone (trained with spatially aware objectives), addresses distributional gap between global/local features with decoupled prompts (fixed for image-level detection, learnable for pixel-level localization), and injects local evidence into global score.

Result: Improves image-level performance by 1.1-3.9% and pixel-level by 1.5-6.9% across seven industrial datasets, delivering strong generalization with lean architecture without CLIP-specific tricks.

Conclusion: Revisiting backbone choice with TIPS VLM and addressing feature distribution gaps through decoupled prompts and local evidence injection provides effective zero-shot anomaly detection with improved performance and generalization.

Abstract: Anomaly detection identifies departures from expected behavior in safety-critical settings. When target-domain normal data are unavailable, zero-shot anomaly detection (ZSAD) leverages vision-language models (VLMs). However, CLIP's coarse image-text alignment limits both localization and detection due to (i) spatial misalignment and (ii) weak sensitivity to fine-grained anomalies; prior work compensates with complex auxiliary modules yet largely overlooks the choice of backbone. We revisit the backbone and use TIPS-a VLM trained with spatially aware objectives. While TIPS alleviates CLIP's issues, it exposes a distributional gap between global and local features. We address this with decoupled prompts-fixed for image-level detection and learnable for pixel-level localization-and by injecting local evidence into the global score. Without CLIP-specific tricks, our TIPS-based pipeline improves image-level performance by 1.1-3.9% and pixel-level by 1.5-6.9% across seven industrial datasets, delivering strong generalization with a lean architecture. Code is available at github.com/AlirezaSalehy/Tipsomaly.

</details>


### [87] [Refer-Agent: A Collaborative Multi-Agent System with Reasoning and Reflection for Referring Video Object Segmentation](https://arxiv.org/abs/2602.03595)
*Haichao Jiang,Tianming Liang,Wei-Shi Zheng,Jian-Fang Hu*

Main category: cs.CV

TL;DR: Refer-Agent: A collaborative multi-agent system with alternating reasoning-reflection mechanisms for zero-shot referring video object segmentation, outperforming both SFT-based and zero-shot methods without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Current RVOS methods rely heavily on supervised fine-tuning of MLLMs, which suffers from data dependence and limited scalability. Zero-shot approaches offer flexibility but underperform due to simplistic workflow designs.

Method: Proposes Refer-Agent, a multi-agent system with reasoning-reflection mechanisms. Features: 1) Coarse-to-Fine frame selection for diversity and relevance, 2) Dynamic Focus Layout for adaptive visual attention, 3) Chain-of-Reflection mechanism with Questioner-Responder pair for self-verification and feedback.

Result: Significantly outperforms state-of-the-art methods on five challenging benchmarks, including both SFT-based models and zero-shot approaches. Enables fast integration of new MLLMs without additional fine-tuning costs.

Conclusion: Refer-Agent provides an effective zero-shot solution for RVOS that addresses limitations of both SFT-based and existing zero-shot methods, offering superior performance, flexibility, and scalability without fine-tuning requirements.

Abstract: Referring Video Object Segmentation (RVOS) aims to segment objects in videos based on textual queries. Current methods mainly rely on large-scale supervised fine-tuning (SFT) of Multi-modal Large Language Models (MLLMs). However, this paradigm suffers from heavy data dependence and limited scalability against the rapid evolution of MLLMs. Although recent zero-shot approaches offer a flexible alternative, their performance remains significantly behind SFT-based methods, due to the straightforward workflow designs. To address these limitations, we propose \textbf{Refer-Agent}, a collaborative multi-agent system with alternating reasoning-reflection mechanisms. This system decomposes RVOS into step-by-step reasoning process. During reasoning, we introduce a Coarse-to-Fine frame selection strategy to ensure the frame diversity and textual relevance, along with a Dynamic Focus Layout that adaptively adjusts the agent's visual focus. Furthermore, we propose a Chain-of-Reflection mechanism, which employs a Questioner-Responder pair to generate a self-reflection chain, enabling the system to verify intermediate results and generates feedback for next-round reasoning refinement. Extensive experiments on five challenging benchmarks demonstrate that Refer-Agent significantly outperforms state-of-the-art methods, including both SFT-based models and zero-shot approaches. Moreover, Refer-Agent is flexible and enables fast integration of new MLLMs without any additional fine-tuning costs. Code will be released.

</details>


### [88] [A Lightweight Library for Energy-Based Joint-Embedding Predictive Architectures](https://arxiv.org/abs/2602.03604)
*Basile Terver,Randall Balestriero,Megi Dervishi,David Fan,Quentin Garrido,Tushar Nagarajan,Koustuv Sinha,Wancong Zhang,Mike Rabbat,Yann LeCun,Amir Bar*

Main category: cs.CV

TL;DR: EB-JEPA is an open-source library for learning representations and world models using Joint-Embedding Predictive Architectures that predict in representation space rather than pixel space, with implementations for images, video, and action-conditioned world models.


<details>
  <summary>Details</summary>
Motivation: To provide accessible, modular implementations of JEPA-based representation learning that avoids generative modeling pitfalls while capturing semantically meaningful features, and to demonstrate how these techniques scale from images to video and action-conditioned world models.

Method: Joint-Embedding Predictive Architectures that learn to predict in representation space rather than pixel space, with modular implementations for image-level SSL, video temporal modeling, and action-conditioned world models, all designed for single-GPU training within hours.

Result: Achieves 91% accuracy on CIFAR-10 probing, demonstrates multi-step prediction on Moving MNIST, and achieves 97% planning success rate on Two Rooms navigation task, with comprehensive ablations showing importance of regularization components.

Conclusion: EB-JEPA successfully provides accessible JEPA implementations that learn useful representations across image, video, and world modeling domains, with demonstrated effectiveness and comprehensive analysis of critical regularization components.

Abstract: We present EB-JEPA, an open-source library for learning representations and world models using Joint-Embedding Predictive Architectures (JEPAs). JEPAs learn to predict in representation space rather than pixel space, avoiding the pitfalls of generative modeling while capturing semantically meaningful features suitable for downstream tasks. Our library provides modular, self-contained implementations that illustrate how representation learning techniques developed for image-level self-supervised learning can transfer to video, where temporal dynamics add complexity, and ultimately to action-conditioned world models, where the model must additionally learn to predict the effects of control inputs. Each example is designed for single-GPU training within a few hours, making energy-based self-supervised learning accessible for research and education. We provide ablations of JEA components on CIFAR-10. Probing these representations yields 91% accuracy, indicating that the model learns useful features. Extending to video, we include a multi-step prediction example on Moving MNIST that demonstrates how the same principles scale to temporal modeling. Finally, we show how these representations can drive action-conditioned world models, achieving a 97% planning success rate on the Two Rooms navigation task. Comprehensive ablations reveal the critical importance of each regularization component for preventing representation collapse. Code is available at https://github.com/facebookresearch/eb_jepa.

</details>


### [89] [KTV: Keyframes and Key Tokens Selection for Efficient Training-Free Video LLMs](https://arxiv.org/abs/2602.03615)
*Baiyang Song,Jun Peng,Yuxin Zhang,Guangyao Chen,Feidiao Yang,Jianyuan Guo*

Main category: cs.CV

TL;DR: KTV is a two-stage training-free video understanding framework that reduces visual redundancy by first selecting keyframes through clustering, then pruning redundant visual tokens, achieving state-of-the-art performance with minimal computational overhead.


<details>
  <summary>Details</summary>
Motivation: Training-free video understanding suffers from severe visual redundancy and high computational overhead when processing long videos. Existing keyframe selection strategies based on CLIP similarity are prone to biases and may overlook critical frames, leading to suboptimal video comprehension.

Method: KTV uses a two-stage framework: 1) Question-agnostic keyframe selection by clustering frame-level visual features to get a compact, diverse subset of frames, and 2) Key visual token selection that prunes redundant or less informative tokens from each selected keyframe based on token importance and redundancy.

Result: KTV outperforms state-of-the-art training-free baselines on Multiple-Choice VideoQA tasks while using significantly fewer visual tokens (only 504 tokens for a 60-min video with 10800 frames), achieving 44.8% accuracy on MLVU-Test benchmark. It also exceeds several training-based approaches on certain benchmarks.

Conclusion: KTV provides an efficient and effective training-free video understanding framework that addresses visual redundancy and computational overhead through intelligent two-stage selection, achieving superior performance with minimal token usage.

Abstract: Training-free video understanding leverages the strong image comprehension capabilities of pre-trained vision language models (VLMs) by treating a video as a sequence of static frames, thus obviating the need for costly video-specific training. However, this paradigm often suffers from severe visual redundancy and high computational overhead, especially when processing long videos. Crucially, existing keyframe selection strategies, especially those based on CLIP similarity, are prone to biases and may inadvertently overlook critical frames, resulting in suboptimal video comprehension. To address these significant challenges, we propose \textbf{KTV}, a novel two-stage framework for efficient and effective training-free video understanding. In the first stage, KTV performs question-agnostic keyframe selection by clustering frame-level visual features, yielding a compact, diverse, and representative subset of frames that mitigates temporal redundancy. In the second stage, KTV applies key visual token selection, pruning redundant or less informative tokens from each selected keyframe based on token importance and redundancy, which significantly reduces the number of tokens fed into the LLM. Extensive experiments on the Multiple-Choice VideoQA task demonstrate that KTV outperforms state-of-the-art training-free baselines while using significantly fewer visual tokens, \emph{e.g.}, only 504 visual tokens for a 60-min video with 10800 frames, achieving $44.8\%$ accuracy on the MLVU-Test benchmark. In particular, KTV also exceeds several training-based approaches on certain benchmarks.

</details>


### [90] [Quasi-multimodal-based pathophysiological feature learning for retinal disease diagnosis](https://arxiv.org/abs/2602.03622)
*Lu Zhang,Huizhen Yu,Zuowei Wang,Fu Gui,Yatu Guo,Wei Zhang,Mengyu Jia*

Main category: cs.CV

TL;DR: A unified multimodal framework for retinal disease classification using synthesized FFA, MSI, and saliency maps with adaptive feature calibration and cross-modality integration.


<details>
  <summary>Details</summary>
Motivation: Multimodal retinal diagnosis faces challenges with data heterogeneity, invasiveness, and registration complexity, requiring a unified approach to integrate diverse imaging data for better disease identification.

Method: Proposes a framework that synthesizes multimodal data (FFA, MSI, saliency maps), trains parallel models for modality-specific representations, then adaptively calibrates features within and across modalities for information pruning and flexible integration.

Result: Achieves superior performance on two public datasets: multi-label classification (F1-score: 0.683, AUC: 0.953) and diabetic retinopathy grading (Accuracy: 0.842, Kappa: 0.861), outperforming state-of-the-art methods.

Conclusion: The framework enhances retinal disease screening accuracy and efficiency while providing a scalable approach for medical imaging data augmentation across various modalities.

Abstract: Retinal diseases spanning a broad spectrum can be effectively identified and diagnosed using complementary signals from multimodal data. However, multimodal diagnosis in ophthalmic practice is typically challenged in terms of data heterogeneity, potential invasiveness, registration complexity, and so on. As such, a unified framework that integrates multimodal data synthesis and fusion is proposed for retinal disease classification and grading. Specifically, the synthesized multimodal data incorporates fundus fluorescein angiography (FFA), multispectral imaging (MSI), and saliency maps that emphasize latent lesions as well as optic disc/cup regions. Parallel models are independently trained to learn modality-specific representations that capture cross-pathophysiological signatures. These features are then adaptively calibrated within and across modalities to perform information pruning and flexible integration according to downstream tasks. The proposed learning system is thoroughly interpreted through visualizations in both image and feature spaces. Extensive experiments on two public datasets demonstrated the superiority of our approach over state-of-the-art ones in the tasks of multi-label classification (F1-score: 0.683, AUC: 0.953) and diabetic retinopathy grading (Accuracy:0.842, Kappa: 0.861). This work not only enhances the accuracy and efficiency of retinal disease screening but also offers a scalable framework for data augmentation across various medical imaging modalities.

</details>


### [91] [Multi-Objective Optimization for Synthetic-to-Real Style Transfer](https://arxiv.org/abs/2602.03625)
*Estelle Chigot,Thomas Oberlin,Manon Huguenin,Dennis Wilson*

Main category: cs.CV

TL;DR: Evolutionary optimization of style transfer pipelines for synthetic-to-real domain adaptation in semantic segmentation, using multi-objective genetic algorithms to balance structural coherence and style similarity.


<details>
  <summary>Details</summary>
Motivation: Semantic segmentation requires costly pixel-level annotations. Synthetic data from graphics engines is cheaper but suffers from domain gap. Style transfer can bridge this gap, but designing effective transformation pipelines is challenging due to large combinatorial search space.

Method: Use multi-objective genetic algorithms to optimize style transfer pipelines, balancing structural coherence and style similarity. Employ paired-image metrics on individual samples during evolution for rapid evaluation, then validate with distributional metrics and segmentation performance on target datasets.

Result: Evolutionary algorithms can propose diverse augmentation pipelines adapted to different objectives. Applied to GTA5→Cityscapes/ACDC domain adaptation, the approach demonstrates effective pipeline optimization for adverse conditions.

Conclusion: Formulating style transfer as a sequencing problem suitable for evolutionary optimization, with efficient metrics enabling feasible search, provides an effective approach for synthetic-to-real domain adaptation in semantic segmentation.

Abstract: Semantic segmentation networks require large amounts of pixel-level annotated data, which are costly to obtain for real-world images. Computer graphics engines can generate synthetic images alongside their ground-truth annotations. However, models trained on such images can perform poorly on real images due to the domain gap between real and synthetic images. Style transfer methods can reduce this difference by applying a realistic style to synthetic images. Choosing effective data transformations and their sequence is difficult due to the large combinatorial search space of style transfer operators. Using multi-objective genetic algorithms, we optimize pipelines to balance structural coherence and style similarity to target domains. We study the use of paired-image metrics on individual image samples during evolution to enable rapid pipeline evaluation, as opposed to standard distributional metrics that require the generation of many images. After optimization, we evaluate the resulting Pareto front using distributional metrics and segmentation performance. We apply this approach to standard datasets in synthetic-to-real domain adaptation: from the video game GTA5 to real image datasets Cityscapes and ACDC, focusing on adverse conditions. Results demonstrate that evolutionary algorithms can propose diverse augmentation pipelines adapted to different objectives. The contribution of this work is the formulation of style transfer as a sequencing problem suitable for evolutionary optimization and the study of efficient metrics that enable feasible search in this space. The source code is available at: https://github.com/echigot/MOOSS.

</details>


### [92] [SPWOOD: Sparse Partial Weakly-Supervised Oriented Object Detection](https://arxiv.org/abs/2602.03634)
*Wei Zhang,Xiang Liu,Ningjing Liu,Mingxin Liu,Wei Liao,Chunyan Xu,Xue Yang*

Main category: cs.CV

TL;DR: First Sparse Partial Weakly-Supervised Oriented Object Detection framework that uses few sparse weakly-labeled data and plenty of unlabeled data to reduce annotation costs in remote sensing.


<details>
  <summary>Details</summary>
Motivation: Reduce prohibitively high annotation costs in remote sensing due to dense object distribution and wide variety of categories, by using fewer and weaker annotations while maintaining performance.

Method: SPWOOD framework with three innovations: SOS-Student model for separating unlabeled objects and learning from weak annotations, Multi-level Pseudo-label Filtering using model prediction distributions, and sparse partitioning for equal category treatment.

Result: Extensive experiments on DOTA and DIOR datasets show significant performance gain over traditional oriented object detection methods, offering highly cost-effective solution.

Conclusion: Proposed framework successfully addresses large-scale labeling challenges in remote sensing by efficiently leveraging sparse weakly-labeled and unlabeled data, achieving state-of-the-art performance with reduced annotation costs.

Abstract: A consistent trend throughout the research of oriented object detection has been the pursuit of maintaining comparable performance with fewer and weaker annotations. This is particularly crucial in the remote sensing domain, where the dense object distribution and a wide variety of categories contribute to prohibitively high costs. Based on the supervision level, existing oriented object detection algorithms can be broadly grouped into fully supervised, semi-supervised, and weakly supervised methods. Within the scope of this work, we further categorize them to include sparsely supervised and partially weakly-supervised methods. To address the challenges of large-scale labeling, we introduce the first Sparse Partial Weakly-Supervised Oriented Object Detection framework, designed to efficiently leverage only a few sparse weakly-labeled data and plenty of unlabeled data. Our framework incorporates three key innovations: (1) We design a Sparse-annotation-Orientation-and-Scale-aware Student (SOS-Student) model to separate unlabeled objects from the background in a sparsely-labeled setting, and learn orientation and scale information from orientation-agnostic or scale-agnostic weak annotations. (2) We construct a novel Multi-level Pseudo-label Filtering strategy that leverages the distribution of model predictions, which is informed by the model's multi-layer predictions. (3) We propose a unique sparse partitioning approach, ensuring equal treatment for each category. Extensive experiments on the DOTA and DIOR datasets show that our framework achieves a significant performance gain over traditional oriented object detection methods mentioned above, offering a highly cost-effective solution. Our code is publicly available at https://github.com/VisionXLab/SPWOOD.

</details>


### [93] [MM-SCALE: Grounded Multimodal Moral Reasoning via Scalar Judgment and Listwise Alignment](https://arxiv.org/abs/2602.03665)
*Eunkyu Park,Wesley Hanwen Deng,Cheyon Jin,Matheus Kunzler Maldaner,Jordan Wheeler,Jason I. Hong,Hong Shen,Adam Perer,Ken Holstein,Motahhare Eslami,Gunhee Kim*

Main category: cs.CV

TL;DR: MM-SCALE is a multimodal moral reasoning dataset with 5-point scalar ratings and modality grounding for better VLM alignment with human moral preferences.


<details>
  <summary>Details</summary>
Motivation: Current VLMs struggle with morally salient judgments in ambiguous contexts, and prior binary/pairwise supervision fails to capture the continuous, pluralistic nature of human moral reasoning.

Method: Created MM-SCALE dataset with image-scenario pairs annotated with 5-point moral acceptability scores and grounded reasoning labels using a tailored interface, enabling listwise preference optimization over ranked scenario sets.

Result: VLMs fine-tuned on MM-SCALE achieve higher ranking fidelity and more stable safety calibration compared to those trained with binary signals.

Conclusion: Scalar supervision provides richer alignment signals and finer calibration of multimodal moral reasoning, moving beyond discrete supervision to better capture human moral preferences.

Abstract: Vision-Language Models (VLMs) continue to struggle to make morally salient judgments in multimodal and socially ambiguous contexts. Prior works typically rely on binary or pairwise supervision, which often fail to capture the continuous and pluralistic nature of human moral reasoning. We present MM-SCALE (Multimodal Moral Scale), a large-scale dataset for aligning VLMs with human moral preferences through 5-point scalar ratings and explicit modality grounding. Each image-scenario pair is annotated with moral acceptability scores and grounded reasoning labels by humans using an interface we tailored for data collection, enabling listwise preference optimization over ranked scenario sets. By moving from discrete to scalar supervision, our framework provides richer alignment signals and finer calibration of multimodal moral reasoning. Experiments show that VLMs fine-tuned on MM-SCALE achieve higher ranking fidelity and more stable safety calibration than those trained with binary signals.

</details>


### [94] [Efficient Sequential Neural Network with Spatial-Temporal Attention and Linear LSTM for Robust Lane Detection Using Multi-Frame Images](https://arxiv.org/abs/2602.03669)
*Sandeep Patil,Yongqi Dong,Haneen Farah,Hans Hellendoorn*

Main category: cs.CV

TL;DR: Novel sequential neural network with spatial-temporal attention mechanism for robust lane detection in autonomous vehicles, outperforming SOTA methods with fewer parameters and MACs.


<details>
  <summary>Details</summary>
Motivation: Current lane detection methods lack versatility for accurate, robust, real-time performance in mixed-traffic environments, especially vision-based methods that neglect critical image regions and spatial-temporal salience, leading to poor performance in challenging conditions like occlusion and lighting issues.

Method: Proposes a sequential neural network model with spatial-temporal attention mechanism to focus on key lane line features and exploit salient correlations among continuous image frames, built on standard encoder-decoder structure with common neural network backbones.

Result: Extensive experiments on three large-scale open-source datasets demonstrate superior performance over state-of-the-art methods in various testing scenarios, with fewer parameters and reduced Multiply-Accumulate Operations (MACs) compared to baseline sequential models.

Conclusion: The proposed spatial-temporal attention mechanism enables robust, accurate, and computationally efficient lane detection suitable for real-time autonomous vehicle applications in challenging mixed-traffic environments.

Abstract: Lane detection is a crucial perception task for all levels of automated vehicles (AVs) and Advanced Driver Assistance Systems, particularly in mixed-traffic environments where AVs must interact with human-driven vehicles (HDVs) and challenging traffic scenarios. Current methods lack versatility in delivering accurate, robust, and real-time compatible lane detection, especially vision-based methods often neglect critical regions of the image and their spatial-temporal (ST) salience, leading to poor performance in difficult circumstances such as serious occlusion and dazzle lighting. This study introduces a novel sequential neural network model with a spatial-temporal attention mechanism to focus on key features of lane lines and exploit salient ST correlations among continuous image frames. The proposed model, built on a standard encoder-decoder structure and common neural network backbones, is trained and evaluated on three large-scale open-source datasets. Extensive experiments demonstrate the strength and robustness of the proposed model, outperforming state-of-the-art methods in various testing scenarios. Furthermore, with the ST attention mechanism, the developed sequential neural network models exhibit fewer parameters and reduced Multiply-Accumulate Operations (MACs) compared to baseline sequential models, highlighting their computational efficiency. Relevant data, code, and models are released at https://doi.org/10.4121/4619cab6-ae4a-40d5-af77-582a77f3d821.

</details>


### [95] [Referring Industrial Anomaly Segmentation](https://arxiv.org/abs/2602.03673)
*Pengfei Yue,Xiaokang Jiang,Yilin Lu,Jianghang Lin,Shengchuan Zhang,Liujuan Cao*

Main category: cs.CV

TL;DR: RIAS introduces a language-guided paradigm for industrial anomaly detection using text descriptions to generate precise masks without manual thresholds, enabling single-model detection of diverse anomalies.


<details>
  <summary>Details</summary>
Motivation: Traditional IAD methods face challenges: unsupervised approaches need manual thresholds for rough localizations, supervised methods overfit due to scarce imbalanced data, and both suffer from "One Anomaly Class, One Model" limitation requiring separate models for different anomaly types.

Method: Proposes RIAS paradigm using language to guide detection, introduces MVTec-Ref dataset with diverse referring expressions focusing on anomaly patterns, and develops DQFormer benchmark with Dual Query Token (Anomaly/Background) and Language-Gated Multi-Level Aggregation for efficient visual-textual integration.

Result: Experiments demonstrate RIAS's effectiveness in advancing IAD toward open-set capabilities, with MVTec-Ref dataset containing 95% small anomalies and diverse referring expressions to support language-guided detection.

Conclusion: RIAS represents a significant advancement in industrial anomaly detection by leveraging language to overcome traditional limitations, enabling precise mask generation without manual thresholds and single-model detection of diverse anomalies through universal prompts.

Abstract: Industrial Anomaly Detection (IAD) is vital for manufacturing, yet traditional methods face significant challenges: unsupervised approaches yield rough localizations requiring manual thresholds, while supervised methods overfit due to scarce, imbalanced data. Both suffer from the "One Anomaly Class, One Model" limitation. To address this, we propose Referring Industrial Anomaly Segmentation (RIAS), a paradigm leveraging language to guide detection. RIAS generates precise masks from text descriptions without manual thresholds and uses universal prompts to detect diverse anomalies with a single model. We introduce the MVTec-Ref dataset to support this, designed with diverse referring expressions and focusing on anomaly patterns, notably with 95% small anomalies. We also propose the Dual Query Token with Mask Group Transformer (DQFormer) benchmark, enhanced by Language-Gated Multi-Level Aggregation (LMA) to improve multi-scale segmentation. Unlike traditional methods using redundant queries, DQFormer employs only "Anomaly" and "Background" tokens for efficient visual-textual integration. Experiments demonstrate RIAS's effectiveness in advancing IAD toward open-set capabilities. Code: https://github.com/swagger-coder/RIAS-MVTec-Ref.

</details>


### [96] [RegionReasoner: Region-Grounded Multi-Round Visual Reasoning](https://arxiv.org/abs/2602.03733)
*Wenfang Sun,Hao Chen,Yingjun Du,Yefeng Zheng,Cees G. M. Snoek*

Main category: cs.CV

TL;DR: RegionReasoner is a reinforcement learning framework for multi-round visual reasoning that enforces grounded reasoning by citing reference bounding boxes and maintains semantic coherence via global-local consistency rewards, achieving improved accuracy on new benchmark RegionDial-Bench.


<details>
  <summary>Details</summary>
Motivation: Current vision-language models rely on single-step or text-only reasoning, limiting their ability to iteratively refine understanding across multiple visual contexts. There's a need for systems that can perform multi-round visual reasoning with explicit spatial grounding.

Method: Proposes RegionReasoner, a reinforcement learning framework that requires each reasoning trace to explicitly cite corresponding reference bounding boxes. Uses global-local consistency reward that extracts key objects/nouns from global scene captions and region-level captions, aligning them with reasoning traces. Optimized with structured rewards combining grounding fidelity and semantic alignment.

Result: RegionReasoner-7B with new benchmark RegionDial-Bench considerably improves multi-round reasoning accuracy, spatial grounding precision, and global-local consistency on detection and segmentation tasks, establishing a strong baseline for multi-round visual reasoning.

Conclusion: The proposed RegionReasoner framework addresses limitations of single-step reasoning by enabling iterative visual reasoning with explicit spatial grounding and semantic coherence, demonstrating effectiveness through systematic evaluation on a new multi-round reasoning benchmark.

Abstract: Large vision-language models have achieved remarkable progress in visual reasoning, yet most existing systems rely on single-step or text-only reasoning, limiting their ability to iteratively refine understanding across multiple visual contexts. To address this limitation, we introduce a new multi-round visual reasoning benchmark with training and test sets spanning both detection and segmentation tasks, enabling systematic evaluation under iterative reasoning scenarios. We further propose RegionReasoner, a reinforcement learning framework that enforces grounded reasoning by requiring each reasoning trace to explicitly cite the corresponding reference bounding boxes, while maintaining semantic coherence via a global-local consistency reward. This reward extracts key objects and nouns from both global scene captions and region-level captions, aligning them with the reasoning trace to ensure consistency across reasoning steps. RegionReasoner is optimized with structured rewards combining grounding fidelity and global-local semantic alignment. Experiments on detection and segmentation tasks show that RegionReasoner-7B, together with our newly introduced benchmark RegionDial-Bench, considerably improves multi-round reasoning accuracy, spatial grounding precision, and global-local consistency, establishing a strong baseline for this emerging research direction.

</details>


### [97] [Zero-shot large vision-language model prompting for automated bone identification in paleoradiology x-ray archives](https://arxiv.org/abs/2602.03750)
*Owen Dong,Lily Gao,Manish Kota,Bennett A. Landmana,Jelena Bekvalac,Gaynor Western,Katherine D. Van Schaik*

Main category: cs.CV

TL;DR: Using large vision-language models to automatically identify bones, projection views, and laterality in heterogeneous paleoradiology images through zero-shot prompting, achieving 92% bone accuracy and 100% laterality accuracy.


<details>
  <summary>Details</summary>
Motivation: Paleoradiology images are highly heterogeneous with disarticulated bones, inconsistent positioning, and missing laterality markers, making manual content navigation time-consuming and inefficient for expert analysis. There's a need for automated triaging to accelerate analysis of large archaeological datasets.

Method: Zero-shot prompting strategy using a state-of-the-art Large Vision Language Model (LVLM). Pipeline converts raw DICOM files to bone-windowed PNGs, submits them to LVLM with carefully engineered prompts, and extracts structured JSON outputs that are formatted into spreadsheets for validation.

Result: On 100 expert-reviewed images: 92% main bone accuracy, 80% projection view accuracy, and 100% laterality accuracy. System includes confidence flags for ambiguous cases (low/medium confidence).

Conclusion: LVLMs can substantially accelerate code word development for large paleoradiology datasets, enabling efficient content navigation and triaging in future anthropology workflows despite the heterogeneity of archaeological remains.

Abstract: Paleoradiology, the use of modern imaging technologies to study archaeological and anthropological remains, offers new windows on millennial scale patterns of human health. Unfortunately, the radiographs collected during field campaigns are heterogeneous: bones are disarticulated, positioning is ad hoc, and laterality markers are often absent. Additionally, factors such as age at death, age of bone, sex, and imaging equipment introduce high variability. Thus, content navigation, such as identifying a subset of images with a specific projection view, can be time consuming and difficult, making efficient triaging a bottleneck for expert analysis. We report a zero shot prompting strategy that leverages a state of the art Large Vision Language Model (LVLM) to automatically identify the main bone, projection view, and laterality in such images. Our pipeline converts raw DICOM files to bone windowed PNGs, submits them to the LVLM with a carefully engineered prompt, and receives structured JSON outputs, which are extracted and formatted onto a spreadsheet in preparation for validation. On a random sample of 100 images reviewed by an expert board certified paleoradiologist, the system achieved 92% main bone accuracy, 80% projection view accuracy, and 100% laterality accuracy, with low or medium confidence flags for ambiguous cases. These results suggest that LVLMs can substantially accelerate code word development for large paleoradiology datasets, allowing for efficient content navigation in future anthropology workflows.

</details>


### [98] [Edge-Optimized Vision-Language Models for Underground Infrastructure Assessment](https://arxiv.org/abs/2602.03742)
*Johny J. Lopez,Md Meftahul Ferdaus,Mahdi Abdelguerfi*

Main category: cs.CV

TL;DR: Lightweight two-stage pipeline for real-time defect segmentation and natural language summarization of underground infrastructure deficiencies on edge devices.


<details>
  <summary>Details</summary>
Motivation: Automated generation of human-readable summaries from robotic inspection detections is challenging on resource-constrained edge devices, creating a gap between automated defect detection and actionable maintenance insights.

Method: Two-stage pipeline: 1) RAPID-SCAN segmentation model (0.64M parameters) for defect detection, 2) Fine-tuned Phi-3.5 Vision-Language Model for natural language summarization, with post-training quantization and hardware optimization for edge deployment.

Result: RAPID-SCAN achieves 0.834 F1-score for segmentation; complete pipeline successfully deployed on mobile robotic platform with reduced model size and inference latency while maintaining summarization quality in real-world scenarios.

Conclusion: Edge-deployable integrated AI systems can bridge automated defect detection with actionable maintenance insights, enabling scalable and autonomous underground infrastructure inspection solutions.

Abstract: Autonomous inspection of underground infrastructure, such as sewer and culvert systems, is critical to public safety and urban sustainability. Although robotic platforms equipped with visual sensors can efficiently detect structural deficiencies, the automated generation of human-readable summaries from these detections remains a significant challenge, especially on resource-constrained edge devices. This paper presents a novel two-stage pipeline for end-to-end summarization of underground deficiencies, combining our lightweight RAPID-SCAN segmentation model with a fine-tuned Vision-Language Model (VLM) deployed on an edge computing platform. The first stage employs RAPID-SCAN (Resource-Aware Pipeline Inspection and Defect Segmentation using Compact Adaptive Network), achieving 0.834 F1-score with only 0.64M parameters for efficient defect segmentation. The second stage utilizes a fine-tuned Phi-3.5 VLM that generates concise, domain-specific summaries in natural language from the segmentation outputs. We introduce a curated dataset of inspection images with manually verified descriptions for VLM fine-tuning and evaluation. To enable real-time performance, we employ post-training quantization with hardware-specific optimization, achieving significant reductions in model size and inference latency without compromising summarization quality. We deploy and evaluate our complete pipeline on a mobile robotic platform, demonstrating its effectiveness in real-world inspection scenarios. Our results show the potential of edge-deployable integrated AI systems to bridge the gap between automated defect detection and actionable insights for infrastructure maintenance, paving the way for more scalable and autonomous inspection solutions.

</details>


### [99] [LIVE: Long-horizon Interactive Video World Modeling](https://arxiv.org/abs/2602.03747)
*Junchao Huang,Ziyang Ye,Xinting Hu,Tianyu He,Guiyu Zhang,Shaoshuai Shi,Jiang Bian,Li Jiang*

Main category: cs.CV

TL;DR: LIVE introduces a cycle-consistency objective for autoregressive video world models to bound error accumulation in long-horizon video generation without teacher distillation.


<details>
  <summary>Details</summary>
Motivation: Autoregressive video world models struggle with long-horizon generation due to error accumulation, and existing methods using teacher models are computationally expensive and fail to prevent error propagation beyond training horizons.

Method: LIVE uses a cycle-consistency objective: forward rollout from ground-truth frames followed by reverse generation to reconstruct initial state, with diffusion loss computed on reconstructed terminal state to constrain error propagation. Also introduces progressive training curriculum.

Result: LIVE achieves state-of-the-art performance on long-horizon benchmarks, generating stable, high-quality videos far beyond training rollout lengths.

Conclusion: The cycle-consistency approach effectively bounds error accumulation in long-horizon video generation without needing teacher-based distillation, providing a unified framework for different approaches.

Abstract: Autoregressive video world models predict future visual observations conditioned on actions. While effective over short horizons, these models often struggle with long-horizon generation, as small prediction errors accumulate over time. Prior methods alleviate this by introducing pre-trained teacher models and sequence-level distribution matching, which incur additional computational cost and fail to prevent error propagation beyond the training horizon. In this work, we propose LIVE, a Long-horizon Interactive Video world modEl that enforces bounded error accumulation via a novel cycle-consistency objective, thereby eliminating the need for teacher-based distillation. Specifically, LIVE first performs a forward rollout from ground-truth frames and then applies a reverse generation process to reconstruct the initial state. The diffusion loss is subsequently computed on the reconstructed terminal state, providing an explicit constraint on long-horizon error propagation. Moreover, we provide an unified view that encompasses different approaches and introduce progressive training curriculum to stabilize training. Experiments demonstrate that LIVE achieves state-of-the-art performance on long-horizon benchmarks, generating stable, high-quality videos far beyond training rollout lengths.

</details>


### [100] [See-through: Single-image Layer Decomposition for Anime Characters](https://arxiv.org/abs/2602.03749)
*Jian Lin,Chengze Li,Haoyun Qin,Kwun Wang Chan,Yanghua Jin,Hanyuan Liu,Stephen Chun Wang Choy,Xueting Liu*

Main category: cs.CV

TL;DR: Automated framework converts static anime illustrations into manipulatable 2.5D models using layer decomposition and diffusion-based consistency modules.


<details>
  <summary>Details</summary>
Motivation: Current professional workflows require tedious manual segmentation and artistic "hallucination" of occluded regions to enable motion in anime illustrations. There's a need to automate this process to reduce manual effort while maintaining quality.

Method: Decomposes single images into fully inpainted, semantically distinct layers with inferred drawing orders. Uses a scalable engine that bootstraps supervision from commercial Live2D models. Combines diffusion-based Body Part Consistency Module for global geometric coherence with pixel-level pseudo-depth inference to resolve intricate stratification of anime characters.

Result: The approach yields high-fidelity, manipulatable models suitable for professional, real-time animation applications, overcoming the scarcity of training data through the scalable supervision engine.

Conclusion: The framework successfully automates the transformation of static anime illustrations into manipulatable 2.5D models, addressing the limitations of current manual workflows and enabling professional animation applications.

Abstract: We introduce a framework that automates the transformation of static anime illustrations into manipulatable 2.5D models. Current professional workflows require tedious manual segmentation and the artistic ``hallucination'' of occluded regions to enable motion. Our approach overcomes this by decomposing a single image into fully inpainted, semantically distinct layers with inferred drawing orders. To address the scarcity of training data, we introduce a scalable engine that bootstraps high-quality supervision from commercial Live2D models, capturing pixel-perfect semantics and hidden geometry. Our methodology couples a diffusion-based Body Part Consistency Module, which enforces global geometric coherence, with a pixel-level pseudo-depth inference mechanism. This combination resolves the intricate stratification of anime characters, e.g., interleaving hair strands, allowing for dynamic layer reconstruction. We demonstrate that our approach yields high-fidelity, manipulatable models suitable for professional, real-time animation applications.

</details>


### [101] [Test-Time Conditioning with Representation-Aligned Visual Features](https://arxiv.org/abs/2602.03753)
*Nicolas Sereyjol-Garros,Ellington Kirby,Victor Letzelter,Victor Besnier,Nermin Samet*

Main category: cs.CV

TL;DR: REPA-G is an inference-time guidance framework that uses aligned representations from self-supervised models to condition diffusion models, enabling versatile control from fine textures to broad semantics without retraining.


<details>
  <summary>Details</summary>
Motivation: While representation alignment helps diffusion model training, its potential for inference-time conditioning remains unexplored. Current methods rely on ambiguous text prompts or coarse class labels, lacking precise control over generation.

Method: REPA-G optimizes a similarity objective (potential) at inference to steer denoising toward conditioned representations from pre-trained feature extractors. It enables multi-scale control from patch-level textures to global semantics and multi-concept composition.

Result: Quantitative results on ImageNet and COCO show high-quality, diverse generations. The method provides flexible, precise control without retraining and theoretically enables sampling from potential-induced tilted distributions.

Conclusion: REPA-G offers a flexible, precise alternative to text prompts and class labels for diffusion model conditioning, operating entirely at inference time with versatile multi-scale and multi-concept control capabilities.

Abstract: While representation alignment with self-supervised models has been shown to improve diffusion model training, its potential for enhancing inference-time conditioning remains largely unexplored. We introduce Representation-Aligned Guidance (REPA-G), a framework that leverages these aligned representations, with rich semantic properties, to enable test-time conditioning from features in generation. By optimizing a similarity objective (the potential) at inference, we steer the denoising process toward a conditioned representation extracted from a pre-trained feature extractor. Our method provides versatile control at multiple scales, ranging from fine-grained texture matching via single patches to broad semantic guidance using global image feature tokens. We further extend this to multi-concept composition, allowing for the faithful combination of distinct concepts. REPA-G operates entirely at inference time, offering a flexible and precise alternative to often ambiguous text prompts or coarse class labels. We theoretically justify how this guidance enables sampling from the potential-induced tilted distribution. Quantitative results on ImageNet and COCO demonstrate that our approach achieves high-quality, diverse generations. Code is available at https://github.com/valeoai/REPA-G.

</details>


### [102] [RAWDet-7: A Multi-Scenario Benchmark for Object Detection and Description on Quantized RAW Images](https://arxiv.org/abs/2602.03760)
*Mishal Fatima,Shashank Agnihotri,Kanchana Vaishnavi Gandikota,Michael Moeller,Margret Keuper*

Main category: cs.CV

TL;DR: RAWDet-7 is a large-scale RAW image dataset with object detection and description annotations to study machine vision using unprocessed sensor data instead of human-optimized RGB images.


<details>
  <summary>Details</summary>
Motivation: Most vision models use RGB images processed through ISP pipelines optimized for human perception, which discards sensor-level information that could be useful for machine reasoning. RAW images preserve unprocessed scene data with richer cues for object detection and description.

Method: Introduce RAWDet-7 dataset with ~25k training and 7.6k test RAW images collected across diverse cameras, lighting conditions, and environments. Images are densely annotated for seven object categories following MS-COCO and LVIS conventions, with object-level descriptions derived from corresponding high-resolution sRGB images.

Result: Dataset supports evaluation under simulated 4-bit, 6-bit, and 8-bit quantization reflecting realistic sensor constraints. Provides benchmark for studying detection performance, description quality & detail, and generalization in low-bit RAW image processing.

Conclusion: RAWDet-7 enables research on leveraging unprocessed sensor data for machine vision tasks, addressing limitations of human-optimized image processing pipelines and supporting studies on information preservation under RAW processing and quantization.

Abstract: Most vision models are trained on RGB images processed through ISP pipelines optimized for human perception, which can discard sensor-level information useful for machine reasoning. RAW images preserve unprocessed scene data, enabling models to leverage richer cues for both object detection and object description, capturing fine-grained details, spatial relationships, and contextual information often lost in processed images. To support research in this domain, we introduce RAWDet-7, a large-scale dataset of ~25k training and 7.6k test RAW images collected across diverse cameras, lighting conditions, and environments, densely annotated for seven object categories following MS-COCO and LVIS conventions. In addition, we provide object-level descriptions derived from the corresponding high-resolution sRGB images, facilitating the study of object-level information preservation under RAW image processing and low-bit quantization. The dataset allows evaluation under simulated 4-bit, 6-bit, and 8-bit quantization, reflecting realistic sensor constraints, and provides a benchmark for studying detection performance, description quality & detail, and generalization in low-bit RAW image processing. Dataset & code upon acceptance.

</details>


### [103] [FOVI: A biologically-inspired foveated interface for deep vision models](https://arxiv.org/abs/2602.03766)
*Nicholas M. Blauch,George A. Alvarez,Talia Konkle*

Main category: cs.CV

TL;DR: Foveated vision interface (FOVI) mimics human retina/V1 to process variable-resolution images efficiently using kNN-convolution, reducing computational costs while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Human vision uses foveated sensing with variable resolution for efficient active sensing, while computer vision typically uses uniform resolution, creating inefficiencies for high-resolution images.

Method: FOVI reformats variable-resolution retina-like sensor arrays into uniformly dense V1-like manifolds, uses k-nearest-neighborhood receptive fields with novel kernel mapping for kNN-convolution, and adapts models like DINOv3 ViT with LoRA.

Result: Models achieve competitive performance at a fraction of computational cost compared to non-foveated baselines, enabling efficient high-resolution egocentric vision.

Conclusion: FOVI opens pathways for scalable active sensing in high-resolution vision applications by mimicking biological foveation for computational efficiency.

Abstract: Human vision is foveated, with variable resolution peaking at the center of a large field of view; this reflects an efficient trade-off for active sensing, allowing eye-movements to bring different parts of the world into focus with other parts of the world in context. In contrast, most computer vision systems encode the visual world at a uniform resolution, raising challenges for processing full-field high-resolution images efficiently. We propose a foveated vision interface (FOVI) based on the human retina and primary visual cortex, that reformats a variable-resolution retina-like sensor array into a uniformly dense, V1-like sensor manifold. Receptive fields are defined as k-nearest-neighborhoods (kNNs) on the sensor manifold, enabling kNN-convolution via a novel kernel mapping technique. We demonstrate two use cases: (1) an end-to-end kNN-convolutional architecture, and (2) a foveated adaptation of the foundational DINOv3 ViT model, leveraging low-rank adaptation (LoRA). These models provide competitive performance at a fraction of the computational cost of non-foveated baselines, opening pathways for efficient and scalable active sensing for high-resolution egocentric vision. Code and pre-trained models are available at https://github.com/nblauch/fovi and https://huggingface.co/fovi-pytorch.

</details>


### [104] [From Pre- to Intra-operative MRI: Predicting Brain Shift in Temporal Lobe Resection for Epilepsy Surgery](https://arxiv.org/abs/2602.03785)
*Jingjing Peng,Giorgio Fiore,Yang Liu,Ksenia Ellum,Debayan Daspupta,Keyoumars Ashkan,Andrew McEvoy,Anna Miserocchi,Sebastien Ourselin,John Duncan,Alejandro Granados*

Main category: cs.CV

TL;DR: NeuralShift: A U-Net model that predicts brain shift from preoperative MRI for temporal lobe resection, achieving accurate deformation prediction with 0.97 DICE score and 1.12 mm landmark error.


<details>
  <summary>Details</summary>
Motivation: Brain shift during neurosurgery invalidates preoperative MRI guidance, creating need for intraoperative brain shift compensation to enhance neuronavigation precision and surgical outcomes.

Method: U-Net-based model (NeuralShift) that predicts brain shift entirely from preoperative MRI for temporal lobe resection patients, evaluated using Target Registration Errors on anatomical landmarks and DICE scores comparing predicted vs. actual intraoperative masks.

Result: Model predicts global brain deformation with DICE score of 0.97 and accurate local displacements achieving landmark TRE as low as 1.12 mm, effectively compensating for large brain shifts during temporal lobe removal.

Conclusion: The model successfully predicts brain deformation during temporal lobe resection using only preoperative images, offering potential to increase neurosurgery safety and efficiency while improving patient outcomes.

Abstract: Introduction: In neurosurgery, image-guided Neurosurgery Systems (IGNS) highly rely on preoperative brain magnetic resonance images (MRI) to assist surgeons in locating surgical targets and determining surgical paths. However, brain shift invalidates the preoperative MRI after dural opening. Updated intraoperative brain MRI with brain shift compensation is crucial for enhancing the precision of neuronavigation systems and ensuring the optimal outcome of surgical interventions. Methodology: We propose NeuralShift, a U-Net-based model that predicts brain shift entirely from pre-operative MRI for patients undergoing temporal lobe resection. We evaluated our results using Target Registration Errors (TREs) computed on anatomical landmarks located on the resection side and along the midline, and DICE scores comparing predicted intraoperative masks with masks derived from intraoperative MRI. Results: Our experimental results show that our model can predict the global deformation of the brain (DICE of 0.97) with accurate local displacements (achieve landmark TRE as low as 1.12 mm), compensating for large brain shifts during temporal lobe removal neurosurgery. Conclusion: Our proposed model is capable of predicting the global deformation of the brain during temporal lobe resection using only preoperative images, providing potential opportunities to the surgical team to increase safety and efficiency of neurosurgery and better outcomes to patients. Our contributions will be publicly available after acceptance in https://github.com/SurgicalDataScienceKCL/NeuralShift.

</details>


### [105] [3D-Aware Implicit Motion Control for View-Adaptive Human Video Generation](https://arxiv.org/abs/2602.03796)
*Zhixue Fang,Xu He,Songlin Tang,Haoxian Zhang,Qingfeng Li,Xiaoqiang Liu,Pengfei Wan,Kun Gai*

Main category: cs.CV

TL;DR: 3DiMo introduces an implicit, view-agnostic motion representation for video generation that avoids rigid 2D pose constraints and inaccurate 3D models, enabling faithful motion reproduction with flexible camera control.


<details>
  <summary>Details</summary>
Motivation: Existing motion control methods have limitations: 2D poses bind motion to specific viewpoints preventing novel-view synthesis, while explicit 3D models (like SMPL) have inherent inaccuracies that override the generator's intrinsic 3D awareness. The paper advocates for an implicit motion representation that aligns with the generator's spatial priors rather than depending on external constraints.

Method: 3DiMo jointly trains a motion encoder with a pretrained video generator to distill driving frames into compact, view-agnostic motion tokens injected via cross-attention. Uses view-rich supervision (single-view, multi-view, moving-camera videos) to enforce motion consistency across viewpoints. Employs auxiliary geometric supervision with SMPL for early initialization only, then anneals to zero to transition from external guidance to learning genuine 3D understanding from data and generator priors.

Result: Experiments confirm 3DiMo faithfully reproduces driving motions with flexible, text-driven camera control, significantly surpassing existing methods in both motion fidelity and visual quality.

Conclusion: The implicit, view-agnostic motion representation approach enables better alignment with video generators' spatial priors, resulting in superior motion control and novel-view synthesis capabilities compared to existing 2D pose or explicit 3D model-based methods.

Abstract: Existing methods for human motion control in video generation typically rely on either 2D poses or explicit 3D parametric models (e.g., SMPL) as control signals. However, 2D poses rigidly bind motion to the driving viewpoint, precluding novel-view synthesis. Explicit 3D models, though structurally informative, suffer from inherent inaccuracies (e.g., depth ambiguity and inaccurate dynamics) which, when used as a strong constraint, override the powerful intrinsic 3D awareness of large-scale video generators. In this work, we revisit motion control from a 3D-aware perspective, advocating for an implicit, view-agnostic motion representation that naturally aligns with the generator's spatial priors rather than depending on externally reconstructed constraints. We introduce 3DiMo, which jointly trains a motion encoder with a pretrained video generator to distill driving frames into compact, view-agnostic motion tokens, injected semantically via cross-attention. To foster 3D awareness, we train with view-rich supervision (i.e., single-view, multi-view, and moving-camera videos), forcing motion consistency across diverse viewpoints. Additionally, we use auxiliary geometric supervision that leverages SMPL only for early initialization and is annealed to zero, enabling the model to transition from external 3D guidance to learning genuine 3D spatial motion understanding from the data and the generator's priors. Experiments confirm that 3DiMo faithfully reproduces driving motions with flexible, text-driven camera control, significantly surpassing existing methods in both motion fidelity and visual quality.

</details>


### [106] [Progressive Checkerboards for Autoregressive Multiscale Image Generation](https://arxiv.org/abs/2602.03811)
*David Eigen*

Main category: cs.CV

TL;DR: Proposes progressive checkerboard ordering for parallel multiscale autoregressive image generation, achieving competitive ImageNet results with fewer sampling steps.


<details>
  <summary>Details</summary>
Motivation: Addresses the challenge of efficiently sampling independent locations in parallel while modeling mutual dependencies with serial conditioning in autoregressive image generation.

Method: Uses a flexible, fixed ordering based on progressive checkerboards that draws samples in parallel from evenly spaced regions at each scale, maintaining full balance in all levels of a quadtree subdivision.

Result: Achieves competitive performance on class-conditional ImageNet compared to recent state-of-the-art autoregressive systems with similar model capacity, using fewer sampling steps.

Conclusion: Progressive checkerboard ordering enables effective conditioning both between and within scales, with evidence showing that a wide range of scale-up factors lead to similar results when total serial steps are constant.

Abstract: A key challenge in autoregressive image generation is to efficiently sample independent locations in parallel, while still modeling mutual dependencies with serial conditioning. Some recent works have addressed this by conditioning between scales in a multiscale pyramid. Others have looked at parallelizing samples in a single image using regular partitions or randomized orders. In this work we examine a flexible, fixed ordering based on progressive checkerboards for multiscale autoregressive image generation. Our ordering draws samples in parallel from evenly spaced regions at each scale, maintaining full balance in all levels of a quadtree subdivision at each step. This enables effective conditioning both between and within scales. Intriguingly, we find evidence that in our balanced setting, a wide range of scale-up factors lead to similar results, so long as the total number of serial steps is constant. On class-conditional ImageNet, our method achieves competitive performance compared to recent state-of-the-art autoregressive systems with like model capacity, using fewer sampling steps.

</details>


### [107] [Fast-Slow Efficient Training for Multimodal Large Language Models via Visual Token Pruning](https://arxiv.org/abs/2602.03815)
*Dingkun Zhang,Shuhan Qi,Yulin Wu,Xinyu Xiao,Xuan Wang,Long Chen*

Main category: cs.CV

TL;DR: DualSpeed is a fast-slow training framework for MLLMs that uses visual token pruning for efficiency while maintaining performance through dual-mode training and self-distillation.


<details>
  <summary>Details</summary>
Motivation: Multimodal LLMs suffer from severe training inefficiency due to massive model sizes and visual token numbers. Existing efficient training methods focus on reducing model sizes or parameters, but visual token pruning causes training-inference mismatch when applied during training.

Method: DualSpeed uses a fast-slow framework: fast-mode incorporates visual token pruning methods to reduce tokens, with mode isolator to isolate behaviors; slow-mode trains on full visual sequences for consistency, enhanced by self-distillation from the fast-mode.

Result: DualSpeed accelerates training of LLaVA-1.5 by 2.1× and LLaVA-NeXT by 4.0× while retaining over 99% performance, achieving both training efficiency and non-degraded performance.

Conclusion: DualSpeed successfully addresses the training-inference mismatch problem in visual token pruning for MLLMs, enabling efficient training without performance degradation through a novel dual-mode framework with self-distillation.

Abstract: Multimodal Large Language Models (MLLMs) suffer from severe training inefficiency issue, which is associated with their massive model sizes and visual token numbers. Existing efforts in efficient training focus on reducing model sizes or trainable parameters. Inspired by the success of Visual Token Pruning (VTP) in improving inference efficiency, we are exploring another substantial research direction for efficient training by reducing visual tokens. However, applying VTP at the training stage results in a training-inference mismatch: pruning-trained models perform poorly when inferring on non-pruned full visual token sequences. To close this gap, we propose DualSpeed, a fast-slow framework for efficient training of MLLMs. The fast-mode is the primary mode, which incorporates existing VTP methods as plugins to reduce visual tokens, along with a mode isolator to isolate the model's behaviors. The slow-mode is the auxiliary mode, where the model is trained on full visual sequences to retain training-inference consistency. To boost its training, it further leverages self-distillation to learn from the sufficiently trained fast-mode. Together, DualSpeed can achieve both training efficiency and non-degraded performance. Experiments show DualSpeed accelerates the training of LLaVA-1.5 by 2.1$\times$ and LLaVA-NeXT by 4.0$\times$, retaining over 99% performance. Code: https://github.com/dingkun-zhang/DualSpeed

</details>


### [108] [Continuous Control of Editing Models via Adaptive-Origin Guidance](https://arxiv.org/abs/2602.03826)
*Alon Wolf,Chen Katzir,Kfir Aberman,Or Patashnik*

Main category: cs.CV

TL;DR: AdaOr enables smooth control of text-guided edit intensity in diffusion models by adjusting the guidance origin with an identity-conditioned adaptive origin.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based editing models lack smooth control over edit intensity. While CFG affects prompt adherence in generation, scaling it in editing models doesn't produce smooth transitions between input and edited results due to unconditional prediction dominating at low scales.

Method: Introduces Adaptive-Origin Guidance (AdaOr) that replaces the standard unconditional prediction with an identity-conditioned adaptive origin using identity instructions. Interpolates between identity prediction and unconditional prediction based on edit strength to ensure continuous transitions.

Result: AdaOr provides smoother and more consistent control over edit intensity compared to current slider-based approaches, demonstrated on both image and video editing tasks.

Conclusion: The method enables fine-grained control at inference without per-edit procedures or specialized datasets by incorporating identity instructions into standard training frameworks.

Abstract: Diffusion-based editing models have emerged as a powerful tool for semantic image and video manipulation. However, existing models lack a mechanism for smoothly controlling the intensity of text-guided edits. In standard text-conditioned generation, Classifier-Free Guidance (CFG) impacts prompt adherence, suggesting it as a potential control for edit intensity in editing models. However, we show that scaling CFG in these models does not produce a smooth transition between the input and the edited result. We attribute this behavior to the unconditional prediction, which serves as the guidance origin and dominates the generation at low guidance scales, while representing an arbitrary manipulation of the input content. To enable continuous control, we introduce Adaptive-Origin Guidance (AdaOr), a method that adjusts this standard guidance origin with an identity-conditioned adaptive origin, using an identity instruction corresponding to the identity manipulation. By interpolating this identity prediction with the standard unconditional prediction according to the edit strength, we ensure a continuous transition from the input to the edited result. We evaluate our method on image and video editing tasks, demonstrating that it provides smoother and more consistent control compared to current slider-based editing approaches. Our method incorporates an identity instruction into the standard training framework, enabling fine-grained control at inference time without per-edit procedure or reliance on specialized datasets.

</details>


### [109] [EventNeuS: 3D Mesh Reconstruction from a Single Event Camera](https://arxiv.org/abs/2602.03847)
*Shreyas Sachan,Viktor Rudnev,Mohamed Elgharib,Christian Theobalt,Vladislav Golyanik*

Main category: cs.CV

TL;DR: EventNeuS: A self-supervised neural model that combines 3D signed distance functions with density fields for accurate 3D mesh reconstruction from monocular event streams, outperforming previous methods by 34-31%.


<details>
  <summary>Details</summary>
Motivation: Event cameras offer advantages over RGB cameras in many scenarios, but existing event-based 3D mesh reconstruction methods have limited accuracy. There's a need for better 3D reconstruction from monocular event streams.

Method: Combines 3D signed distance function and density field learning with event-based supervision. Introduces spherical harmonics encodings to handle view-dependent effects in a self-supervised neural model.

Result: Outperforms existing approaches by significant margins: 34% lower Chamfer distance and 31% lower mean absolute error on average compared to the best previous method.

Conclusion: EventNeuS successfully addresses the accuracy limitations of existing event-based 3D reconstruction methods by combining SDF and density field learning with event supervision, achieving state-of-the-art performance.

Abstract: Event cameras offer a considerable alternative to RGB cameras in many scenarios. While there are recent works on event-based novel-view synthesis, dense 3D mesh reconstruction remains scarcely explored and existing event-based techniques are severely limited in their 3D reconstruction accuracy. To address this limitation, we present EventNeuS, a self-supervised neural model for learning 3D representations from monocular colour event streams. Our approach, for the first time, combines 3D signed distance function and density field learning with event-based supervision. Furthermore, we introduce spherical harmonics encodings into our model for enhanced handling of view-dependent effects. EventNeuS outperforms existing approaches by a significant margin, achieving 34% lower Chamfer distance and 31% lower mean absolute error on average compared to the best previous method.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [110] [HMVLA: Hyperbolic Multimodal Fusion for Vision-Language-Action Models](https://arxiv.org/abs/2602.02533)
*Kun Wang,Xiao Feng,Mingcheng Qu,Tonghua Su*

Main category: cs.RO

TL;DR: HMVLA is a novel Vision Language Action framework that uses hyperbolic space embeddings and sparse MoE for better hierarchical semantic alignment between vision and language, outperforming baseline methods in accuracy and generalization.


<details>
  <summary>Details</summary>
Motivation: Existing VLA models often directly fine-tune pre-trained VLMs without fully addressing the unique semantic alignment challenges in the VLA domain, particularly hierarchical relationships in vision-language data.

Method: Proposes HMVLA framework that: 1) embeds multimodal features in hyperbolic space instead of Euclidean space to better model hierarchical relationships, and 2) introduces a sparsely gated Mixture of Experts mechanism tailored for semantic alignment to enhance multimodal comprehension and efficiency.

Result: Extensive experiments show HMVLA surpasses baseline methods in both accuracy and generalization. Additional validation through dataset reconstruction demonstrates robustness and cross-domain adaptability.

Conclusion: HMVLA effectively addresses hierarchical semantic alignment challenges in VLA models through hyperbolic space embeddings and specialized MoE mechanisms, achieving superior performance and generalization compared to existing approaches.

Abstract: Vision Language Action (VLA) models have recently shown great potential in bridging multimodal perception with robotic control. However, existing methods often rely on direct fine-tuning of pre-trained Vision-Language Models (VLMs), feeding semantic and visual features directly into a policy network without fully addressing the unique semantic alignment challenges in the VLA domain. In this paper, we propose HMVLA, a novel VLA framework that exploits the inherent hierarchical structures in vision and language for comprehensive semantic alignment. Unlike traditional methods that perform alignment in Euclidean space, our HMVLA embeds multimodal features in hyperbolic space, enabling more effective modeling of the hierarchical relationships present in image text data. Furthermore, we introduce a sparsely gated Mixture of Experts (MoE) mechanism tailored for semantic alignment, which enhances multimodal comprehension between images and text while improving efficiency. Extensive experiments demonstrate that HMVLA surpasses baseline methods in both accuracy and generalization. In addition, we validate its robustness by reconstructing datasets to further test cross domain adaptability.

</details>


### [111] [StepNav: Structured Trajectory Priors for Efficient and Multimodal Visual Navigation](https://arxiv.org/abs/2602.02590)
*Xubo Luo,Aodi Wu,Haodong Han,Xue Wan,Wei Zhang,Leizheng Shu,Ruisuo Wang*

Main category: cs.RO

TL;DR: StepNav: A novel framework for visual navigation that uses structured, multimodal trajectory priors from variational principles instead of unstructured noise, enabling safer, more efficient trajectory generation in cluttered environments.


<details>
  <summary>Details</summary>
Motivation: Current generative models for visual navigation rely on unstructured noise priors, which often produce unsafe, inefficient, or unimodal plans that can't meet real-time requirements in cluttered and uncertain environments.

Method: StepNav learns a geometry-aware success probability field to identify feasible navigation corridors, constructs explicit multimodal mixture priors from these corridors, and refines trajectories using conditional flow-matching formulated as an optimal control problem with smoothness and safety regularization.

Result: Experiments in simulation and real-world benchmarks show consistent improvements in robustness, efficiency, and safety over state-of-the-art generative planners, with significantly fewer steps needed for trajectory generation.

Conclusion: StepNav advances reliable trajectory generation for practical autonomous navigation by replacing unstructured noise with physically-grounded candidates, enabling safer and more efficient plans in cluttered environments.

Abstract: Visual navigation is fundamental to autonomous systems, yet generating reliable trajectories in cluttered and uncertain environments remains a core challenge. Recent generative models promise end-to-end synthesis, but their reliance on unstructured noise priors often yields unsafe, inefficient, or unimodal plans that cannot meet real-time requirements. We propose StepNav, a novel framework that bridges this gap by introducing structured, multimodal trajectory priors derived from variational principles. StepNav first learns a geometry-aware success probability field to identify all feasible navigation corridors. These corridors are then used to construct an explicit, multi-modal mixture prior that initializes a conditional flow-matching process. This refinement is formulated as an optimal control problem with explicit smoothness and safety regularization. By replacing unstructured noise with physically-grounded candidates, StepNav generates safer and more efficient plans in significantly fewer steps. Experiments in both simulation and real-world benchmarks demonstrate consistent improvements in robustness, efficiency, and safety over state-of-the-art generative planners, advancing reliable trajectory generation for practical autonomous navigation. The code has been released at https://github.com/LuoXubo/StepNav.

</details>


### [112] [AROLA: A Modular Layered Architecture for Scaled Autonomous Racing](https://arxiv.org/abs/2602.02730)
*Fam Shihata,Mohammed Abdelazim,Ahmed Hussein*

Main category: cs.RO

TL;DR: AROLA is a modular ROS 2-based software architecture for autonomous racing that reorganizes fragmented systems into interchangeable layers, with Race Monitor providing real-time performance evaluation and standardized analysis.


<details>
  <summary>Details</summary>
Motivation: Autonomous racing software stacks need to evolve beyond fragmented and monolithic designs to enable rapid development, module replacement, and objective benchmarking without custom message definitions.

Method: Introduces AROLA as a modular layered architecture decomposing the autonomous-driving pipeline into 8 standardized layers (sensing to actuation) with ROS 2 interfaces, plus Race Monitor for real-time performance logging and post-race analysis.

Result: Validated in simulation and on RoboRacer hardware platform, including deployment at 2025 RoboRacer IV25 competition, demonstrating that modularity and systematic evaluation accelerate development and improve reproducibility.

Conclusion: Modular architecture with transparent interfaces and systematic evaluation (AROLA + Race Monitor) can accelerate development and improve reproducibility in scaled autonomous racing.

Abstract: Autonomous racing has advanced rapidly, particularly on scaled platforms, and software stacks must evolve accordingly. In this work, AROLA is introduced as a modular, layered software architecture in which fragmented and monolithic designs are reorganized into interchangeable layers and components connected through standardized ROS 2 interfaces. The autonomous-driving pipeline is decomposed into sensing, pre-processing, perception, localization and mapping, planning, behavior, control, and actuation, enabling rapid module replacement and objective benchmarking without reliance on custom message definitions. To support consistent performance evaluation, a Race Monitor framework is introduced as a lightweight system through which lap timing, trajectory quality, and computational load are logged in real time and standardized post-race analyses are generated. AROLA is validated in simulation and on hardware using the RoboRacer platform, including deployment at the 2025 RoboRacer IV25 competition. Together, AROLA and Race Monitor demonstrate that modularity, transparent interfaces, and systematic evaluation can accelerate development and improve reproducibility in scaled autonomous racing.

</details>


### [113] [PokeNet: Learning Kinematic Models of Articulated Objects from Human Observations](https://arxiv.org/abs/2602.02741)
*Anmol Gupta,Weiwei Gu,Omkar Patil,Jun Ki Lee,Nakul Gopalan*

Main category: cs.RO

TL;DR: PokeNet is an end-to-end framework that learns articulation models from single human demonstrations without prior object knowledge, outperforming existing methods by 27% in joint axis and state estimation accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing articulation modeling approaches have limitations: they require prior knowledge about objects (joint types/numbers), fail to recover occluded joints, need large multi-view image datasets, and neglect manipulation order constraints that are essential for multi-DoF objects like dishwashers.

Method: PokeNet is an end-to-end framework that takes a sequence of point cloud observations from a single human demonstration and predicts joint parameters, infers manipulation order, and tracks joint states over time without requiring prior object knowledge.

Result: PokeNet outperforms state-of-the-art methods, improving joint axis and state estimation accuracy by an average of over 27% across diverse objects including novel and unseen categories, demonstrated in both simulation and real-world environments.

Conclusion: PokeNet provides an effective solution for learning articulation models from minimal human demonstrations without prior object knowledge, addressing key limitations of existing approaches and enabling practical robotic manipulation of articulated objects.

Abstract: Articulation modeling enables robots to learn joint parameters of articulated objects for effective manipulation which can then be used downstream for skill learning or planning. Existing approaches often rely on prior knowledge about the objects, such as the number or type of joints. Some of these approaches also fail to recover occluded joints that are only revealed during interaction. Others require large numbers of multi-view images for every object, which is impractical in real-world settings. Furthermore, prior works neglect the order of manipulations, which is essential for many multi-DoF objects where one joint must be operated before another, such as a dishwasher. We introduce PokeNet, an end-to-end framework that estimates articulation models from a single human demonstration without prior object knowledge. Given a sequence of point cloud observations of a human manipulating an unknown object, PokeNet predicts joint parameters, infers manipulation order, and tracks joint states over time. PokeNet outperforms existing state-of-the-art methods, improving joint axis and state estimation accuracy by an average of over 27% across diverse objects, including novel and unseen categories. We demonstrate these gains in both simulation and real-world environments.

</details>


### [114] [Bimanual High-Density EMG Control for In-Home Mobile Manipulation by a User with Quadriplegia](https://arxiv.org/abs/2602.02773)
*Jehan Yang,Eleanor Hodgson,Cindy Sun,Zackory Erickson,Doug Weber*

Main category: cs.RO

TL;DR: First system enabling quadriplegic user to control home mobile manipulator using bimanual HDEMG forearm sleeves for gesture-based control, integrated with vision/language/motion planning for shared autonomy.


<details>
  <summary>Details</summary>
Motivation: People with cervical spinal cord injury (cSCI) need assistance with daily household tasks but paralysis limits access to traditional robot control interfaces like joysticks or keyboards.

Method: 1) Custom fabric-integrated HDEMG forearm sleeves capture residual neuromotor activity from paralyzed arms for gesture-based control; 2) Shared autonomy framework integrates vision, language, and motion planning for robust teleoperation; 3) 12-day in-home user study evaluating real-time use.

Result: System enables effective robot control for performing activities of daily living and household tasks in real home environment, as demonstrated through in-home deployment.

Conclusion: The integrated system of wearable EMG interface and shared autonomy framework successfully enables users with quadriplegia to control mobile manipulators for household tasks in their own homes.

Abstract: Mobile manipulators in the home can enable people with cervical spinal cord injury (cSCI) to perform daily physical household tasks that they could not otherwise do themselves. However, paralysis in these users often limits access to traditional robot control interfaces such as joysticks or keyboards. In this work, we introduce and deploy the first system that enables a user with quadriplegia to control a mobile manipulator in their own home using bimanual high-density electromyography (HDEMG). We develop a pair of custom, fabric-integrated HDEMG forearm sleeves, worn on both arms, that capture residual neuromotor activity from clinically paralyzed degrees of freedom and support real-time gesture-based robot control. Second, by integrating vision, language, and motion planning modules, we introduce a shared autonomy framework that supports robust and user-driven teleoperation, with particular benefits for navigation-intensive tasks in home environments. Finally, to demonstrate the system in the wild, we present a twelve-day in-home user study evaluating real-time use of the wearable EMG interface for daily robot control. Together, these system components enable effective robot control for performing activities of daily living and other household tasks in a real home environment.

</details>


### [115] [Adaptive Linear Path Model-Based Diffusion](https://arxiv.org/abs/2602.02831)
*Yutaka Shimizu,Masayoshi Tomizuka*

Main category: cs.RO

TL;DR: LP-MBD replaces complex diffusion scheduling with linear probability paths for simpler tuning, and ALP-MBD adds adaptive scheduling via RL for better robustness and efficiency.


<details>
  <summary>Details</summary>
Motivation: Diffusion models show impressive robotic control results but are highly sensitive to scheduling parameters, making tuning a critical challenge that needs simplification and automation.

Method: LP-MBD replaces variance-preserving schedule with flow-matching-inspired linear probability path for geometric interpretability and decoupled parameterization. ALP-MBD adds reinforcement learning to adapt diffusion steps and noise levels based on task complexity and environment.

Result: LP-MBD simplifies scheduling while maintaining strong performance across numerical studies, Brax benchmarks, and mobile-robot trajectory tracking. ALP-MBD further improves robustness, adaptability, and real-time efficiency.

Conclusion: The proposed linear probability path approach reduces tuning complexity and provides a stable foundation for adaptation, with reinforcement learning-based scheduling adaptation offering improved performance and efficiency in model-based diffusion control.

Abstract: The interest in combining model-based control approaches with diffusion models has been growing. Although we have seen many impressive robotic control results in difficult tasks, the performance of diffusion models is highly sensitive to the choice of scheduling parameters, making parameter tuning one of the most critical challenges. We introduce Linear Path Model-Based Diffusion (LP-MBD), which replaces the variance-preserving schedule with a flow-matching-inspired linear probability path. This yields a geometrically interpretable and decoupled parameterization that reduces tuning complexity and provides a stable foundation for adaptation. Building on this, we propose Adaptive LP-MBD (ALP-MBD), which leverages reinforcement learning to adjust diffusion steps and noise levels according to task complexity and environmental conditions. Across numerical studies, Brax benchmarks, and mobile-robot trajectory tracking, LP-MBD simplifies scheduling while maintaining strong performance, and ALP-MBD further improves robustness, adaptability, and real-time efficiency.

</details>


### [116] [Language Movement Primitives: Grounding Language Models in Robot Motion](https://arxiv.org/abs/2602.02839)
*Yinlong Dai,Benjamin A. Christie,Daniel J. Evans,Dylan P. Losey,Simon Stepputtis*

Main category: cs.RO

TL;DR: LMPs bridge VLM reasoning with DMP motion control for zero-shot robot manipulation from natural language instructions, achieving 80% task success vs 31% baseline.


<details>
  <summary>Details</summary>
Motivation: Current approaches face a disconnect: VLMs can reason about tasks but struggle with motion grounding, while robotics models need fine-tuning for novel tasks. There's a fundamental gap between abstract task reasoning and low-level motion control.

Method: Proposes Language Movement Primitives (LMPs) that ground VLM reasoning in Dynamic Movement Primitive (DMP) parameterization. VLMs set DMP parameters to generate continuous, stable trajectories, bridging high-level language understanding with low-level motion control.

Result: Achieves 80% task success across 20 real-world manipulation tasks, compared to 31% for the best-performing baseline, demonstrating effective zero-shot manipulation from natural language.

Conclusion: LMPs successfully connect VLM-based task reasoning with DMP-based motion control, enabling robots to perform novel manipulation tasks from natural language instructions without fine-tuning.

Abstract: Enabling robots to perform novel manipulation tasks from natural language instructions remains a fundamental challenge in robotics, despite significant progress in generalized problem solving with foundational models. Large vision and language models (VLMs) are capable of processing high-dimensional input data for visual scene and language understanding, as well as decomposing tasks into a sequence of logical steps; however, they struggle to ground those steps in embodied robot motion. On the other hand, robotics foundation models output action commands, but require in-domain fine-tuning or experience before they are able to perform novel tasks successfully. At its core, there still remains the fundamental challenge of connecting abstract task reasoning with low-level motion control. To address this disconnect, we propose Language Movement Primitives (LMPs), a framework that grounds VLM reasoning in Dynamic Movement Primitive (DMP) parameterization. Our key insight is that DMPs provide a small number of interpretable parameters, and VLMs can set these parameters to specify diverse, continuous, and stable trajectories. Put another way: VLMs can reason over free-form natural language task descriptions, and semantically ground their desired motions into DMPs -- bridging the gap between high-level task reasoning and low-level position and velocity control. Building on this combination of VLMs and DMPs, we formulate our LMP pipeline for zero-shot robot manipulation that effectively completes tabletop manipulation problems by generating a sequence of DMP motions. Across 20 real-world manipulation tasks, we show that LMP achieves 80% task success as compared to 31% for the best-performing baseline. See videos at our website: https://collab.me.vt.edu/lmp

</details>


### [117] [Kino-PAX$^+$: Near-Optimal Massively Parallel Kinodynamic Sampling-based Motion Planner](https://arxiv.org/abs/2602.02846)
*Nicolas Perrault,Qi Heng Ho,Morteza Lahijanian*

Main category: cs.RO

TL;DR: Kino-PAX⁺ is a massively parallel kinodynamic motion planner with asymptotic near-optimal guarantees that achieves up to 1000x speedup over serial methods.


<details>
  <summary>Details</summary>
Motivation: Sampling-based motion planners (SBMPs) struggle with real-time performance due to serial computation, and existing parallel approaches lack optimality guarantees while only focusing on finding feasible solutions.

Method: Decomposes traditionally serial operations into three massively parallel subroutines, builds a sparse tree of dynamically feasible trajectories, and focuses computation on the most promising nodes within local neighborhoods for propagation and refinement.

Result: Achieves solutions up to three orders of magnitude faster than existing serial methods and obtains lower solution costs than state-of-the-art GPU-based planners while maintaining probabilistic δ-robust completeness.

Conclusion: Kino-PAX⁺ provides a breakthrough in parallel kinodynamic motion planning by combining massive parallelism with asymptotic near-optimality guarantees, enabling real-time performance for complex robotic systems.

Abstract: Sampling-based motion planners (SBMPs) are widely used for robot motion planning with complex kinodynamic constraints in high-dimensional spaces, yet they struggle to achieve \emph{real-time} performance due to their serial computation design. Recent efforts to parallelize SBMPs have achieved significant speedups in finding feasible solutions; however, they provide no guarantees of optimizing an objective function. We introduce Kino-PAX$^{+}$, a massively parallel kinodynamic SBMP with asymptotic near-optimal guarantees. Kino-PAX$^{+}$ builds a sparse tree of dynamically feasible trajectories by decomposing traditionally serial operations into three massively parallel subroutines. The algorithm focuses computation on the most promising nodes within local neighborhoods for propagation and refinement, enabling rapid improvement of solution cost. We prove that, while maintaining probabilistic $δ$-robust completeness, this focus on promising nodes ensures asymptotic $δ$-robust near-optimality. Our results show that Kino-PAX$^{+}$ finds solutions up to three orders of magnitude faster than existing serial methods and achieves lower solution costs than a state-of-the-art GPU-based planner.

</details>


### [118] [Latent Perspective-Taking via a Schrödinger Bridge in Influence-Augmented Local Models](https://arxiv.org/abs/2602.02857)
*Kevin Alcedo,Pedro U. Lima,Rachid Alami*

Main category: cs.RO

TL;DR: The paper presents a neuro-symbolic approach for robots to reason about others' mental states in social environments, combining learned structured mental models with perspective-shift operators for socially-aware decision-making.


<details>
  <summary>Details</summary>
Motivation: Robots operating alongside humans need to make decisions under uncertainty about others' hidden mental models and states. Existing approaches like Interactive POMDPs and Bayesian Theory of Mind are either computationally intractable for exact nested-belief inference or too brittle with hand-specified models for open-world settings.

Method: The method builds on Influence-Based Abstraction to create an Influence-Augmented Local Model that decomposes social tasks. It proposes: (1) a neuro-symbolic world model as a factored, discrete Dynamic Bayesian Network, and (2) a perspective-shift operator modeled as an amortized Schrödinger Bridge that transforms egocentric beliefs into other-centric beliefs. This enables decision-time mental-state planning via Schrödinger Bridge in belief space.

Result: The architecture enables agents to synthesize socially-aware policies in model-based reinforcement learning through decision-time mental-state planning, with preliminary results demonstrated in a MiniGrid social navigation task.

Conclusion: The approach addresses both computational tractability and model brittleness issues by learning structured mental models and estimators of others' mental states, providing a practical framework for robots to reason about social interactions in uncertain environments.

Abstract: Operating in environments alongside humans requires robots to make decisions under uncertainty. In addition to exogenous dynamics, they must reason over others' hidden mental-models and mental-states. While Interactive POMDPs and Bayesian Theory of Mind formulations are principled, exact nested-belief inference is intractable, and hand-specified models are brittle in open-world settings. We address both by learning structured mental-models and an estimator of others' mental-states. Building on the Influence-Based Abstraction, we instantiate an Influence-Augmented Local Model to decompose socially-aware robot tasks into local dynamics, social influences, and exogenous factors. We propose (a) a neuro-symbolic world model instantiating a factored, discrete Dynamic Bayesian Network, and (b) a perspective-shift operator modeled as an amortized Schrödinger Bridge over the learned local dynamics that transports factored egocentric beliefs into other-centric beliefs. We show that this architecture enables agents to synthesize socially-aware policies in model-based reinforcement learning, via decision-time mental-state planning (a Schrödinger Bridge in belief space), with preliminary results in a MiniGrid social navigation task.

</details>


### [119] [IMAGINE: Intelligent Multi-Agent Godot-based Indoor Networked Exploration](https://arxiv.org/abs/2602.02858)
*Tiago Leite,Maria Conceição,António Grilo*

Main category: cs.RO

TL;DR: MARL enables UAV teams to autonomously explore GNSS-denied indoor environments with communication constraints using continuous actions and high-fidelity simulation.


<details>
  <summary>Details</summary>
Motivation: Address challenges in coordinating autonomous UAV teams for exploration in GNSS-denied environments with communication constraints, perception limitations, and decentralized decision-making.

Method: Multi-Agent Reinforcement Learning (MARL) with continuous action spaces in Godot simulation, using ND-POMDPs, LiDAR sensors, local map sharing, and Curriculum Learning across five complexity levels.

Result: Scalable training paradigm with simplified architecture enables rapid autonomous indoor exploration; Curriculum Learning accelerates robust training; addresses prior limitations in discrete actions, centralized approaches, and connectivity assumptions.

Conclusion: Combination of high-fidelity simulation, MARL formulation, and computational efficiency provides strong foundation for deploying learned cooperative strategies in physical robotic systems.

Abstract: The exploration of unknown, Global Navigation Satellite System (GNSS) denied environments by an autonomous communication-aware and collaborative group of Unmanned Aerial Vehicles (UAVs) presents significant challenges in coordination, perception, and decentralized decision-making. This paper implements Multi-Agent Reinforcement Learning (MARL) to address these challenges in a 2D indoor environment, using high-fidelity game-engine simulations (Godot) and continuous action spaces. Policy training aims to achieve emergent collaborative behaviours and decision-making under uncertainty using Network-Distributed Partially Observable Markov Decision Processes (ND-POMDPs). Each UAV is equipped with a Light Detection and Ranging (LiDAR) sensor and can share data (sensor measurements and a local occupancy map) with neighbouring agents. Inter-agent communication constraints include limited range, bandwidth and latency. Extensive ablation studies evaluated MARL training paradigms, reward function, communication system, neural network (NN) architecture, memory mechanisms, and POMDP formulations. This work jointly addresses several key limitations in prior research, namely reliance on discrete actions, single-agent or centralized formulations, assumptions of a priori knowledge and permanent connectivity, inability to handle dynamic obstacles, short planning horizons and architectural complexity in Recurrent NNs/Transformers. Results show that the scalable training paradigm, combined with a simplified architecture, enables rapid autonomous exploration of an indoor area. The implementation of Curriculum-Learning (five increasingly complex levels) also enabled faster, more robust training. This combination of high-fidelity simulation, MARL formulation, and computational efficiency establishes a strong foundation for deploying learned cooperative strategies in physical robotic systems.

</details>


### [120] [Accelerating Structured Chain-of-Thought in Autonomous Vehicles](https://arxiv.org/abs/2602.02864)
*Yi Gu,Yan Wang,Yuxiao Chen,Yurong You,Wenjie Luo,Yue Wang,Wenhao Ding,Boyi Li,Heng Yang,Boris Ivanovic,Marco Pavone*

Main category: cs.RO

TL;DR: FastDriveCoT accelerates CoT reasoning for autonomous driving by parallelizing independent reasoning steps, achieving 3-4× speedup while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Chain-of-Thought reasoning improves vision-language-action models for autonomous driving but introduces significant inference latency due to its autoregressive nature, making it impractical for real-time applications.

Method: Introduces FastDriveCoT, a parallel decoding method that decomposes CoT reasoning into a dependency graph of distinct sub-tasks (e.g., identifying critical objects, summarizing traffic rules) and generates multiple independent reasoning steps concurrently in a single forward pass.

Result: Achieves 3-4× speedup in CoT generation and substantial reduction in end-to-end latency across various model architectures while preserving downstream task improvements from CoT reasoning.

Conclusion: FastDriveCoT enables practical real-time CoT reasoning for autonomous driving by significantly reducing inference latency through parallel decoding while maintaining the decision-making benefits of CoT.

Abstract: Chain-of-Thought (CoT) reasoning enhances the decision-making capabilities of vision-language-action models in autonomous driving, but its autoregressive nature introduces significant inference latency, making it impractical for real-time applications. To address this, we introduce FastDriveCoT, a novel parallel decoding method that accelerates template-structured CoT. Our approach decomposes the reasoning process into a dependency graph of distinct sub-tasks, such as identifying critical objects and summarizing traffic rules, some of which can be generated in parallel. By generating multiple independent reasoning steps concurrently within a single forward pass, we significantly reduce the number of sequential computations. Experiments demonstrate a 3-4$\times$ speedup in CoT generation and a substantial reduction in end-to-end latency across various model architectures, all while preserving the original downstream task improvements brought by incorporating CoT reasoning.

</details>


### [121] [Moving On, Even When You're Broken: Fail-Active Trajectory Generation via Diffusion Policies Conditioned on Embodiment and Task](https://arxiv.org/abs/2602.02895)
*Gilberto G. Briscoe-Martinez,Yaashia Gautam,Rahul Shetty,Anuj Pasricha,Marco M. Nicotra,Alessandro Roncone*

Main category: cs.RO

TL;DR: DEFT: A diffusion-based trajectory generator for robotic arms that enables task completion under arbitrary actuation failures by generating trajectories conditioned on current embodiment and task constraints.


<details>
  <summary>Details</summary>
Motivation: Robot failures require human intervention and disrupt operations. The goal is to achieve "fail-active" operation - maintaining safe operation under impairment to complete tasks despite actuation failures.

Method: DEFT uses diffusion-based trajectory generation conditioned on the robot's current embodiment (including failures) and task constraints. It generalizes across failure types and supports both constrained and unconstrained motions.

Result: In simulation over thousands of joint-failure cases, DEFT outperformed baselines by up to 2x. It showed robust generalization to unseen failures. Real-world experiments on drawer manipulation and whiteboard erasing demonstrated success where classical methods failed.

Conclusion: DEFT achieves fail-active manipulation across arbitrary failure configurations and real-world deployments, enabling robots to complete tasks despite actuation impairments without human intervention.

Abstract: Robot failure is detrimental and disruptive, often requiring human intervention to recover. Maintaining safe operation under impairment to achieve task completion, i.e. fail-active operation, is our target. Focusing on actuation failures, we introduce DEFT, a diffusion-based trajectory generator conditioned on the robot's current embodiment and task constraints. DEFT generalizes across failure types, supports constrained and unconstrained motions, and enables task completion under arbitrary failure. We evaluated DEFT in both simulation and real-world scenarios using a 7-DoF robotic arm. In simulation over thousands of joint-failure cases across multiple tasks, DEFT outperformed the baseline by up to 2 times. On failures unseen during training, it continued to outperform the baseline, indicating robust generalization in simulation. Further, we performed real-world evaluations on two multi-step tasks, drawer manipulation and whiteboard erasing. These experiments demonstrated DEFT succeeding on tasks where classical methods failed. Our results show that DEFT achieves fail-active manipulation across arbitrary failure configurations and real-world deployments.

</details>


### [122] [Modular Isoperimetric Soft Robotic Truss for Lunar Applications](https://arxiv.org/abs/2602.02915)
*Mihai Stanciu,Isaac Weaver,Adam Rose,James Wade,Kaden Paxton,Chris Paul,Spencer Stowell,Nathan Usevitch*

Main category: cs.RO

TL;DR: A modular robotic system using inflatable fabric tubes and pinch-roller joints for lunar applications, enabling shape-changing structures and locomotion without additional compressed air.


<details>
  <summary>Details</summary>
Motivation: To address key challenges for sustainable lunar operations and future space missions by creating lightweight, modular, and reconfigurable robotic structures that can adapt to various functions.

Method: System uses truss-like robotic triangles formed by continuous inflated fabric tubes routed through robotic roller units and a connecting unit. Spherical joints allow up to three triangles to connect. Roller units pinch tubes to create effective joints, and electric motors translate rollers along tubes to change shape while maintaining constant perimeter (isoperimetric).

Result: Achieves 1:18.3 stowed-to-deployed volume ratio. Demonstrates as 12-DOF solar array capable of 60° tilt and 360° sweep, and as 14-DOF locomotion device using step-and-slide gait. Enables untethered operation after initial inflation.

Conclusion: The modular, shape-adaptive system provides a versatile solution for lunar applications, enabling both structural reconfiguration and locomotion capabilities with efficient packaging and operation.

Abstract: We introduce a large-scale robotic system designed as a lightweight, modular, and reconfigurable structure for lunar applications. The system consists of truss-like robotic triangles formed by continuous inflated fabric tubes routed through two robotic roller units and a connecting unit. A newly developed spherical joint enables up to three triangles to connect at a vertex, allowing construction of truss assemblies beyond a single octahedron. When deflated, the triangles compact to approximately the volume of the roller units, achieving a stowed-to-deployed volume ratio of 1:18.3. Upon inflation, the roller units pinch the tubes, locally reducing bending stiffness to form effective joints. Electric motors then translate the roller units along the tube, shifting the pinch point by lengthening one edge while shortening another at the same rate, thereby preserving a constant perimeter (isoperimetric). This shape-changing process requires no additional compressed air, enabling untethered operation after initial inflation. We demonstrate the system as a 12-degree-of-freedom solar array capable of tilting up to 60 degrees and sweeping 360 degrees, and as a 14-degree-of-freedom locomotion device using a step-and-slide gait. This modular, shape-adaptive system addresses key challenges for sustainable lunar operations and future space missions.

</details>


### [123] [Embodiment-Aware Generalist Specialist Distillation for Unified Humanoid Whole-Body Control](https://arxiv.org/abs/2602.02960)
*Quanquan Peng,Yunfeng Lin,Yufei Xue,Jiangmiao Pang,Weinan Zhang*

Main category: cs.RO

TL;DR: EAGLE is an iterative generalist-specialist distillation framework that produces a single unified policy controlling multiple heterogeneous humanoid robots without per-robot reward tuning.


<details>
  <summary>Details</summary>
Motivation: Current RL-trained humanoid whole-body controllers target single robot embodiments and struggle with variations in dynamics, DoFs, and kinematic topology. There's a need for generalist policies that transfer across different humanoid embodiments while supporting richer behaviors beyond simple walking.

Method: EAGLE uses an iterative distillation framework: embodiment-specific specialists are forked from the current generalist, refined on their respective robots, and new skills are distilled back into the generalist by training on the pooled embodiment set. This loop repeats until performance convergence.

Result: Validated on robots like Unitree H1, G1, and Fourier N1, with experiments on five different robots in simulation and four in real-world settings. Achieves high tracking accuracy and robustness compared to other methods.

Conclusion: EAGLE marks a step toward scalable, fleet-level humanoid control by producing a single unified policy that controls multiple heterogeneous humanoids without per-robot reward tuning.

Abstract: Humanoid Whole-Body Controllers trained with reinforcement learning (RL) have recently achieved remarkable performance, yet many target a single robot embodiment. Variations in dynamics, degrees of freedom (DoFs), and kinematic topology still hinder a single policy from commanding diverse humanoids. Moreover, obtaining a generalist policy that not only transfers across embodiments but also supports richer behaviors-beyond simple walking to squatting, leaning-remains especially challenging. In this work, we tackle these obstacles by introducing EAGLE, an iterative generalist-specialist distillation framework that produces a single unified policy that controls multiple heterogeneous humanoids without per-robot reward tuning. During each cycle, embodiment-specific specialists are forked from the current generalist, refined on their respective robots, and new skills are distilled back into the generalist by training on the pooled embodiment set. Repeating this loop until performance convergence produces a robust Whole-Body Controller validated on robots such as Unitree H1, G1, and Fourier N1. We conducted experiments on five different robots in simulation and four in real-world settings. Through quantitative evaluations, EAGLE achieves high tracking accuracy and robustness compared to other methods, marking a step toward scalable, fleet-level humanoid control. See more details at https://eagle-wbc.github.io/

</details>


### [124] [RPL: Learning Robust Humanoid Perceptive Locomotion on Challenging Terrains](https://arxiv.org/abs/2602.03002)
*Yuanhang Zhang,Younggyo Seo,Juyue Chen,Yifu Yuan,Koushil Sreenath,Pieter Abbeel,Carmelo Sferrazza,Karen Liu,Rocky Duan,Guanya Shi*

Main category: cs.RO

TL;DR: RPL: Two-stage training framework for robust multi-directional humanoid locomotion on complex terrains with payloads using depth cameras and transformer policies.


<details>
  <summary>Details</summary>
Motivation: Humanoid perceptive locomotion has made progress but achieving robust multi-directional locomotion on complex terrains with payloads remains underexplored and challenging.

Method: Two-stage framework: 1) Train terrain-specific expert policies with privileged height maps for decoupled locomotion/manipulation skills, 2) Distill into transformer policy using multiple depth cameras. Introduces depth feature scaling based on velocity commands and random side masking for robustness. Develops efficient multi-depth system with ray-casting for scalable depth distillation.

Result: Achieves robust multi-directional locomotion with 2kg payloads across challenging terrains: 20° slopes, staircases with varying step lengths (22-30cm), and stepping stones with 60cm gaps. System achieves 5x speedup over existing depth rendering pipelines.

Conclusion: RPL framework enables robust multi-directional humanoid locomotion on complex terrains with payloads, demonstrating practical applicability through extensive real-world experiments and efficient simulation-to-real transfer.

Abstract: Humanoid perceptive locomotion has made significant progress and shows great promise, yet achieving robust multi-directional locomotion on complex terrains remains underexplored. To tackle this challenge, we propose RPL, a two-stage training framework that enables multi-directional locomotion on challenging terrains, and remains robust with payloads. RPL first trains terrain-specific expert policies with privileged height map observations to master decoupled locomotion and manipulation skills across different terrains, and then distills them into a transformer policy that leverages multiple depth cameras to cover a wide range of views. During distillation, we introduce two techniques to robustify multi-directional locomotion, depth feature scaling based on velocity commands and random side masking, which are critical for asymmetric depth observations and unseen widths of terrains. For scalable depth distillation, we develop an efficient multi-depth system that ray-casts against both dynamic robot meshes and static terrain meshes in massively parallel environments, achieving a 5-times speedup over the depth rendering pipelines in existing simulators while modeling realistic sensor latency, noise, and dropout. Extensive real-world experiments demonstrate robust multi-directional locomotion with payloads (2kg) across challenging terrains, including 20° slopes, staircases with different step lengths (22 cm, 25 cm, 30 cm), and 25 cm by 25 cm stepping stones separated by 60 cm gaps.

</details>


### [125] [Training and Simulation of Quadrupedal Robot in Adaptive Stair Climbing for Indoor Firefighting: An End-to-End Reinforcement Learning Approach](https://arxiv.org/abs/2602.03087)
*Baixiao Huang,Baiyu Huang,Yu Hou*

Main category: cs.RO

TL;DR: Two-stage end-to-end deep RL enables quadruped robots to climb various indoor staircases for fire search missions, transferring skills from abstract to realistic stair environments.


<details>
  <summary>Details</summary>
Motivation: Quadruped robots need to perform primary searches in indoor fires, but face challenges with situational awareness in complex environments and rapid stair climbing across different staircase types.

Method: Two-stage end-to-end deep RL approach: 1) Train on pyramid-stair terrain, 2) Transfer to realistic indoor staircases (straight, L-shaped, spiral) using centerline-based navigation with local height-map perception.

Result: Developed framework enables policy generalization across diverse staircases, with empirical analysis of success, efficiency, and failure modes under increasing stair difficulty.

Conclusion: The approach successfully balances navigation and locomotion, enabling quadrupeds to adapt to different stair shapes using end-to-end RL methods without hierarchical planning.

Abstract: Quadruped robots are used for primary searches during the early stages of indoor fires. A typical primary search involves quickly and thoroughly looking for victims under hazardous conditions and monitoring flammable materials. However, situational awareness in complex indoor environments and rapid stair climbing across different staircases remain the main challenges for robot-assisted primary searches. In this project, we designed a two-stage end-to-end deep reinforcement learning (RL) approach to optimize both navigation and locomotion. In the first stage, the quadrupeds, Unitree Go2, were trained to climb stairs in Isaac Lab's pyramid-stair terrain. In the second stage, the quadrupeds were trained to climb various realistic indoor staircases in the Isaac Lab engine, with the learned policy transferred from the previous stage. These indoor staircases are straight, L-shaped, and spiral, to support climbing tasks in complex environments. This project explores how to balance navigation and locomotion and how end-to-end RL methods can enable quadrupeds to adapt to different stair shapes. Our main contributions are: (1) A two-stage end-to-end RL framework that transfers stair-climbing skills from abstract pyramid terrain to realistic indoor stair topologies. (2) A centerline-based navigation formulation that enables unified learning of navigation and locomotion without hierarchical planning. (3) Demonstration of policy generalization across diverse staircases using only local height-map perception. (4) An empirical analysis of success, efficiency, and failure modes under increasing stair difficulty.

</details>


### [126] [A Unified Candidate Set with Scene-Adaptive Refinement via Diffusion for End-to-End Autonomous Driving](https://arxiv.org/abs/2602.03112)
*Zhengfei Wu,Shuaixi Pan,Shuohan Chen,Shuo Yang,Yanjun Huang*

Main category: cs.RO

TL;DR: CdDrive enhances autonomous driving planning by combining fixed trajectory vocabulary with scene-adaptive diffusion-generated candidates, using HATNA for smoothness and joint scoring for reliable performance across scenarios.


<details>
  <summary>Details</summary>
Motivation: Current multimodal planning approaches face a trade-off: fixed trajectory vocabularies provide stable coverage but miss optimal solutions in complex interactions, while scene-adaptive refinement can over-correct in simple scenarios by unnecessarily perturbing already good trajectories.

Method: CdDrive preserves original vocabulary candidates and augments them with scene-adaptive candidates generated by vocabulary-conditioned diffusion denoising. Both candidate types are jointly scored by a shared selection module. HATNA (Horizon-Aware Trajectory Noise Adapter) improves smoothness and geometric continuity via temporal smoothing and horizon-aware noise modulation.

Result: Experiments on NAVSIM v1 and NAVSIM v2 demonstrate leading performance. Ablation studies verify the contribution of each component.

Conclusion: CdDrive effectively balances the strengths of fixed vocabulary and scene-adaptive approaches, achieving reliable performance across both routine and highly interactive driving scenarios through its hybrid candidate generation and joint scoring framework.

Abstract: End-to-end autonomous driving is increasingly adopting a multimodal planning paradigm that generates multiple trajectory candidates and selects the final plan, making candidate-set design critical. A fixed trajectory vocabulary provides stable coverage in routine driving but often misses optimal solutions in complex interactions, while scene-adaptive refinement can cause over-correction in simple scenarios by unnecessarily perturbing already strong vocabulary trajectories.We propose CdDrive, which preserves the original vocabulary candidates and augments them with scene-adaptive candidates generated by vocabulary-conditioned diffusion denoising. Both candidate types are jointly scored by a shared selection module, enabling reliable performance across routine and highly interactive scenarios. We further introduce HATNA (Horizon-Aware Trajectory Noise Adapter) to improve the smoothness and geometric continuity of diffusion candidates via temporal smoothing and horizon-aware noise modulation. Experiments on NAVSIM v1 and NAVSIM v2 demonstrate leading performance, and ablations verify the contribution of each component.

</details>


### [127] [Multi-function Robotized Surgical Dissector for Endoscopic Pulmonary Thromboendarterectomy: Preclinical Study and Evaluation](https://arxiv.org/abs/2602.03147)
*Runfeng Zhu,Xin Zhong,Qingxiang Zhao,Jing Lin,Zhong Wu,Kang Li*

Main category: cs.RO

TL;DR: Novel robotized dissector based on concentric push/pull robot structure enables endoscopic pulmonary thromboendarterectomy with slender 3.5mm diameter body and dual-segment bending for accessing deep thin branches of pulmonary arteries.


<details>
  <summary>Details</summary>
Motivation: Current tools for pulmonary thromboendarterectomy are rigid and straight, lacking distal dexterity to access thin branches of tortuous pulmonary arteries, limiting surgical capabilities.

Method: Developed a CPPR-based robotized dissector with 3.5mm diameter, dual-segment bending, central lumen for irrigation/tip tool/camera wires, and optimization-based kinematics model for accurate positioning.

Result: Achieved 2mm positioning accuracy for 60mm tip tool under open-loop control, demonstrated dexterity in ex vivo porcine lung surgery simulation, enabling endoscopic PTE upgrade.

Conclusion: The robotized dissector successfully addresses limitations of conventional tools, offering slenderness, dexterity, and endoscopic capability for improved pulmonary thromboendarterectomy procedures.

Abstract: Patients suffering chronic severe pulmonary thromboembolism need Pulmonary Thromboendarterectomy (PTE) to remove the thromb and intima located inside pulmonary artery (PA). During the surgery, a surgeon holds tweezers and a dissector to delicately strip the blockage, but available tools for this surgery are rigid and straight, lacking distal dexterity to access into thin branches of PA. Therefore, this work presents a novel robotized dissector based on concentric push/pull robot (CPPR) structure, enabling entering deep thin branch of tortuous PA. Compared with conventional rigid dissectors, our design characterizes slenderness and dual-segment-bending dexterity. Owing to the hollow and thin-walled structure of the CPPR-based dissector as it has a slender body of 3.5mm in diameter, the central lumen accommodates two channels for irrigation and tip tool, and space for endoscopic camera's signal wire. To provide accurate surgical manipulation, optimization-based kinematics model was established, realizing a 2mm accuracy in positioning the tip tool (60mm length) under open-loop control strategy. As such, with the endoscopic camera, traditional PTE is possible to be upgraded as endoscopic PTE. Basic physic performance of the robotized dissector including stiffness, motion accuracy and maneuverability was evaluated through experiments. Surgery simulation on ex vivo porcine lung also demonstrates its dexterity and notable advantages in PTE.

</details>


### [128] [When Attention Betrays: Erasing Backdoor Attacks in Robotic Policies by Reconstructing Visual Tokens](https://arxiv.org/abs/2602.03153)
*Xuetao Li,Pinhan Fu,Wenke Huang,Nengyuan Pan,Songhua Yang,Kaiyan Zhao,Guancheng Wan,Mengde Li,Jifeng Xuan,Miao Li*

Main category: cs.RO

TL;DR: Bera is a test-time backdoor erasure framework for vision-language-action models that detects and removes backdoor triggers without retraining, using attention analysis and image reconstruction to maintain performance while eliminating harmful behavior.


<details>
  <summary>Details</summary>
Motivation: Vision-language-action models used in robotics are vulnerable to backdoor attacks during fine-tuning, where poisoned pretraining data can implant stealthy triggers that cause harmful behavior. Existing defenses lack mechanistic understanding of multimodal backdoors or require expensive full-model retraining.

Method: Bera analyzes the attention grabbing mechanism where backdoors redirect late-stage attention and form compact embedding clusters. It detects anomalous tokens via latent-space localization, masks suspicious regions using deep-layer attention cues, and reconstructs trigger-free images to break the trigger-unsafe-action mapping while restoring correct behavior.

Result: Extensive experiments across multiple embodied platforms and tasks show Bera effectively maintains nominal performance, significantly reduces attack success rates, and consistently restores benign behavior from backdoored outputs without requiring model retraining.

Conclusion: Bera provides a robust and practical defense mechanism for securing robotic systems against backdoor attacks in vision-language-action models, offering a computationally efficient solution that doesn't require changes to the training pipeline.

Abstract: Downstream fine-tuning of vision-language-action (VLA) models enhances robotics, yet exposes the pipeline to backdoor risks. Attackers can pretrain VLAs on poisoned data to implant backdoors that remain stealthy but can trigger harmful behavior during inference. However, existing defenses either lack mechanistic insight into multimodal backdoors or impose prohibitive computational costs via full-model retraining. To this end, we uncover a deep-layer attention grabbing mechanism: backdoors redirect late-stage attention and form compact embedding clusters near the clean manifold. Leveraging this insight, we introduce Bera, a test-time backdoor erasure framework that detects tokens with anomalous attention via latent-space localization, masks suspicious regions using deep-layer cues, and reconstructs a trigger-free image to break the trigger-unsafe-action mapping while restoring correct behavior. Unlike prior defenses, Bera requires neither retraining of VLAs nor any changes to the training pipeline. Extensive experiments across multiple embodied platforms and tasks show that Bera effectively maintains nominal performance, significantly reduces attack success rates, and consistently restores benign behavior from backdoored outputs, thereby offering a robust and practical defense mechanism for securing robotic systems.

</details>


### [129] [Estimation of Ground Reaction Forces from Kinematic Data during Locomotion](https://arxiv.org/abs/2602.03177)
*Gautami Golani,Dong Anh Khoa To,Ananda Sidarta,Arun-Kumar Kaliya-Perumal,Oliver Roberts,Lek Syn Lim,Jim Patton,Domenico Campolo*

Main category: cs.RO

TL;DR: A force-plate-free method to estimate ground reaction forces using only marker-based motion capture data, enabling clinical gait analysis without dedicated force plates.


<details>
  <summary>Details</summary>
Motivation: Ground reaction forces (GRFs) are clinically important for assessing gait mechanics, joint loading, symmetry, balance, and motor function, but their use is limited in clinical settings due to the practical constraints of force plate systems.

Method: Uses kinematics from sixteen body segments to estimate center of mass (CoM), then computes GRFs and decomposes them into individual components through a minimization-based approach. This kinematics-only method identifies gait stance phases and provides kinetic measures without force plates.

Result: Experimental results demonstrate the viability of CoM and GRF estimation based solely on kinematic data, supporting force-plate-free gait analysis.

Conclusion: The presented force-plate-free approach enables widespread clinical deployment of GRF analysis by eliminating the need for dedicated force plate systems, making clinically meaningful kinetic measures accessible using only motion capture data.

Abstract: Ground reaction forces (GRFs) provide fundamental insight into human gait mechanics and are widely used to assess joint loading, limb symmetry, balance control, and motor function. Despite their clinical relevance, the use of GRF remains underutilised in clinical workflows due to the practical limitations of force plate systems. In this work, we present a force-plate-free approach for estimating GRFs using only marker-based motion capture data. This kinematics only method to estimate and decompose GRF makes it well suited for widespread clinical depolyment. By using kinematics from sixteen body segments, we estimate the centre of mass (CoM) and compute GRFs, which are subsequently decomposed into individual components through a minimization-based approach. Through this framework, we can identify gait stance phases and provide access to clinically meaningful kinetic measures without a dedicated force plate system. Experimental results demonstrate the viability of CoM and GRF estimation based solely on kinematic data, supporting force-plate-free gait analysis.

</details>


### [130] [Hierarchical Proportion Models for Motion Generation via Integration of Motion Primitives](https://arxiv.org/abs/2602.03188)
*Yu-Han Shu,Toshiaki Tsuji,Sho Sakaino*

Main category: cs.RO

TL;DR: Hierarchical imitation learning framework using motion primitives with proportion-based synthesis improves data efficiency and adaptability for complex robot tasks.


<details>
  <summary>Details</summary>
Motivation: Imitation learning requires extensive high-quality data and retraining for complex/long-horizon tasks, needing improved data efficiency and adaptability.

Method: Two-layer hierarchical IL framework: upper layer for long-term planning, lower layer learns motion primitives combined via proportion-based synthesis. Three variants: learning-based, sampling-based, and playback-based proportion models.

Result: Successfully generated complex motions not in primitive set via real-robot pick-and-place experiments. Sampling-based and playback-based models achieved more stable/adaptable motion than standard hierarchical model.

Conclusion: Proportion-based motion integration effectively enables practical robot learning with improved data efficiency and adaptability for complex tasks.

Abstract: Imitation learning (IL) enables robots to acquire human-like motion skills from demonstrations, but it still requires extensive high-quality data and retraining to handle complex or long-horizon tasks. To improve data efficiency and adaptability, this study proposes a hierarchical IL framework that integrates motion primitives with proportion-based motion synthesis. The proposed method employs a two-layer architecture, where the upper layer performs long-term planning, while a set of lower-layer models learn individual motion primitives, which are combined according to specific proportions. Three model variants are introduced to explore different trade-offs between learning flexibility, computational cost, and adaptability: a learning-based proportion model, a sampling-based proportion model, and a playback-based proportion model, which differ in how the proportions are determined and whether the upper layer is trainable. Through real-robot pick-and-place experiments, the proposed models successfully generated complex motions not included in the primitive set. The sampling-based and playback-based proportion models achieved more stable and adaptable motion generation than the standard hierarchical model, demonstrating the effectiveness of proportion-based motion integration for practical robot learning.

</details>


### [131] [HUSKY: Humanoid Skateboarding System via Physics-Aware Whole-Body Control](https://arxiv.org/abs/2602.03205)
*Jinrui Han,Dewei Wang,Chenyun Zhang,Xinzhe Liu,Ping Luo,Chenjia Bai,Xuelong Li*

Main category: cs.RO

TL;DR: HUSKY framework enables humanoid robots to perform stable skateboarding by integrating system modeling with physics-aware whole-body control, overcoming challenges of dynamic maneuvering on underactuated wheeled platforms.


<details>
  <summary>Details</summary>
Motivation: Current humanoid whole-body control frameworks rely on static environment assumptions, making it difficult to handle highly dynamic tasks with complex interactions like skateboarding, which requires managing non-holonomic constraints and tightly coupled human-object interactions.

Method: Proposes HUSKY framework that: 1) models coupling between board tilt and truck steering angles for system dynamics analysis, 2) uses Adversarial Motion Priors (AMP) to learn human-like pushing motions, 3) implements physics-guided heading-oriented strategy for lean-to-steer behaviors, and 4) employs trajectory-guided mechanism for smooth transitions between pushing and steering.

Result: Experimental results on Unitree G1 humanoid platform demonstrate stable and agile maneuvering on skateboards in real-world scenarios, successfully executing the challenging skateboarding task.

Conclusion: HUSKY framework effectively addresses the challenges of humanoid skateboarding by integrating system modeling with physics-aware whole-body control, enabling stable dynamic maneuvering on underactuated wheeled platforms with complex interactions.

Abstract: While current humanoid whole-body control frameworks predominantly rely on the static environment assumptions, addressing tasks characterized by high dynamism and complex interactions presents a formidable challenge. In this paper, we address humanoid skateboarding, a highly challenging task requiring stable dynamic maneuvering on an underactuated wheeled platform. This integrated system is governed by non-holonomic constraints and tightly coupled human-object interactions. Successfully executing this task requires simultaneous mastery of hybrid contact dynamics and robust balance control on a mechanically coupled, dynamically unstable skateboard. To overcome the aforementioned challenges, we propose HUSKY, a learning-based framework that integrates humanoid-skateboard system modeling and physics-aware whole-body control. We first model the coupling relationship between board tilt and truck steering angles, enabling a principled analysis of system dynamics. Building upon this, HUSKY leverages Adversarial Motion Priors (AMP) to learn human-like pushing motions and employs a physics-guided, heading-oriented strategy for lean-to-steer behaviors. Moreover, a trajectory-guided mechanism ensures smooth and stable transitions between pushing and steering. Experimental results on the Unitree G1 humanoid platform demonstrate that our framework enables stable and agile maneuvering on skateboards in real-world scenarios. The project page is available on https://husky-humanoid.github.io/.

</details>


### [132] [Depth Completion in Unseen Field Robotics Environments Using Extremely Sparse Depth Measurements](https://arxiv.org/abs/2602.03209)
*Marco Job,Thomas Stastny,Eleni Kelasidi,Roland Siegwart,Michael Pantic*

Main category: cs.RO

TL;DR: A depth completion model using synthetic data and sparse depth measurements for real-time metric depth prediction in field robotics.


<details>
  <summary>Details</summary>
Motivation: Field robots need robust perception for safe operations in unstructured environments. Monocular depth estimation has potential but lacks reliable scale cues, struggles with ambiguous/low-texture conditions, and lacks large-scale datasets for field robotics.

Method: Proposes a depth completion model trained on synthetic data that uses extremely sparse measurements from depth sensors to predict dense metric depth. Uses a synthetic dataset generation pipeline with textured 3D meshes from Structure from Motion and photorealistic rendering with novel viewpoint synthesis.

Result: Achieves end-to-end latency of 53 ms per frame on Nvidia Jetson AGX Orin, enabling real-time deployment. Demonstrates competitive performance across diverse real-world field robotics scenarios.

Conclusion: The approach enables robust depth perception for field robots using synthetic training data and sparse depth measurements, addressing key limitations of monocular depth estimation while maintaining real-time performance on embedded platforms.

Abstract: Autonomous field robots operating in unstructured environments require robust perception to ensure safe and reliable operations. Recent advances in monocular depth estimation have demonstrated the potential of low-cost cameras as depth sensors; however, their adoption in field robotics remains limited due to the absence of reliable scale cues, ambiguous or low-texture conditions, and the scarcity of large-scale datasets. To address these challenges, we propose a depth completion model that trains on synthetic data and uses extremely sparse measurements from depth sensors to predict dense metric depth in unseen field robotics environments. A synthetic dataset generation pipeline tailored to field robotics enables the creation of multiple realistic datasets for training purposes. This dataset generation approach utilizes textured 3D meshes from Structure from Motion and photorealistic rendering with novel viewpoint synthesis to simulate diverse field robotics scenarios. Our approach achieves an end-to-end latency of 53 ms per frame on a Nvidia Jetson AGX Orin, enabling real-time deployment on embedded platforms. Extensive evaluation demonstrates competitive performance across diverse real-world field robotics scenarios.

</details>


### [133] [Omnidirectional Solid-State mmWave Radar Perception for UAV Power Line Collision Avoidance](https://arxiv.org/abs/2602.03229)
*Nicolaj Haarhøj Malle,Emad Ebeid*

Main category: cs.RO

TL;DR: mmWave radar system enables UAVs to detect and avoid power lines up to 10m away, even for thin wires (1.2mm), providing safety for autonomous and manual flight.


<details>
  <summary>Details</summary>
Motivation: Power line detection is challenging for UAV pilots and autonomous systems, increasing collision risks during flight operations near electrical infrastructure.

Method: Multiple compact solid-state mmWave radar modules integrated to create omnidirectional sensing coverage around UAV, with specialized detection-and-avoidance algorithm tailored to power line environments.

Result: Reliable detection up to 10m range, successful avoidance at speeds over 10 m/s, detection of wires as thin as 1.2mm diameter in field experiments on real power lines.

Conclusion: The mmWave radar-based system provides robust power line detection and avoidance suitable as an additional safety layer for both autonomous and manual UAV flight.

Abstract: Detecting and estimating distances to power lines is a challenge for both human UAV pilots and autonomous systems, which increases the risk of unintended collisions. We present a mmWave radar-based perception system that provides spherical sensing coverage around a small UAV for robust power line detection and avoidance. The system integrates multiple compact solid-state mmWave radar modules to synthesize an omnidirectional field of view while remaining lightweight. We characterize the sensing behavior of this omnidirectional radar arrangement in power line environments and develop a robust detection-and-avoidance algorithm tailored to that behavior. Field experiments on real power lines demonstrate reliable detection at ranges up to 10 m, successful avoidance maneuvers at flight speeds upwards of 10 m/s, and detection of wires as thin as 1.2 mm in diameter. These results indicate the approach's suitability as an additional safety layer for both autonomous and manual UAV flight.

</details>


### [134] [A thin and soft optical tactile sensor for highly sensitive object perception](https://arxiv.org/abs/2602.03248)
*Yanchen Shen,Kohei Tsuji,Haruto Koizumi,Jiseon Hong,Tomoaki Niiyama,Hiroyuki Kuwabara,Hayato Ishida,Jun Hiramitsu,Mitsuhito Mase,Satoshi Sunada*

Main category: cs.RO

TL;DR: A thin, soft optical tactile sensor uses speckle pattern changes in silicone for force measurement and texture recognition, achieving 40 mN force accuracy and 93.33% classification accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing optical tactile sensors (especially vision-based) are bulky, rigid, and alignment-sensitive due to complex optical assemblies with lenses and cameras. There's a need for compact, soft, alignment-free tactile sensors for robotics and wearable devices.

Method: Developed a thin, compact, soft optical tactile sensor with alignment-free configuration that captures deformation-induced changes in speckle patterns within soft silicone material. Uses machine learning for force measurement and texture recognition.

Result: Achieved root-mean-square error of 40 mN in force measurement and 93.33% classification accuracy over nine classes of textured surfaces (including Mahjong tiles).

Conclusion: The speckle-based approach provides a compact, easily fabricated, mechanically compliant platform that bridges optical sensing with flexible shape-adaptive architectures, demonstrating potential as a novel tactile-sensing paradigm for soft robotics and wearable haptic interfaces.

Abstract: Tactile sensing is crucial in robotics and wearable devices for safe perception and interaction with the environment. Optical tactile sensors have emerged as promising solutions, as they are immune to electromagnetic interference and have high spatial resolution. However, existing optical approaches, particularly vision-based tactile sensors, rely on complex optical assemblies that involve lenses and cameras, resulting in bulky, rigid, and alignment-sensitive designs. In this study, we present a thin, compact, and soft optical tactile sensor featuring an alignment-free configuration. The soft optical sensor operates by capturing deformation-induced changes in speckle patterns generated within a soft silicone material, thereby enabling precise force measurements and texture recognition via machine learning. The experimental results show a root-mean-square error of 40 mN in the force measurement and a classification accuracy of 93.33% over nine classes of textured surfaces, including Mahjong tiles. The proposed speckle-based approach provides a compact, easily fabricated, and mechanically compliant platform that bridges optical sensing with flexible shape-adaptive architectures, thereby demonstrating its potential as a novel tactile-sensing paradigm for soft robotics and wearable haptic interfaces.

</details>


### [135] [Collision Detection with Analytical Derivatives of Contact Kinematics](https://arxiv.org/abs/2602.03250)
*Anup Teejo Mathew,Anees Peringal,Daniele Caradonna,Frederic Boyer,Federico Renda*

Main category: cs.RO

TL;DR: iDCOL: An implicit differentiable collision detection framework that uses geometric regularization of degenerate shapes to enable smooth contact kinematics for gradient-based robotics applications.


<details>
  <summary>Details</summary>
Motivation: Differentiable contact kinematics are essential for gradient-based robotics methods, but degenerate configurations (shapes with zero or undefined curvature) cause non-smooth contact maps, limiting their applicability.

Method: Regularize degenerate geometries into strictly convex implicit representations, then solve a fixed-size nonlinear system from a geometric scaling-based convex optimization to compute collision detection and contact kinematics. Apply Implicit Function Theorem to derive analytical derivatives.

Result: Developed iDCOL framework with fast Newton-based solver and open-source C++ implementation. Demonstrated robustness through extensive collision simulations and applicability in gradient-based kinematic path planning and differentiable contact physics.

Conclusion: iDCOL provides a robust differentiable collision detection framework that handles degenerate geometries through geometric regularization, enabling smooth contact kinematics for gradient-based robotics applications.

Abstract: Differentiable contact kinematics are essential for gradient-based methods in robotics, yet the mapping from robot state to contact distance, location, and normal becomes non-smooth in degenerate configurations of shapes with zero or undefined curvature. We address this inherent limitation by selectively regularizing such geometries into strictly convex implicit representations, restoring uniqueness and smoothness of the contact map. Leveraging this geometric regularization, we develop iDCOL, an implicit differentiable collision detection and contact kinematics framework. iDCOL represents colliding bodies using strictly convex implicit surfaces and computes collision detection and contact kinematics by solving a fixed-size nonlinear system derived from a geometric scaling-based convex optimization formulation. By applying the Implicit Function Theorem to the resulting system residual, we derive analytical derivatives of the contact kinematic quantities. We develop a fast Newton-based solver for iDCOL and provide an open-source C++ implementation of the framework. The robustness of the approach is evaluated through extensive collision simulations and benchmarking, and applicability is demonstrated in gradient-based kinematic path planning and differentiable contact physics, including multi-body rigid collisions and a soft-robot interaction example.

</details>


### [136] [RDT2: Exploring the Scaling Limit of UMI Data Towards Zero-Shot Cross-Embodiment Generalization](https://arxiv.org/abs/2602.03310)
*Songming Liu,Bangguo Li,Kai Ma,Lingxuan Wu,Hengkai Tan,Xiao Ouyang,Hang Su,Jun Zhu*

Main category: cs.RO

TL;DR: RDT2 is a 7B parameter vision-language-action foundation model that achieves zero-shot generalization to unseen objects, scenes, instructions, and robotic platforms through a novel three-stage training approach on a massive 10,000-hour diverse robotic dataset.


<details>
  <summary>Details</summary>
Motivation: Current Vision-Language-Action models face three major challenges: data scarcity, architectural inefficiencies, and inability to generalize across different hardware platforms, limiting their potential for generalist robotics.

Method: 1) Collected over 10,000 hours of robotic demonstrations using an enhanced embodiment-agnostic Universal Manipulation Interface (UMI); 2) Developed a novel three-stage training recipe aligning discrete linguistic knowledge with continuous control via Residual Vector Quantization (RVQ), flow-matching, and distillation for real-time inference.

Result: RDT2 becomes one of the first models to simultaneously zero-shot generalize to unseen objects, scenes, instructions, and robotic platforms, and outperforms state-of-the-art baselines in dexterous, long-horizon, and dynamic tasks like playing table tennis.

Conclusion: RDT2 demonstrates that with sufficient diverse data and appropriate architectural innovations, VLA models can achieve unprecedented generalization capabilities across robotic platforms, representing a significant step toward generalist robotics.

Abstract: Vision-Language-Action (VLA) models hold promise for generalist robotics but currently struggle with data scarcity, architectural inefficiencies, and the inability to generalize across different hardware platforms. We introduce RDT2, a robotic foundation model built upon a 7B parameter VLM designed to enable zero-shot deployment on novel embodiments for open-vocabulary tasks. To achieve this, we collected one of the largest open-source robotic datasets--over 10,000 hours of demonstrations in diverse families--using an enhanced, embodiment-agnostic Universal Manipulation Interface (UMI). Our approach employs a novel three-stage training recipe that aligns discrete linguistic knowledge with continuous control via Residual Vector Quantization (RVQ), flow-matching, and distillation for real-time inference. Consequently, RDT2 becomes one of the first models that simultaneously zero-shot generalizes to unseen objects, scenes, instructions, and even robotic platforms. Besides, it outperforms state-of-the-art baselines in dexterous, long-horizon, and dynamic downstream tasks like playing table tennis. See https://rdt-robotics.github.io/rdt2/ for more information.

</details>


### [137] [Manipulation via Force Distribution at Contact](https://arxiv.org/abs/2602.03350)
*Haegu Lee,Yitaek Kim,Casper Hewson Rask,Christoffer Sloth*

Main category: cs.RO

TL;DR: The paper introduces a Force-Distributed Line Contact (FDLC) model for contact-rich manipulation that outperforms point contact models by enabling non-uniform force distributions, requiring less control effort and robot motion.


<details>
  <summary>Details</summary>
Motivation: Existing point contact models are computationally efficient but limited for achieving human-like, contact-rich manipulation because they fail to capture key frictional dynamics and torque generation observed in human manipulation.

Method: Introduces a Force-Distributed Line Contact (FDLC) model and constructs a bi-level optimization framework: lower-level solves optimization for contact force computation, and upper-level applies iLQR for trajectory optimization.

Result: Demonstrates limitations of point contact models and establishes benefits of FDLC in generating efficient and robust trajectories. FDLC enables non-uniform force distributions along contact line while requiring lower control effort and less robot motion.

Conclusion: The FDLC model is effective for contact-rich manipulation, outperforming conventional point contact models by better capturing real-world frictional dynamics and enabling more efficient, robust trajectories with reduced control effort.

Abstract: Efficient and robust trajectories play a crucial role in contact-rich manipulation, which demands accurate mod- eling of object-robot interactions. Many existing approaches rely on point contact models due to their computational effi- ciency. Simple contact models are computationally efficient but inherently limited for achieving human-like, contact-rich ma- nipulation, as they fail to capture key frictional dynamics and torque generation observed in human manipulation. This study introduces a Force-Distributed Line Contact (FDLC) model in contact-rich manipulation and compares it against conventional point contact models. A bi-level optimization framework is constructed, in which the lower-level solves an optimization problem for contact force computation, and the upper-level optimization applies iLQR for trajectory optimization. Through this framework, the limitations of point contact are demon- strated, and the benefits of the FDLC in generating efficient and robust trajectories are established. The effectiveness of the proposed approach is validated by a box rotating task, demonstrating that FDLC enables trajectories generated via non-uniform force distributions along the contact line, while requiring lower control effort and less motion of the robot.

</details>


### [138] [Learning-based Adaptive Control of Quadruped Robots for Active Stabilization on Moving Platforms](https://arxiv.org/abs/2602.03367)
*Minsung Yoon,Heechan Shin,Jeil Jeong,Sung-Eui Yoon*

Main category: cs.RO

TL;DR: Learning-based Active Stabilization on Moving Platforms (LAS-MP) enables quadruped robots to maintain balance on 6-DOF moving platforms like vehicles by using adaptive posture adjustment policies and state estimators trained across diverse platform motions.


<details>
  <summary>Details</summary>
Motivation: Quadruped robots struggle to maintain balance on moving platforms (subways, buses, airplanes, yachts) due to independent 6-DOF platform motions and resulting diverse inertia forces acting on the robot.

Method: LAS-MP combines a self-balancing policy that adaptively adjusts robot posture in response to platform motion with system state estimators that infer robot and platform states from proprioceptive sensor data. Includes platform trajectory generation and scheduling methods for systematic training across various platform motions.

Result: Superior balancing performance across multiple metrics compared to three baselines. Ablation studies and estimator evaluations validate the effectiveness of each component.

Conclusion: LAS-MP successfully addresses the challenge of quadruped robot stabilization on moving platforms through learning-based adaptive posture control and state estimation, demonstrating robust performance across diverse motion scenarios.

Abstract: A quadruped robot faces balancing challenges on a six-degrees-of-freedom moving platform, like subways, buses, airplanes, and yachts, due to independent platform motions and resultant diverse inertia forces on the robot. To alleviate these challenges, we present the Learning-based Active Stabilization on Moving Platforms (\textit{LAS-MP}), featuring a self-balancing policy and system state estimators. The policy adaptively adjusts the robot's posture in response to the platform's motion. The estimators infer robot and platform states based on proprioceptive sensor data. For a systematic training scheme across various platform motions, we introduce platform trajectory generation and scheduling methods. Our evaluation demonstrates superior balancing performance across multiple metrics compared to three baselines. Furthermore, we conduct a detailed analysis of the \textit{LAS-MP}, including ablation studies and evaluation of the estimators, to validate the effectiveness of each component.

</details>


### [139] [PlanTRansformer: Unified Prediction and Planning with Goal-conditioned Transformer](https://arxiv.org/abs/2602.03376)
*Constantin Selzer,Fabina B. Flohr*

Main category: cs.RO

TL;DR: PlanTRansformer (PTR) unifies trajectory prediction and planning in autonomous driving using a Gaussian Mixture Transformer framework with teacher-student training, achieving significant improvements in prediction accuracy and planning error reduction.


<details>
  <summary>Details</summary>
Motivation: Current autonomous driving systems have disconnected prediction and planning components. Prediction models forecast agent motion without knowing intentions, while planning assumes known objectives. This mismatch creates a critical bottleneck where prediction lacks supervision for agent intentions that planning requires, and existing models ignore planning constraints like collision avoidance.

Method: Introduces PlanTRansformer (PTR), a unified Gaussian Mixture Transformer framework integrating goal-conditioned prediction, dynamic feasibility, interaction awareness, and lane-level topology reasoning. Uses teacher-student training strategy that progressively masks surrounding agent commands during training to align with inference conditions where agent intentions are unavailable.

Result: PTR achieves 4.3%/3.5% improvement in marginal/joint mAP compared to baseline Motion Transformer (MTR) and 15.5% planning error reduction at 5s horizon compared to GameFormer. The architecture-agnostic design enables application to diverse Transformer-based prediction models.

Conclusion: PTR successfully bridges the gap between prediction and planning in autonomous driving by creating a unified framework that handles both tasks while addressing the intention supervision problem through progressive masking training, resulting in improved performance on both prediction accuracy and planning quality.

Abstract: Trajectory prediction and planning are fundamental yet disconnected components in autonomous driving. Prediction models forecast surrounding agent motion under unknown intentions, producing multimodal distributions, while planning assumes known ego objectives and generates deterministic trajectories. This mismatch creates a critical bottleneck: prediction lacks supervision for agent intentions, while planning requires this information. Existing prediction models, despite strong benchmarking performance, often remain disconnected from planning constraints such as collision avoidance and dynamic feasibility. We introduce Plan TRansformer (PTR), a unified Gaussian Mixture Transformer framework integrating goal-conditioned prediction, dynamic feasibility, interaction awareness, and lane-level topology reasoning. A teacher-student training strategy progressively masks surrounding agent commands during training to align with inference conditions where agent intentions are unavailable. PTR achieves 4.3%/3.5% improvement in marginal/joint mAP compared to the baseline Motion Transformer (MTR) and 15.5% planning error reduction at 5s horizon compared to GameFormer. The architecture-agnostic design enables application to diverse Transformer-based prediction models. Project Website: https://github.com/SelzerConst/PlanTRansformer

</details>


### [140] [Enhancing Navigation Efficiency of Quadruped Robots via Leveraging Personal Transportation Platforms](https://arxiv.org/abs/2602.03397)
*Minsung Yoon,Sung-Eui Yoon*

Main category: cs.RO

TL;DR: RL-ATR enables quadruped robots to ride personal transporters like Segways using reinforcement learning, improving navigation efficiency and reducing energy consumption compared to legged locomotion.


<details>
  <summary>Details</summary>
Motivation: Quadruped robots have limited long-range navigation efficiency due to their reliance on legs alone, similar to how humans use personal transporters to extend mobility range.

Method: Reinforcement Learning-based Active Transporter Riding (RL-ATR) with a transporter riding policy and two state estimators that handle sensor ambiguities in non-inertial frames by inferring unobservable robot and transporter states.

Result: Simulation evaluations show proficient command tracking across various transporter-robot models and reduced energy consumption compared to legged locomotion, with ablation studies quantifying component contributions.

Conclusion: The riding ability broadens quadruped robots' locomotion modalities, potentially expanding their operational range and efficiency by leveraging personal transporters.

Abstract: Quadruped robots face limitations in long-range navigation efficiency due to their reliance on legs. To ameliorate the limitations, we introduce a Reinforcement Learning-based Active Transporter Riding method (\textit{RL-ATR}), inspired by humans' utilization of personal transporters, including Segways. The \textit{RL-ATR} features a transporter riding policy and two state estimators. The policy devises adequate maneuvering strategies according to transporter-specific control dynamics, while the estimators resolve sensor ambiguities in non-inertial frames by inferring unobservable robot and transporter states. Comprehensive evaluations in simulation validate proficient command tracking abilities across various transporter-robot models and reduced energy consumption compared to legged locomotion. Moreover, we conduct ablation studies to quantify individual component contributions within the \textit{RL-ATR}. This riding ability could broaden the locomotion modalities of quadruped robots, potentially expanding the operational range and efficiency.

</details>


### [141] [Deep-Learning-Based Control of a Decoupled Two-Segment Continuum Robot for Endoscopic Submucosal Dissection](https://arxiv.org/abs/2602.03406)
*Yuancheng Shao,Yao Zhang,Jia Gu,Zixi Chen,Di Wu,Yuqiao Chen,Bo Lu,Wenjie Liu,Cesare Stefanini,Peng Qi*

Main category: cs.RO

TL;DR: DESectBot: A dual-segment continuum robot with 6-DoF dexterity for endoscopic submucosal dissection, controlled by GRU-based deep learning for superior precision in surgical tasks.


<details>
  <summary>Details</summary>
Motivation: Manual ESD is technically demanding, and existing single-segment robotic tools offer limited dexterity, motivating development of more advanced robotic solutions for improved lesion targeting.

Method: Developed DESectBot - a dual-segment continuum robot with decoupled structure and integrated forceps. Proposed GRU-based deep learning controllers for simultaneous tip position/orientation control, benchmarked against Jacobian IK, MPC, FNN, and LSTM.

Result: GRU achieved lowest RMSEs in trajectory tracking (1.11mm/4.62° and 0.81mm/2.59°), best orientation control (0.14mm/0.72° mean RMSE), 100% success rate in peg transfer (11.8s average), and demonstrated sufficient stiffness for ex vivo ESD tissue resection.

Conclusion: GRU-based control significantly enhances precision, reliability, and usability in ESD surgical training scenarios, with DESectBot providing adequate workspace and stiffness for complex surgical procedures.

Abstract: Manual endoscopic submucosal dissection (ESD) is technically demanding, and existing single-segment robotic tools offer limited dexterity. These limitations motivate the development of more advanced solutions. To address this, DESectBot, a novel dual segment continuum robot with a decoupled structure and integrated surgical forceps, enabling 6 degrees of freedom (DoFs) tip dexterity for improved lesion targeting in ESD, was developed in this work. Deep learning controllers based on gated recurrent units (GRUs) for simultaneous tip position and orientation control, effectively handling the nonlinear coupling between continuum segments, were proposed. The GRU controller was benchmarked against Jacobian based inverse kinematics, model predictive control (MPC), a feedforward neural network (FNN), and a long short-term memory (LSTM) network. In nested-rectangle and Lissajous trajectory tracking tasks, the GRU achieved the lowest position/orientation RMSEs: 1.11 mm/ 4.62° and 0.81 mm/ 2.59°, respectively. For orientation control at a fixed position (four target poses), the GRU attained a mean RMSE of 0.14 mm and 0.72°, outperforming all alternatives. In a peg transfer task, the GRU achieved a 100% success rate (120 success/120 attempts) with an average transfer time of 11.8s, the STD significantly outperforms novice-controlled systems. Additionally, an ex vivo ESD demonstration grasping, elevating, and resecting tissue as the scalpel completed the cut confirmed that DESectBot provides sufficient stiffness to divide thick gastric mucosa and an operative workspace adequate for large lesions.These results confirm that GRU-based control significantly enhances precision, reliability, and usability in ESD surgical training scenarios.

</details>


### [142] [Learning-based Initialization of Trajectory Optimization for Path-following Problems of Redundant Manipulators](https://arxiv.org/abs/2602.03418)
*Minsung Yoon,Mincheul Kang,Daehyung Park,Sung-Eui Yoon*

Main category: cs.RO

TL;DR: Learning-based initial trajectory generation for manipulator trajectory optimization using example-guided reinforcement learning with null-space projected imitation reward.


<details>
  <summary>Details</summary>
Motivation: Trajectory optimization performance depends heavily on initial trajectory quality, but selecting good initial trajectories is difficult due to large solution space and lack of prior knowledge about task constraints in configuration space.

Method: Proposes learning-based initial trajectory generation using example-guided reinforcement learning with null-space projected imitation reward to efficiently learn kinematically feasible motion from expert demonstrations while considering null-space constraints.

Result: Statistical evaluation shows improved optimality, efficiency, and applicability of trajectory optimization when using the method's output, outperforming three baselines. Real-world experiments with 7-DOF manipulator demonstrate performance improvement and feasibility.

Conclusion: The proposed method effectively generates high-quality initial trajectories for trajectory optimization in a short time budget, addressing the challenge of initial trajectory selection for redundant manipulators following Cartesian paths.

Abstract: Trajectory optimization (TO) is an efficient tool to generate a redundant manipulator's joint trajectory following a 6-dimensional Cartesian path. The optimization performance largely depends on the quality of initial trajectories. However, the selection of a high-quality initial trajectory is non-trivial and requires a considerable time budget due to the extremely large space of the solution trajectories and the lack of prior knowledge about task constraints in configuration space. To alleviate the issue, we present a learning-based initial trajectory generation method that generates high-quality initial trajectories in a short time budget by adopting example-guided reinforcement learning. In addition, we suggest a null-space projected imitation reward to consider null-space constraints by efficiently learning kinematically feasible motion captured in expert demonstrations. Our statistical evaluation in simulation shows the improved optimality, efficiency, and applicability of TO when we plug in our method's output, compared with three other baselines. We also show the performance improvement and feasibility via real-world experiments with a seven-degree-of-freedom manipulator.

</details>


### [143] [ProAct: A Benchmark and Multimodal Framework for Structure-Aware Proactive Response](https://arxiv.org/abs/2602.03430)
*Xiaomeng Zhu,Fengming Zhu,Weijie Zhou,Ye Tian,Zhenlin Hu,Yufei Huang,Yuchun Guo,Xinyu Wu,Zhengyou Zhang,Fangzhen Lin,Xuantang Xiong*

Main category: cs.RO

TL;DR: ProAct-75 benchmark for proactive agents with task graphs, plus ProAct-Helper baseline using MLLM and entropy-driven search for parallel action execution.


<details>
  <summary>Details</summary>
Motivation: Proactive agents that align with higher-level objectives (assistance, safety) need specialized resources for development, which are currently lacking.

Method: Introduce ProAct-75 benchmark with 75 tasks, 91,581 step-level annotations and explicit task graphs. Propose ProAct-Helper baseline using Multimodal LLM for state detection and entropy-driven heuristic search leveraging task graphs for parallel action selection.

Result: ProAct-Helper outperforms strong closed-source models: improves trigger detection mF1 by 6.21%, saves 0.25 more steps in online one-step decision, and increases parallel action rate by 15.58%.

Conclusion: ProAct-75 provides essential resources for proactive agent development, and ProAct-Helper demonstrates effective use of task graphs for complex decision-making with parallel execution capabilities.

Abstract: While passive agents merely follow instructions, proactive agents align with higher-level objectives, such as assistance and safety by continuously monitoring the environment to determine when and how to act. However, developing proactive agents is hindered by the lack of specialized resources. To address this, we introduce ProAct-75, a benchmark designed to train and evaluate proactive agents across diverse domains, including assistance, maintenance, and safety monitoring. Spanning 75 tasks, our dataset features 91,581 step-level annotations enriched with explicit task graphs. These graphs encode step dependencies and parallel execution possibilities, providing the structural grounding necessary for complex decision-making. Building on this benchmark, we propose ProAct-Helper, a reference baseline powered by a Multimodal Large Language Model (MLLM) that grounds decision-making in state detection, and leveraging task graphs to enable entropy-driven heuristic search for action selection, allowing agents to execute parallel threads independently rather than mirroring the human's next step. Extensive experiments demonstrate that ProAct-Helper outperforms strong closed-source models, improving trigger detection mF1 by 6.21%, saving 0.25 more steps in online one-step decision, and increasing the rate of parallel actions by 15.58%.

</details>


### [144] [Model-based Optimal Control for Rigid-Soft Underactuated Systems](https://arxiv.org/abs/2602.03435)
*Daniele Caradonna,Nikhil Nair,Anup Teejo Mathew,Daniel Feliu Talegón,Imran Afgan,Egidio Falotico,Cosimo Della Santina,Federico Renda*

Main category: cs.RO

TL;DR: This paper investigates optimal control strategies for dynamic swing-up tasks in underactuated continuum soft robots, addressing computational challenges through analytical derivatives and numerical robustness techniques.


<details>
  <summary>Details</summary>
Motivation: Continuum soft robots are underactuated with intrinsic input constraints, making dynamic control challenging. Existing methods focus on quasi-static behaviors or use simplified models that fail to capture real continuum deformations. Model-based optimal control offers a solution but faces computational cost and accuracy issues with numerical differentiation for high-dimensional models.

Method: The work leverages the Geometric Variable Strain model for analytical derivatives and investigates three optimal control strategies: Direct Collocation, Differential Dynamic Programming, and Nonlinear Model Predictive Control. To handle stiff continuum dynamics and constrained actuation, the authors employ implicit integration schemes and warm-start strategies to improve numerical robustness and computational efficiency.

Result: The methods are evaluated in simulation on three benchmark systems: Soft Cart-Pole, Soft Pendubot, and Soft Furuta Pendulum. The study highlights performance and computational trade-offs between the different optimal control approaches for dynamic swing-up tasks.

Conclusion: The paper demonstrates that optimal control strategies can effectively address dynamic swing-up tasks in underactuated soft systems when combined with analytical derivatives from the Geometric Variable Strain model and numerical robustness techniques like implicit integration and warm-starting.

Abstract: Continuum soft robots are inherently underactuated and subject to intrinsic input constraints, making dynamic control particularly challenging, especially in hybrid rigid-soft robots. While most existing methods focus on quasi-static behaviors, dynamic tasks such as swing-up require accurate exploitation of continuum dynamics. This has led to studies on simple low-order template systems that often fail to capture the complexity of real continuum deformations. Model-based optimal control offers a systematic solution; however, its application to rigid-soft robots is often limited by the computational cost and inaccuracy of numerical differentiation for high-dimensional models. Building on recent advances in the Geometric Variable Strain model that enable analytical derivatives, this work investigates three optimal control strategies for underactuated soft systems-Direct Collocation, Differential Dynamic Programming, and Nonlinear Model Predictive Control-to perform dynamic swing-up tasks. To address stiff continuum dynamics and constrained actuation, implicit integration schemes and warm-start strategies are employed to improve numerical robustness and computational efficiency. The methods are evaluated in simulation on three Rigid-Soft and high-order soft benchmark systems-the Soft Cart-Pole, the Soft Pendubot, and the Soft Furuta Pendulum- highlighting their performance and computational trade-offs.

</details>


### [145] [HetroD: A High-Fidelity Drone Dataset and Benchmark for Autonomous Driving in Heterogeneous Traffic](https://arxiv.org/abs/2602.03447)
*Yu-Hsiang Chen,Wei-Jer Chang,Christian Kotulla,Thomas Keutgens,Steffen Runde,Tobias Moers,Christoph Klas,Wei Zhan,Masayoshi Tomizuka,Yi-Ting Chen*

Main category: cs.RO

TL;DR: HetroD is a dataset and benchmark for autonomous driving in heterogeneous traffic environments dominated by vulnerable road users (VRUs), addressing gaps in existing datasets focused on structured traffic.


<details>
  <summary>Details</summary>
Motivation: Existing autonomous driving datasets focus on structured, lane-disciplined traffic, but real-world heterogeneous traffic with vulnerable road users (pedestrians, cyclists, motorcyclists) exhibiting complex behaviors like hook turns and lane splitting remains underrepresented.

Method: Collected large-scale drone-based dataset with centimeter-accurate annotations, HD maps, and traffic signal states. Developed modular toolkit for extracting per-agent scenarios. Dataset includes over 65.4k high-fidelity agent trajectories (70% VRUs).

Result: State-of-the-art prediction and planning models struggle with HetroD's challenges: fail to predict lateral VRU movements, cannot handle unstructured maneuvers, and show limited performance in dense multi-agent scenarios.

Conclusion: HetroD bridges the gap for autonomous driving in heterogeneous traffic, provides standardized benchmarks for forecasting, planning, and simulation tasks, and highlights the need for more robust approaches to handle real-world mixed traffic scenarios.

Abstract: We present HetroD, a dataset and benchmark for developing autonomous driving systems in heterogeneous environments. HetroD targets the critical challenge of navi- gating real-world heterogeneous traffic dominated by vulner- able road users (VRUs), including pedestrians, cyclists, and motorcyclists that interact with vehicles. These mixed agent types exhibit complex behaviors such as hook turns, lane splitting, and informal right-of-way negotiation. Such behaviors pose significant challenges for autonomous vehicles but remain underrepresented in existing datasets focused on structured, lane-disciplined traffic. To bridge the gap, we collect a large- scale drone-based dataset to provide a holistic observation of traffic scenes with centimeter-accurate annotations, HD maps, and traffic signal states. We further develop a modular toolkit for extracting per-agent scenarios to support downstream task development. In total, the dataset comprises over 65.4k high- fidelity agent trajectories, 70% of which are from VRUs. HetroD supports modeling of VRU behaviors in dense, het- erogeneous traffic and provides standardized benchmarks for forecasting, planning, and simulation tasks. Evaluation results reveal that state-of-the-art prediction and planning models struggle with the challenges presented by our dataset: they fail to predict lateral VRU movements, cannot handle unstructured maneuvers, and exhibit limited performance in dense and multi-agent scenarios, highlighting the need for more robust approaches to heterogeneous traffic. See our project page for more examples: https://hetroddata.github.io/HetroD/

</details>


### [146] [CMR: Contractive Mapping Embeddings for Robust Humanoid Locomotion on Unstructured Terrains](https://arxiv.org/abs/2602.03511)
*Qixin Zeng,Hongyin Zhang,Shangke Lyu,Junxi Jin,Donglin Wang,Chao Huang*

Main category: cs.RO

TL;DR: CMR framework maps noisy observations to contractive latent space to improve humanoid locomotion robustness under sensor noise and model mismatch.


<details>
  <summary>Details</summary>
Motivation: Humanoid locomotion struggles with disturbance rejection on unstructured terrains due to unreliable sensing, sensor noise, and sim-to-real gaps that destabilize policies.

Method: Contractive Mapping for Robustness (CMR) framework maps high-dimensional noisy observations to latent space with contractive dynamics, using contrastive representation learning with Lipschitz regularization to attenuate local perturbations over time.

Result: CMR significantly outperforms other locomotion algorithms under increased noise conditions in extensive humanoid experiments.

Conclusion: The CMR framework provides a theoretically grounded approach to robust disturbance rejection that can be easily integrated into modern deep RL pipelines as an auxiliary loss term.

Abstract: Robust disturbance rejection remains a longstanding challenge in humanoid locomotion, particularly on unstructured terrains where sensing is unreliable and model mismatch is pronounced. While perception information, such as height map, enhances terrain awareness, sensor noise and sim-to-real gaps can destabilize policies in practice. In this work, we provide theoretical analysis that bounds the return gap under observation noise, when the induced latent dynamics are contractive. Furthermore, we present Contractive Mapping for Robustness (CMR) framework that maps high-dimensional, disturbance-prone observations into a latent space, where local perturbations are attenuated over time. Specifically, this approach couples contrastive representation learning with Lipschitz regularization to preserve task-relevant geometry while explicitly controlling sensitivity. Notably, the formulation can be incorporated into modern deep reinforcement learning pipelines as an auxiliary loss term with minimal additional technical effort required. Further, our extensive humanoid experiments show that CMR potently outperforms other locomotion algorithms under increased noise.

</details>


### [147] [Investigating the Influence of Spatial Ability in Augmented Reality-assisted Robot Programming](https://arxiv.org/abs/2602.03544)
*Nicolas Leins,Jana Gonnermann-Müller,Malte Teichmann,Sebastian Pokutta*

Main category: cs.RO

TL;DR: AR-assisted robot programming doesn't improve overall learning experience but compensates for low spatial ability, reducing cognitive barriers for learners with varying cognitive profiles.


<details>
  <summary>Details</summary>
Motivation: While AR shows promise for enhancing learning, its mechanisms and effects remain unclear. As learning becomes more personalized, understanding how AR interacts with individual learner characteristics (like spatial ability) is crucial for designing effective educational environments.

Method: Between-subjects experiment with 71 participants comparing conventional robot programming to AR-assisted approach using head-mounted display. Spatial ability measured via Mental Rotation Test. Learning experience assessed through System Usability Scale (SUS) and cognitive load measurements.

Result: AR support didn't significantly improve overall learning experience compared to conventional approach. However, AR showed compensatory effect: in control group, higher spatial ability predicted better SUS scores and lower extraneous cognitive load; in AR condition, these relationships disappeared, suggesting AR mitigated disadvantages for learners with lower spatial abilities.

Conclusion: AR can serve compensatory function by reducing influence of learner characteristics like spatial ability. This suggests AR could help create more equitable learning environments. Future research should explore AR's compensatory role to guide design of personalized learning that addresses diverse needs and reduces barriers for varying cognitive profiles.

Abstract: Augmented Reality (AR) offers promising opportunities to enhance learning, but its mechanisms and effects are not yet fully understood. As learning becomes increasingly personalized, considering individual learner characteristics becomes more important. This study investigates the moderating effect of spatial ability on learning experience with AR in the context of robot programming. A between-subjects experiment ($N=71$) compared conventional robot programming to an AR-assisted approach using a head-mounted display. Participants' spatial ability was assessed using the Mental Rotation Test. The learning experience was measured through the System Usability Scale (SUS) and cognitive load. The results indicate that AR support does not significantly improve the learning experience compared to the conventional approach. However, AR appears to have a compensatory effect on the influence of spatial ability. In the control group, spatial ability was significantly positively associated with SUS scores and negatively associated with extraneous cognitive load, indicating that higher spatial ability predicts a better learning experience. In the AR condition, these relationships were not observable, suggesting that AR mitigated the disadvantage typically experienced by learners with lower spatial abilities. These findings suggest that AR can serve a compensatory function by reducing the influence of learner characteristics. Future research should further explore this compensatory role of AR to guide the design of personalized learning environments that address diverse learner needs and reduce barriers for learners with varying cognitive profiles.

</details>


### [148] [AffordanceGrasp-R1:Leveraging Reasoning-Based Affordance Segmentation with Reinforcement Learning for Robotic Grasping](https://arxiv.org/abs/2602.03547)
*Dingyi Zhou,Mu He,Zhuowei Fang,Xiangtong Yao,Yinlong Liu,Alois Knoll,Hu Cao*

Main category: cs.RO

TL;DR: AffordanceGrasp-R1: A reasoning-driven framework combining chain-of-thought cold-start with RL for robotic grasping, using context-aware pipeline with instruction-conditioned affordance masks.


<details>
  <summary>Details</summary>
Motivation: To improve robotic grasping by enhancing deduction and spatial grounding through reasoning-driven affordance segmentation, addressing limitations in current methods for complex language-conditioned manipulation scenarios.

Method: Combines chain-of-thought (CoT) cold-start strategy with reinforcement learning; redesigns grasping pipeline to be context-aware by generating grasp candidates from global scene point cloud and filtering them using instruction-conditioned affordance masks.

Result: Consistently outperforms state-of-the-art methods on benchmark datasets; real-world robotic grasping evaluations validate robustness and generalization under complex language-conditioned manipulation scenarios.

Conclusion: AffordanceGrasp-R1 demonstrates superior performance and generalization capabilities for robotic grasping through its reasoning-driven approach combining CoT cold-start with RL and context-aware pipeline design.

Abstract: We introduce AffordanceGrasp-R1, a reasoning-driven affordance segmentation framework for robotic grasping that combines a chain-of-thought (CoT) cold-start strategy with reinforcement learning to enhance deduction and spatial grounding. In addition, we redesign the grasping pipeline to be more context-aware by generating grasp candidates from the global scene point cloud and subsequently filtering them using instruction-conditioned affordance masks. Extensive experiments demonstrate that AffordanceGrasp-R1 consistently outperforms state-of-the-art (SOTA) methods on benchmark datasets, and real-world robotic grasping evaluations further validate its robustness and generalization under complex language-conditioned manipulation scenarios.

</details>


### [149] [Multi-Player, Multi-Strategy Quantum Game Model for Interaction-Aware Decision-Making in Autonomous Driving](https://arxiv.org/abs/2602.03571)
*Karim Essalmi,Fernando Garrido,Fawzi Nashashibi*

Main category: cs.RO

TL;DR: Proposes Quantum Game Decision-Making (QGDM) model combining classical game theory with quantum mechanics principles for automated driving decision-making, improving success rates and reducing collisions in interactive scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing automated driving decision-making approaches oversimplify interactions between ego vehicle and surrounding agents, often neglecting interactions among agents themselves. Classical game theory assumes rational players, but human behavior is frequently uncertain or irrational.

Method: Quantum Game Decision-Making (QGDM) model that combines classical game theory with quantum mechanics principles (superposition, entanglement, interference) for multi-player, multi-strategy decision-making. Runs in real time on standard computers without requiring quantum hardware.

Result: QGDM significantly improves success rates and reduces collision rates compared to classical approaches, particularly in scenarios with high interaction (roundabouts, merging, highways). One of the first studies to apply quantum game theory to automated driving decision-making.

Conclusion: QGDM provides a novel framework for addressing interaction-aware decision-making in automated driving by incorporating quantum mechanics principles to better model uncertain and irrational human behavior, outperforming classical approaches in interactive scenarios.

Abstract: Although significant progress has been made in decision-making for automated driving, challenges remain for deployment in the real world. One challenge lies in addressing interaction-awareness. Most existing approaches oversimplify interactions between the ego vehicle and surrounding agents, and often neglect interactions among the agents themselves. A common solution is to model these interactions using classical game theory. However, its formulation assumes rational players, whereas human behavior is frequently uncertain or irrational. To address these challenges, we propose the Quantum Game Decision-Making (QGDM) model, a novel framework that combines classical game theory with quantum mechanics principles (such as superposition, entanglement, and interference) to tackle multi-player, multi-strategy decision-making problems. To the best of our knowledge, this is one of the first studies to apply quantum game theory to decision-making for automated driving. QGDM runs in real time on a standard computer, without requiring quantum hardware. We evaluate QGDM in simulation across various scenarios, including roundabouts, merging, and highways, and compare its performance with multiple baseline methods. Results show that QGDM significantly improves success rates and reduces collision rates compared to classical approaches, particularly in scenarios with high interaction.

</details>


### [150] [Human-in-the-Loop Failure Recovery with Adaptive Task Allocation](https://arxiv.org/abs/2602.03603)
*Lorena Maria Genua,Nikita Boguslavskii,Zhi Li*

Main category: cs.RO

TL;DR: ARFA: Adaptive method for allocating robotic failures to human operators based on capability modeling, task urgency, and workload distribution to improve system performance.


<details>
  <summary>Details</summary>
Motivation: Despite increased adoption of autonomous robots for patient care and assistance, robots still struggle in dynamic environments and require human intervention for failure recovery. Current systems lack effective human-robot collaboration mechanisms to assign failures to the most competent operator while reducing workload and minimizing task disruptions.

Method: ARFA models human operator capabilities and continuously updates these beliefs based on actual performance. For each failure, a reward function calculates expected outcomes considering operator capabilities, historical data, task urgency, and current workload distribution. Failures are assigned to the operator with highest expected reward.

Result: Simulations and user studies show ARFA outperforms random allocation, significantly reducing robot idle time, improving overall system performance, and leading to more distributed workload among operators.

Conclusion: ARFA provides an effective adaptive method for failure allocation in human-robot collaboration systems, enabling robots to receive assistance from the most competent operators while optimizing system performance and workload distribution.

Abstract: Since the recent Covid-19 pandemic, mobile manipulators and humanoid assistive robots with higher levels of autonomy have increasingly been adopted for patient care and living assistance. Despite advancements in autonomy, these robots often struggle to perform reliably in dynamic and unstructured environments and require human intervention to recover from failures. Effective human-robot collaboration is essential to enable robots to receive assistance from the most competent operator, in order to reduce their workload and minimize disruptions in task execution. In this paper, we propose an adaptive method for allocating robotic failures to human operators (ARFA). Our proposed approach models the capabilities of human operators, and continuously updates these beliefs based on their actual performance for failure recovery. For every failure to be resolved, a reward function calculates expected outcomes based on operator capabilities and historical data, task urgency, and current workload distribution. The failure is then assigned to the operator with the highest expected reward. Our simulations and user studies show that ARFA outperforms random allocation, significantly reducing robot idle time, improving overall system performance, and leading to a more distributed workload among operators.

</details>


### [151] [Self-supervised Physics-Informed Manipulation of Deformable Linear Objects with Non-negligible Dynamics](https://arxiv.org/abs/2602.03623)
*Youyuan Long,Gokhan Solak,Sara Zeynalpour,Heng Zhang,Arash Ajoudani*

Main category: cs.RO

TL;DR: SPiD is a physics-informed self-supervised learning framework for dynamic manipulation of deformable linear objects, combining an accurate mass-spring model with augmented self-supervised training to achieve robust rope stabilization and trajectory tracking.


<details>
  <summary>Details</summary>
Motivation: To enable dynamic manipulation of deformable linear objects (like ropes) with a data-efficient, robust framework that features strong sim-to-real generalization, addressing the challenges of accurate modeling and learning for complex deformable object dynamics.

Method: Extends a mass-spring model for accurate yet lightweight deformable object dynamics, trains neural controllers using task-oriented costs with differentiable object models, and introduces a self-supervised DAgger variant for distribution shift detection and offline self-correction without expert supervision.

Result: Achieves fast and smooth rope stabilization in simulation and real-world experiments, generalizing across unseen initial states, rope lengths, masses, non-uniform mass distributions, and external disturbances. Maintains performance with noisy, low-frequency state updates using affordable markerless perception.

Conclusion: SPiD provides a data-efficient, robust, physically grounded framework for dynamic manipulation of deformable linear objects with strong sim-to-real generalization, demonstrated through rope stabilization and trajectory tracking tasks.

Abstract: We address dynamic manipulation of deformable linear objects by presenting SPiD, a physics-informed self-supervised learning framework that couples an accurate deformable object model with an augmented self-supervised training strategy. On the modeling side, we extend a mass-spring model to more accurately capture object dynamics while remaining lightweight enough for high-throughput rollouts during self-supervised learning. On the learning side, we train a neural controller using a task-oriented cost, enabling end-to-end optimization through interaction with the differentiable object model. In addition, we propose a self-supervised DAgger variant that detects distribution shift during deployment and performs offline self-correction to further enhance robustness without expert supervision. We evaluate our method primarily on the rope stabilization task, where a robot must bring a swinging rope to rest as quickly and smoothly as possible. Extensive experiments in both simulation and the real world demonstrate that the proposed controller achieves fast and smooth rope stabilization, generalizing across unseen initial states, rope lengths, masses, non-uniform mass distributions, and external disturbances. Additionally, we develop an affordable markerless rope perception method and demonstrate that our controller maintains performance with noisy and low-frequency state updates. Furthermore, we demonstrate the generality of the framework by extending it to the rope trajectory tracking task. Overall, SPiD offers a data-efficient, robust, and physically grounded framework for dynamic manipulation of deformable linear objects, featuring strong sim-to-real generalization.

</details>


### [152] [Variance-Reduced Model Predictive Path Integral via Quadratic Model Approximation](https://arxiv.org/abs/2602.03639)
*Fabian Schramm,Franki Nguimatsia Tiofack,Nicolas Perrin-Gilbert,Marc Toussaint,Justin Carpentier*

Main category: cs.RO

TL;DR: Hybrid variance-reduced MPPI framework integrates prior models into sampling to reduce variance and improve sample efficiency in optimization and control tasks.


<details>
  <summary>Details</summary>
Motivation: Sampling-based controllers like MPPI offer flexibility but suffer from high variance and low sample efficiency, making them impractical when obtaining samples is expensive or limited.

Method: Decompose objective into known approximate model and residual term; use quadratic approximation to derive closed-form, model-guided prior that concentrates samples in informative regions; framework is agnostic to geometric information source (exact derivatives, structural approximations, or gradient-free smoothing).

Result: Achieved faster convergence and superior performance in low-sample regimes on optimization benchmarks, nonlinear cart-pole control, and contact-rich manipulation with non-smooth dynamics compared to standard MPPI.

Conclusion: The hybrid variance-reduced MPPI framework makes sample-based control strategies more practical for scenarios with expensive or limited sampling by effectively reducing variance and improving sample efficiency.

Abstract: Sampling-based controllers, such as Model Predictive Path Integral (MPPI) methods, offer substantial flexibility but often suffer from high variance and low sample efficiency. To address these challenges, we introduce a hybrid variance-reduced MPPI framework that integrates a prior model into the sampling process. Our key insight is to decompose the objective function into a known approximate model and a residual term. Since the residual captures only the discrepancy between the model and the objective, it typically exhibits a smaller magnitude and lower variance than the original objective. Although this principle applies to general modeling choices, we demonstrate that adopting a quadratic approximation enables the derivation of a closed-form, model-guided prior that effectively concentrates samples in informative regions. Crucially, the framework is agnostic to the source of geometric information, allowing the quadratic model to be constructed from exact derivatives, structural approximations (e.g., Gauss- or Quasi-Newton), or gradient-free randomized smoothing. We validate the approach on standard optimization benchmarks, a nonlinear, underactuated cart-pole control task, and a contact-rich manipulation problem with non-smooth dynamics. Across these domains, we achieve faster convergence and superior performance in low-sample regimes compared to standard MPPI. These results suggest that the method can make sample-based control strategies more practical in scenarios where obtaining samples is expensive or limited.

</details>


### [153] [MVP-LAM: Learning Action-Centric Latent Action via Cross-Viewpoint Reconstruction](https://arxiv.org/abs/2602.03668)
*Jung Min Lee,Dohyeok Lee,Seokhun Ju,Taehyun Cho,Jin Woo Koo,Li Zhao,Sangwoo Hong,Jungwoo Lee*

Main category: cs.RO

TL;DR: MVP-LAM learns discrete latent actions from multi-view videos using cross-viewpoint reconstruction, improving action informativeness and downstream robot manipulation performance.


<details>
  <summary>Details</summary>
Motivation: Latent actions from human videos enable scaling robot learning beyond robot-specific datasets, but need to capture underlying agent actions despite lacking ground-truth labels.

Method: MVP-LAM learns discrete latent actions from time-synchronized multi-view videos using cross-viewpoint reconstruction - latent actions from one view must explain future in another view.

Result: On Bridge V2, MVP-LAM produces more action-centric latent actions with higher mutual information with ground-truth actions, better action prediction, and improved out-of-distribution performance.

Conclusion: Pretraining vision-language-action models with MVP-LAM latent actions improves downstream manipulation performance on SIMPLER and LIBERO-Long benchmarks.

Abstract: Learning \emph{latent actions} from diverse human videos enables scaling robot learning beyond embodiment-specific robot datasets, and these latent actions have recently been used as pseudo-action labels for vision-language-action (VLA) model pretraining. To make VLA pretraining effective, latent actions should contain information about the underlying agent's actions despite the absence of ground-truth labels. We propose \textbf{M}ulti-\textbf{V}iew\textbf{P}oint \textbf{L}atent \textbf{A}ction \textbf{M}odel (\textbf{MVP-LAM}), which learns discrete latent actions that are highly informative about ground-truth actions from time-synchronized multi-view videos. MVP-LAM trains latent actions with a \emph{cross-viewpoint reconstruction} objective, so that a latent action inferred from one view must explain the future in another view, reducing reliance on viewpoint-specific cues. On Bridge V2, MVP-LAM produces more action-centric latent actions, achieving higher mutual information with ground-truth actions and improved action prediction, including under out-of-distribution evaluation. Finally, pretraining VLAs with MVP-LAM latent actions improves downstream manipulation performance on the SIMPLER and LIBERO-Long benchmarks.

</details>


### [154] [A Scene Graph Backed Approach to Open Set Semantic Mapping](https://arxiv.org/abs/2602.03781)
*Martin Günther,Felix Igelbrink,Oscar Lima,Lennart Niecksch,Marian Renz,Martin Atzmueller*

Main category: cs.RO

TL;DR: A novel mapping architecture that uses 3D Semantic Scene Graphs as the foundational backend for robotic perception, enabling real-time graph updates and bridging sensor data with high-level reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing approaches decouple perception from representation, treating scene graphs as derivative layers generated post hoc, which limits consistency and scalability for high-level reasoning in large-scale real-world environments.

Method: Proposes a mapping architecture where 3D Semantic Scene Graphs serve as the primary knowledge representation, leveraging incremental scene graph prediction to infer and update graph structure in real-time during environment exploration.

Result: Creates a topologically consistent and computationally efficient map that maintains explicit, spatially grounded representations supporting both flat and hierarchical topologies, enabling direct exploitation by knowledge-driven frameworks.

Conclusion: This approach bridges the gap between raw sensor data and high-level symbolic reasoning, providing a stable, verifiable structure that enhances agent interpretability, trustworthiness, and alignment with human concepts.

Abstract: While Open Set Semantic Mapping and 3D Semantic Scene Graphs (3DSSGs) are established paradigms in robotic perception, deploying them effectively to support high-level reasoning in large-scale, real-world environments remains a significant challenge. Most existing approaches decouple perception from representation, treating the scene graph as a derivative layer generated post hoc. This limits both consistency and scalability. In contrast, we propose a mapping architecture where the 3DSSG serves as the foundational backend, acting as the primary knowledge representation for the entire mapping process.
  Our approach leverages prior work on incremental scene graph prediction to infer and update the graph structure in real-time as the environment is explored. This ensures that the map remains topologically consistent and computationally efficient, even during extended operations in large-scale settings. By maintaining an explicit, spatially grounded representation that supports both flat and hierarchical topologies, we bridge the gap between sub-symbolic raw sensor data and high-level symbolic reasoning. Consequently, this provides a stable, verifiable structure that knowledge-driven frameworks, ranging from knowledge graphs and ontologies to Large Language Models (LLMs), can directly exploit, enabling agents to operate with enhanced interpretability, trustworthiness, and alignment to human concepts.

</details>


### [155] [BridgeV2W: Bridging Video Generation Models to Embodied World Models via Embodiment Masks](https://arxiv.org/abs/2602.03793)
*Yixiang Chen,Peiyan Li,Jiabing Yang,Keji He,Xiangnan Wu,Yuan Xu,Kai Wang,Jing Liu,Nianfeng Liu,Yan Huang,Liang Wang*

Main category: cs.RO

TL;DR: BridgeV2W is a unified world model that bridges coordinate-space actions to pixel-space videos using URDF-based embodiment masks and ControlNet-style conditioning, with flow-based motion loss to focus on dynamic regions.


<details>
  <summary>Details</summary>
Motivation: Existing embodied world models face challenges: misalignment between coordinate-space actions and pixel-space videos, sensitivity to camera viewpoint, and non-unified architectures across different robot embodiments.

Method: Converts coordinate actions to pixel-aligned embodiment masks from URDF and camera parameters, injects them into pretrained video generation model via ControlNet-style pathway, and uses flow-based motion loss to focus on dynamic regions.

Result: Improves video generation quality on single-arm (DROID) and dual-arm (AgiBot-G1) datasets under diverse challenging conditions with unseen viewpoints and scenes, and shows potential for downstream tasks like policy evaluation and goal-conditioned planning.

Conclusion: BridgeV2W provides a unified world model architecture that effectively bridges action spaces to video generation while handling viewpoint variations and different robot embodiments, enabling better downstream robotic applications.

Abstract: Embodied world models have emerged as a promising paradigm in robotics, most of which leverage large-scale Internet videos or pretrained video generation models to enrich visual and motion priors. However, they still face key challenges: a misalignment between coordinate-space actions and pixel-space videos, sensitivity to camera viewpoint, and non-unified architectures across embodiments. To this end, we present BridgeV2W, which converts coordinate-space actions into pixel-aligned embodiment masks rendered from the URDF and camera parameters. These masks are then injected into a pretrained video generation model via a ControlNet-style pathway, which aligns the action control signals with predicted videos, adds view-specific conditioning to accommodate camera viewpoints, and yields a unified world model architecture across embodiments. To mitigate overfitting to static backgrounds, BridgeV2W further introduces a flow-based motion loss that focuses on learning dynamic and task-relevant regions. Experiments on single-arm (DROID) and dual-arm (AgiBot-G1) datasets, covering diverse and challenging conditions with unseen viewpoints and scenes, show that BridgeV2W improves video generation quality compared to prior state-of-the-art methods. We further demonstrate the potential of BridgeV2W on downstream real-world tasks, including policy evaluation and goal-conditioned planning. More results can be found on our project website at https://BridgeV2W.github.io .

</details>


### [156] [Conformal Reachability for Safe Control in Unknown Environments](https://arxiv.org/abs/2602.03799)
*Xinhang Ma,Junlin Wu,Yiannis Kantaros,Yevgeniy Vorobeychik*

Main category: cs.RO

TL;DR: Probabilistic verification framework for unknown dynamical systems combining conformal prediction with reachability analysis to provide safety guarantees while optimizing reward.


<details>
  <summary>Details</summary>
Motivation: Most prior safe control work assumes known/deterministic dynamics or finite state/action spaces, limiting real-world application. Need framework for unknown dynamical systems with probabilistic safety guarantees.

Method: Combine conformal prediction to obtain valid uncertainty intervals for unknown dynamics with reachability analysis to verify safety within those bounds. Develop algorithm to train policies optimizing nominal reward while maximizing planning horizon with sound probabilistic safety guarantees.

Result: Evaluated in 7 safe control settings across 4 domains (cartpole, lane following, drone control, safe navigation) for both affine and nonlinear safety specifications. Learned policies achieve strongest provable safety guarantees while maintaining high average reward.

Conclusion: Proposed framework successfully addresses limitations of prior work by providing probabilistic safety guarantees for unknown dynamical systems while maintaining performance, enabling trustworthy autonomy in real-world applications.

Abstract: Designing provably safe control is a core problem in trustworthy autonomy. However, most prior work in this regard assumes either that the system dynamics are known or deterministic, or that the state and action space are finite, significantly limiting application scope. We address this limitation by developing a probabilistic verification framework for unknown dynamical systems which combines conformal prediction with reachability analysis. In particular, we use conformal prediction to obtain valid uncertainty intervals for the unknown dynamics at each time step, with reachability then verifying whether safety is maintained within the conformal uncertainty bounds. Next, we develop an algorithmic approach for training control policies that optimize nominal reward while also maximizing the planning horizon with sound probabilistic safety guarantees. We evaluate the proposed approach in seven safe control settings spanning four domains -- cartpole, lane following, drone control, and safe navigation -- for both affine and nonlinear safety specifications. Our experiments show that the policies we learn achieve the strongest provable safety guarantees while still maintaining high average reward.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [157] [CreditAudit: 2D Auditing for LLM Evaluation and Selection](https://arxiv.org/abs/2602.02515)
*Yiliang Song,Hongjun An,Jiangong Xiao,Haofei Zhao,Jiawei Shao,Xuelong Li*

Main category: cs.AI

TL;DR: CreditAudit is a deployment-oriented framework that evaluates models not just on average performance but also on stability across different system prompts, providing credit grades (AAA-BBB) to help practitioners choose models for real-world use.


<details>
  <summary>Details</summary>
Motivation: Current leaderboard scores are converging with marginal differences, but fail to match real-world experience because small changes in system prompts and interaction modes can cause disproportionate failures, leaving practitioners uncertain about which model to deploy.

Method: CreditAudit evaluates models under a family of semantically aligned, non-adversarial system prompt templates across multiple benchmarks. It reports mean ability (average performance) and scenario-induced fluctuation sigma (stability risk), then maps volatility into interpretable credit grades (AAA to BBB) using cross-model quantiles with diagnostics to mitigate template difficulty drift.

Result: Experiments on GPQA, TruthfulQA, and MMLU Pro show that models with similar mean ability can exhibit substantially different fluctuation, and stability risk can overturn prioritization decisions in agentic or high failure cost regimes.

Conclusion: CreditAudit provides a 2D and grade-based language for regime-specific model selection, supporting tiered deployment and more disciplined allocation of testing/monitoring effort, enabling more objective and trustworthy model evaluation for real-world use.

Abstract: Leaderboard scores on public benchmarks have been steadily rising and converging, with many frontier language models now separated by only marginal differences. However, these scores often fail to match users' day to day experience, because system prompts, output protocols, and interaction modes evolve under routine iteration, and in agentic multi step pipelines small protocol shifts can trigger disproportionate failures, leaving practitioners uncertain about which model to deploy. We propose CreditAudit, a deployment oriented credit audit framework that evaluates models under a family of semantically aligned and non adversarial system prompt templates across multiple benchmarks, reporting mean ability as average performance across scenarios and scenario induced fluctuation sigma as a stability risk signal, and further mapping volatility into interpretable credit grades from AAA to BBB via cross model quantiles with diagnostics that mitigate template difficulty drift. Controlled experiments on GPQA, TruthfulQA, and MMLU Pro show that models with similar mean ability can exhibit substantially different fluctuation, and stability risk can overturn prioritization decisions in agentic or high failure cost regimes. By providing a 2D and grade based language for regime specific selection, CreditAudit supports tiered deployment and more disciplined allocation of testing and monitoring effort, enabling more objective and trustworthy model evaluation for real world use.

</details>


### [158] [Experience-Driven Multi-Agent Systems Are Training-free Context-aware Earth Observers](https://arxiv.org/abs/2602.02559)
*Pengyu Dai,Weihao Xuan,Junjue Wang,Hongruixuan Chen,Jian Song,Yafei Ou,Naoto Yokoya*

Main category: cs.AI

TL;DR: GeoEvolver is a self-evolving multi-agent system that enables LLM agents to acquire Earth Observation expertise through structured interaction without parameter updates, improving task success by 12% on EO benchmarks.


<details>
  <summary>Details</summary>
Motivation: LLM agents struggle in specialized, tool-intensive domains like Earth Observation that require long-horizon execution, multi-modal coordination, and adherence to implicit tool constraints. Existing agents lack mechanisms to learn fine-grained tool-level expertise from interaction, limiting their effectiveness in complex EO workflows.

Method: GeoEvolver decomposes queries into independent sub-goals via a retrieval-augmented multi-agent orchestrator, explores diverse tool-parameter configurations at sub-goal level, and distills successful patterns and root-cause attribution from failures into an evolving memory bank that provides in-context demonstrations for future queries.

Result: Experiments on three tool-integrated EO benchmarks show GeoEvolver consistently improves end-to-end task success with an average gain of 12% across multiple LLM backbones, demonstrating that EO expertise can emerge progressively from efficient, fine-grained interactions.

Conclusion: GeoEvolver enables LLM agents to acquire Earth Observation expertise through structured interaction without parameter updates, addressing the challenge of specialized tool-intensive domains by learning fine-grained tool-level expertise from interaction and evolving memory.

Abstract: Recent advances have enabled large language model (LLM) agents to solve complex tasks by orchestrating external tools. However, these agents often struggle in specialized, tool-intensive domains that demand long-horizon execution, tight coordination across modalities, and strict adherence to implicit tool constraints. Earth Observation (EO) tasks exemplify this challenge due to the multi-modal and multi-temporal data inputs, as well as the requirements of geo-knowledge constraints (spectrum library, spatial reasoning, etc): many high-level plans can be derailed by subtle execution errors that propagate through a pipeline and invalidate final results. A core difficulty is that existing agents lack a mechanism to learn fine-grained, tool-level expertise from interaction. Without such expertise, they cannot reliably configure tool parameters or recover from mid-execution failures, limiting their effectiveness in complex EO workflows. To address this, we introduce \textbf{GeoEvolver}, a self-evolving multi-agent system~(MAS) that enables LLM agents to acquire EO expertise through structured interaction without any parameter updates. GeoEvolver decomposes each query into independent sub-goals via a retrieval-augmented multi-agent orchestrator, then explores diverse tool-parameter configurations at the sub-goal level. Successful patterns and root-cause attribution from failures are then distilled in an evolving memory bank that provides in-context demonstrations for future queries. Experiments on three tool-integrated EO benchmarks show that GeoEvolver consistently improves end-to-end task success, with an average gain of 12\% across multiple LLM backbones, demonstrating that EO expertise can emerge progressively from efficient, fine-grained interactions with the environment.

</details>


### [159] [Uncertainty and Fairness Awareness in LLM-Based Recommendation Systems](https://arxiv.org/abs/2602.02582)
*Chandan Kumar Sah,Xiaoli Lian,Li Zhang,Tony Xu,Syed Shazaib Shah*

Main category: cs.AI

TL;DR: This paper introduces an uncertainty-aware evaluation framework for LLM-based recommendation systems, quantifying predictive uncertainty and fairness gaps while exposing systematic biases in models like Gemini 1.5 Flash that persist across prompt variations.


<details>
  <summary>Details</summary>
Motivation: While LLMs enable powerful zero-shot recommendations, their predictive uncertainty and embedded biases threaten reliability and fairness, necessitating systematic evaluation methodologies to ensure trustworthy deployment.

Method: The authors introduce a benchmark with curated metrics and a dataset annotated for eight demographic attributes across movies and music domains. They quantify predictive uncertainty via entropy, measure fairness gaps using SNSR and SNSV metrics, test under prompt perturbations, and integrate personality-aware fairness into the RecLLM evaluation pipeline.

Result: Gemini 1.5 Flash exhibits systematic unfairness with SNSR at 0.1363 and SNSV at 0.0507, and these disparities persist under typographical errors and multilingual inputs. The study reveals personality-linked bias patterns and exposes trade-offs between personalization and group fairness.

Conclusion: The paper establishes a foundation for safer, more interpretable RecLLMs through novel uncertainty-aware evaluation methodology, personality profile-informed fairness benchmarks, and motivates future work on multi-model benchmarks and adaptive calibration for trustworthy deployment.

Abstract: Large language models (LLMs) enable powerful zero-shot recommendations by leveraging broad contextual knowledge, yet predictive uncertainty and embedded biases threaten reliability and fairness. This paper studies how uncertainty and fairness evaluations affect the accuracy, consistency, and trustworthiness of LLM-generated recommendations. We introduce a benchmark of curated metrics and a dataset annotated for eight demographic attributes (31 categorical values) across two domains: movies and music. Through in-depth case studies, we quantify predictive uncertainty (via entropy) and demonstrate that Google DeepMind's Gemini 1.5 Flash exhibits systematic unfairness for certain sensitive attributes; measured similarity-based gaps are SNSR at 0.1363 and SNSV at 0.0507. These disparities persist under prompt perturbations such as typographical errors and multilingual inputs. We further integrate personality-aware fairness into the RecLLM evaluation pipeline to reveal personality-linked bias patterns and expose trade-offs between personalization and group fairness. We propose a novel uncertainty-aware evaluation methodology for RecLLMs, present empirical insights from deep uncertainty case studies, and introduce a personality profile-informed fairness benchmark that advances explainability and equity in LLM recommendations. Together, these contributions establish a foundation for safer, more interpretable RecLLMs and motivate future work on multi-model benchmarks and adaptive calibration for trustworthy deployment.

</details>


### [160] [PeerRank: Autonomous LLM Evaluation Through Web-Grounded, Bias-Controlled Peer Review](https://arxiv.org/abs/2602.02589)
*Yanki Margalit,Erni Avram,Ran Taig,Oded Margalit,Nurit Cohen-Inger*

Main category: cs.AI

TL;DR: PeerRank is an autonomous LLM evaluation framework where models generate tasks, answer with web grounding, judge peers, and aggregate assessments without human supervision, producing stable rankings and revealing biases.


<details>
  <summary>Details</summary>
Motivation: Current LLM evaluation methods rely on human-authored benchmarks and judgments that scale poorly, become outdated quickly, and don't match open-world deployments that use web retrieval and synthesis.

Method: PeerRank treats evaluation as a multi-agent process where each model participates symmetrically as task designer, respondent, and evaluator. Models generate evaluation tasks, answer them with category-scoped live web grounding, judge peer responses, and aggregate dense peer assessments into relative performance estimates without human supervision or gold references.

Result: In a study with 12 commercial models and 420 autonomously generated questions, PeerRank produced stable, discriminative rankings and revealed measurable identity and presentation biases. Rankings were robust, and mean peer scores agreed with Elo ratings. Peer scores also correlated with objective accuracy on TruthfulQA and GSM8K benchmarks.

Conclusion: Bias-aware peer evaluation with selective web-grounded answering can scale open-world LLM assessment beyond static and human-curated benchmarks, providing an autonomous framework for ongoing model evaluation.

Abstract: Evaluating large language models typically relies on human-authored benchmarks, reference answers, and human or single-model judgments, approaches that scale poorly, become quickly outdated, and mismatch open-world deployments that depend on web retrieval and synthesis. We introduce PeerRank, a fully autonomous end-to-end evaluation framework in which models generate evaluation tasks, answer them with category-scoped live web grounding, judge peer responses and aggregate dense peer assessments into relative performance estimates, without human supervision or gold references. PeerRank treats evaluation as a multi-agent process where each model participates symmetrically as task designer, respondent, and evaluator, while removing biased judgments. In a large-scale study over 12 commercially available models and 420 autonomously generated questions, PeerRank produces stable, discriminative rankings and reveals measurable identity and presentation biases. Rankings are robust, and mean peer scores agree with Elo. We further validate PeerRank on TruthfulQA and GSM8K, where peer scores correlate with objective accuracy. Together, these results suggest that bias-aware peer evaluation with selective web-grounded answering can scale open-world LLM assessment beyond static and human curated benchmarks.

</details>


### [161] [A Positive Case for Faithfulness: LLM Self-Explanations Help Predict Model Behavior](https://arxiv.org/abs/2602.02639)
*Harry Mayne,Justin Singh Kang,Dewi Gould,Kannan Ramchandran,Adam Mahdi,Noah Y. Siegel*

Main category: cs.AI

TL;DR: LLM self-explanations improve prediction of model behavior by 11-37% (NSG metric), showing they encode useful information despite 5-15% being misleading.


<details>
  <summary>Details</summary>
Motivation: Existing faithfulness metrics for LLM self-explanations have critical limitations - they rely on adversarial prompting or error detection, overlooking the predictive value of explanations. There's poor understanding of how faithful these explanations are to models' true reasoning.

Method: Introduced Normalized Simulatability Gain (NSG), a general and scalable metric based on the idea that faithful explanations should allow observers to learn model's decision-making criteria and better predict behavior on related inputs. Evaluated 18 frontier models (Gemini 3, GPT-5.2, Claude 4.5, etc.) on 7,000 counterfactuals from datasets covering health, business, and ethics.

Result: Self-explanations substantially improve prediction of model behavior (11-37% NSG). Self-explanations provide more predictive information than explanations from external models, even stronger ones, suggesting an advantage from self-knowledge. Across models, 5-15% of self-explanations are egregiously misleading.

Conclusion: Despite imperfections (5-15% misleading explanations), self-explanations encode information that helps predict model behavior, making a positive case for their use in AI oversight. The self-knowledge advantage cannot be replicated by external explanation methods.

Abstract: LLM self-explanations are often presented as a promising tool for AI oversight, yet their faithfulness to the model's true reasoning process is poorly understood. Existing faithfulness metrics have critical limitations, typically relying on identifying unfaithfulness via adversarial prompting or detecting reasoning errors. These methods overlook the predictive value of explanations. We introduce Normalized Simulatability Gain (NSG), a general and scalable metric based on the idea that a faithful explanation should allow an observer to learn a model's decision-making criteria, and thus better predict its behavior on related inputs. We evaluate 18 frontier proprietary and open-weight models, e.g., Gemini 3, GPT-5.2, and Claude 4.5, on 7,000 counterfactuals from popular datasets covering health, business, and ethics. We find self-explanations substantially improve prediction of model behavior (11-37% NSG). Self-explanations also provide more predictive information than explanations generated by external models, even when those models are stronger. This implies an advantage from self-knowledge that external explanation methods cannot replicate. Our approach also reveals that, across models, 5-15% of self-explanations are egregiously misleading. Despite their imperfections, we show a positive case for self-explanations: they encode information that helps predict model behavior.

</details>


### [162] [MARS: Modular Agent with Reflective Search for Automated AI Research](https://arxiv.org/abs/2602.02660)
*Jiefeng Chen,Bhavana Dalvi Mishra,Jaehyun Nam,Rui Meng,Tomas Pfister,Jinsung Yoon*

Main category: cs.AI

TL;DR: MARS is a modular AI research agent framework that uses budget-aware MCTS planning, modular construction, and comparative reflective memory to automate AI research while managing computational costs and performance attribution.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based agents struggle with AI research automation due to computationally expensive evaluations (like model training) and opaque performance attribution, often generating monolithic scripts that ignore execution costs and causal factors.

Method: Three pillars: (1) Budget-Aware Planning via cost-constrained Monte Carlo Tree Search to balance performance with execution expense; (2) Modular Construction using "Design-Decompose-Implement" pipeline to manage complex research repositories; (3) Comparative Reflective Memory that analyzes solution differences to address credit assignment and distill high-signal insights.

Result: Achieves state-of-the-art performance among open-source frameworks on MLE-Bench under comparable settings, maintaining competitiveness with global leaderboard's top methods. Shows qualitative "Aha!" moments with 63% of utilized lessons originating from cross-branch transfer, demonstrating effective generalization of insights across search paths.

Conclusion: MARS provides an effective framework for autonomous AI research that addresses key challenges of computational expense and performance attribution through its three-pillar approach, enabling both quantitative performance gains and qualitative insight generalization.

Abstract: Automating AI research differs from general software engineering due to computationally expensive evaluation (e.g., model training) and opaque performance attribution. Current LLM-based agents struggle here, often generating monolithic scripts that ignore execution costs and causal factors. We introduce MARS (Modular Agent with Reflective Search), a framework optimized for autonomous AI research. MARS relies on three pillars: (1) Budget-Aware Planning via cost-constrained Monte Carlo Tree Search (MCTS) to explicitly balance performance with execution expense; (2) Modular Construction, employing a "Design-Decompose-Implement" pipeline to manage complex research repositories; and (3) Comparative Reflective Memory, which addresses credit assignment by analyzing solution differences to distill high-signal insights. MARS achieves state-of-the-art performance among open-source frameworks on MLE-Bench under comparable settings, maintaining competitiveness with the global leaderboard's top methods. Furthermore, the system exhibits qualitative "Aha!" moments, where 63% of all utilized lessons originate from cross-branch transfer, demonstrating that the agent effectively generalizes insights across search paths.

</details>


### [163] [ATLAS : Adaptive Self-Evolutionary Research Agent with Task-Distributed Multi-LLM Supporters](https://arxiv.org/abs/2602.02709)
*Ujin Jeon,Jiyong Kwon,Madison Ann Sullivan,Caleb Eunho Lee,Guang Lin*

Main category: cs.AI

TL;DR: ATLAS is an adaptive multi-agent framework that evolves lightweight research agents while delegating specialized tasks to supporter agents, using EvoDPO algorithm to dynamically update reference policies for better performance on long-horizon tasks.


<details>
  <summary>Details</summary>
Motivation: Current multi-LLM agent systems either keep solvers frozen after fine-tuning or use static preference-optimization loops, which become intractable for long-horizon tasks and lack adaptability to changing environments.

Method: Proposes ATLAS framework with task-distributed architecture: lightweight research agent evolves iteratively while delegating exploration, hyperparameter tuning, and reference policy management to specialized supporter agents. Core algorithm is Evolving Direct Preference Optimization (EvoDPO) that adaptively updates phase-indexed reference policy.

Result: Theoretical regret analysis for preference-based contextual bandit under concept drift. Experimental results on non-stationary linear contextual bandits and SciML loss reweighting for 1D Burgers' equation show ATLAS improves stability and performance over static single-agent baselines.

Conclusion: ATLAS provides an effective adaptive framework for multi-agent systems that outperforms static approaches, particularly beneficial for long-horizon tasks requiring continuous adaptation to changing environments.

Abstract: Recent multi-LLM agent systems perform well in prompt optimization and automated problem-solving, but many either keep the solver frozen after fine-tuning or rely on a static preference-optimization loop, which becomes intractable for long-horizon tasks. We propose ATLAS (Adaptive Task-distributed Learning for Agentic Self-evolution), a task-distributed framework that iteratively develops a lightweight research agent while delegating complementary roles to specialized supporter agents for exploration, hyperparameter tuning, and reference policy management. Our core algorithm, Evolving Direct Preference Optimization (EvoDPO), adaptively updates the phase-indexed reference policy. We provide a theoretical regret analysis for a preference-based contextual bandit under concept drift. In addition, experiments were conducted on non-stationary linear contextual bandits and scientific machine learning (SciML) loss reweighting for the 1D Burgers' equation. Both results show that ATLAS improves stability and performance over a static single-agent baseline.

</details>


### [164] [Dynamic Mix Precision Routing for Efficient Multi-step LLM Interaction](https://arxiv.org/abs/2602.02711)
*Yuanzhe Li,Jianing Deng,Jingtong Hu,Tianlong Chen,Song Wang,Huanrui Yang*

Main category: cs.AI

TL;DR: Dynamic mix-precision routing framework adaptively selects between high/low-precision LLMs for long-horizon decision-making to balance accuracy and cost.


<details>
  <summary>Details</summary>
Motivation: Large LLMs achieve strong performance in long-horizon decision-making but incur prohibitive inference costs. While practitioners believe larger models are needed for higher success rates, multi-step interaction with large LLMs is expensive.

Method: Proposes dynamic mix-precision routing framework that adaptively selects between high-precision and low-precision LLMs at each decision step based on diverse sensitivities among interaction steps. Uses two-stage training: 1) KL-divergence-based supervised learning to identify precision-sensitive steps, 2) Group-Relative Policy Optimization (GRPO) to improve task success rates.

Result: Experiments on ALFWorld demonstrate great improvement on accuracy-cost trade-off over single-precision baselines and heuristic routing methods.

Conclusion: Dynamic mix-precision routing effectively balances LLM inference cost and task performance in long-horizon decision-making, addressing the prohibitive cost of using large LLMs throughout multi-step interactions.

Abstract: Large language models (LLM) achieve strong performance in long-horizon decision-making tasks through multi-step interaction and reasoning at test time. While practitioners commonly believe a higher task success rate necessitates the use of a larger and stronger LLM model, multi-step interaction with a large LLM incurs prohibitive inference cost. To address this problem, we explore the use of low-precision quantized LLM in the long-horizon decision-making process. Based on the observation of diverse sensitivities among interaction steps, we propose a dynamic mix-precision routing framework that adaptively selects between high-precision and low-precision LLMs at each decision step. The router is trained via a two-stage pipeline, consisting of KL-divergence-based supervised learning that identifies precision-sensitive steps, followed by Group-Relative Policy Optimization (GRPO) to further improve task success rates. Experiments on ALFWorld demonstrate that our approach achieves a great improvement on accuracy-cost trade-off over single-precision baselines and heuristic routing methods.

</details>


### [165] [Scaling-Aware Adapter for Structure-Grounded LLM Reasoning](https://arxiv.org/abs/2602.02780)
*Zihao Jing,Qiuhao Zeng,Ruiyi Fang,Yan Yi Li,Yan Sun,Boyu Wang,Pingzhao Hu*

Main category: cs.AI

TL;DR: Cuttlefish is a unified all-atom LLM that grounds language reasoning in geometric cues and adaptively scales structural tokens with complexity to improve biomolecular structure reasoning while reducing hallucinations.


<details>
  <summary>Details</summary>
Motivation: Existing methods for biomolecular structure reasoning are modality-specific and either omit geometric groundings (leading to structural hallucinations) or use inflexible modality fusion bottlenecks that over-compress and suboptimally allocate structural tokens, hindering generalized all-atom reasoning.

Method: Two key components: 1) Scaling-Aware Patching uses instruction-conditioned gating to generate variable-size patches over structural graphs, adaptively scaling query token budget with structural complexity. 2) Geometry Grounding Adapter refines adaptive tokens via cross-attention to modality embeddings and injects resulting modality tokens into the LLM to expose explicit geometric cues.

Result: Experiments across diverse all-atom benchmarks demonstrate that Cuttlefish achieves superior performance in heterogeneous structure-grounded reasoning.

Conclusion: Cuttlefish provides a unified approach that grounds language reasoning in geometric cues while adaptively scaling structural tokens, enabling better all-atom reasoning with reduced structural hallucinations.

Abstract: Large language models (LLMs) are enabling reasoning over biomolecular structures, yet existing methods remain modality-specific and typically compress structural inputs through sequence-based tokenization or fixed-length query connectors. Such architectures either omit the geometric groundings requisite for mitigating structural hallucinations or impose inflexible modality fusion bottlenecks that concurrently over-compress and suboptimally allocate structural tokens, thereby impeding the realization of generalized all-atom reasoning. We introduce Cuttlefish, a unified all-atom LLM that grounds language reasoning in geometric cues while scaling modality tokens with structural complexity. First, Scaling-Aware Patching leverages an instruction-conditioned gating mechanism to generate variable-size patches over structural graphs, adaptively scaling the query token budget with structural complexity to mitigate fixed-length connector bottlenecks. Second, Geometry Grounding Adapter refines these adaptive tokens via cross-attention to modality embeddings and injects the resulting modality tokens into the LLM, exposing explicit geometric cues to reduce structural hallucination. Experiments across diverse all-atom benchmarks demonstrate that Cuttlefish achieves superior performance in heterogeneous structure-grounded reasoning. Code is available at the project repository.

</details>


### [166] [Chain of Simulation: A Dual-Mode Reasoning Framework for Large Language Models with Dynamic Problem Routing](https://arxiv.org/abs/2602.02842)
*Saeid Sheikhi*

Main category: cs.AI

TL;DR: Chain of Simulation (CoS) is a dual-mode reasoning framework that dynamically routes problems to specialized reasoning strategies in LLMs, achieving significant accuracy improvements across mathematical, spatial, and multi-hop reasoning tasks with reduced computational cost.


<details>
  <summary>Details</summary>
Motivation: Existing uniform prompting approaches for LLMs fail to leverage problem-specific reasoning strategies, leading to suboptimal performance across diverse reasoning tasks that require different cognitive approaches.

Method: CoS employs three specialized reasoning modes: computational flow with self-consistency for math problems, symbolic state tracking with JSON for spatial reasoning, and hybrid fact-extraction for multi-hop inference, with dynamic routing between modes based on problem characteristics.

Result: CoS achieves 71.5% accuracy on GSM8K (1.0% absolute improvement), 90.0% on StrategyQA (2.5% improvement), and 19.0% on bAbI (65.2% relative improvement) compared to strongest baselines, with computational mode achieving 81.2% accuracy when correctly applied to math problems.

Conclusion: Problem-specific mode selection is crucial for LLM reasoning, and CoS establishes an effective approach for improving reasoning without additional training, providing superior accuracy-efficiency trade-offs compared to Self-Consistency at 54% lower computational cost.

Abstract: We present Chain of Simulation (CoS), a novel dual-mode reasoning framework that dynamically routes problems to specialized reasoning strategies in Large Language Models (LLMs). Unlike existing uniform prompting approaches, CoS employs three distinct reasoning modes: (1) computational flow with self-consistency for mathematical problems, (2) symbolic state tracking with JSON representations for spatial reasoning, and (3) hybrid fact-extraction for multi-hop inference. Through comprehensive evaluation on GSM8K, StrategyQA, and bAbI benchmarks using four state-of-the-art models (Gemma-3 27B, LLaMA-3.1 8B, Mistral 7B, and Qwen-2.5 14B), we demonstrate that CoS achieves 71.5% accuracy on GSM8K (1.0% absolute improvement), 90.0% on StrategyQA (2.5% improvement), and 19.0% on bAbI (65.2% relative improvement) compared to the strongest baselines. The analysis reveals that problem-specific mode selection is crucial, with computational mode achieving 81.2% accuracy when correctly applied to mathematical problems, while misrouting leads to 0% accuracy. We provide detailed algorithms for mode selection, state tracking, and answer extraction, establishing CoS as an effective approach for improving LLM reasoning without additional training. The framework provides superior trade-offs between accuracy and efficiency compared to Self-Consistency, achieving comparable performance at 54% lower computational cost.

</details>


### [167] [AutoSizer: Automatic Sizing of Analog and Mixed-Signal Circuits via Large Language Model (LLM) Agents](https://arxiv.org/abs/2602.02849)
*Xi Yu,Dmitrii Torbunov,Soumyajit Mandal,Yihui Ren*

Main category: cs.AI

TL;DR: AutoSizer is an LLM-driven meta-optimization framework for AMS circuit sizing that outperforms traditional methods through adaptive search-space refinement and two-loop optimization.


<details>
  <summary>Details</summary>
Motivation: AMS circuit design relies heavily on expert knowledge with transistor sizing being a major bottleneck due to nonlinear behavior, high-dimensional spaces, and strict constraints. Existing EDA methods treat sizing as static black-box optimization, leading to inefficient solutions, while LLMs have reasoning capabilities but lack numerical optimization precision for AMS sizing.

Method: AutoSizer uses a reflective LLM-driven meta-optimization framework with two-loop optimization: inner loop for circuit sizing and outer loop that analyzes optimization dynamics and constraints to iteratively refine search space from simulation feedback. It unifies circuit understanding, adaptive search-space construction, and optimization orchestration in a closed loop.

Result: AutoSizer achieves higher solution quality, faster convergence, and higher success rate across varying circuit difficulties, outperforming both traditional optimization methods and existing LLM-based agents. The paper also introduces AMS-SizingBench, an open benchmark with 24 diverse AMS circuits in SKY130 CMOS technology.

Conclusion: The proposed AutoSizer framework successfully addresses the gap in AMS circuit sizing by combining LLM reasoning with numerical optimization, demonstrating superior performance through adaptive search-space refinement and closed-loop optimization orchestration.

Abstract: The design of Analog and Mixed-Signal (AMS) integrated circuits remains heavily reliant on expert knowledge, with transistor sizing a major bottleneck due to nonlinear behavior, high-dimensional design spaces, and strict performance constraints. Existing Electronic Design Automation (EDA) methods typically frame sizing as static black-box optimization, resulting in inefficient and less robust solutions. Although Large Language Models (LLMs) exhibit strong reasoning abilities, they are not suited for precise numerical optimization in AMS sizing. To address this gap, we propose AutoSizer, a reflective LLM-driven meta-optimization framework that unifies circuit understanding, adaptive search-space construction, and optimization orchestration in a closed loop. It employs a two-loop optimization framework, with an inner loop for circuit sizing and an outer loop that analyzes optimization dynamics and constraints to iteratively refine the search space from simulation feedback. We further introduce AMS-SizingBench, an open benchmark comprising 24 diverse AMS circuits in SKY130 CMOS technology, designed to evaluate adaptive optimization policies under realistic simulator-based constraints. AutoSizer experimentally achieves higher solution quality, faster convergence, and higher success rate across varying circuit difficulties, outperforming both traditional optimization methods and existing LLM-based agents.

</details>


### [168] [STEER: Inference-Time Risk Control via Constrained Quality-Diversity Search](https://arxiv.org/abs/2602.02862)
*Eric Yang,Jong Ha Lee,Jonathan Amar,Elissa Ye,Yugang Jia*

Main category: cs.AI

TL;DR: STEER is a training-free framework that enables tunable control over LLM decision conservativeness in ordinal settings like clinical triage, using evolutionary search to create diverse personas and exposing a single risk percentile parameter for steering behavior.


<details>
  <summary>Details</summary>
Motivation: LLMs trained for average correctness suffer from mode collapse in ordinal decision settings, removing the ability to trade off specificity and sensitivity based on contextual constraints, which is critical in applications like clinical triage.

Method: STEER constructs a population of natural-language personas through offline constrained quality-diversity search that promotes behavioral coverage while enforcing safety, reasoning, and stability thresholds. At inference, a single interpretable control parameter maps user-specified risk percentile to selected persona for monotonic adjustment of decision conservativeness.

Result: On clinical triage benchmarks, STEER achieves broader behavioral coverage than temperature-based sampling and static persona ensembles. Compared to post-training methods, it maintains substantially higher accuracy on unambiguous urgent cases while providing comparable control over ambiguous decisions.

Conclusion: STEER demonstrates a safety-preserving paradigm for risk control that can steer LLM behavior without compromising domain competence, addressing the mode collapse problem in ordinal decision settings.

Abstract: Large Language Models (LLMs) trained for average correctness often exhibit mode collapse, producing narrow decision behaviors on tasks where multiple responses may be reasonable. This limitation is particularly problematic in ordinal decision settings such as clinical triage, where standard alignment removes the ability to trade off specificity and sensitivity (the ROC operating point) based on contextual constraints. We propose STEER (Steerable Tuning via Evolutionary Ensemble Refinement), a training-free framework that reintroduces this tunable control. STEER constructs a population of natural-language personas through an offline, constrained quality-diversity search that promotes behavioral coverage while enforcing minimum safety, reasoning, and stability thresholds. At inference time, STEER exposes a single, interpretable control parameter that maps a user-specified risk percentile to a selected persona, yielding a monotonic adjustment of decision conservativeness. On two clinical triage benchmarks, STEER achieves broader behavioral coverage compared to temperature-based sampling and static persona ensembles. Compared to a representative post-training method, STEER maintains substantially higher accuracy on unambiguous urgent cases while providing comparable control over ambiguous decisions. These results demonstrate STEER as a safety-preserving paradigm for risk control, capable of steering behavior without compromising domain competence.

</details>


### [169] ["I May Not Have Articulated Myself Clearly": Diagnosing Dynamic Instability in LLM Reasoning at Inference Time](https://arxiv.org/abs/2602.02863)
*Jinkun Chen,Fengxiang Cheng,Sijia Han,Vlado Keselj*

Main category: cs.AI

TL;DR: LLMs often fail mid-reasoning by "losing the thread," detectable via inference-time token probabilities without training. A simple instability signal combining distributional shift and entropy predicts failures with above-chance accuracy across models and tasks.


<details>
  <summary>Details</summary>
Motivation: Many LLM reasoning failures occur as process-level breakdowns where models "lose the thread" mid-reasoning, not just at final output. Current evaluation only measures end-point failures, missing these intermediate breakdowns that could be detected from standard API observables.

Method: Define instability signal combining consecutive-step distributional shift (Jensen-Shannon Divergence) and uncertainty (entropy) from token log probabilities. Summarize each reasoning trace by its peak instability strength. Analyze timing of instability relative to remaining decoding horizon.

Result: Instability strength reliably predicts wrong answers with above-chance AUC on GSM8K and HotpotQA. Shows monotonic accuracy decline across model sizes. Early instability can be corrective (followed by stabilization and correct answer), while late instability is more often destructive (leading to failure), indicating recoverability depends on timing.

Conclusion: Mid-reasoning breakdowns in LLMs are detectable from standard inference observables without training. Instability timing matters: early instability can be recovered from, while late instability tends to be destructive. Provides a model-agnostic, training-free diagnostic tool for reasoning failures.

Abstract: Reasoning failures in large language models (LLMs) are typically measured only at the end of a generation, yet many failures manifest as a process-level breakdown: the model "loses the thread" mid-reasoning. We study whether such breakdowns are detectable from inference-time observables available in standard APIs (token log probabilities), without any training or fine-tuning. We define a simple instability signal that combines consecutive-step distributional shift (JSD) and uncertainty (entropy), summarize each trace by its peak instability strength, and show that this signal reliably predicts failure. Across GSM8K and HotpotQA, instability strength predicts wrong answers with above-chance AUC and yields monotonic bucket-level accuracy decline at scale across model sizes. Crucially, we show that instability is not uniformly harmful: early instability can reflect subsequent stabilization and a correct final answer (\emph{corrective instability}), whereas late instability is more often followed by failure (\emph{destructive instability}), even at comparable peak magnitudes, indicating that recoverability depends not only on how strongly the distribution changes but also on when such changes occur relative to the remaining decoding horizon. The method is model-agnostic, training-free, and reproducible, and is presented as a diagnostic lens rather than a corrective or control mechanism.

</details>


### [170] [Aligning Language Model Benchmarks with Pairwise Preferences](https://arxiv.org/abs/2602.02898)
*Marco Gutierrez,Xinyi Leng,Hannah Cyberey,Jonathan Richard Schwarz,Ahmed Alaa,Thomas Hartvigsen*

Main category: cs.AI

TL;DR: BenchAlign: A method to automatically update offline benchmarks using limited real-world performance data to create new benchmarks that better predict model preferences in practical settings.


<details>
  <summary>Details</summary>
Motivation: Current language model benchmarks often fail to predict real-world utility despite being computationally efficient proxies. There's a gap between benchmark performance and practical human preferences that needs to be bridged.

Method: Proposes benchmark alignment and BenchAlign solution, which learns preference-aligned weightings for benchmark questions using question-level performance data and ranked model pairs collected during deployment. Creates new static benchmarks that rank unseen models according to human preferences.

Result: Aligned benchmarks accurately rank unseen models according to human preference models across different model sizes while remaining interpretable.

Conclusion: Benchmark alignment can bridge the gap between offline benchmarks and practical utility, accelerating model development toward real-world usefulness by creating benchmarks that better reflect human preferences.

Abstract: Language model benchmarks are pervasive and computationally-efficient proxies for real-world performance. However, many recent works find that benchmarks often fail to predict real utility. Towards bridging this gap, we introduce benchmark alignment, where we use limited amounts of information about model performance to automatically update offline benchmarks, aiming to produce new static benchmarks that predict model pairwise preferences in given test settings. We then propose BenchAlign, the first solution to this problem, which learns preference-aligned weight- ings for benchmark questions using the question-level performance of language models alongside ranked pairs of models that could be collected during deployment, producing new benchmarks that rank previously unseen models according to these preferences. Our experiments show that our aligned benchmarks can accurately rank unseen models according to models of human preferences, even across different sizes, while remaining interpretable. Overall, our work provides insights into the limits of aligning benchmarks with practical human preferences, which stands to accelerate model development towards real utility.

</details>


### [171] [Minimal Computational Preconditions for Subjective Perspective in Artificial Agents](https://arxiv.org/abs/2602.02902)
*Hongju Pae*

Main category: cs.AI

TL;DR: Paper proposes implementing subjective perspective in AI agents using a slowly evolving global latent state that modulates fast policy dynamics, showing direction-dependent hysteresis as a measurable signature of subjectivity.


<details>
  <summary>Details</summary>
Motivation: To operationalize and measure subjective perspective in artificial agents by grounding it in phenomenologically motivated internal structures rather than just behavioral optimization.

Method: Implement perspective as a slowly evolving global latent state that modulates fast policy dynamics without direct optimization for behavioral consequences, tested in reward-free environments with regime shifts.

Result: The latent structure exhibits direction-dependent hysteresis while policy-level behavior remains reactive, suggesting hysteresis as a measurable signature of perspective-like subjectivity in machines.

Conclusion: Direction-dependent hysteresis in slowly evolving latent states can serve as an operational measure of subjective perspective in artificial agents, providing a way to quantify machine subjectivity.

Abstract: This study operationalizes subjective perspective in artificial agents by grounding it in a minimal, phenomenologically motivated internal structure. The perspective is implemented as a slowly evolving global latent state that modulates fast policy dynamics without being directly optimized for behavioral consequences. In a reward-free environment with regime shifts, this latent structure exhibits direction-dependent hysteresis, while policy-level behavior remains comparatively reactive. I argue that such hysteresis constitutes a measurable signature of perspective-like subjectivity in machine systems.

</details>


### [172] [FIRE-Bench: Evaluating Agents on the Rediscovery of Scientific Insights](https://arxiv.org/abs/2602.02905)
*Zhen Wang,Fan Bai,Zhongyan Luo,Jinyan Su,Kaiser Sun,Xinle Yu,Jieyuan Liu,Kun Zhou,Claire Cardie,Mark Dredze,Eric P. Xing,Zhiting Hu*

Main category: cs.AI

TL;DR: FIRE-Bench is a new benchmark for evaluating LLM-powered autonomous agents' ability to rediscover established scientific findings through full-cycle research, revealing current agents struggle with scientific discovery despite advanced LLM backbones.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for evaluating LLM-powered autonomous agents in scientific discovery either rely too heavily on LLM-as-judge evaluations or use isolated performance metrics that are poor proxies for actual scientific insight. There's a need for rigorous evaluation of agents' capacity for verifiable discovery through the complete research cycle.

Method: FIRE-Bench evaluates agents through rediscovery of established findings from recent, high-impact ML research. Agents are given only high-level research questions from published studies and must autonomously explore ideas, design experiments, implement code, execute plans, and derive evidence-based conclusions - simulating the full scientific research cycle.

Result: Evaluation of state-of-the-art agents with frontier LLMs (like GPT-5) shows full-cycle scientific research remains challenging: strongest agents achieve limited rediscovery success (<50 F1), exhibit high variance across runs, and display recurring failure modes in experimental design, execution, and evidence-based reasoning.

Conclusion: FIRE-Bench provides a rigorous, diagnostic framework for measuring progress toward reliable agent-driven scientific discovery, highlighting that current agent systems still struggle with the complete scientific research process despite advanced LLM capabilities.

Abstract: Autonomous agents powered by large language models (LLMs) promise to accelerate scientific discovery end-to-end, but rigorously evaluating their capacity for verifiable discovery remains a central challenge. Existing benchmarks face a trade-off: they either heavily rely on LLM-as-judge evaluations of automatically generated research outputs or optimize convenient yet isolated performance metrics that provide coarse proxies for scientific insight. To address this gap, we introduce FIRE-Bench (Full-cycle Insight Rediscovery Evaluation), a benchmark that evaluates agents through the rediscovery of established findings from recent, high-impact machine learning research. Agents are given only a high-level research question extracted from a published, verified study and must autonomously explore ideas, design experiments, implement code, execute their plans, and derive conclusions supported by empirical evidence. We evaluate a range of state-of-the-art agents with frontier LLMs backbones like gpt-5 on FIRE-Bench. Our results show that full-cycle scientific research remains challenging for current agent systems: even the strongest agents achieve limited rediscovery success (<50 F1), exhibit high variance across runs, and display recurring failure modes in experimental design, execution, and evidence-based reasoning. FIRE-Bench provides a rigorous and diagnostic framework for measuring progress toward reliable agent-driven scientific discovery.

</details>


### [173] [Reasoning about Reasoning: BAPO Bounds on Chain-of-Thought Token Complexity in LLMs](https://arxiv.org/abs/2602.02909)
*Kiran Tomlinson,Tobias Schnabel,Adith Swaminathan,Jennifer Neville*

Main category: cs.AI

TL;DR: The paper establishes fundamental lower bounds on chain-of-thought reasoning tokens required for solving computational tasks, showing Ω(n) tokens needed for binary majority, triplet matching, and graph reachability problems.


<details>
  <summary>Details</summary>
Motivation: Chain-of-thought reasoning improves LLM performance but incurs high latency and compute costs. The paper aims to understand the fundamental scaling requirements of reasoning tokens as input size grows, providing theoretical foundations for analyzing optimal reasoning length.

Method: Extends the bounded attention prefix oracle (BAPO) model to quantify information flow in LLMs. Proves lower bounds on CoT tokens for three canonical BAPO-hard tasks using theoretical analysis, then provides matching/near-matching upper bounds via explicit constructions. Validates with experiments on frontier reasoning models.

Result: Proves Ω(n) reasoning tokens required for binary majority, triplet matching, and graph reachability when input size is n. Provides matching or near-matching upper bounds. Experiments show approximately linear reasoning token scaling and failures when constrained to smaller reasoning budgets.

Conclusion: Identifies fundamental bottlenecks in inference-time compute through chain-of-thought reasoning and provides a principled theoretical tool for analyzing optimal reasoning length requirements in LLMs.

Abstract: Inference-time scaling via chain-of-thought (CoT) reasoning is a major driver of state-of-the-art LLM performance, but it comes with substantial latency and compute costs. We address a fundamental theoretical question: how many reasoning tokens are required to solve a problem as input size grows? By extending the bounded attention prefix oracle (BAPO) model--an abstraction of LLMs that quantifies the information flow required to solve a task--we prove lower bounds on the CoT tokens required for three canonical BAPO-hard tasks: binary majority, triplet matching, and graph reachability. We show that each requires $Ω(n)$ reasoning tokens when the input size is $n$. We complement these results with matching or near-matching upper bounds via explicit constructions. Finally, our experiments with frontier reasoning models show approximately linear reasoning token scaling on these tasks and failures when constrained to smaller reasoning budgets, consistent with our theoretical lower bounds. Together, our results identify fundamental bottlenecks in inference-time compute through CoT and offer a principled tool for analyzing optimal reasoning length.

</details>


### [174] [DeltaEvolve: Accelerating Scientific Discovery through Momentum-Driven Evolution](https://arxiv.org/abs/2602.02919)
*Jiachen Jiang,Tianyu Ding,Zhihui Zhu*

Main category: cs.AI

TL;DR: DeltaEvolve: A momentum-driven evolutionary framework that uses structured semantic deltas instead of full-code histories to guide LLM-driven program evolution more efficiently.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-driven evolutionary systems like AlphaEvolve use full-code histories which are context-inefficient and provide weak evolutionary guidance. Full-code snapshots contain redundant implementation details that dilute core algorithmic ideas, making it difficult to provide clear inspirations for evolution.

Method: Formalizes evolutionary agents as an Expectation-Maximization framework: LLM samples candidate programs (E-step) and system updates control context based on evaluation feedback (M-step). Proposes DeltaEvolve which replaces full-code history with structured semantic deltas capturing how and why modifications between successive nodes affect performance. Uses multi-level database and progressive disclosure mechanism to reduce input tokens.

Result: Empirical evaluations across diverse scientific domains show that DeltaEvolve can discover better solutions with less token consumption compared to full-code-based evolutionary agents.

Conclusion: Structured semantic deltas provide more informative and transferable evolutionary guidance than full-code histories, enabling more efficient and effective LLM-driven program evolution for automated science discovery.

Abstract: LLM-driven evolutionary systems have shown promise for automated science discovery, yet existing approaches such as AlphaEvolve rely on full-code histories that are context-inefficient and potentially provide weak evolutionary guidance. In this work, we first formalize the evolutionary agents as a general Expectation-Maximization framework, where the language model samples candidate programs (E-step) and the system updates the control context based on evaluation feedback (M-step). Under this view, constructing context via full-code snapshots constitutes a suboptimal M-step, as redundant implement details dilutes core algorithmic ideas, making it difficult to provide clear inspirations for evolution. To address this, we propose DeltaEvolve, a momentum-driven evolutionary framework that replaces full-code history with structured semantic delta capturing how and why modifications between successive nodes affect performance. As programs are often decomposable, semantic delta usually contains many effective components which are transferable and more informative to drive improvement. By organizing semantic delta through multi-level database and progressive disclosure mechanism, input tokens are further reduced. Empirical evaluations on tasks across diverse scientific domains show that our framework can discover better solution with less token consumption over full-code-based evolutionary agents.

</details>


### [175] [UAT-LITE: Inference-Time Uncertainty-Aware Attention for Pretrained Transformers](https://arxiv.org/abs/2602.02952)
*Elias Hossain,Shubhashis Roy Dipta,Subash Neupane,Rajib Rana,Ravid Shwartz-Ziv,Ivan Garibay,Niloofar Yousefi*

Main category: cs.AI

TL;DR: UAT-LITE is an inference-time framework that makes transformer self-attention uncertainty-aware using Monte Carlo dropout, improving calibration without modifying pretrained weights or training objectives.


<details>
  <summary>Details</summary>
Motivation: Neural NLP models are often miscalibrated (assigning high confidence to incorrect predictions), which undermines selective prediction and high-stakes deployment. Existing calibration methods either only adjust output probabilities without changing internal computation, or require substantial training/storage costs.

Method: UAT-LITE uses approximate Bayesian inference via Monte Carlo dropout in pretrained transformer classifiers during inference. Token-level epistemic uncertainty is estimated from stochastic forward passes and used to modulate self-attention during contextualization. Also introduces layerwise variance decomposition to diagnose uncertainty accumulation across transformer depth.

Result: Across SQuAD 2.0 answerability, MNLI, and SST-2, UAT-LITE reduces Expected Calibration Error by ~20% on average relative to fine-tuned BERT-base baseline while preserving task accuracy. Improves selective prediction and robustness under distribution shift.

Conclusion: UAT-LITE provides an effective inference-time framework for making transformer self-attention uncertainty-aware, improving calibration and uncertainty estimation without modifying pretrained weights or requiring expensive training procedures.

Abstract: Neural NLP models are often miscalibrated, assigning high confidence to incorrect predictions, which undermines selective prediction and high-stakes deployment. Post-hoc calibration methods adjust output probabilities but leave internal computation unchanged, while ensemble and Bayesian approaches improve uncertainty at substantial training or storage cost. We propose UAT-LITE, an inference-time framework that makes self-attention uncertainty-aware using approximate Bayesian inference via Monte Carlo dropout in pretrained transformer classifiers. Token-level epistemic uncertainty is estimated from stochastic forward passes and used to modulate self-attention during contextualization, without modifying pretrained weights or training objectives. We additionally introduce a layerwise variance decomposition to diagnose how predictive uncertainty accumulates across transformer depth. Across the SQuAD 2.0 answerability, MNLI, and SST-2, UAT-LITE reduces Expected Calibration Error by approximately 20% on average relative to a fine-tuned BERT-base baseline while preserving task accuracy, and improves selective prediction and robustness under distribution shift.

</details>


### [176] [Generative Engine Optimization: A VLM and Agent Framework for Pinterest Acquisition Growth](https://arxiv.org/abs/2602.02961)
*Faye Zhang,Qianyu Cheng,Jasmine Wan,Vishwakarma Singh,Jinfeng Rao,Kofi Boakye*

Main category: cs.AI

TL;DR: Pinterest develops a Generative Engine Optimization (GEO) framework using reverse search design with fine-tuned VLMs to predict user search queries, creating semantically coherent collection pages optimized for generative search, resulting in 20% organic traffic growth.


<details>
  <summary>Details</summary>
Motivation: Generative AI search systems (ChatGPT, Gemini, Claude) are shifting content discovery from traditional keyword matching to intent inference and contextual answer generation. This poses a challenge for visual platforms like Pinterest where individual images lack semantic depth and authority signals that generative search prioritizes, risking disintermediation as users get answers without visiting sites.

Method: 1. Fine-tune Vision-Language Models (VLMs) to predict what users would actually search for (reverse search design) rather than generating generic image captions. 2. Augment with AI agents that mine real-time internet trends to capture emerging search demand. 3. Use VLM-generated queries to construct semantically coherent Collection Pages via multimodal embeddings. 4. Employ hybrid VLM and two-tower ANN architectures to build authority-aware interlinking structures that propagate signals across billions of visual assets.

Result: Deployed at scale across billions of images and tens of millions of collections, GEO delivers 20% organic traffic growth contributing to multi-million monthly active user (MAU) growth.

Conclusion: Pinterest GEO demonstrates a principled pathway for visual platforms to thrive in the generative search era by pioneering reverse search design and creating indexable aggregations optimized for generative retrieval.

Abstract: Large Language Models are fundamentally reshaping content discovery through AI-native search systems such as ChatGPT, Gemini, and Claude. Unlike traditional search engines that match keywords to documents, these systems infer user intent, synthesize multimodal evidence, and generate contextual answers directly on the search page, introducing a paradigm shift from Search Engine Optimization (SEO) to Generative Engine Optimization (GEO). For visual content platforms hosting billions of assets, this poses an acute challenge: individual images lack the semantic depth and authority signals that generative search prioritizes, risking disintermediation as user needs are satisfied in-place without site visits.
  We present Pinterest GEO, a production-scale framework that pioneers reverse search design: rather than generating generic image captions describing what content is, we fine-tune Vision-Language Models (VLMs) to predict what users would actually search for, augmented this with AI agents that mine real-time internet trends to capture emerging search demand. These VLM-generated queries then drive construction of semantically coherent Collection Pages via multimodal embeddings, creating indexable aggregations optimized for generative retrieval. Finally, we employ hybrid VLM and two-tower ANN architectures to build authority-aware interlinking structures that propagate signals across billions of visual assets. Deployed at scale across billions of images and tens of millions of collections, GEO delivers 20\% organic traffic growth contributing to multi-million monthly active user (MAU) growth, demonstrating a principled pathway for visual platforms to thrive in the generative search era.

</details>


### [177] [Structuring Value Representations via Geometric Coherence in Markov Decision Processes](https://arxiv.org/abs/2602.02978)
*Zuyuan Zhang,Zeyu Fang,Tian Lan*

Main category: cs.AI

TL;DR: GCR-RL uses order theory to structure value function learning as poset refinements, improving RL stability and efficiency.


<details>
  <summary>Details</summary>
Motivation: Geometric properties can stabilize and speed up RL, but existing approaches focus on symmetry, data augmentation, or structural restrictions. This paper introduces a novel perspective using order theory to ensure geometric coherence in value function learning.

Method: Proposes GCR-RL (Geometric Coherence Regularized RL) that frames value function estimation as learning desired posets. It computes sequences of super-poset refinements by refining previous posets and learning additional order relationships from temporal difference signals. Two algorithms are developed: one using Q-learning and another using actor-critic methods.

Result: Theoretical analysis shows convergence properties and rates. Empirical evaluation across tasks demonstrates significant improvements in sample efficiency and stable performance compared to strong baselines.

Conclusion: Order theory provides a powerful framework for ensuring geometric coherence in RL, leading to more stable and sample-efficient learning through structured poset refinement.

Abstract: Geometric properties can be leveraged to stabilize and speed reinforcement learning. Existing examples include encoding symmetry structure, geometry-aware data augmentation, and enforcing structural restrictions. In this paper, we take a novel view of RL through the lens of order theory and recast value function estimates into learning a desired poset (partially ordered set). We propose \emph{GCR-RL} (Geometric Coherence Regularized Reinforcement Learning) that computes a sequence of super-poset refinements -- by refining posets in previous steps and learning additional order relationships from temporal difference signals -- thus ensuring geometric coherence across the sequence of posets underpinning the learned value functions. Two novel algorithms by Q-learning and by actor--critic are developed to efficiently realize these super-poset refinements. Their theoretical properties and convergence rates are analyzed. We empirically evaluate GCR-RL in a range of tasks and demonstrate significant improvements in sample efficiency and stable performance over strong baselines.

</details>


### [178] [Are LLMs Biased Like Humans? Causal Reasoning as a Function of Prior Knowledge, Irrelevant Information, and Reasoning Budget](https://arxiv.org/abs/2602.02983)
*Hanna M. Dettki,Charley M. Wu,Bob Rehder*

Main category: cs.AI

TL;DR: LLMs show more rule-like causal reasoning than humans, don't mirror human collider biases, and CoT improves robustness, suggesting they can complement humans but may break down with uncertainty.


<details>
  <summary>Details</summary>
Motivation: To understand whether LLMs' causal judgments reflect normative computation, human shortcuts, or pattern matching, and to benchmark them against human reasoning on causal tasks.

Method: Benchmarked 20+ LLMs against human baseline on 11 causal judgment tasks using collider structure (C1→E←C2), tested robustness under semantic abstraction and prompt overloading, and evaluated CoT effects.

Result: LLMs exhibit more rule-like reasoning than humans, don't show human collider biases (weak explaining away, Markov violations), and CoT improves robustness to semantic abstraction and prompt overloading.

Conclusion: LLMs can complement humans when known biases are undesirable, but their rule-like reasoning may break down with uncertainty, highlighting need to characterize LLM reasoning for safe deployment.

Abstract: Large language models (LLMs) are increasingly used in domains where causal reasoning matters, yet it remains unclear whether their judgments reflect normative causal computation, human-like shortcuts, or brittle pattern matching. We benchmark 20+ LLMs against a matched human baseline on 11 causal judgment tasks formalized by a collider structure ($C_1 \!\rightarrow\! E\! \leftarrow \!C_2$). We find that a small interpretable model compresses LLMs' causal judgments well and that most LLMs exhibit more rule-like reasoning strategies than humans who seem to account for unmentioned latent factors in their probability judgments. Furthermore, most LLMs do not mirror the characteristic human collider biases of weak explaining away and Markov violations. We probe LLMs' causal judgment robustness under (i) semantic abstraction and (ii) prompt overloading (injecting irrelevant text), and find that chain-of-thought (CoT) increases robustness for many LLMs. Together, this divergence suggests LLMs can complement humans when known biases are undesirable, but their rule-like reasoning may break down when uncertainty is intrinsic -- highlighting the need to characterize LLM reasoning strategies for safe, effective deployment.

</details>


### [179] [Large Language Models Can Take False First Steps at Inference-time Planning](https://arxiv.org/abs/2602.02991)
*Haijiang Yan,Jian-Qiao Zhu,Adam Sanborn*

Main category: cs.AI

TL;DR: LLMs show planning capabilities during training but appear short-sighted at inference; this gap is explained by Bayesian modeling of how self-generated context drives a "planning-shift" during generation.


<details>
  <summary>Details</summary>
Motivation: To explain why LLMs exhibit planning abilities during training but appear to have compromised planning behavior during inference, despite having acquired sequence-level planning capabilities.

Method: Proposes a Bayesian account that grounds planning behavior in the evolving generative context, suggesting that accumulated self-generated context drives a planning-shift during inference. Validates through two controlled experiments: random-generation task showing constrained planning under human prompts and increasing planning strength with accumulated context, and Gaussian-sampling task showing reduced initial bias when conditioning on self-generated sequences.

Result: The experiments demonstrate: 1) constrained planning under human prompts and increasing planning strength as self-generated context accumulates, and 2) reduced initial bias when conditioning on self-generated sequences in Gaussian-sampling tasks.

Conclusion: The findings provide a theoretical explanation with empirical evidence for characterizing how LLMs plan ahead during inference, explaining the apparent gap between training-acquired planning capabilities and inference-time planning behavior through the lens of self-generated context accumulation.

Abstract: Large language models (LLMs) have been shown to acquire sequence-level planning abilities during training, yet their planning behavior exhibited at inference time often appears short-sighted and inconsistent with these capabilities. We propose a Bayesian account for this gap by grounding planning behavior in the evolving generative context: given the subtle differences between natural language and the language internalized by LLMs, accumulated self-generated context drives a planning-shift during inference and thereby creates the appearance of compromised planning behavior. We further validate the proposed model through two controlled experiments: a random-generation task demonstrating constrained planning under human prompts and increasing planning strength as self-generated context accumulates, and a Gaussian-sampling task showing reduced initial bias when conditioning on self-generated sequences. These findings provide a theoretical explanation along with empirical evidence for characterizing how LLMs plan ahead during inference.

</details>


### [180] [Agent Alpha: Tree Search Unifying Generation, Exploration and Evaluation for Computer-Use Agents](https://arxiv.org/abs/2602.02995)
*Sizhe Tang,Rongqian Chen,Tian Lan*

Main category: cs.AI

TL;DR: Agent Alpha uses step-level MCTS with alpha-UCT search to enable GUI agents to reuse partial successes and recover from errors, achieving 77% success rate on OSWorld benchmark.


<details>
  <summary>Details</summary>
Motivation: Current GUI agents using trajectory-level sampling lack regressive ability - they can't reuse partial successes or recover from early missteps, limiting their effectiveness despite increased test-time compute.

Method: Unified framework combining generation, exploration, and evaluation through step-level Monte Carlo Tree Search (MCTS) with alpha-UCT guided search. Includes comparison-driven evaluation to mitigate scoring biases and diversity-constrained expansion to maintain compact search space.

Result: Achieves state-of-the-art 77% success rate on OSWorld benchmark, significantly outperforming trajectory-level baselines under equivalent computational resources.

Conclusion: Agent Alpha's step-level MCTS approach with alpha-UCT search enables effective planning, early pruning of suboptimal branches, and efficient prefix reuse, addressing key limitations of trajectory-level sampling methods for GUI agents.

Abstract: While scaling test-time compute through trajectory-level sampling has significantly improved Graphical User Interface (GUI) agents, the lack of regressive ability prevents the reuse of partial successes and the recovery from early missteps. In this paper, we introduce Agent Alpha, a unified framework that synergizes generation, exploration, and evaluation through step-level Monte Carlo Tree Search (MCTS). It enables active modeling or exploiting structures of the planning space. By integrating alpha-UCT guided search into the interaction loop, Agent Alpha enables deliberate planning, facilitating early pruning of suboptimal branches and efficient prefix reuse. We also employ comparison-driven evaluation to mitigate absolute scoring biases and diversity-constrained expansion to maintain a compact, informative search space. Regret bound of alpha-UCT is analyzed. On the OSWorld benchmark, Agent Alpha achieves a state-of-the-art success rate of $\sim 77\%$, significantly outperforming trajectory-level baselines under equivalent compute.

</details>


### [181] [Methods and Open Problems in Differentiable Social Choice: Learning Mechanisms, Decisions, and Alignment](https://arxiv.org/abs/2602.03003)
*Zhiyu An,Wan Du*

Main category: cs.AI

TL;DR: Differentiable social choice is an emerging paradigm that formulates voting rules and aggregation procedures as learnable, differentiable models optimized from data, bridging machine learning with social choice theory.


<details>
  <summary>Details</summary>
Motivation: Social choice has become foundational to modern ML systems (auctions, federated learning, LLM alignment, etc.), but current implementations often happen implicitly without normative scrutiny. There's a need to explicitly integrate social choice theory into ML pipelines.

Method: Survey approach synthesizing work across auctions, voting, budgeting, liquid democracy, decentralized aggregation, and inverse mechanism learning. Formulates social choice mechanisms as differentiable, learnable models that can be optimized from data.

Result: Shows how classical social choice axioms and impossibility results reappear as objectives, constraints, and optimization trade-offs in differentiable frameworks. Identifies 36 open problems defining a new research agenda.

Conclusion: Differentiable social choice represents a paradigm shift that bridges machine learning, economics, and democratic theory, creating opportunities for more principled and transparent collective decision-making in ML systems.

Abstract: Social choice is no longer a peripheral concern of political theory or economics-it has become a foundational component of modern machine learning systems. From auctions and resource allocation to federated learning, participatory governance, and the alignment of large language models, machine learning pipelines increasingly aggregate heterogeneous preferences, incentives, and judgments into collective decisions. In effect, many contemporary machine learning systems already implement social choice mechanisms, often implicitly and without explicit normative scrutiny.
  This Review surveys differentiable social choice: an emerging paradigm that formulates voting rules, mechanisms, and aggregation procedures as learnable, differentiable models optimized from data. We synthesize work across auctions, voting, budgeting, liquid democracy, decentralized aggregation, and inverse mechanism learning, showing how classical axioms and impossibility results reappear as objectives, constraints, and optimization trade-offs. We conclude by identifying 36 open problems defining a new research agenda at the intersection of machine learning, economics, and democratic theory.

</details>


### [182] [Distilling LLM Reasoning into Graph of Concept Predictors](https://arxiv.org/abs/2602.03006)
*Ziyang Yu,Liang Zhao*

Main category: cs.AI

TL;DR: GCP is a reasoning-aware active distillation framework that externalizes LLM reasoning as a graph and trains modular concept predictors, improving sample efficiency and interpretability.


<details>
  <summary>Details</summary>
Motivation: Current active distillation methods for LLMs only distill final labels, discarding valuable intermediate reasoning signals and offering limited diagnostics for understanding where reasoning errors occur.

Method: Proposes Graph of Concept Predictors (GCP) that externalizes teacher's decision process as a directed acyclic graph and mirrors it with modular concept predictors in the student, using graph-aware acquisition strategy and targeted sub-module retraining.

Result: Experiments on eight NLP classification benchmarks show GCP enhances performance under limited annotation budgets while yielding more interpretable and controllable training dynamics.

Conclusion: GCP provides an effective reasoning-aware distillation framework that improves sample efficiency, interpretability, and training stability compared to traditional label-only distillation approaches.

Abstract: Deploying Large Language Models (LLMs) for discriminative workloads is often limited by inference latency, compute, and API costs at scale. Active distillation reduces these costs by querying an LLM oracle to train compact discriminative students, but most pipelines distill only final labels, discarding intermediate reasoning signals and offering limited diagnostics of what reasoning is missing and where errors arise. We propose Graph of Concept Predictors (GCP), a reasoning-aware active distillation framework that externalizes the teacher's decision process as a directed acyclic graph and mirrors it with modular concept predictors in the student. GCP enhances sample efficiency through a graph-aware acquisition strategy that targets uncertainty and disagreement at critical reasoning nodes. Additionally, it improves training stability and efficiency by performing targeted sub-module retraining, which attributes downstream loss to specific concept predictors and updates only the most influential modules. Experiments on eight NLP classification benchmarks demonstrate that GCP enhances performance under limited annotation budgets while yielding more interpretable and controllable training dynamics. Code is available at: https://github.com/Ziyang-Yu/GCP.

</details>


### [183] [STAR: Similarity-guided Teacher-Assisted Refinement for Super-Tiny Function Calling Models](https://arxiv.org/abs/2602.03022)
*Jiliang Ni,Jiachen Pu,Zhongyi Yang,Jingfeng Luo,Conggang Hu*

Main category: cs.AI

TL;DR: STAR is a holistic framework that transfers LLM capabilities to super-tiny models for function calling, using constrained knowledge distillation and similarity-guided RL to overcome training challenges and achieve state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Large LLMs are crucial for AI agents but too big for widespread adoption, while existing transfer methods suffer from overfitting, training instability, ineffective binary rewards for multi-solution tasks, and difficulty synergizing techniques.

Method: STAR framework with two innovations: (1) Constrained Knowledge Distillation (CKD) - augments top-k forward KL divergence to suppress confidently incorrect predictions, ensuring training stability while preserving exploration capacity; (2) Similarity-guided RL (Sim-RL) - introduces fine-grained, similarity-based rewards for better policy optimization by evaluating similarity between generated outputs and ground truth.

Result: STAR models achieve SOTA in their size classes, significantly outperforming baselines. The 0.6B STAR model achieves best performance among all open models under 1B, surpassing several well-known larger open models.

Conclusion: STAR demonstrates an effective training framework for distilling LLM capabilities into super-tiny models, paving the way for powerful, accessible, and efficient AI agents.

Abstract: The proliferation of Large Language Models (LLMs) in function calling is pivotal for creating advanced AI agents, yet their large scale hinders widespread adoption, necessitating transferring their capabilities into smaller ones. However, existing paradigms are often plagued by overfitting, training instability, ineffective binary rewards for multi-solution tasks, and the difficulty of synergizing techniques. We introduce STAR: Similarity-guided Teacher-Assisted Refinement, a novel holistic framework that effectively transfers LLMs' capabilities to super-tiny models. STAR consists of two core technical innovations: (1) Constrained Knowledge Distillation (CKD), a training objective that augments top-k forward KL divergence to suppress confidently incorrect predictions, ensuring training stability while preserving exploration capacity for downstream RL. STAR holistically synergizes these strategies within a cohesive training curriculum, enabling super-tiny models to achieve exceptional performance on complex function calling tasks; (2) Similarity-guided RL (Sim-RL), a RL mechanism that introduces a fine-grained, similarity-based reward. This provides a robust, continuous, and rich signal for better policy optimization by evaluating the similarity between generated outputs and the ground truth. Extensive experiments on challenging and renowned benchmarks demonstrate the effectiveness of our method. Our STAR models establish SOTA in their size classes, significantly outperforming baselines. Remarkably, our 0.6B STAR model achieves the best performance among all open models under 1B, surpassing even several well-known open models at a larger scale. STAR demonstrates a training framework that distills capabilities of LLMs into super-tiny models, paving the way for powerful, accessible, and efficient AI agents.

</details>


### [184] [RC-GRPO: Reward-Conditioned Group Relative Policy Optimization for Multi-Turn Tool Calling Agents](https://arxiv.org/abs/2602.03025)
*Haitian Zhong,Jixiu Zhai,Lei Song,Jiang Bian,Qiang Liu,Tieniu Tan*

Main category: cs.AI

TL;DR: RC-GRPO improves multi-turn tool calling in LLMs by using reward-conditioned tokens to increase within-group diversity during RL, overcoming GRPO's limitations when reward variation is low.


<details>
  <summary>Details</summary>
Motivation: Standard SFT+GRPO approach stalls when within-group reward variation is low (e.g., all rollouts get 0 or 1 rewards), making group-normalized advantages uninformative and causing vanishing updates in multi-turn tool calling tasks.

Method: Two-stage approach: 1) Fine-tune Reward-Conditioned Trajectory Policy (RCTP) on mixed-quality trajectories with reward goal tokens (<|high_reward|>, <|low_reward|>) injected into prompts, 2) During RL, sample diverse reward tokens within each GRPO group and condition rollouts on sampled tokens to improve within-group diversity and advantage gains.

Result: On Berkeley Function Calling Leaderboard v4 multi-turn benchmark, RC-GRPO consistently outperforms baselines, with Qwen-2.5-7B-Instruct surpassing all closed-source API models.

Conclusion: Treating exploration as controllable steering via discrete reward tokens effectively addresses GRPO's limitations in low-reward-variance scenarios, enabling more effective multi-turn tool calling in LLMs.

Abstract: Multi-turn tool calling is challenging for Large Language Models (LLMs) because rewards are sparse and exploration is expensive. A common recipe, SFT followed by GRPO, can stall when within-group reward variation is low (e.g., more rollouts in a group receive the all 0 or all 1 reward), making the group-normalized advantage uninformative and yielding vanishing updates. To address this problem, we propose RC-GRPO (Reward-Conditioned Group Relative Policy Optimization), which treats exploration as a controllable steering problem via discrete reward tokens. We first fine-tune a Reward-Conditioned Trajectory Policy (RCTP) on mixed-quality trajectories with reward goal special tokens (e.g., <|high_reward|>, <|low_reward|>) injected into the prompts, enabling the model to learn how to generate distinct quality trajectories on demand. Then during RL, we sample diverse reward tokens within each GRPO group and condition rollouts on the sampled token to improve within-group diversity, improving advantage gains. On the Berkeley Function Calling Leaderboard v4 (BFCLv4) multi-turn benchmark, our method yields consistently improved performance than baselines, and the performance on Qwen-2.5-7B-Instruct even surpasses all closed-source API models.

</details>


### [185] [Visual Reasoning over Time Series via Multi-Agent System](https://arxiv.org/abs/2602.03026)
*Weilin Ruan,Yuxuan Liang*

Main category: cs.AI

TL;DR: MAS4TS is a multi-agent system for time series analysis that combines visual reasoning with tool usage to achieve state-of-the-art performance across diverse tasks.


<details>
  <summary>Details</summary>
Motivation: Existing time series methods lack intuitive visual reasoning capabilities and struggle to generalize across tasks with adaptive tool usage, limiting their real-world applicability.

Method: Proposes MAS4TS with Analyzer-Reasoner-Executor paradigm: uses Vision-Language Model for visual reasoning over time series plots, reconstructs predictive trajectories in latent space, coordinates three specialized agents via shared memory and gated communication, and employs a router for task-specific tool chain selection.

Result: Achieves state-of-the-art performance across multiple benchmarks, demonstrates strong generalization capabilities, and enables efficient inference.

Conclusion: MAS4TS provides a unified framework that successfully integrates visual reasoning, agent coordination, and adaptive tool usage for general time series analysis, addressing limitations of existing approaches.

Abstract: Time series analysis underpins many real-world applications, yet existing time-series-specific methods and pretrained large-model-based approaches remain limited in integrating intuitive visual reasoning and generalizing across tasks with adaptive tool usage. To address these limitations, we propose MAS4TS, a tool-driven multi-agent system for general time series tasks, built upon an Analyzer-Reasoner-Executor paradigm that integrates agent communication, visual reasoning, and latent reconstruction within a unified framework. MAS4TS first performs visual reasoning over time series plots with structured priors using a Vision-Language Model to extract temporal structures, and subsequently reconstructs predictive trajectories in latent space. Three specialized agents coordinate via shared memory and gated communication, while a router selects task-specific tool chains for execution. Extensive experiments on multiple benchmarks demonstrate that MAS4TS achieves state-of-the-art performance across a wide range of time series tasks, while exhibiting strong generalization and efficient inference.

</details>


### [186] [KANFIS A Neuro-Symbolic Framework for Interpretable and Uncertainty-Aware Learning](https://arxiv.org/abs/2602.03034)
*Binbin Yong,Haoran Pei,Jun Shen,Haoran Li,Qingguo Zhou,Zhao Su*

Main category: cs.AI

TL;DR: KANFIS replaces ANFIS's exponential rule explosion with linear-scaling additive decomposition for compact, interpretable neuro-fuzzy systems.


<details>
  <summary>Details</summary>
Motivation: Conventional ANFIS suffers from structural complexity and exponential rule explosion in high-dimensional spaces, limiting interpretability and scalability.

Method: Proposes KANFIS using additive function decomposition with linear-scaling parameters, sparse masking for compact rules, and compatibility with Type-1 and Interval Type-2 fuzzy logic.

Result: KANFIS achieves competitive performance against neural and neuro-fuzzy baselines while maintaining interpretability and linear complexity scaling.

Conclusion: KANFIS provides a compact, interpretable neuro-fuzzy architecture that overcomes ANFIS's exponential complexity problem through additive decomposition.

Abstract: Adaptive Neuro-Fuzzy Inference System (ANFIS) was designed to combine the learning capabilities of neural network with the reasoning transparency of fuzzy logic. However, conventional ANFIS architectures suffer from structural complexity, where the product-based inference mechanism causes an exponential explosion of rules in high-dimensional spaces. We herein propose the Kolmogorov-Arnold Neuro-Fuzzy Inference System (KANFIS), a compact neuro-symbolic architecture that unifies fuzzy reasoning with additive function decomposition. KANFIS employs an additive aggregation mechanism, under which both model parameters and rule complexity scale linearly with input dimensionality rather than exponentially. Furthermore, KANFIS is compatible with both Type-1 (T1) and Interval Type-2 (IT2) fuzzy logic systems, enabling explicit modeling of uncertainty and ambiguity in fuzzy representations. By using sparse masking mechanisms, KANFIS generates compact and structured rule sets, resulting in an intrinsically interpretable model with clear rule semantics and transparent inference processes. Empirical results demonstrate that KANFIS achieves competitive performance against representative neural and neuro-fuzzy baselines.

</details>


### [187] [MAS-ProVe: Understanding the Process Verification of Multi-Agent Systems](https://arxiv.org/abs/2602.03053)
*Vishal Venkataramani,Haizhou Shi,Zixuan Ke,Austin Xu,Xiaoxiao He,Yingbo Zhou,Semih Yavuz,Hao Wang,Shafiq Joty*

Main category: cs.AI

TL;DR: Process verification for multi-agent systems doesn't consistently improve performance and shows high variance, with LLM-as-a-Judge performing best but still facing reliability challenges.


<details>
  <summary>Details</summary>
Motivation: While process verification shows promise for evaluating intermediate reasoning steps in single-agent settings, its effectiveness in multi-agent systems remains unclear due to the complexity of partial multi-agent trajectories.

Method: Systematic empirical study (MAS-ProVe) evaluating three verification paradigms (LLM-as-a-Judge, reward models, process reward models) across two granularity levels (agent-level, iteration-level), five verifiers, four context management strategies, six MAS frameworks, and multiple reasoning benchmarks.

Result: Process verification doesn't consistently improve MAS performance and exhibits high variance. LLM-as-a-Judge generally outperforms reward-based approaches, with trained judges beating general-purpose LLMs. Small gap exists between LLMs as judges vs. single agents, with context-length-performance trade-off observed.

Conclusion: Effective and robust process verification for multi-agent systems remains an open challenge requiring advances beyond current paradigms, despite LLM-as-a-Judge showing relative promise.

Abstract: Multi-Agent Systems (MAS) built on Large Language Models (LLMs) often exhibit high variance in their reasoning trajectories. Process verification, which evaluates intermediate steps in trajectories, has shown promise in general reasoning settings, and has been suggested as a potential tool for guiding coordination of MAS; however, its actual effectiveness in MAS remains unclear. To fill this gap, we present MAS-ProVe, a systematic empirical study of process verification for multi-agent systems (MAS). Our study spans three verification paradigms (LLM-as-a-Judge, reward models, and process reward models), evaluated across two levels of verification granularity (agent-level and iteration-level). We further examine five representative verifiers and four context management strategies, and conduct experiments over six diverse MAS frameworks on multiple reasoning benchmarks. We find that process-level verification does not consistently improve performance and frequently exhibits high variance, highlighting the difficulty of reliably evaluating partial multi-agent trajectories. Among the methods studied, LLM-as-a-Judge generally outperforms reward-based approaches, with trained judges surpassing general-purpose LLMs. We further observe a small performance gap between LLMs acting as judges and as single agents, and identify a context-length-performance trade-off in verification. Overall, our results suggest that effective and robust process verification for MAS remains an open challenge, requiring further advances beyond current paradigms. Code is available at https://github.com/Wang-ML-Lab/MAS-ProVe.

</details>


### [188] [De-conflating Preference and Qualification: Constrained Dual-Perspective Reasoning for Job Recommendation with Large Language Models](https://arxiv.org/abs/2602.03097)
*Bryce Kan,Wei Yang,Emily Nguyen,Ganghui Yi,Bowen Yi,Chenxiao Yu,Yan Liu*

Main category: cs.AI

TL;DR: JobRec is a generative job recommendation framework that decouples candidate preference from employer qualification using dual-perspective reasoning and constrained optimization for controllable professional matching.


<details>
  <summary>Details</summary>
Motivation: Existing job recommendation systems conflate subjective candidate preferences with objective employer qualifications into a single interaction signal, leading to confounded supervision under recruitment-funnel censoring and limited policy controllability.

Method: Proposes JobRec with: 1) Unified Semantic Alignment Schema to structure candidate/job attributes, 2) Two-Stage Cooperative Training Strategy to learn separate preference and qualification experts, and 3) Lagrangian-based Policy Alignment for optimizing recommendations under explicit eligibility constraints.

Result: JobRec consistently outperforms strong baselines and provides improved controllability for strategy-aware professional matching, with experiments validated using a synthetic dataset refined by experts.

Conclusion: JobRec successfully addresses the challenges of de-conflating preference and qualification in professional job recommendation through structured semantic alignment, decoupled expert learning, and constrained optimization, enabling more controllable and effective matching.

Abstract: Professional job recommendation involves a complex bipartite matching process that must reconcile a candidate's subjective preference with an employer's objective qualification. While Large Language Models (LLMs) are well-suited for modeling the rich semantics of resumes and job descriptions, existing paradigms often collapse these two decision dimensions into a single interaction signal, yielding confounded supervision under recruitment-funnel censoring and limiting policy controllability. To address these challenges, We propose JobRec, a generative job recommendation framework for de-conflating preference and qualification via constrained dual-perspective reasoning. JobRec introduces a Unified Semantic Alignment Schema that aligns candidate and job attributes into structured semantic layers, and a Two-Stage Cooperative Training Strategy that learns decoupled experts to separately infer preference and qualification. Building on these experts, a Lagrangian-based Policy Alignment module optimizes recommendations under explicit eligibility requirements, enabling controllable trade-offs. To mitigate data scarcity, we construct a synthetic dataset refined by experts. Experiments show that JobRec consistently outperforms strong baselines and provides improved controllability for strategy-aware professional matching.

</details>


### [189] [Risky-Bench: Probing Agentic Safety Risks under Real-World Deployment](https://arxiv.org/abs/2602.03100)
*Jingnan Zheng,Yanzhen Luo,Jingjun Xu,Bingnan Liu,Yuxin Chen,Chenhang Cui,Gelei Deng,Chaochao Lu,Xiang Wang,An Zhang,Tat-Seng Chua*

Main category: cs.AI

TL;DR: Risky-Bench is a framework for systematic agent safety evaluation that uses domain-agnostic safety principles to create context-aware rubrics and assesses safety risks through realistic task execution under varying threat assumptions.


<details>
  <summary>Details</summary>
Motivation: Existing agent safety evaluations are limited: they focus on risk-oriented tasks specific to particular agent settings, have limited coverage of safety risk space, fail to assess safety during long-horizon interactive tasks, and lack adaptability across diverse agent configurations.

Method: Risky-Bench organizes evaluation around domain-agnostic safety principles to derive context-aware safety rubrics that delineate safety space, and systematically evaluates safety risks across this space through realistic task execution under varying threat assumptions.

Result: When applied to life-assist agent settings, Risky-Bench uncovers substantial safety risks in state-of-the-art agents under realistic execution conditions.

Conclusion: Risky-Bench provides an extensible methodology for agent safety assessment that is not confined to life-assist scenarios and can be adapted to other deployment settings to construct environment-specific safety evaluations.

Abstract: Large Language Models (LLMs) are increasingly deployed as agents that operate in real-world environments, introducing safety risks beyond linguistic harm. Existing agent safety evaluations rely on risk-oriented tasks tailored to specific agent settings, resulting in limited coverage of safety risk space and failing to assess agent safety behavior during long-horizon, interactive task execution in complex real-world deployments. Moreover, their specialization to particular agent settings limits adaptability across diverse agent configurations. To address these limitations, we propose Risky-Bench, a framework that enables systematic agent safety evaluation grounded in real-world deployment. Risky-Bench organizes evaluation around domain-agnostic safety principles to derive context-aware safety rubrics that delineate safety space, and systematically evaluates safety risks across this space through realistic task execution under varying threat assumptions. When applied to life-assist agent settings, Risky-Bench uncovers substantial safety risks in state-of-the-art agents under realistic execution conditions. Moreover, as a well-structured evaluation pipeline, Risky-Bench is not confined to life-assist scenarios and can be adapted to other deployment settings to construct environment-specific safety evaluations, providing an extensible methodology for agent safety assessment.

</details>


### [190] [Understanding Multi-Agent LLM Frameworks: A Unified Benchmark and Experimental Analysis](https://arxiv.org/abs/2602.03128)
*Abdelghny Orogat,Ana Rostam,Essam Mansour*

Main category: cs.AI

TL;DR: MAFBench is a unified evaluation suite that reveals how multi-agent LLM framework architectures dramatically impact performance (100x latency differences, 30% accuracy drops), providing design principles and selection guidance.


<details>
  <summary>Details</summary>
Motivation: Multi-agent LLM frameworks have significant but poorly understood architectural impacts on system performance, with architectural choices alone causing order-of-magnitude differences in latency/throughput and substantial accuracy/scalability variations. Existing benchmarks focus on individual capabilities and lack standardized framework-level evaluation.

Method: Introduce an architectural taxonomy for comparing multi-agent LLM frameworks along fundamental dimensions, and develop MAFBench - a unified evaluation suite that integrates existing benchmarks under a standardized execution pipeline for controlled empirical study across widely used frameworks.

Result: Framework-level design choices alone can increase latency by over 100x, reduce planning accuracy by up to 30%, and lower coordination success from above 90% to below 30%. These dramatic performance variations highlight the critical importance of architectural decisions.

Conclusion: The paper translates findings into concrete architectural design principles and framework selection guidance, and outlines promising future research directions for multi-agent LLM systems, emphasizing the need for systematic framework-level evaluation.

Abstract: Multi-agent LLM frameworks are widely used to accelerate the development of agent systems powered by large language models (LLMs). These frameworks impose distinct architectural structures that govern how agents interact, store information, and coordinate tasks. However, their impact on system performance remains poorly understood. This gap is critical, as architectural choices alone can induce order-of-magnitude differences in latency and throughput, as well as substantial variation in accuracy and scalability. Addressing this challenge requires (i) jointly evaluating multiple capabilities, such as orchestration overhead, memory behavior, planning, specialization, and coordination, and (ii) conducting these evaluations under controlled, framework-level conditions to isolate architectural effects. Existing benchmarks focus on individual capabilities and lack standardized framework-level evaluation. We address these limitations by (i) introducing an architectural taxonomy for systematically comparing multi-agent LLM frameworks along fundamental dimensions, and (ii) developing MAFBench, a unified evaluation suite that integrates existing benchmarks under a standardized execution pipeline. Using MAFBench, we conduct a controlled empirical study across several widely used frameworks. Our results show that framework-level design choices alone can increase latency by over 100x, reduce planning accuracy by up to 30%, and lower coordination success from above 90% to below 30%. Finally, we translate our findings into concrete architectural design principles and framework selection guidance, and outline promising future research directions.

</details>


### [191] [General Agents Contain World Models, even under Partial Observability and Stochasticity](https://arxiv.org/abs/2602.03146)
*Santiago Cifuentes*

Main category: cs.AI

TL;DR: Extends previous theorem to stochastic agents in partially observable environments, showing they cannot avoid learning their environment through randomization.


<details>
  <summary>Details</summary>
Motivation: Previous work showed optimal deterministic agents in fully observable environments contain sufficient knowledge to reconstruct their environment, but this relied on restrictive assumptions of determinism and full observability.

Method: Extends the theoretical framework to stochastic agents operating in partially observable environments, weakening the notion of generality to show less powerful agents already contain world models.

Result: Proves that stochastic agents cannot avoid learning their environment through randomization, and strengthens the result by showing even less powerful agents contain models of their world.

Conclusion: The work removes key assumptions from previous results, showing that learning environment models is fundamental to agent behavior even in stochastic and partially observable settings.

Abstract: Deciding whether an agent possesses a model of its surrounding world is a fundamental step toward understanding its capabilities and limitations. In [10], it was shown that, within a particular framework, every almost optimal and general agent necessarily contains sufficient knowledge of its environment to allow an approximate reconstruction of it by querying the agent as a black box. This result relied on the assumptions that the agent is deterministic and that the environment is fully observable.
  In this work, we remove both assumptions by extending the theorem to stochastic agents operating in partially observable environments. Fundamentally, this shows that stochastic agents cannot avoid learning their environment through the usage of randomization. We also strengthen the result by weakening the notion of generality, proving that less powerful agents already contain a model of the world in which they operate.

</details>


### [192] [Enhancing Foundation VLM Robustness to Missing Modality: Scalable Diffusion for Bi-directional Feature Restoration](https://arxiv.org/abs/2602.03151)
*Wei Dai,Haoyu Wang,Honghao Chang,Lijun He,Fan Li,Jian Sun,Haixia Bi*

Main category: cs.AI

TL;DR: A diffusion-based restoration strategy for VLMs that handles missing modalities via dynamic gating and cross-modal mutual learning, outperforming existing methods in zero-shot evaluations.


<details>
  <summary>Details</summary>
Motivation: VLMs struggle when modalities are incomplete, with prompt methods failing to restore essential features and imputation methods generating semantic noise. There's a need for precise semantic restoration while maintaining VLM generalization.

Method: Enhanced diffusion model as pluggable mid-stage module with two innovations: (1) Dynamic Modality Gating adaptively uses conditional features to guide semantically consistent feature generation, and (2) Cross-Modal Mutual Learning bridges semantic spaces of dual encoders for bidirectional alignment.

Result: Outperforms existing baseline methods in zero-shot evaluations across benchmark datasets. Extensive experiments confirm robustness and scalability across diverse missing rates and environments.

Conclusion: Proposed approach serves as a robust and scalable extension for VLMs in missing modality scenarios, ensuring reliability with publicly available code and models.

Abstract: Vision Language Models (VLMs) typically assume complete modality input during inference. However, their effectiveness drops sharply when certain modalities are unavailable or incomplete. Current research primarily faces two dilemmas: Prompt-based methods struggle to restore missing yet indispensable features and impair generalization of VLMs. Imputation-based approaches, lacking effective guidance, are prone to generating semantically irrelevant noise. Restoring precise semantics while sustaining VLM generalization remains challenging. Therefore, we propose a general missing modality restoration strategy in this paper. We introduce an enhanced diffusion model as a pluggable mid-stage training module to effectively restore missing features. Our strategy introduces two key innovations: (I) Dynamic Modality Gating, which adaptively leverages conditional features to steer the generation of semantically consistent features; (II) Cross-Modal Mutual Learning mechanism, which bridges the semantic spaces of dual encoders to achieve bidirectional alignment. Zero-shot evaluations across benchmark datasets demonstrate that our approach outperforms existing baseline methods. Extensive experiments and ablation studies confirm our model as a robust and scalable extension for VLMs in missing modality scenarios, ensuring reliability across diverse missing rates and environments. Our code and models will be publicly available.

</details>


### [193] [VALUEFLOW: Toward Pluralistic and Steerable Value-based Alignment in Large Language Models](https://arxiv.org/abs/2602.03160)
*Woojin Kim,Sieun Hyeon,Jusang Oh,Jaeyoung Do*

Main category: cs.AI

TL;DR: VALUEFLOW is a unified framework for hierarchical value extraction, intensity evaluation, and calibrated steering in LLMs, addressing gaps in current value alignment methods.


<details>
  <summary>Details</summary>
Motivation: Current LLM alignment methods fail to capture hierarchical value structure, measure calibrated intensity, or enable controlled value steering, limiting principled alignment with diverse human values.

Method: Three-component framework: HIVES for hierarchical value embedding, VIDB for large-scale value-labeled texts with intensity estimates, and anchor-based evaluator for consistent intensity scoring.

Result: Comprehensive study across 10 models and 4 value theories reveals asymmetries in steerability and composition laws for multi-value control, establishing scalable infrastructure.

Conclusion: VALUEFLOW advances pluralistic LLM alignment by providing unified framework for extraction, evaluation, and calibrated intensity steering of values.

Abstract: Aligning Large Language Models (LLMs) with the diverse spectrum of human values remains a central challenge: preference-based methods often fail to capture deeper motivational principles. Value-based approaches offer a more principled path, yet three gaps persist: extraction often ignores hierarchical structure, evaluation detects presence but not calibrated intensity, and the steerability of LLMs at controlled intensities remains insufficiently understood. To address these limitations, we introduce VALUEFLOW, the first unified framework that spans extraction, evaluation, and steering with calibrated intensity control. The framework integrates three components: (i) HIVES, a hierarchical value embedding space that captures intra- and cross-theory value structure; (ii) the Value Intensity DataBase (VIDB), a large-scale resource of value-labeled texts with intensity estimates derived from ranking-based aggregation; and (iii) an anchor-based evaluator that produces consistent intensity scores for model outputs by ranking them against VIDB panels. Using VALUEFLOW, we conduct a comprehensive large-scale study across ten models and four value theories, identifying asymmetries in steerability and composition laws for multi-value control. This paper establishes a scalable infrastructure for evaluating and controlling value intensity, advancing pluralistic alignment of LLMs.

</details>


### [194] [Beyond Quantity: Trajectory Diversity Scaling for Code Agents](https://arxiv.org/abs/2602.03219)
*Guhong Chen,Chenghao Sun,Cheng Fu,Qiyao Wang,Zhihong Huang,Chaopeng Wei,Guangxu Chen,Feiteng Fang,Ahmadreza Argha,Bing Zhao,Xander Xu,Qi Han,Hamid Alinejad-Rokny,Qiang Qu,Binhua Li,Shiwen Ni,Min Yang,Hu Wei,Yongbin Li*

Main category: cs.AI

TL;DR: TDScaling is a trajectory diversity scaling framework for code agents that improves performance through diverse data synthesis rather than raw volume scaling, addressing limitations of current LLM agent training approaches.


<details>
  <summary>Details</summary>
Motivation: Current code LLM agents face generalization limits due to low-quality synthetic data and diminishing returns from quantity scaling. There's an early bottleneck in trajectory data utilization, and quantity-centric approaches underperform compared to diversity-focused methods.

Method: TDScaling integrates four innovations: 1) Business Cluster mechanism for real-service logical dependencies, 2) blueprint-driven multi-agent paradigm for trajectory coherence, 3) adaptive evolution mechanism using Domain Entropy, Reasoning Mode Entropy, and Cumulative Action Complexity to prevent mode collapse, and 4) sandboxed code tool to mitigate catastrophic forgetting.

Result: Experiments on general tool-use benchmarks (BFCL, tau^2-Bench) and code agent tasks (RebenchT, CodeCI, BIRD) show TDScaling improves both tool-use generalization and inherent coding proficiency, achieving win-win outcomes with better performance-cost trade-offs.

Conclusion: TDScaling demonstrates that scaling through trajectory diversity rather than raw volume yields superior performance for code agents, with plans to release the full codebase and synthesized dataset (30,000+ tool clusters) upon publication.

Abstract: As code large language models (LLMs) evolve into tool-interactive agents via the Model Context Protocol (MCP), their generalization is increasingly limited by low-quality synthetic data and the diminishing returns of quantity scaling. Moreover, quantity-centric scaling exhibits an early bottleneck that underutilizes trajectory data. We propose TDScaling, a Trajectory Diversity Scaling-based data synthesis framework for code agents that scales performance through diversity rather than raw volume. Under a fixed training budget, increasing trajectory diversity yields larger gains than adding more trajectories, improving the performance-cost trade-off for agent training. TDScaling integrates four innovations: (1) a Business Cluster mechanism that captures real-service logical dependencies; (2) a blueprint-driven multi-agent paradigm that enforces trajectory coherence; (3) an adaptive evolution mechanism that steers synthesis toward long-tail scenarios using Domain Entropy, Reasoning Mode Entropy, and Cumulative Action Complexity to prevent mode collapse; and (4) a sandboxed code tool that mitigates catastrophic forgetting of intrinsic coding capabilities. Experiments on general tool-use benchmarks (BFCL, tau^2-Bench) and code agent tasks (RebenchT, CodeCI, BIRD) demonstrate a win-win outcome: TDScaling improves both tool-use generalization and inherent coding proficiency. We plan to release the full codebase and the synthesized dataset (including 30,000+ tool clusters) upon publication.

</details>


### [195] [TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking](https://arxiv.org/abs/2602.03224)
*Yu Cheng,Jiuan Zhou,Yongkang Hu,Yihang Chen,Huichi Zhou,Mingang Chen,Zhizhong Zhang,Kun Shao,Yuan Xie,Zhaoxia Yin*

Main category: cs.AI

TL;DR: TAME framework uses dual-memory evolution to prevent trustworthiness degradation during agent memory evolution while maintaining task performance.


<details>
  <summary>Details</summary>
Motivation: Agent memory evolution during test-time can degrade safety alignment even in benign tasks (Agent Memory Misevolution), creating a need to preserve trustworthiness while improving reasoning.

Method: TAME framework with dual-memory evolution: executor memory evolves for task performance via methodology distillation, and evaluator memory evolves for safety/utility assessment via historical feedback. Uses closed-loop process of memory filtering, draft generation, trustworthy refinement, execution, and dual-track memory updating.

Result: TAME mitigates misevolution, achieving joint improvement in both trustworthiness and task performance across various domains and evaluation settings.

Conclusion: The proposed dual-memory evolutionary framework effectively addresses Agent Memory Misevolution, preserving trustworthiness without sacrificing utility during agent memory evolution.

Abstract: Test-time evolution of agent memory serves as a pivotal paradigm for achieving AGI by bolstering complex reasoning through experience accumulation. However, even during benign task evolution, agent safety alignment remains vulnerable-a phenomenon known as Agent Memory Misevolution. To evaluate this phenomenon, we construct the Trust-Memevo benchmark to assess multi-dimensional trustworthiness during benign task evolution, revealing an overall decline in trustworthiness across various task domains and evaluation settings. To address this issue, we propose TAME, a dual-memory evolutionary framework that separately evolves executor memory to improve task performance by distilling generalizable methodologies, and evaluator memory to refine assessments of both safety and task utility based on historical feedback. Through a closed loop of memory filtering, draft generation, trustworthy refinement, execution, and dual-track memory updating, TAME preserves trustworthiness without sacrificing utility. Experiments demonstrate that TAME mitigates misevolution, achieving a joint improvement in both trustworthiness and task performance.

</details>


### [196] [The Necessity of a Unified Framework for LLM-Based Agent Evaluation](https://arxiv.org/abs/2602.03238)
*Pengyu Zhu,Li Sun,Philip S. Yu,Sen Su*

Main category: cs.AI

TL;DR: Proposes a unified evaluation framework to address standardization issues in LLM agent benchmarks, which are currently confounded by inconsistent prompts, tools, and environments.


<details>
  <summary>Details</summary>
Motivation: Current LLM agent evaluations suffer from confounding factors like inconsistent system prompts, tool configurations, and environmental dynamics, leading to unfair comparisons, non-reproducible results, and difficulty attributing performance gains to the model itself.

Method: Introduces a proposal for standardizing agent evaluation through a unified framework that addresses prompt engineering consistency, toolset standardization, and environmental data management.

Result: The paper identifies critical flaws in current agent evaluation practices and proposes a solution, but doesn't present empirical results since it's a proposal paper rather than an implementation study.

Conclusion: A unified evaluation framework is essential for rigorous advancement of agent evaluation, addressing current issues of unfairness, opacity, and non-reproducibility in LLM agent benchmarking.

Abstract: With the advent of Large Language Models (LLMs), general-purpose agents have seen fundamental advancements. However, evaluating these agents presents unique challenges that distinguish them from static QA benchmarks. We observe that current agent benchmarks are heavily confounded by extraneous factors, including system prompts, toolset configurations, and environmental dynamics. Existing evaluations often rely on fragmented, researcher-specific frameworks where the prompt engineering for reasoning and tool usage varies significantly, making it difficult to attribute performance gains to the model itself. Additionally, the lack of standardized environmental data leads to untraceable errors and non-reproducible results. This lack of standardization introduces substantial unfairness and opacity into the field. We propose that a unified evaluation framework is essential for the rigorous advancement of agent evaluation. To this end, we introduce a proposal aimed at standardizing agent evaluation.

</details>


### [197] [Accordion-Thinking: Self-Regulated Step Summaries for Efficient and Readable LLM Reasoning](https://arxiv.org/abs/2602.03249)
*Zhicheng Yang,Zhijiang Guo,Yinya Huang,Yongxin Wang,Wenlei Shi,Yiwei Wang,Xiaodan Liang,Jing Tang*

Main category: cs.AI

TL;DR: LLMs learn to dynamically compress reasoning steps through self-summarization, enabling 3x throughput with same accuracy by reducing KV cache dependency.


<details>
  <summary>Details</summary>
Motivation: Long Chain-of-Thought reasoning scales poorly due to linear KV cache growth and quadratic attention complexity, creating practical deployment limits despite performance gains.

Method: Accordion-Thinking framework where LLMs learn to self-regulate reasoning granularity via dynamic summarization, with reinforcement learning to incentivize compression. Features Fold mode (periodic summarization) vs Unfold mode (exhaustive reasoning).

Result: Models learn to encode essential reasoning into compact summaries, achieving 3x throughput while maintaining accuracy on 48GB GPU memory. Accuracy gap between Fold and Unfold modes vanishes during training.

Conclusion: LLMs can tackle complex reasoning with minimal dependency token overhead through learned self-compression, while structured summaries provide human-readable reasoning accounts.

Abstract: Scaling test-time compute via long Chain-ofThought unlocks remarkable gains in reasoning capabilities, yet it faces practical limits due to the linear growth of KV cache and quadratic attention complexity. In this paper, we introduce Accordion-Thinking, an end-to-end framework where LLMs learn to self-regulate the granularity of the reasoning steps through dynamic summarization. This mechanism enables a Fold inference mode, where the model periodically summarizes its thought process and discards former thoughts to reduce dependency on historical tokens. We apply reinforcement learning to incentivize this capability further, uncovering a critical insight: the accuracy gap between the highly efficient Fold mode and the exhaustive Unfold mode progressively narrows and eventually vanishes over the course of training. This phenomenon demonstrates that the model learns to encode essential reasoning information into compact summaries, achieving effective compression of the reasoning context. Our Accordion-Thinker demonstrates that with learned self-compression, LLMs can tackle complex reasoning tasks with minimal dependency token overhead without compromising solution quality, and it achieves a 3x throughput while maintaining accuracy on a 48GB GPU memory configuration, while the structured step summaries provide a human-readable account of the reasoning process.

</details>


### [198] [LPS-Bench: Benchmarking Safety Awareness of Computer-Use Agents in Long-Horizon Planning under Benign and Adversarial Scenarios](https://arxiv.org/abs/2602.03255)
*Tianyu Chen,Chujia Hu,Ge Gao,Dongrui Liu,Xia Hu,Wenjie Wang*

Main category: cs.AI

TL;DR: LPS-Bench is a benchmark for evaluating planning-time safety awareness of computer-use agents in long-horizon tasks, revealing significant safety deficiencies in existing systems.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks focus on short-horizon or GUI-based tasks, evaluating execution-time errors but overlooking planning-time risk anticipation. Computer-use agents face critical safety risks from ambiguous instructions and adversarial manipulation.

Method: Developed LPS-Bench with 65 scenarios across 7 task domains and 9 risk types, using a multi-agent automated pipeline for scalable data generation and LLM-as-a-judge evaluation protocol to assess safety awareness through planning trajectories.

Result: Experiments revealed substantial deficiencies in existing computer-use agents' ability to maintain safe behavior during long-horizon planning.

Conclusion: The benchmark fills a critical gap in evaluating planning-time safety awareness, analyzes risks in MCP-based computer-use agent systems, and proposes mitigation strategies to improve long-horizon planning safety.

Abstract: Computer-use agents (CUAs) that interact with real computer systems can perform automated tasks but face critical safety risks. Ambiguous instructions may trigger harmful actions, and adversarial users can manipulate tool execution to achieve malicious goals. Existing benchmarks mostly focus on short-horizon or GUI-based tasks, evaluating on execution-time errors but overlooking the ability to anticipate planning-time risks. To fill this gap, we present LPS-Bench, a benchmark that evaluates the planning-time safety awareness of MCP-based CUAs under long-horizon tasks, covering both benign and adversarial interactions across 65 scenarios of 7 task domains and 9 risk types. We introduce a multi-agent automated pipeline for scalable data generation and adopt an LLM-as-a-judge evaluation protocol to assess safety awareness through the planning trajectory. Experiments reveal substantial deficiencies in existing CUAs' ability to maintain safe behavior. We further analyze the risks and propose mitigation strategies to improve long-horizon planning safety in MCP-based CUA systems. We open-source our code at https://github.com/tychenn/LPS-Bench.

</details>


### [199] [CSR-Bench: A Benchmark for Evaluating the Cross-modal Safety and Reliability of MLLMs](https://arxiv.org/abs/2602.03263)
*Yuxuan Liu,Yuntian Shi,Kun Wang,Haoting Shen,Kun Yang*

Main category: cs.AI

TL;DR: CSR-Bench is a new benchmark for evaluating cross-modal reliability in MLLMs through four stress-testing patterns (Safety, Over-rejection, Bias, Hallucination) across 61 fine-grained types, revealing systematic alignment gaps and trade-offs between safety and over-rejection.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs may exhibit safety behaviors driven by unimodal shortcuts rather than true joint intent understanding, creating a need for systematic evaluation of cross-modal reliability to identify modality-induced behavior shifts.

Method: Developed CSR-Bench with four stress-testing interaction patterns covering 61 fine-grained types, requiring integrated image-text interpretation. Included paired text-only controls to diagnose modality-induced behavior shifts. Evaluated 16 state-of-the-art MLLMs.

Result: Models show systematic cross-modal alignment gaps: weak safety awareness, strong language dominance under interference, consistent performance degradation from text-only to multimodal inputs, and a clear trade-off between reducing over-rejection and maintaining safe, non-discriminatory behavior.

Conclusion: MLLMs exhibit systematic cross-modal reliability issues, with apparent safety gains potentially coming from refusal-oriented heuristics rather than robust intent understanding, highlighting the need for better cross-modal alignment in multimodal AI systems.

Abstract: Multimodal large language models (MLLMs) enable interaction over both text and images, but their safety behavior can be driven by unimodal shortcuts instead of true joint intent understanding. We introduce CSR-Bench, a benchmark for evaluating cross-modal reliability through four stress-testing interaction patterns spanning Safety, Over-rejection, Bias, and Hallucination, covering 61 fine-grained types. Each instance is constructed to require integrated image-text interpretation, and we additionally provide paired text-only controls to diagnose modality-induced behavior shifts. We evaluate 16 state-of-the-art MLLMs and observe systematic cross-modal alignment gaps. Models show weak safety awareness, strong language dominance under interference, and consistent performance degradation from text-only controls to multimodal inputs. We also observe a clear trade-off between reducing over-rejection and maintaining safe, non-discriminatory behavior, suggesting that some apparent safety gains may come from refusal-oriented heuristics rather than robust intent understanding. WARNING: This paper contains unsafe contents.

</details>


### [200] [Agentic Proposing: Enhancing Large Language Model Reasoning via Compositional Skill Synthesis](https://arxiv.org/abs/2602.03279)
*Zhengbo Jiao,Shaobo Wang,Zifan Zhang,Xuan Ren,Wei Wang,Bing Zhao,Hu Wei,Linfeng Zhang*

Main category: cs.AI

TL;DR: Agentic Proposing framework uses specialized agents to generate high-quality synthetic reasoning data, enabling small models to achieve SOTA performance with minimal training data.


<details>
  <summary>Details</summary>
Motivation: High-quality reasoning datasets are expensive to create manually, and current synthetic data generation methods face trade-offs between structural validity and problem complexity, often producing inconsistent or unsolvable instances.

Method: Agentic Proposing models problem synthesis as goal-driven sequential decision process where specialized agents dynamically select and compose modular reasoning skills using internal reflection and tool-use. Developed Agentic-Proposer-4B using Multi-Granularity Policy Optimization (MGPO) to generate verifiable training trajectories.

Result: Downstream solvers trained on agent-synthesized data significantly outperform leading baselines with robust cross-domain generalization. A 30B solver trained on only 11,000 synthesized trajectories achieves 91.6% accuracy on AIME25, rivaling frontier-scale proprietary models like GPT-5.

Conclusion: A small volume of high-quality synthetic signals can effectively substitute for massive human-curated datasets, demonstrating the power of agent-driven synthesis for advancing complex reasoning in LLMs.

Abstract: Advancing complex reasoning in large language models relies on high-quality, verifiable datasets, yet human annotation remains cost-prohibitive and difficult to scale. Current synthesis paradigms often face a recurring trade-off: maintaining structural validity typically restricts problem complexity, while relaxing constraints to increase difficulty frequently leads to inconsistent or unsolvable instances. To address this, we propose Agentic Proposing, a framework that models problem synthesis as a goal-driven sequential decision process where a specialized agent dynamically selects and composes modular reasoning skills. Through an iterative workflow of internal reflection and tool-use, we develop the Agentic-Proposer-4B using Multi-Granularity Policy Optimization (MGPO) to generate high-precision, verifiable training trajectories across mathematics, coding, and science. Empirical results demonstrate that downstream solvers trained on agent-synthesized data significantly outperform leading baselines and exhibit robust cross-domain generalization. Notably, a 30B solver trained on only 11,000 synthesized trajectories achieves a state-of-the-art 91.6% accuracy on AIME25, rivaling frontier-scale proprietary models such as GPT-5 and proving that a small volume of high-quality synthetic signals can effectively substitute for massive human-curated datasets.

</details>


### [201] [MeetBench-XL: Calibrated Multi-Dimensional Evaluation and Learned Dual-Policy Agents for Real-Time Meetings](https://arxiv.org/abs/2602.03285)
*Yuelin Hu,Jun Xu,Bingcong Lu,Zhengxue Cheng,Hongwei Hu,Ronghua Wu,Li Song*

Main category: cs.AI

TL;DR: MeetAll dataset and MeetMaster XL agent framework for enterprise meeting AI assistants that handle diverse operational tasks with strict latency/cost/privacy constraints, using bilingual multimodal corpus and dual-policy routing.


<details>
  <summary>Details</summary>
Motivation: Existing meeting benchmarks focus on simplified QA and fail to reflect real enterprise workflows where queries arise from multi-stakeholder collaboration, span long temporal contexts, and require tool-augmented reasoning under strict latency, cost, and privacy constraints.

Method: 1) MeetAll dataset: bilingual multimodal corpus from 231 enterprise meetings (140 hours) with questions injected using enterprise-informed protocol validated by domain experts. 2) MeetBench XL: multi-dimensional evaluation protocol. 3) MeetMaster XL: learned dual-policy agent that optimizes query routing between fast/slow reasoning paths and tool invocation (retrieval, cross-meeting aggregation, web search) with lightweight classifier.

Result: Experiments against commercial systems show consistent gains, with ablations, robustness tests, and real-world deployment case study demonstrating superior quality-latency tradeoff over single model baselines.

Conclusion: The paper addresses the gap in enterprise meeting AI assistants by introducing a grounded dataset and learned agent framework that handles diverse operational tasks under real-world constraints, validated through comprehensive evaluation and deployment.

Abstract: Enterprise meeting environments require AI assistants that handle diverse operational tasks, from rapid fact checking during live discussions to cross meeting analysis for strategic planning, under strict latency, cost, and privacy constraints. Existing meeting benchmarks mainly focus on simplified question answering and fail to reflect real world enterprise workflows, where queries arise organically from multi stakeholder collaboration, span long temporal contexts, and require tool augmented reasoning.
  We address this gap through a grounded dataset and a learned agent framework. First, we introduce MeetAll, a bilingual and multimodal corpus derived from 231 enterprise meetings totaling 140 hours. Questions are injected using an enterprise informed protocol validated by domain expert review and human discriminability studies. Unlike purely synthetic benchmarks, this protocol is grounded in four enterprise critical dimensions: cognitive load, temporal context span, domain expertise, and actionable task execution, calibrated through interviews with stakeholders across finance, healthcare, and technology sectors.
  Second, we propose MeetBench XL, a multi dimensional evaluation protocol aligned with human judgment that measures factual fidelity, intent alignment, response efficiency, structural clarity, and completeness. Third, we present MeetMaster XL, a learned dual policy agent that jointly optimizes query routing between fast and slow reasoning paths and tool invocation, including retrieval, cross meeting aggregation, and web search. A lightweight classifier enables accurate routing with minimal overhead, achieving a superior quality latency tradeoff over single model baselines. Experiments against commercial systems show consistent gains, supported by ablations, robustness tests, and a real world deployment case study.Resources: https://github.com/huyuelin/MeetBench.

</details>


### [202] [CRL-VLA: Continual Vision-Language-Action Learning](https://arxiv.org/abs/2602.03445)
*Qixin Zeng,Shuo Zhang,Hongyin Zhang,Renjie Wang,Han Zhao,Libang Zhao,Runze Li,Donglin Wang,Chao Huang*

Main category: cs.AI

TL;DR: CRL-VLA is a continual reinforcement learning framework for Vision-Language-Action models that balances stability-plasticity trade-off through asymmetric regulation and dual-critic architecture with theoretical guarantees.


<details>
  <summary>Details</summary>
Motivation: Lifelong learning is essential for embodied agents in open-world environments, but existing methods struggle to balance stability (retaining old skills) and plasticity (learning new skills) in continual reinforcement learning for VLA models.

Method: CRL-VLA uses asymmetric regulation: constraining advantage magnitudes on prior tasks while enabling controlled growth on new tasks. It employs a dual-critic architecture with Goal-Conditioned Value Formulation (GCVF), where a frozen critic maintains semantic consistency and a trainable estimator drives adaptation.

Result: Experiments on the LIBERO benchmark show CRL-VLA effectively harmonizes stability-plasticity objectives, outperforming baselines in both anti-forgetting and forward adaptation.

Conclusion: CRL-VLA provides a theoretically grounded framework for continual post-training of VLA models that successfully addresses the stability-plasticity dilemma in lifelong robotic scenarios.

Abstract: Lifelong learning is critical for embodied agents in open-world environments, where reinforcement learning fine-tuning has emerged as an important paradigm to enable Vision-Language-Action (VLA) models to master dexterous manipulation through environmental interaction. Thus, Continual Reinforcement Learning (CRL) is a promising pathway for deploying VLA models in lifelong robotic scenarios, yet balancing stability (retaining old skills) and plasticity (learning new ones) remains a formidable challenge for existing methods. We introduce CRL-VLA, a framework for continual post-training of VLA models with rigorous theoretical bounds. We derive a unified performance bound linking the stability-plasticity trade-off to goal-conditioned advantage magnitude, scaled by policy divergence. CRL-VLA resolves this dilemma via asymmetric regulation: constraining advantage magnitudes on prior tasks while enabling controlled growth on new tasks. This is realized through a simple but effective dual-critic architecture with novel Goal-Conditioned Value Formulation (GCVF), where a frozen critic anchors semantic consistency and a trainable estimator drives adaptation. Experiments on the LIBERO benchmark demonstrate that CRL-VLA effectively harmonizes these conflicting objectives, outperforming baselines in both anti-forgetting and forward adaptation.

</details>


### [203] [Rejecting Arguments Based on Doubt in Structured Bipolar Argumentation](https://arxiv.org/abs/2602.03286)
*Michael A. Müller,Srdjan Vesic,Bruno Yun*

Main category: cs.AI

TL;DR: A new computational argumentation approach incorporating philosophical/linguistic views: allows rejecting arguments based on doubt, focuses on sentence-level acceptance, and introduces structured bipolar frameworks with semantics between admissible and complete.


<details>
  <summary>Details</summary>
Motivation: To address two overlooked ideas in computational argumentation: 1) agents can rationally reject arguments based on mere doubt (not all defended arguments must be accepted), and 2) it's sometimes more natural to think in terms of which individual sentences/claims are accepted rather than which arguments.

Method: Define structured bipolar argumentation frameworks (SBAFs) where arguments consist of sentences with attack and support relations. Provide semantics with two features: 1) doesn't force acceptance of all defended arguments (unlike completeness-based semantics), 2) provides both argument extensions and language extensions for acceptable sentences.

Result: Developed semantics that represent reasonable positions in debates, lying between admissible and complete semantics of abstract argumentation. The approach provides new perspectives on existing methods, specifying conditions for ignoring support relations and showing deductive support semantics as a special case.

Conclusion: The paper presents a philosophically/linguistically informed computational argumentation approach that better captures rational agent behavior by allowing doubt-based rejection and sentence-level reasoning, bridging abstract argumentation with more nuanced real-world reasoning patterns.

Abstract: This paper develops a new approach to computational argumentation that is informed by philosophical and linguistic views. Namely, it takes into account two ideas that have received little attention in the literature on computational argumentation: First, an agent may rationally reject an argument based on mere doubt, thus not all arguments they could defend must be accepted; and, second, that it is sometimes more natural to think in terms of which individual sentences or claims an agent accepts in a debate, rather than which arguments. In order to incorporate these two ideas into a computational approach, we first define the notion of structured bipolar argumentation frameworks (SBAFs), where arguments consist of sentences and we have both an attack and a support relation between them. Then, we provide semantics for SBAFs with two features: (1) Unlike with completeness-based semantics, our semantics do not force agents to accept all defended arguments. (2) In addition to argument extensions, which give acceptable sets of arguments, we also provide semantics for language extensions that specify acceptable sets of sentences. These semantics represent reasonable positions an agent might have in a debate. Our semantics lie between the admissible and complete semantics of abstract argumentation. Further, our approach can be used to provide a new perspective on existing approaches. For instance, we can specify the conditions under which an agent can ignore support between arguments (i.e. under which the use of abstract argumentation is warranted) and we show that deductive support semantics is a special case of our approach.

</details>


### [204] [Memora: A Harmonic Memory Representation Balancing Abstraction and Specificity](https://arxiv.org/abs/2602.03315)
*Menglin Xia,Xuchao Zhang,Shantanu Dixit,Paramaguru Harimurugan,Rujia Wang,Victor Ruhle,Robert Sim,Chetan Bansal,Saravan Rajmohan*

Main category: cs.AI

TL;DR: Memora introduces a harmonic memory representation that balances abstraction and specificity for agents, organizing information through primary abstractions and cue anchors while using a retrieval policy that exploits memory connections beyond semantic similarity.


<details>
  <summary>Details</summary>
Motivation: Agent memory systems need to handle continuously growing information while supporting efficient, context-aware retrieval. Current abstraction methods for scaling often sacrifice specificity, losing fine-grained details needed for effective reasoning.

Method: Memora uses a harmonic memory representation with primary abstractions that index concrete memory values and consolidate related updates. Cue anchors expand retrieval access across diverse memory aspects and connect related memories. A retrieval policy actively exploits these memory connections to retrieve relevant information beyond direct semantic similarity.

Result: Memora establishes new state-of-the-art on LoCoMo and LongMemEval benchmarks, demonstrating better retrieval relevance and reasoning effectiveness as memory scales. Theoretically, standard RAG and KG-based memory systems emerge as special cases of the framework.

Conclusion: Memora provides a balanced approach to agent memory that maintains both abstraction for scalability and specificity for detailed reasoning, with theoretical generality and empirical superiority over existing methods.

Abstract: Agent memory systems must accommodate continuously growing information while supporting efficient, context-aware retrieval for downstream tasks. Abstraction is essential for scaling agent memory, yet it often comes at the cost of specificity, obscuring the fine-grained details required for effective reasoning. We introduce Memora, a harmonic memory representation that structurally balances abstraction and specificity. Memora organizes information via its primary abstractions that index concrete memory values and consolidate related updates into unified memory entries, while cue anchors expand retrieval access across diverse aspects of the memory and connect related memories. Building on this structure, we employ a retrieval policy that actively exploits these memory connections to retrieve relevant information beyond direct semantic similarity. Theoretically, we show that standard Retrieval-Augmented Generation (RAG) and Knowledge Graph (KG)-based memory systems emerge as special cases of our framework. Empirically, Memora establishes a new state-of-the-art on the LoCoMo and LongMemEval benchmarks, demonstrating better retrieval relevance and reasoning effectiveness as memory scales.

</details>


### [205] [MentalSeek-Dx: Towards Progressive Hypothetico-Deductive Reasoning for Real-world Psychiatric Diagnosis](https://arxiv.org/abs/2602.03340)
*Xiao Sun,Yuming Yang,Junnan Zhu,Jiang Zhong,Xinyu Zhou,Kaiwen Wei*

Main category: cs.AI

TL;DR: MentalDx Bench is the first benchmark for disorder-level psychiatric diagnosis using real clinical data, revealing LLMs' failure at fine-grained diagnosis despite good coarse categorization. MentalSeek-Dx, a specialized 14B-parameter LLM trained with clinical reasoning, achieves SOTA performance.


<details>
  <summary>Details</summary>
Motivation: Current LLM benchmarks for psychiatric assessment lack ecological validity and fine-grained diagnostic supervision, limiting their clinical utility. There's a need for clinically grounded evaluation that reflects real-world diagnostic challenges.

Method: Created MentalDx Bench with 712 de-identified EHRs annotated by board-certified psychiatrists under ICD-11 guidelines covering 76 disorders across 16 categories. Developed MentalSeek-Dx using supervised trajectory construction and curriculum-based reinforcement learning to internalize clinical hypothetico-deductive reasoning.

Result: Evaluation of 18 LLMs revealed paradigm misalignment: strong performance at coarse diagnostic categorization but systematic failure at disorder-level diagnosis. MentalSeek-Dx achieved state-of-the-art performance on MentalDx Bench with only 14B parameters.

Conclusion: The work establishes a clinically grounded framework for reliable psychiatric diagnosis, demonstrating that specialized training to internalize clinical reasoning processes can overcome the limitations of pattern-based LLM approaches.

Abstract: Mental health disorders represent a burgeoning global public health challenge. While Large Language Models (LLMs) have demonstrated potential in psychiatric assessment, their clinical utility is severely constrained by benchmarks that lack ecological validity and fine-grained diagnostic supervision. To bridge this gap, we introduce \textbf{MentalDx Bench}, the first benchmark dedicated to disorder-level psychiatric diagnosis within real-world clinical settings. Comprising 712 de-identified electronic health records annotated by board-certified psychiatrists under ICD-11 guidelines, the benchmark covers 76 disorders across 16 diagnostic categories. Evaluation of 18 LLMs reveals a critical \textit{paradigm misalignment}: strong performance at coarse diagnostic categorization contrasts with systematic failure at disorder-level diagnosis, underscoring a gap between pattern-based modeling and clinical hypothetico-deductive reasoning. In response, we propose \textbf{MentalSeek-Dx}, a medical-specialized LLM trained to internalize this clinical reasoning process through supervised trajectory construction and curriculum-based reinforcement learning. Experiments on MentalDx Bench demonstrate that MentalSeek-Dx achieves state-of-the-art (SOTA) performance with only 14B parameters, establishing a clinically grounded framework for reliable psychiatric diagnosis.

</details>


### [206] [Building Interpretable Models for Moral Decision-Making](https://arxiv.org/abs/2602.03351)
*Mayank Goel,Aritra Das,Paras Chopra*

Main category: cs.AI

TL;DR: A custom transformer model achieves 77% accuracy on Moral Machine trolley dilemmas while remaining small enough for interpretability analysis.


<details>
  <summary>Details</summary>
Motivation: To study how neural networks make moral decisions on trolley-style dilemmas and understand the computational mechanisms behind moral reasoning.

Method: Build a custom transformer model with structured scenario embeddings encoding affected parties, quantities, and outcomes, using a 2-layer architecture small enough for detailed interpretability analysis.

Result: Achieves 77% accuracy on Moral Machine data, with interpretability techniques revealing that moral biases localize to distinct computational stages in the network.

Conclusion: The small transformer model successfully captures moral decision-making patterns while enabling detailed analysis of how moral reasoning distributes across computational stages.

Abstract: We build a custom transformer model to study how neural networks make moral decisions on trolley-style dilemmas. The model processes structured scenarios using embeddings that encode who is affected, how many people, and which outcome they belong to. Our 2-layer architecture achieves 77% accuracy on Moral Machine data while remaining small enough for detailed analysis. We use different interpretability techniques to uncover how moral reasoning distributes across the network, demonstrating that biases localize to distinct computational stages among other findings.

</details>


### [207] [GFlowPO: Generative Flow Network as a Language Model Prompt Optimizer](https://arxiv.org/abs/2602.03358)
*Junmo Cho,Suhan Kim,Sangjune An,Minsu Kim,Dong Bok Lee,Heejun Lee,Sung Ju Hwang,Hae Beom Lee*

Main category: cs.AI

TL;DR: GFlowPO is a probabilistic prompt optimization framework that uses GFlowNets for sample-efficient prompt search and dynamic memory updates to concentrate on high-reward regions.


<details>
  <summary>Details</summary>
Motivation: Prompt optimization is critical but difficult due to the combinatorially large search space and sparse rewards from expensive LM evaluations. Existing RL-based methods suffer from poor sample efficiency due to on-policy updates and fixed meta-prompt distributions.

Method: Two-step approach: 1) Fine-tune a lightweight prompt-LM using off-policy GFlowNet objective with replay-based training for sample-efficient exploration. 2) Dynamic Memory Update (DMU) that updates the meta-prompt by injecting diverse prompts from replay buffer and top-performing prompts from a priority queue.

Result: GFlowPO consistently outperforms recent discrete prompt optimization baselines across few-shot text classification, instruction induction benchmarks, and question answering tasks.

Conclusion: GFlowPO provides an effective probabilistic framework for prompt optimization that addresses sample efficiency issues through off-policy learning and dynamic meta-prompt updates, demonstrating superior performance across multiple NLP tasks.

Abstract: Finding effective prompts for language models (LMs) is critical yet notoriously difficult: the prompt space is combinatorially large, rewards are sparse due to expensive target-LM evaluation. Yet, existing RL-based prompt optimizers often rely on on-policy updates and a meta-prompt sampled from a fixed distribution, leading to poor sample efficiency. We propose GFlowPO, a probabilistic prompt optimization framework that casts prompt search as a posterior inference problem over latent prompts regularized by a meta-prompted reference-LM prior. In the first step, we fine-tune a lightweight prompt-LM with an off-policy Generative Flow Network (GFlowNet) objective, using a replay-based training policy that reuses past prompt evaluations to enable sample-efficient exploration. In the second step, we introduce Dynamic Memory Update (DMU), a training-free mechanism that updates the meta-prompt by injecting both (i) diverse prompts from a replay buffer and (ii) top-performing prompts from a small priority queue, thereby progressively concentrating the search process on high-reward regions. Across few-shot text classification, instruction induction benchmarks, and question answering tasks, GFlowPO consistently outperforms recent discrete prompt optimization baselines.

</details>


### [208] [Risk Awareness Injection: Calibrating Vision-Language Models for Safety without Compromising Utility](https://arxiv.org/abs/2602.03402)
*Mengxuan Wang,Yuxin Chen,Gang Xu,Tao He,Hongjie Jiang,Ming Li*

Main category: cs.AI

TL;DR: RAI is a lightweight, training-free defense framework that amplifies unsafe signals in VLMs by modulating high-risk visual tokens using unsafe prototypes, restoring LLM-like risk recognition without degrading utility.


<details>
  <summary>Details</summary>
Motivation: VLMs are vulnerable to multimodal jailbreak attacks, existing defenses are costly (safety fine-tuning) or degrade utility (aggressive token manipulations), and visual inputs dilute risk signals that LLMs naturally recognize.

Method: Constructs Unsafe Prototype Subspace from language embeddings, performs targeted modulation on selected high-risk visual tokens to activate safety-critical signals while preserving semantic integrity for cross-modal reasoning.

Result: Extensive experiments show RAI substantially reduces attack success rate across multiple jailbreak benchmarks without compromising task performance on utility benchmarks.

Conclusion: RAI provides an effective, lightweight, training-free safety calibration framework that restores LLM-like risk recognition in VLMs by amplifying unsafe signals through targeted visual token modulation.

Abstract: Vision language models (VLMs) extend the reasoning capabilities of large language models (LLMs) to cross-modal settings, yet remain highly vulnerable to multimodal jailbreak attacks. Existing defenses predominantly rely on safety fine-tuning or aggressive token manipulations, incurring substantial training costs or significantly degrading utility. Recent research shows that LLMs inherently recognize unsafe content in text, and the incorporation of visual inputs in VLMs frequently dilutes risk-related signals. Motivated by this, we propose Risk Awareness Injection (RAI), a lightweight and training-free framework for safety calibration that restores LLM-like risk recognition by amplifying unsafe signals in VLMs. Specifically, RAI constructs an Unsafe Prototype Subspace from language embeddings and performs targeted modulation on selected high-risk visual tokens, explicitly activating safety-critical signals within the cross-modal feature space. This modulation restores the model's LLM-like ability to detect unsafe content from visual inputs, while preserving the semantic integrity of original tokens for cross-modal reasoning. Extensive experiments across multiple jailbreak and utility benchmarks demonstrate that RAI substantially reduces attack success rate without compromising task performance.

</details>


### [209] [Feasible strategies for conflict resolution within intuitionistic fuzzy preference-based conflict situations](https://arxiv.org/abs/2602.03403)
*Guangming Lang,Mingchuan Shang,Mengjun Hu,Jie Zhou,Feng Xu*

Main category: cs.AI

TL;DR: Proposes intuitionistic fuzzy preference-based conflict analysis with finer granularity than classical models, enabling three-way conflict measures and adjustment strategies.


<details>
  <summary>Details</summary>
Motivation: Existing preference-based conflict models use only three qualitative relations (preference, converse, indifference), which limits their ability to capture conflict essence. Need finer granularity to better model agents' attitudes.

Method: Introduces intuitionistic fuzzy preference-based conflict situations, develops conflict measures within this framework, constructs three-way analysis models for trisecting agent pairs, agents, and issues, uses relative loss functions for threshold calculation, and proposes adjustment mechanism-based feasible strategies with an algorithm.

Result: Develops a comprehensive framework for conflict analysis with finer granularity, including mathematical models, threshold calculation methods, and practical adjustment strategies with an illustrative example demonstrating validity.

Conclusion: The proposed intuitionistic fuzzy preference-based conflict analysis model overcomes limitations of classical models, provides more nuanced conflict representation, and offers practical tools for conflict resolution with demonstrated effectiveness.

Abstract: In three-way conflict analysis, preference-based conflict situations characterize agents' attitudes towards issues by formally modeling their preferences over pairs of issues. However, existing preference-based conflict models rely exclusively on three qualitative relations, namely, preference, converse, and indifference, to describe agents' attitudes towards issue pairs, which significantly limits their capacity in capturing the essence of conflict. To overcome this limitation, we introduce the concept of an intuitionistic fuzzy preference-based conflict situation that captures agents' attitudes towards issue pairs with finer granularity than that afforded by classical preference-based models. Afterwards, we develop intuitionistic fuzzy preference-based conflict measures within this framework, and construct three-way conflict analysis models for trisecting the set of agent pairs, the agent set, and the issue set. Additionally, relative loss functions built on the proposed conflict functions are employed to calculate thresholds for three-way conflict analysis. Finally, we present adjustment mechanism-based feasible strategies that simultaneously account for both adjustment magnitudes and conflict degrees, together with an algorithm for constructing such feasible strategies, and provide an illustrative example to demonstrate the validity and effectiveness of the proposed model.

</details>


### [210] [DiscoverLLM: From Executing Intents to Discovering Them](https://arxiv.org/abs/2602.03429)
*Tae Soo Kim,Yoonjoo Lee,Jaesang Yu,John Joon Young Chung,Juho Kim*

Main category: cs.AI

TL;DR: DiscoverLLM trains LLMs to help users discover their intents through interactive exploration rather than just asking clarification questions, using a novel user simulator with hierarchical intent modeling.


<details>
  <summary>Details</summary>
Motivation: Current LLMs struggle with ambiguous requests because users often don't know what they want yet - they need to explore options to discover their intents. Simply asking clarification questions fails when users themselves are uncertain.

Method: Introduces DiscoverLLM framework with a novel user simulator that models cognitive state using a hierarchy of intents that progressively concretize as relevant options are surfaced. The degree of concretization serves as a reward signal for training models to optimize intent discovery.

Result: Achieves over 10% higher task performance while reducing conversation length by up to 40% across interactive benchmarks in creative writing, technical writing, and SVG drawing. User study with 75 participants shows improved conversation satisfaction and efficiency.

Conclusion: DiscoverLLM enables LLMs to effectively collaborate with users by adaptively exploring options when intents are unclear and refining/implementing when intents concretize, representing a significant advancement in human-AI interaction for intent discovery.

Abstract: To handle ambiguous and open-ended requests, Large Language Models (LLMs) are increasingly trained to interact with users to surface intents they have not yet expressed (e.g., ask clarification questions). However, users are often ambiguous because they have not yet formed their intents: they must observe and explore outcomes to discover what they want. Simply asking "what kind of tone do you want?" fails when users themselves do not know. We introduce DiscoverLLM, a novel and generalizable framework that trains LLMs to help users form and discover their intents. Central to our approach is a novel user simulator that models cognitive state with a hierarchy of intents that progressively concretize as the model surfaces relevant options -- where the degree of concretization serves as a reward signal that models can be trained to optimize. Resulting models learn to collaborate with users by adaptively diverging (i.e., explore options) when intents are unclear, and converging (i.e., refine and implement) when intents concretize. Across proposed interactive benchmarks in creative writing, technical writing, and SVG drawing, DiscoverLLM achieves over 10% higher task performance while reducing conversation length by up to 40%. In a user study with 75 human participants, DiscoverLLM improved conversation satisfaction and efficiency compared to baselines.

</details>


### [211] [Ontology-to-tools compilation for executable semantic constraint enforcement in LLM agents](https://arxiv.org/abs/2602.03439)
*Xiaochi Zhou,Patrick Bulter,Changxuan Yang,Simon D. Rihm,Thitikarn Angkanaporn,Jethro Akroyd,Sebastian Mosbach,Markus Kraft*

Main category: cs.AI

TL;DR: LLMs are coupled with formal domain knowledge via ontology-to-tools compilation, where ontological specifications are compiled into executable tool interfaces that enforce semantic constraints during knowledge graph generation rather than through post-hoc validation.


<details>
  <summary>Details</summary>
Motivation: To reduce manual schema and prompt engineering by embedding formal knowledge into generative systems, enabling structured interaction between generative models, symbolic constraints, and external resources.

Method: Ontology-to-tools compilation within The World Avatar (TWA) framework, where ontological specifications are compiled into executable tool interfaces. The Model Context Protocol (MCP) and associated agents enable structured interaction. An agent-based workflow translates ontologies into ontology-aware tools and iteratively applies them to extract, validate, and repair structured knowledge from unstructured scientific text.

Result: Demonstrated using metal-organic polyhedra synthesis literature as a case study, showing how executable ontological semantics can guide LLM behavior and reduce manual schema and prompt engineering.

Conclusion: Establishes a general paradigm for embedding formal knowledge into generative systems through ontology-to-tools compilation, enabling semantic constraint enforcement during generation rather than through post-hoc validation.

Abstract: We introduce ontology-to-tools compilation as a proof-of-principle mechanism for coupling large language models (LLMs) with formal domain knowledge. Within The World Avatar (TWA), ontological specifications are compiled into executable tool interfaces that LLM-based agents must use to create and modify knowledge graph instances, enforcing semantic constraints during generation rather than through post-hoc validation. Extending TWA's semantic agent composition framework, the Model Context Protocol (MCP) and associated agents are integral components of the knowledge graph ecosystem, enabling structured interaction between generative models, symbolic constraints, and external resources. An agent-based workflow translates ontologies into ontology-aware tools and iteratively applies them to extract, validate, and repair structured knowledge from unstructured scientific text. Using metal-organic polyhedra synthesis literature as an illustrative case, we show how executable ontological semantics can guide LLM behaviour and reduce manual schema and prompt engineering, establishing a general paradigm for embedding formal knowledge into generative systems.

</details>


### [212] [The Dual Role of Abstracting over the Irrelevant in Symbolic Explanations: Cognitive Effort vs. Understanding](https://arxiv.org/abs/2602.03467)
*Zeynep G. Saribatur,Johannes Langer,Ute Schmid*

Main category: cs.AI

TL;DR: Formal abstractions (removal and clustering) improve human reasoning with symbolic AI explanations by reducing cognitive load and enhancing understanding.


<details>
  <summary>Details</summary>
Motivation: AI systems often produce outputs that are difficult for humans to understand, and while symbolic AI offers transparency, raw logical traces impose high cognitive load. The paper investigates how formal abstractions can make symbolic explanations more human-centered.

Method: Used Answer Set Programming (ASP) as a formal framework to define irrelevant details for abstraction. Conducted cognitive experiments where participants classified stimuli across domains using explanations derived from answer set programs, testing the effects of detail removal and clustering.

Result: Clustering details significantly improved participants' understanding, while removal of details significantly reduced cognitive effort. Both abstraction techniques enhanced human-centered symbolic explanations.

Conclusion: Formal abstractions (specifically removal and clustering of details) effectively enhance human reasoning performance and reduce cognitive effort when interacting with symbolic AI explanations, supporting the hypothesis that abstraction improves human-centered symbolic explanations.

Abstract: Explanations are central to human cognition, yet AI systems often produce outputs that are difficult to understand. While symbolic AI offers a transparent foundation for interpretability, raw logical traces often impose a high extraneous cognitive load. We investigate how formal abstractions, specifically removal and clustering, impact human reasoning performance and cognitive effort. Utilizing Answer Set Programming (ASP) as a formal framework, we define a notion of irrelevant details to be abstracted over to obtain simplified explanations. Our cognitive experiments, in which participants classified stimuli across domains with explanations derived from an answer set program, show that clustering details significantly improve participants' understanding, while removal of details significantly reduce cognitive effort, supporting the hypothesis that abstraction enhances human-centered symbolic explanations.

</details>


### [213] [IntentRL: Training Proactive User-intent Agents for Open-ended Deep Research via Reinforcement Learning](https://arxiv.org/abs/2602.03468)
*Haohao Luo,Zexi Li,Yuexiang Xie,Wenhao Zhang,Yaliang Li,Ying Shen*

Main category: cs.AI

TL;DR: IntentRL trains proactive agents to clarify user intents before starting computationally expensive deep research, improving efficiency and outcomes through scalable data generation and two-stage reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Deep Research agents face an autonomy-interaction dilemma: high autonomy on ambiguous queries leads to prolonged execution with unsatisfactory outcomes due to computational expense and time consumption.

Method: Proposed IntentRL framework with scalable pipeline for generating high-quality dialogue data via shallow-to-deep intent refinement graph, plus two-stage RL: Stage I learns from offline dialogues, Stage II uses online rollouts with user simulator.

Result: IntentRL significantly improves both intent hit rate and downstream task performance, outperforming built-in clarify modules of closed-source DR agents and proactive LLM baselines.

Conclusion: Proactive intent clarification before deep research execution addresses the autonomy-interaction dilemma, making deep research agents more efficient and effective through better understanding of user needs.

Abstract: Deep Research (DR) agents extend Large Language Models (LLMs) beyond parametric knowledge by autonomously retrieving and synthesizing evidence from large web corpora into long-form reports, enabling a long-horizon agentic paradigm. However, unlike real-time conversational assistants, DR is computationally expensive and time-consuming, creating an autonomy-interaction dilemma: high autonomy on ambiguous user queries often leads to prolonged execution with unsatisfactory outcomes. To address this, we propose IntentRL, a framework that trains proactive agents to clarify latent user intents before starting long-horizon research. To overcome the scarcity of open-ended research data, we introduce a scalable pipeline that expands a few seed samples into high-quality dialogue turns via a shallow-to-deep intent refinement graph. We further adopt a two-stage reinforcement learning (RL) strategy: Stage I applies RL on offline dialogues to efficiently learn general user-interaction behavior, while Stage II uses the trained agent and a user simulator for online rollouts to strengthen adaptation to diverse user feedback. Extensive experiments show that IntentRL significantly improves both intent hit rate and downstream task performance, outperforming the built-in clarify modules of closed-source DR agents and proactive LLM baselines.

</details>


### [214] [When Routing Collapses: On the Degenerate Convergence of LLM Routers](https://arxiv.org/abs/2602.03478)
*Guannan Lai,Han-Jia Ye*

Main category: cs.AI

TL;DR: EquiRouter addresses routing collapse in LLM routing by directly learning model rankings instead of scalar scores, reducing costs by ~17% at GPT-4-level performance.


<details>
  <summary>Details</summary>
Motivation: Existing LLM routers systematically default to expensive models even when cheaper ones suffice, wasting computation and cost - a phenomenon called "routing collapse" caused by objective-decision mismatch.

Method: Proposes EquiRouter, a decision-aware router that directly learns model rankings rather than predicting scalar performance scores, bridging the gap between training objectives and routing decisions.

Result: On RouterBench, EquiRouter reduces cost by about 17% at GPT-4-level performance compared to the strongest prior router, effectively mitigating routing collapse.

Conclusion: Directly learning model rankings rather than scalar scores is key to preventing routing collapse and achieving better quality-cost trade-offs in LLM routing systems.

Abstract: LLM routing aims to achieve a favorable quality--cost trade-off by dynamically assigning easy queries to smaller models and harder queries to stronger ones. However, across both unimodal and multimodal settings, we uncover a pervasive yet underexplored failure mode in existing routers: as the user's cost budget increases, routers systematically default to the most capable and most expensive model even when cheaper models already suffice. As a result, current routers under-utilize small models, wasting computation and monetary cost and undermining the core promise of routing; we term this phenomenon routing collapse. We attribute routing collapse to an objective--decision mismatch: many routers are trained to predict scalar performance scores, whereas routing decisions ultimately depend on discrete comparisons among candidate models. Consequently, small prediction errors can flip relative orderings and trigger suboptimal selections. To bridge this gap, we propose EquiRouter, a decision-aware router that directly learns model rankings, restoring the role of smaller models and mitigating routing collapse. On RouterBench, EquiRouter reduces cost by about 17\% at GPT-4-level performance compared to the strongest prior router. Our code is available at https://github.com/AIGNLAI/EquiRouter.

</details>


### [215] [Group Selection as a Safeguard Against AI Substitution](https://arxiv.org/abs/2602.03541)
*Qiankun Zhong,Thomas F. Eisenmann,Julian Garcia,Iyad Rahwan*

Main category: cs.AI

TL;DR: AI use can reduce cultural diversity, potentially causing "cultural collapse" where AI-generated content decreases human variation and innovation. AI-substitute users (minimal input, AI produces output) spread under individual selection despite harming cultural variance, while AI-complement users (AI as guidance, human produces output) benefit groups and can be favored by cultural group selection.


<details>
  <summary>Details</summary>
Motivation: The paper addresses concerns that reliance on generative AI reduces cultural variance and diversity in creative work, which has already caused problems like model collapse and hallucination. The authors want to examine long-term consequences for human cultural evolution and identify conditions where widespread AI use might lead to "cultural collapse" - a process where AI-generated content reduces human variation, innovation, and slows cumulative cultural evolution.

Method: The researchers use an agent-based model and evolutionary game theory to compare two types of AI use: complement (users seek AI suggestions but remain main producers) and substitute (users provide minimal input and rely on AI for most output). They study how these strategies compete and spread under evolutionary dynamics, examining individual-level selection versus cultural group selection.

Result: AI-substitute users prevail under individual-level selection despite causing stronger reduction in cultural variance. AI-complement users can benefit their groups by maintaining the variance needed for exploration, and can therefore be favored under cultural group selection when group boundaries are strong. The findings show tension between individual incentives and group-level benefits.

Conclusion: Widespread AI adoption poses risks to cultural evolution, with AI-substitute use potentially dominating despite harming cultural diversity. However, AI-complement use can preserve cultural variance and be favored through group selection mechanisms. The research informs policy and organizational strategies to mitigate risks of cultural collapse by promoting complement rather than substitute AI use patterns.

Abstract: Reliance on generative AI can reduce cultural variance and diversity, especially in creative work. This reduction in variance has already led to problems in model performance, including model collapse and hallucination. In this paper, we examine the long-term consequences of AI use for human cultural evolution and the conditions under which widespread AI use may lead to "cultural collapse", a process in which reliance on AI-generated content reduces human variation and innovation and slows cumulative cultural evolution. Using an agent-based model and evolutionary game theory, we compare two types of AI use: complement and substitute. AI-complement users seek suggestions and guidance while remaining the main producers of the final output, whereas AI-substitute users provide minimal input, and rely on AI to produce most of the output. We then study how these use strategies compete and spread under evolutionary dynamics. We find that AI-substitute users prevail under individual-level selection despite the stronger reduction in cultural variance. By contrast, AI-complement users can benefit their groups by maintaining the variance needed for exploration, and can therefore be favored under cultural group selection when group boundaries are strong. Overall, our findings shed light on the long-term, population-level effects of AI adoption and inform policy and organizational strategies to mitigate these risks.

</details>


### [216] [Persona Generators: Generating Diverse Synthetic Personas at Scale](https://arxiv.org/abs/2602.03545)
*Davide Paglieri,Logan Cross,William A. Cunningham,Joel Z. Leibo,Alexander Sasha Vezhnevets*

Main category: cs.AI

TL;DR: Persona Generators use LLM-based evolutionary optimization to create diverse synthetic populations from minimal descriptions, outperforming baselines in covering rare trait combinations.


<details>
  <summary>Details</summary>
Motivation: Evaluating AI systems requires understanding diverse human behaviors, but collecting representative human data is expensive/infeasible, especially for novel technologies or future scenarios. Existing synthetic persona methods need detailed population data and focus on density matching rather than covering long-tail behaviors.

Method: Introduce Persona Generators that produce diverse synthetic populations. Apply AlphaEvolve iterative improvement loop using LLMs as mutation operators to refine generator code over hundreds of iterations, creating lightweight generators that expand small descriptions into diverse populations maximizing coverage of opinions/preferences.

Result: Evolved generators substantially outperform existing baselines across six diversity metrics on held-out contexts, producing populations that span rare trait combinations difficult to achieve in standard LLM outputs.

Conclusion: Persona Generators enable efficient creation of diverse synthetic populations for AI evaluation without expensive human data collection, addressing the coverage gap for long-tail behaviors in existing methods.

Abstract: Evaluating AI systems that interact with humans requires understanding their behavior across diverse user populations, but collecting representative human data is often expensive or infeasible, particularly for novel technologies or hypothetical future scenarios. Recent work in Generative Agent-Based Modeling has shown that large language models can simulate human-like synthetic personas with high fidelity, accurately reproducing the beliefs and behaviors of specific individuals. However, most approaches require detailed data about target populations and often prioritize density matching (replicating what is most probable) rather than support coverage (spanning what is possible), leaving long-tail behaviors underexplored. We introduce Persona Generators, functions that can produce diverse synthetic populations tailored to arbitrary contexts. We apply an iterative improvement loop based on AlphaEvolve, using large language models as mutation operators to refine our Persona Generator code over hundreds of iterations. The optimization process produces lightweight Persona Generators that can automatically expand small descriptions into populations of diverse synthetic personas that maximize coverage of opinions and preferences along relevant diversity axes. We demonstrate that evolved generators substantially outperform existing baselines across six diversity metrics on held-out contexts, producing populations that span rare trait combinations difficult to achieve in standard LLM outputs.

</details>


### [217] [EHRWorld: A Patient-Centric Medical World Model for Long-Horizon Clinical Trajectories](https://arxiv.org/abs/2602.03569)
*Linjie Mu,Zhongzhen Huang,Yannian Gu,Shengqian Qin,Shaoting Zhang,Xiaofan Zhang*

Main category: cs.AI

TL;DR: EHRWorld is a patient-centric medical world model trained on causal sequential data that outperforms naive LLM baselines in simulating disease progression and treatment outcomes over time.


<details>
  <summary>Details</summary>
Motivation: While world models offer a framework for simulating future states under interventions, implementing them in complex medical domains is challenging. LLMs perform well on static medical tasks but struggle with maintaining consistent patient states under sequential interventions, leading to error accumulation in long-term clinical simulations.

Method: Introduces EHRWorld, a patient-centric medical world model trained under a causal sequential paradigm, along with EHRWorld-110K - a large-scale longitudinal clinical dataset derived from real-world electronic health records.

Result: EHRWorld significantly outperforms naive LLM-based baselines, achieving more stable long-horizon simulation, improved modeling of clinically sensitive events, and favorable reasoning efficiency.

Conclusion: Training on causally grounded, temporally evolving clinical data is necessary for reliable and robust medical world modeling, as demonstrated by EHRWorld's superior performance over LLMs that only incorporate medical knowledge without proper causal sequential training.

Abstract: World models offer a principled framework for simulating future states under interventions, but realizing such models in complex, high-stakes domains like medicine remains challenging. Recent large language models (LLMs) have achieved strong performance on static medical reasoning tasks, raising the question of whether they can function as dynamic medical world models capable of simulating disease progression and treatment outcomes over time. In this work, we show that LLMs only incorporating medical knowledge struggle to maintain consistent patient states under sequential interventions, leading to error accumulation in long-horizon clinical simulation. To address this limitation, we introduce EHRWorld, a patient-centric medical world model trained under a causal sequential paradigm, together with EHRWorld-110K, a large-scale longitudinal clinical dataset derived from real-world electronic health records. Extensive evaluations demonstrate that EHRWorld significantly outperforms naive LLM-based baselines, achieving more stable long-horizon simulation, improved modeling of clinically sensitive events, and favorable reasoning efficiency, highlighting the necessity of training on causally grounded, temporally evolving clinical data for reliable and robust medical world modeling.

</details>


### [218] [Can LLMs Do Rocket Science? Exploring the Limits of Complex Reasoning with GTOC 12](https://arxiv.org/abs/2602.03630)
*Iñaki del Campo,Pablo Cuervo,Victor Rodriguez-Fernandez,Roberto Armellin,Jack Yarndley*

Main category: cs.AI

TL;DR: LLMs show improved strategic planning for complex space missions but fail at practical implementation due to technical execution errors.


<details>
  <summary>Details</summary>
Motivation: To investigate LLMs' capacity for autonomous multi-stage planning in high-dimensional, physically constrained environments using a complex astrodynamics challenge (GTOC 12 asteroid mining competition).

Method: Adapted MLE-Bench framework to orbital mechanics, deployed AIDE-based agent architecture for autonomous mission solution generation, and used "LLM-as-a-Judge" methodology with expert-developed rubric to evaluate strategic viability across five categories.

Result: Strategic viability scores nearly doubled in two years (9.3 to 17.2/26), but models consistently fail at implementation due to unit inconsistencies, boundary condition errors, and inefficient debugging loops.

Conclusion: Current LLMs demonstrate sufficient knowledge for space science tasks but remain limited by an implementation barrier, functioning as domain facilitators rather than fully autonomous engineers.

Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency in code generation and general reasoning, yet their capacity for autonomous multi-stage planning in high-dimensional, physically constrained environments remains an open research question. This study investigates the limits of current AI agents by evaluating them against the 12th Global Trajectory Optimization Competition (GTOC 12), a complex astrodynamics challenge requiring the design of a large-scale asteroid mining campaign. We adapt the MLE-Bench framework to the domain of orbital mechanics and deploy an AIDE-based agent architecture to autonomously generate and refine mission solutions. To assess performance beyond binary validity, we employ an "LLM-as-a-Judge" methodology, utilizing a rubric developed by domain experts to evaluate strategic viability across five structural categories. A comparative analysis of models, ranging from GPT-4-Turbo to reasoning-enhanced architectures like Gemini 2.5 Pro, and o3, reveals a significant trend: the average strategic viability score has nearly doubled in the last two years (rising from 9.3 to 17.2 out of 26). However, we identify a critical capability gap between strategy and execution. While advanced models demonstrate sophisticated conceptual understanding, correctly framing objective functions and mission architectures, they consistently fail at implementation due to physical unit inconsistencies, boundary condition errors, and inefficient debugging loops. We conclude that, while current LLMs often demonstrate sufficient knowledge and intelligence to tackle space science tasks, they remain limited by an implementation barrier, functioning as powerful domain facilitators rather than fully autonomous engineers.

</details>


### [219] [Search-R2: Enhancing Search-Integrated Reasoning via Actor-Refiner Collaboration](https://arxiv.org/abs/2602.03647)
*Bowei He,Minda Hu,Zenan Xu,Hongru Wang,Licheng Zong,Yankai Chen,Chen Ma,Xue Liu,Pluto Zhou,Irwin King*

Main category: cs.AI

TL;DR: Search-R2: Actor-Refiner framework for training search-integrated reasoning agents with fine-grained supervision via hybrid rewards and selective intervention.


<details>
  <summary>Details</summary>
Motivation: Existing RL methods for training search-integrated reasoning agents suffer from sparse trajectory-level rewards that fail to distinguish between high-quality reasoning and lucky guesses, leading to inefficient or misleading search behaviors.

Method: Proposes Search-R2 with Actor-Refiner collaboration: Actor generates initial reasoning trajectories, Meta-Refiner selectively diagnoses/repairs flawed steps via 'cut-and-regenerate'. Uses hybrid rewards combining outcome correctness with dense process reward measuring information density of retrieved evidence.

Result: Outperforms strong RAG and RL-based baselines across various general and multi-hop QA datasets and model scales, achieving superior reasoning accuracy with minimal overhead.

Conclusion: Search-R2 effectively addresses the multi-scale credit assignment problem in training search-integrated reasoning agents through targeted intervention and fine-grained supervision, enabling more efficient and accurate reasoning.

Abstract: Search-integrated reasoning enables language agents to transcend static parametric knowledge by actively querying external sources. However, training these agents via reinforcement learning is hindered by the multi-scale credit assignment problem: existing methods typically rely on sparse, trajectory-level rewards that fail to distinguish between high-quality reasoning and fortuitous guesses, leading to redundant or misleading search behaviors. To address this, we propose Search-R2, a novel Actor-Refiner collaboration framework that enhances reasoning through targeted intervention, with both components jointly optimized during training. Our approach decomposes the generation process into an Actor, which produces initial reasoning trajectories, and a Meta-Refiner, which selectively diagnoses and repairs flawed steps via a 'cut-and-regenerate' mechanism. To provide fine-grained supervision, we introduce a hybrid reward design that couples outcome correctness with a dense process reward quantifying the information density of retrieved evidence. Theoretically, we formalize the Actor-Refiner interaction as a smoothed mixture policy, proving that selective correction yields strict performance gains over strong baselines. Extensive experiments across various general and multi-hop QA datasets demonstrate that Search-R2 consistently outperforms strong RAG and RL-based baselines across model scales, achieving superior reasoning accuracy with minimal overhead.

</details>


### [220] [Mitigating Conversational Inertia in Multi-Turn Agents](https://arxiv.org/abs/2602.03664)
*Yang Wan,Zheng Cao,Zhenhao Zhang,Zhengwen Zeng,Shuheng Shen,Changhua Meng,Linchao Zhu*

Main category: cs.AI

TL;DR: LLMs in multi-turn agent scenarios suffer from "conversational inertia" where they mimic previous responses, limiting exploration. The paper proposes Context Preference Learning to calibrate models to favor low-inertia responses.


<details>
  <summary>Details</summary>
Motivation: While LLMs excel as few-shot learners, this strength becomes problematic in multi-turn agent scenarios where they erroneously mimic their own previous responses as few-shot examples, creating a tension between context enrichment and exploration constraints.

Method: Through attention analysis identifying conversational inertia, the paper proposes Context Preference Learning to calibrate model preferences to favor low-inertia responses over high-inertia ones, plus context management strategies for inference-time exploration-exploitation balance.

Result: Experimental results across eight agentic environments and one deep research scenario validate that the framework reduces conversational inertia and achieves performance improvements.

Conclusion: The paper reveals a fundamental tension in transforming few-shot LLMs into agents and provides a solution through preference learning and context management to mitigate conversational inertia while maintaining exploitation benefits.

Abstract: Large language models excel as few-shot learners when provided with appropriate demonstrations, yet this strength becomes problematic in multiturn agent scenarios, where LLMs erroneously mimic their own previous responses as few-shot examples. Through attention analysis, we identify conversational inertia, a phenomenon where models exhibit strong diagonal attention to previous responses, which is associated with imitation bias that constrains exploration. This reveals a tension when transforming few-shot LLMs into agents: longer context enriches environmental feedback for exploitation, yet also amplifies conversational inertia that undermines exploration. Our key insight is that for identical states, actions generated with longer contexts exhibit stronger inertia than those with shorter contexts, enabling construction of preference pairs without environment rewards. Based on this, we propose Context Preference Learning to calibrate model preferences to favor low-inertia responses over highinertia ones. We further provide context management strategies at inference time to balance exploration and exploitation. Experimental results across eight agentic environments and one deep research scenario validate that our framework reduces conversational inertia and achieves performance improvements.

</details>


### [221] [TodyComm: Task-Oriented Dynamic Communication for Multi-Round LLM-based Multi-Agent System](https://arxiv.org/abs/2602.03688)
*Wenzhe Fan,Tommaso Tognoli,Henry Peng Zou,Chunyu Miao,Yibo Wang,Xinhua Zhang*

Main category: cs.AI

TL;DR: TodyComm is a task-oriented dynamic communication algorithm for multi-agent LLM systems that adapts communication topologies across rounds based on task dynamics, outperforming fixed-topology methods.


<details>
  <summary>Details</summary>
Motivation: Existing multi-agent LLM systems use fixed communication topologies during inference, which fails to handle realistic scenarios where agents' roles change across rounds due to dynamic adversaries, task progression, or time-varying constraints like communication bandwidth.

Method: TodyComm is a task-oriented dynamic communication algorithm that produces behavior-driven collaboration topologies that adapt to dynamics at each round, optimizing task utility through policy gradient methods.

Result: Experiments on five benchmarks show that under both dynamic adversary and communication budget constraints, TodyComm delivers superior task effectiveness while maintaining token efficiency and scalability.

Conclusion: Dynamic communication topologies that adapt to changing task conditions are essential for effective multi-agent LLM collaboration, and TodyComm provides a practical solution that outperforms fixed-topology approaches.

Abstract: Multi-round LLM-based multi-agent systems rely on effective communication structures to support collaboration across rounds. However, most existing methods employ a fixed communication topology during inference, which falls short in many realistic applications where the agents' roles may change \textit{across rounds} due to dynamic adversary, task progression, or time-varying constraints such as communication bandwidth. In this paper, we propose addressing this issue through TodyComm, a \textbf{t}ask-\textbf{o}riented \textbf{dy}namic \textbf{comm}unication algorithm. It produces behavior-driven collaboration topologies that adapt to the dynamics at each round, optimizing the utility for the task through policy gradient. Experiments on five benchmarks demonstrate that under both dynamic adversary and communications budgets, TodyComm delivers superior task effectiveness while retaining token efficiency and scalability.

</details>


### [222] [AOrchestra: Automating Sub-Agent Creation for Agentic Orchestration](https://arxiv.org/abs/2602.03786)
*Jianhao Ruan,Zhihao Xu,Yiran Peng,Fashen Ren,Zhaoyang Yu,Xinbing Liang,Jinyu Xiang,Bang Liu,Chenglin Wu,Yuyu Luo,Jiayi Zhang*

Main category: cs.AI

TL;DR: AOrchestra introduces a unified agent abstraction (Instruction, Context, Tools, Model) that enables dynamic creation of specialized sub-agents for complex tasks, achieving significant performance improvements across benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing language agent systems lack dynamic abstraction views of sub-agents, which limits adaptability for complex, long-horizon tasks. The current sub-agent-as-tools paradigm needs better abstraction to improve flexibility and reduce human engineering efforts.

Method: Proposes a unified framework-agnostic agent abstraction as a tuple (Instruction, Context, Tools, Model). AOrchestra system uses a central orchestrator that concretizes this tuple at each step: curates task-relevant context, selects tools and models, and delegates execution via on-the-fly automatic agent creation.

Result: Across three challenging benchmarks (GAIA, SWE-Bench, Terminal-Bench), AOrchestra achieves 16.28% relative improvement against the strongest baseline when paired with Gemini-3-Flash. The system enables controllable performance-cost trade-off approaching Pareto efficiency.

Conclusion: The unified agent abstraction enables dynamic, on-demand agent creation for complex tasks, reducing engineering effort while maintaining framework-agnostic flexibility. AOrchestra demonstrates significant performance gains through its orchestration approach.

Abstract: Language agents have shown strong promise for task automation. Realizing this promise for increasingly complex, long-horizon tasks has driven the rise of a sub-agent-as-tools paradigm for multi-turn task solving. However, existing designs still lack a dynamic abstraction view of sub-agents, thereby hurting adaptability. We address this challenge with a unified, framework-agnostic agent abstraction that models any agent as a tuple Instruction, Context, Tools, Model. This tuple acts as a compositional recipe for capabilities, enabling the system to spawn specialized executors for each task on demand. Building on this abstraction, we introduce an agentic system AOrchestra, where the central orchestrator concretizes the tuple at each step: it curates task-relevant context, selects tools and models, and delegates execution via on-the-fly automatic agent creation. Such designs enable reducing human engineering efforts, and remain framework-agnostic with plug-and-play support for diverse agents as task executors. It also enables a controllable performance-cost trade-off, allowing the system to approach Pareto-efficient. Across three challenging benchmarks (GAIA, SWE-Bench, Terminal-Bench), AOrchestra achieves 16.28% relative improvement against the strongest baseline when paired with Gemini-3-Flash. The code is available at: https://github.com/FoundationAgents/AOrchestra

</details>


### [223] [Understanding Agent Scaling in LLM-Based Multi-Agent Systems via Diversity](https://arxiv.org/abs/2602.03794)
*Yingxuan Yang,Chengrui Qu,Muning Wen,Laixi Shi,Ying Wen,Weinan Zhang,Adam Wierman,Shangding Gu*

Main category: cs.AI

TL;DR: Scaling LLM-based multi-agent systems with homogeneous agents shows diminishing returns, while heterogeneous agents (different models/prompts/tools) provide substantial gains due to complementary evidence and effective channel diversity.


<details>
  <summary>Details</summary>
Motivation: Multi-agent systems are promising for complex tasks, but simply increasing the number of homogeneous agents shows diminishing returns. Understanding why diversity helps and what fundamentally limits scaling performance is crucial for building efficient systems.

Method: Developed an information-theoretic framework showing MAS performance is bounded by intrinsic task uncertainty, not agent count. Introduced K* (effective channel count) to quantify diversity without ground-truth labels. Empirically compared homogeneous vs heterogeneous agent configurations.

Result: Heterogeneous configurations consistently outperform homogeneous scaling: 2 diverse agents can match or exceed the performance of 16 homogeneous agents. Homogeneous agents saturate early due to correlated outputs, while heterogeneous agents provide complementary evidence.

Conclusion: Diversity-aware design is essential for efficient and robust multi-agent systems. The effective channel count K* provides a principled way to quantify diversity and guide system architecture, showing that diversity matters more than simply increasing agent count.

Abstract: LLM-based multi-agent systems (MAS) have emerged as a promising approach to tackle complex tasks that are difficult for individual LLMs. A natural strategy is to scale performance by increasing the number of agents; however, we find that such scaling exhibits strong diminishing returns in homogeneous settings, while introducing heterogeneity (e.g., different models, prompts, or tools) continues to yield substantial gains. This raises a fundamental question: what limits scaling, and why does diversity help? We present an information-theoretic framework showing that MAS performance is bounded by the intrinsic task uncertainty, not by agent count. We derive architecture-agnostic bounds demonstrating that improvements depend on how many effective channels the system accesses. Homogeneous agents saturate early because their outputs are strongly correlated, whereas heterogeneous agents contribute complementary evidence. We further introduce $K^*$, an effective channel count that quantifies the number of effective channels without ground-truth labels. Empirically, we show that heterogeneous configurations consistently outperform homogeneous scaling: 2 diverse agents can match or exceed the performance of 16 homogeneous agents. Our results provide principled guidelines for building efficient and robust MAS through diversity-aware design. Code and Dataset are available at the link: https://github.com/SafeRL-Lab/Agent-Scaling.

</details>


### [224] [Conformal Thinking: Risk Control for Reasoning on a Compute Budget](https://arxiv.org/abs/2602.03814)
*Xi Wang,Anushri Suresh,Alvin Zhang,Rishi More,William Jurayj,Benjamin Van Durme,Mehrdad Farajtabar,Daniel Khashabi,Eric Nalisnick*

Main category: cs.AI

TL;DR: A risk control framework for adaptive reasoning in LLMs that sets token budgets via upper/lower thresholds to limit error rates while minimizing compute, validated across diverse reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: LLMs enable test-time scaling where accuracy improves with more tokens, but setting token budgets involves a risk-accuracy trade-off. Current approaches lack systematic methods to control error rates while minimizing computational costs.

Method: Proposes a risk control framework with: 1) upper threshold to stop reasoning when confident, 2) novel parametric lower threshold to preemptively stop unsolvable instances, 3) distribution-free risk control to optimally specify stopping mechanisms given target risk and validation set, and 4) efficiency loss for selecting most computationally efficient exiting mechanism with multiple criteria.

Result: Empirical results across diverse reasoning tasks and models show computational efficiency gains from the lower threshold and ensemble stopping mechanisms while adhering to user-specified risk targets.

Conclusion: The risk control framework effectively manages the risk-accuracy trade-off in adaptive reasoning for LLMs, enabling systematic control of error rates while optimizing computational efficiency through intelligent stopping mechanisms.

Abstract: Reasoning Large Language Models (LLMs) enable test-time scaling, with dataset-level accuracy improving as the token budget increases, motivating adaptive reasoning -- spending tokens when they improve reliability and stopping early when additional computation is unlikely to help. However, setting the token budget, as well as the threshold for adaptive reasoning, is a practical challenge that entails a fundamental risk-accuracy trade-off. We re-frame the budget setting problem as risk control, limiting the error rate while minimizing compute. Our framework introduces an upper threshold that stops reasoning when the model is confident (risking incorrect output) and a novel parametric lower threshold that preemptively stops unsolvable instances (risking premature stoppage). Given a target risk and a validation set, we use distribution-free risk control to optimally specify these stopping mechanisms. For scenarios with multiple budget controlling criteria, we incorporate an efficiency loss to select the most computationally efficient exiting mechanism. Empirical results across diverse reasoning tasks and models demonstrate the effectiveness of our risk control approach, demonstrating computational efficiency gains from the lower threshold and ensemble stopping mechanisms while adhering to the user-specified risk target.

</details>


### [225] [AutoFigure: Generating and Refining Publication-Ready Scientific Illustrations](https://arxiv.org/abs/2602.03828)
*Minjun Zhu,Zhen Lin,Yixuan Weng,Panzhong Lu,Qiujie Xie,Yifan Wei,Sifan Liu,Qiyao Sun,Yue Zhang*

Main category: cs.AI

TL;DR: FigureBench is the first large-scale benchmark for generating scientific illustrations from text, and AutoFigure is an agentic framework that automatically creates high-quality, publication-ready scientific illustrations through extensive thinking and validation processes.


<details>
  <summary>Details</summary>
Motivation: Manual creation of scientific illustrations is a bottleneck in academia and industry, despite being crucial for communicating complex concepts. There's a need for automated tools to generate high-quality scientific illustrations from text.

Method: Created FigureBench with 3,300 text-figure pairs covering diverse sources. Developed AutoFigure, an agentic framework that engages in extensive thinking, recombination, and validation to produce structurally sound and aesthetically refined layouts before rendering final illustrations.

Result: AutoFigure consistently surpasses all baseline methods, producing publication-ready scientific illustrations. The framework demonstrates superior performance in generating both structurally complete and aesthetically appealing scientific illustrations.

Conclusion: The proposed AutoFigure framework successfully addresses the bottleneck in scientific illustration creation, providing an automated solution that generates high-quality illustrations from long-form scientific text, with code, dataset, and demo publicly available.

Abstract: High-quality scientific illustrations are crucial for effectively communicating complex scientific and technical concepts, yet their manual creation remains a well-recognized bottleneck in both academia and industry. We present FigureBench, the first large-scale benchmark for generating scientific illustrations from long-form scientific texts. It contains 3,300 high-quality scientific text-figure pairs, covering diverse text-to-illustration tasks from scientific papers, surveys, blogs, and textbooks. Moreover, we propose AutoFigure, the first agentic framework that automatically generates high-quality scientific illustrations based on long-form scientific text. Specifically, before rendering the final result, AutoFigure engages in extensive thinking, recombination, and validation to produce a layout that is both structurally sound and aesthetically refined, outputting a scientific illustration that achieves both structural completeness and aesthetic appeal. Leveraging the high-quality data from FigureBench, we conduct extensive experiments to test the performance of AutoFigure against various baseline methods. The results demonstrate that AutoFigure consistently surpasses all baseline methods, producing publication-ready scientific illustrations. The code, dataset and huggingface space are released in https://github.com/ResearAI/AutoFigure.

</details>
